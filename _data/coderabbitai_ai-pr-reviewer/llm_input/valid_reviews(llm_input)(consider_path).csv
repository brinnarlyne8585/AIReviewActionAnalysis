Comment_URL,Review_Start_Line,Review_End_Line,Original_Commit_id,Merge_Commit_id,Diff_path,New_path,Body,Diff_hunk,Change_Until_Merged
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747008244,,63,1a2ade923bf7d19b188ba54d5f67974f22dc171f,a5bc4194980d9927b6017c36d9661d78af09d29f,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `**kwargs` to the function signature is a good way to allow for more flexible parameterization of data readers. However, it's important to ensure that all possible keyword arguments are properly handled in the `reader.read()` method. If not, this could lead to unexpected behavior or errors.

```diff
- def load(root, reader, start=None, end=None, time=None, tolerance=None, epoch=None, **kwargs):
+ def load(root, reader, start=None, end=None, time=None, tolerance=None, epoch=None, **kwargs):
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 
-def load(root, reader, start=None, end=None, time=None, tolerance=None, epoch=None):
+def load(root, reader, start=None, end=None, time=None, tolerance=None, epoch=None, **kwargs):",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747008248,,76,1a2ade923bf7d19b188ba54d5f67974f22dc171f,a5bc4194980d9927b6017c36d9661d78af09d29f,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring has been updated to reflect the new `kwargs` parameter. This is good practice as it helps maintain the documentation up-to-date with the code changes. 

```diff
-     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
+     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
+     :param optional kwargs: Optional keyword arguments to forward to the reader when reading chunk data.
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     :param datetime, optional tolerance:
     The maximum distance between original and new timestamps for inexact matches.
     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
+    :param optional kwargs: Optional keyword arguments to forward to the reader when reading chunk data.",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747008253,,106,1a2ade923bf7d19b188ba54d5f67974f22dc171f,a5bc4194980d9927b6017c36d9661d78af09d29f,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `reader.read()` method now accepts `**kwargs`. This is consistent with the change made to the function signature and allows for more flexible parameterization of data readers. 

```diff
- frame = reader.read(files[i])
+ frame = reader.read(files[i], **kwargs)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             i = bisect.bisect_left(filetimes, key)
             if i < len(filetimes):
-                frame = reader.read(files[i])
+                frame = reader.read(files[i], **kwargs)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747008257,,117,1a2ade923bf7d19b188ba54d5f67974f22dc171f,a5bc4194980d9927b6017c36d9661d78af09d29f,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `reader.read()` method now accepts `**kwargs`. This is consistent with the change made to the function signature and allows for more flexible parameterization of data readers. 

```diff
- previous = reader.read(files[i - 1])
+ previous = reader.read(files[i - 1], **kwargs)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","                 # expand reindex to allow adjacent chunks
                 # to fill missing values
-                previous = reader.read(files[i - 1])
+                previous = reader.read(files[i - 1], **kwargs)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747008264,,138,1a2ade923bf7d19b188ba54d5f67974f22dc171f,a5bc4194980d9927b6017c36d9661d78af09d29f,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `reader.read()` method now accepts `**kwargs`. This is consistent with the change made to the function signature and allows for more flexible parameterization of data readers. 

```diff
- data = pd.concat([reader.read(file) for _, file in files])
+ data = pd.concat([reader.read(file, **kwargs) for _, file in files])
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         return _empty(reader.columns)
 
-    data = pd.concat([reader.read(file) for _, file in files])
+    data = pd.concat([reader.read(file, **kwargs) for _, file in files])",
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536655630,,13,1604cb614dc4f064292490dbd463e9cd72882b1a,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `SONAR_HOST_URL` environment variable is being fetched from `vars.SONAR_HOST_URL`, which is not a valid syntax. It should be fetched from GitHub secrets or directly set in the workflow file.
```diff
-       SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
+       SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","     runs-on: ubuntu-latest
+    env:
+      SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
+      SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}","--- 

+++ 

@@ -1,12 +1,15 @@

-name: Code Quality Check
+name: üîç Code Quality Check
 
 on:
   pull_request:
-    types: [opened, synchronize]
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
-    name: üîç SonarQube Quality Gate
+    name: üîç SonarQube Quality Check
     runs-on: ubuntu-latest
     env:
       SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
@@ -24,21 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
 
-  # gpt-review:
-  #   name: üìã Chat GPT code review
-  #   runs-on: ubuntu-latest
-  #   needs: sonarqube
-  #   steps:
-  #     - name: üèóÔ∏è Setup repo
-  #       uses: actions/checkout@v3
-  #       with:
-  #         fetch-depth: 0
-
-  #     - name: üìã Perform Chat GPT code review
-  #       uses: mattzcarey/code-review-gpt@v0.1.4-alpha
-  #       with:
-  #         OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
-  #         MODEL: 'gpt-3.5-turbo'
-  #         GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536655631,3.0,5,1604cb614dc4f064292490dbd463e9cd72882b1a,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The permissions set for this workflow are `contents: read` and `pull-requests: write`. This is a good practice as it follows the principle of least privilege, granting only the necessary permissions for the workflow to function.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+permissions:
+  contents: read
+  pull-requests: write","--- 

+++ 

@@ -1,23 +1,24 @@

-name: Code Review
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
-    types: [created]
+    types:
+      - opened
 
 concurrency:
-  group:
-    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+  group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
     github.sha }}-${{ github.workflow }}-${{ github.event_name ==
     'pull_request_review_comment' && 'pr_comment' || 'pr' }}
   cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
 
 jobs:
-  review:
+  summarize:
+    name: üê∞ CodeRabbit summary
     runs-on: ubuntu-latest
     steps:
       - uses: coderabbitai/ai-pr-reviewer@latest
@@ -28,3 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
+          disable_review: true
+          summarize: |
+            Provide your final response in markdown with the following content:
+
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
+
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536655632,12.0,17,1604cb614dc4f064292490dbd463e9cd72882b1a,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The concurrency control setup here is well done. It ensures that only one job or workflow runs per pull request or pull request comment. The `cancel-in-progress` setting also helps to prevent unnecessary resource usage by cancelling any in-progress jobs when a new one starts.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+concurrency:
+  group:
+    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}","--- 

+++ 

@@ -1,23 +1,24 @@

-name: Code Review
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
-    types: [created]
+    types:
+      - opened
 
 concurrency:
-  group:
-    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+  group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
     github.sha }}-${{ github.workflow }}-${{ github.event_name ==
     'pull_request_review_comment' && 'pr_comment' || 'pr' }}
   cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
 
 jobs:
-  review:
+  summarize:
+    name: üê∞ CodeRabbit summary
     runs-on: ubuntu-latest
     steps:
       - uses: coderabbitai/ai-pr-reviewer@latest
@@ -28,3 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
+          disable_review: true
+          summarize: |
+            Provide your final response in markdown with the following content:
+
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
+
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536655633,19.0,30,1604cb614dc4f064292490dbd463e9cd72882b1a,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This job uses the `coderabbitai/ai-pr-reviewer@latest` action to automate code reviews. The environment variables `GITHUB_TOKEN` and `OPENAI_API_KEY` are correctly fetched from secrets, which is a secure way to handle sensitive data. However, please ensure that these secrets have been properly set up in your GitHub repository settings.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+jobs:
+  review:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false","--- 

+++ 

@@ -1,23 +1,24 @@

-name: Code Review
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
-    types: [created]
+    types:
+      - opened
 
 concurrency:
-  group:
-    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+  group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
     github.sha }}-${{ github.workflow }}-${{ github.event_name ==
     'pull_request_review_comment' && 'pr_comment' || 'pr' }}
   cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
 
 jobs:
-  review:
+  summarize:
+    name: üê∞ CodeRabbit summary
     runs-on: ubuntu-latest
     steps:
       - uses: coderabbitai/ai-pr-reviewer@latest
@@ -28,3 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
+          disable_review: true
+          summarize: |
+            Provide your final response in markdown with the following content:
+
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
+
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536655634,,30,1604cb614dc4f064292490dbd463e9cd72882b1a,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's a good practice to end files with a newline character. This can help prevent issues with certain tools that expect or require it.
```diff
          review_comment_lgtm: false
+31: 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false","--- 

+++ 

@@ -1,23 +1,24 @@

-name: Code Review
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
-    types: [created]
+    types:
+      - opened
 
 concurrency:
-  group:
-    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+  group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
     github.sha }}-${{ github.workflow }}-${{ github.event_name ==
     'pull_request_review_comment' && 'pr_comment' || 'pr' }}
   cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
 
 jobs:
-  review:
+  summarize:
+    name: üê∞ CodeRabbit summary
     runs-on: ubuntu-latest
     steps:
       - uses: coderabbitai/ai-pr-reviewer@latest
@@ -28,3 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
+          disable_review: true
+          summarize: |
+            Provide your final response in markdown with the following content:
+
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
+
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536659557,14.0,18,a2f6c10a61c8310250c76a17d8514b7f4063d2cc,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The fetch-depth has been set to 0 which disables shallow clone. This is recommended for improving relevancy of reporting as it allows SonarQube to have a complete history of the project.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      - name: üèóÔ∏è Setup repo
+        uses: actions/checkout@v3
+        with:
+          fetch-depth: 0
 ","--- 

+++ 

@@ -1,7 +1,11 @@

-name: Code Quality Check
+name: üîç Code Quality Check
 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536659558,19.0,26,a2f6c10a61c8310250c76a17d8514b7f4063d2cc,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the new hunk, the environment variables `SONAR_TOKEN` and `SONAR_HOST_URL` are set at the job level instead of the step level. This is a good practice as it reduces redundancy and makes the code cleaner. However, please ensure that these secrets and variables are properly set in your GitHub repository settings or they will not be available during the workflow execution.

Also, the `id` for the ""SonarQube Quality Gate check"" step has been removed. If you're not using the output from this step in another step (as shown in the old hunk), then this change is fine. If you need to use the output, you should add the `id` back.

Lastly, the `timeout-minutes: 5` setting is a good addition as it forces the step to fail after a specific time, preventing the workflow from running indefinitely if there are issues with the quality gate check.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # Triggering SonarQube analysis as results of it are required by Quality Gate check.
+      - name: üîç SonarQube Scan
+        uses: sonarsource/sonarqube-scan-action@master
 
-    # Optionally you can use the output from the Quality Gate in another step.
-    # The possible outputs of the `quality-gate-status` variable are `PASSED`, `WARN` or `FAILED`.
-    - name: ""Example show SonarQube Quality Gate Status value""
-      run: echo ""The Quality Gate status is ${{ steps.sonarqube-quality-gate-check.outputs.quality-gate-status }}""
+      # Check the Quality Gate status.
+      - name: üìã SonarQube Quality Gate check
+        uses: sonarsource/sonarqube-quality-gate-action@master
+        timeout-minutes: 5","--- 

+++ 

@@ -1,7 +1,11 @@

-name: Code Quality Check
+name: üîç Code Quality Check
 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536659559,,12,a2f6c10a61c8310250c76a17d8514b7f4063d2cc,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/preview.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `EXPO_PROJECT` and `NODE_VERSION` environment variables are being referenced from `vars`, but it's not clear where these `vars` are defined. If they are supposed to be secrets, they should be accessed via `secrets`. If they are global environment variables, they should be accessed via `env`.

```diff
-       EXPO_PROJECT: ${{ vars.EXPO_PROJECT }}
+       EXPO_PROJECT: ${{ secrets.EXPO_PROJECT }}



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    runs-on: ubuntu-latest
+    env:
+      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+      EXPO_PROJECT: ${{ vars.EXPO_PROJECT }}","--- 

+++ 

@@ -3,9 +3,13 @@

 on:
   pull_request:
 
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
+
 jobs:
   update-preview:
-    name: üöÄ Create Expo Preview
+    name: üî® Create
     runs-on: ubuntu-latest
     env:
       GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
@@ -16,13 +20,12 @@

 
       - name: üèó Setup Node
         uses: actions/setup-node@v3
-        with:
-          node-version: ${{ vars.NODE_VERSION }}
 
       - name: üèó Setup EAS
         uses: expo/expo-github-action@v8
         with:
           eas-version: latest
+          packager: npm
           token: ${{ secrets.EXPO_TOKEN }}
 
       - name: üì¶ Install dependencies
@@ -32,12 +35,3 @@

         uses: expo/expo-github-action/preview@v8
         with:
           command: eas update --auto --branch ${{ github.event.pull_request.head.ref }}
-
-      # TODO: Add a comment to the PR with the QR code
-      # - name: Add Comment To PR
-      #   uses: mshick/add-pr-comment@v1
-      #   with:
-      #     message: |
-      #       ## Application
-      #       ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
-      #       Published to https://exp.host/${{ env.EXPO_PROJECT }}?"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536659560,,20,a2f6c10a61c8310250c76a17d8514b7f4063d2cc,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/preview.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

-           node-version: ${{ vars.NODE_VERSION }}
+           node-version: ${{ env.NODE_VERSION }}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      - name: üèó Setup Node
+        uses: actions/setup-node@v3
+        with:
+          node-version: ${{ vars.NODE_VERSION }}","--- 

+++ 

@@ -3,9 +3,13 @@

 on:
   pull_request:
 
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
+
 jobs:
   update-preview:
-    name: üöÄ Create Expo Preview
+    name: üî® Create
     runs-on: ubuntu-latest
     env:
       GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
@@ -16,13 +20,12 @@

 
       - name: üèó Setup Node
         uses: actions/setup-node@v3
-        with:
-          node-version: ${{ vars.NODE_VERSION }}
 
       - name: üèó Setup EAS
         uses: expo/expo-github-action@v8
         with:
           eas-version: latest
+          packager: npm
           token: ${{ secrets.EXPO_TOKEN }}
 
       - name: üì¶ Install dependencies
@@ -32,12 +35,3 @@

         uses: expo/expo-github-action/preview@v8
         with:
           command: eas update --auto --branch ${{ github.event.pull_request.head.ref }}
-
-      # TODO: Add a comment to the PR with the QR code
-      # - name: Add Comment To PR
-      #   uses: mshick/add-pr-comment@v1
-      #   with:
-      #     message: |
-      #       ## Application
-      #       ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
-      #       Published to https://exp.host/${{ env.EXPO_PROJECT }}?"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536659561,36.0,43,a2f6c10a61c8310250c76a17d8514b7f4063d2cc,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/preview.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This commented-out section for adding a comment to the PR with the QR code seems like a useful feature. If there's no specific reason for it being commented out, consider uncommenting it to provide this functionality.

```diff
-       # TODO: Add a comment to the PR with the QR code
-       # - name: Add Comment To PR
-       #   uses: mshick/add-pr-comment@v1
-       #   with:
-       #     message: |
-       #       ## Application
-       #       ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
-       #       Published to https://exp.host/${{ env.EXPO_PROJECT }}?
+       - name: Add Comment To PR
+         uses: mshick/add-pr-comment@v1
+         with:
+           message: |
+             ## Application
+             ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
+             Published to https://exp.host/${{ env.EXPO_PROJECT }}?
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # TODO: Add a comment to the PR with the QR code
+      # - name: Add Comment To PR
+      #   uses: mshick/add-pr-comment@v1
+      #   with:
+      #     message: |
+      #       ## Application
+      #       ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
+      #       Published to https://exp.host/${{ env.EXPO_PROJECT }}?","--- 

+++ 

@@ -3,9 +3,13 @@

 on:
   pull_request:
 
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
+
 jobs:
   update-preview:
-    name: üöÄ Create Expo Preview
+    name: üî® Create
     runs-on: ubuntu-latest
     env:
       GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
@@ -16,13 +20,12 @@

 
       - name: üèó Setup Node
         uses: actions/setup-node@v3
-        with:
-          node-version: ${{ vars.NODE_VERSION }}
 
       - name: üèó Setup EAS
         uses: expo/expo-github-action@v8
         with:
           eas-version: latest
+          packager: npm
           token: ${{ secrets.EXPO_TOKEN }}
 
       - name: üì¶ Install dependencies
@@ -32,12 +35,3 @@

         uses: expo/expo-github-action/preview@v8
         with:
           command: eas update --auto --branch ${{ github.event.pull_request.head.ref }}
-
-      # TODO: Add a comment to the PR with the QR code
-      # - name: Add Comment To PR
-      #   uses: mshick/add-pr-comment@v1
-      #   with:
-      #     message: |
-      #       ## Application
-      #       ![Expo QR](https://api.qrserver.com/v1/create-qr-code/?size=250x250&data=exp://exp.host/${{ env.EXPO_PROJECT }}?release-channel=pr${{ github.event.number }})
-      #       Published to https://exp.host/${{ env.EXPO_PROJECT }}?"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663252,22.0,32,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The workflow definition for the `pr-review.yml` file looks well-structured and organized. However, there are a few points to consider:

1. Line 13: The `concurrency` configuration seems incomplete and contains a syntax error. It needs to be corrected for proper functionality.
   
2. Lines 30-31: The boolean values for `review_simple_changes` and `review_comment_lgtm` are set to `false`. Ensure these settings align with the intended behavior of the workflow.

```diff
-   concurrency:
-     group: ${{ github.repository }}-${{ github.event.number || github.head_ref || github.sha }}-${{ github.workflow }}-${{ github.event_name == 'pull_request_review_comment' && 'pr_comment' || 'pr' }}
-     cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
+   concurrency:
+     group: ${{ github.repository }}-${{ github.event.number || github.head_ref || github.sha }}-${{ github.workflow }}-${{ github.event_name == 'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+     cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}

-           review_simple_changes: false
-           review_comment_lgtm: false
+           review_simple_changes: true
+           review_comment_lgtm: true
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    runs-on: ubuntu-latest
+    steps:
+      - uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false
+          openai_heavy_model: gpt-3.5-turbo","--- 

+++ 

@@ -1,14 +1,14 @@

-name: ‚úÖ Code Review
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
     types:
-      - created
+      - opened
 
 concurrency:
   group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
@@ -17,8 +17,8 @@

   cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
 
 jobs:
-  review:
-    name: üê∞ CodeRabbit review and summary
+  summarize:
+    name: üê∞ CodeRabbit summary
     runs-on: ubuntu-latest
     steps:
       - uses: coderabbitai/ai-pr-reviewer@latest
@@ -29,11 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
-          openai_heavy_model: gpt-3.5-turbo
+          disable_review: true
           summarize: |
             Provide your final response in markdown with the following content:
 
-            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
-            - **Changes**: A summary of changes made to each file. Format this by having each file name in H3 and the changes in a markdown list.
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
 
-            Avoid additional commentary as this summary will be added as a comment on the GitHub pull request. Use the titles ""Summary"", and ""Changes"" and they must be H2.
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663253,,22,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `name` field should be updated to reflect the new workflow name.
```diff
- name: Quality Gate
+ name: üîç Code Quality Check
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # Triggering SonarQube analysis as results of it are required by Quality Gate check.
+      - name: üîç SonarQube Scan
+        uses: sonarsource/sonarqube-scan-action@master
 ","--- 

+++ 

@@ -2,6 +2,10 @@

 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663254,4.0,5,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `on` field should be adjusted to trigger on pull requests only.
```diff
-  on:
-    push:
-      branches:
-        - master
-    pull_request:
-      types: [opened, synchronize, reopened]
+  on:
+    pull_request:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   pull_request:
-      types: [opened, synchronize, reopened]
-      
+","--- 

+++ 

@@ -2,6 +2,10 @@

 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663255,7.0,12,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Indentation issue in the `sonarqube` job definition.
```diff
-     jobs:
-       sonarqube:
-         runs-on: ubuntu-latest
-         steps:
+   jobs:
+     sonarqube:
+       name: üîç SonarQube Quality Check
+       runs-on: ubuntu-latest
+       env:
+         SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
+         SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   sonarqube:
+    name: üîç SonarQube Quality Check
     runs-on: ubuntu-latest
+    env:
+      SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
+      SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}","--- 

+++ 

@@ -2,6 +2,10 @@

 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663256,14.0,17,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Adjust the indentation for the `actions/checkout@v3` step.
```diff
-       - uses: actions/checkout@v3
-         with:
-           # Disabling shallow clone is recommended for improving relevancy of reporting.
-           fetch-depth: 0
+       - name: üèóÔ∏è Setup repo
+         uses: actions/checkout@v3
+         with:
+           fetch-depth: 0
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      - name: üèóÔ∏è Setup repo
+        uses: actions/checkout@v3
+        with:
+          fetch-depth: 0","--- 

+++ 

@@ -2,6 +2,10 @@

 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536663257,19.0,25,46793c649333414b445f1d42ca1ff4bd8d8a51a2,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/quality-gate.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Update the steps for SonarQube analysis and quality gate check.
```diff
-       # Triggering SonarQube analysis as results of it are required by Quality Gate check.
-       - name: SonarQube Scan
-         uses: sonarsource/sonarqube-scan-action@master
-         env:
-           SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
-           SONAR_HOST_URL: ${{ vars.SONAR_HOST_URL }}
- 
-       # Check the Quality Gate status.
-       - name: SonarQube Quality Gate check
-         id: sonarqube-quality-gate-check
-         uses: sonarsource/sonarqube-quality-gate-action@master
-         # Force to fail step after specific time.
-         timeout-minutes: 5
+       - name: üîç SonarQube Scan
+         uses: sonarsource/sonarqube-scan-action@master
+ 
+       - name: üìã SonarQube Quality Gate check
+         uses: sonarsource/sonarqube-quality-gate-action@master
+         timeout-minutes: 5
```

This review addresses key issues related to workflow naming, triggering events, indentation, and step adjustments for SonarQube analysis and quality gate checks.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # Triggering SonarQube analysis as results of it are required by Quality Gate check.
+      - name: üîç SonarQube Scan
+        uses: sonarsource/sonarqube-scan-action@master
 
-    # Optionally you can use the output from the Quality Gate in another step.
-    # The possible outputs of the `quality-gate-status` variable are `PASSED`, `WARN` or `FAILED`.
-    - name: ""Example show SonarQube Quality Gate Status value""
-      run: echo ""The Quality Gate status is ${{ steps.sonarqube-quality-gate-check.outputs.quality-gate-status }}""
+      # Check the Quality Gate status.
+      - name: üìã SonarQube Quality Gate check
+        uses: sonarsource/sonarqube-quality-gate-action@master","--- 

+++ 

@@ -2,6 +2,10 @@

 
 on:
   pull_request:
+
+concurrency:
+  group: ${{ github.workflow }}-${{ github.ref }}
+  cancel-in-progress: true
 
 jobs:
   sonarqube:
@@ -23,4 +27,14 @@

       # Check the Quality Gate status.
       - name: üìã SonarQube Quality Gate check
         uses: sonarsource/sonarqube-quality-gate-action@master
+        id: gate
         timeout-minutes: 5
+
+      - name: üí¨ Comment on PR
+        uses: peter-evans/create-or-update-comment@v4
+        if: failure()
+        with:
+          issue-number: ${{ github.event.number }}
+          body: |
+            # Quality Gate ${{ steps.gate.outputs.quality-gate-status }}
+            Check the results [here](${{ vars.SONAR_HOST_URL }}dashboard?id=${{ vars.SONAR_REPO_ID }})"
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536666478,9.0,11,2f44081b2889cadab72391e7fbbba33932d065ad,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `on` section specifies triggering the workflow on `pull_request` and `pull_request_review_comment` events with the types of `created`. This setup seems appropriate for the intended functionality.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  pull_request_review_comment:
+    types:
+      - created","--- 

+++ 

@@ -1,14 +1,14 @@

-name: ‚úÖ PR Summary
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
     types:
-      - created
+      - opened
 
 concurrency:
   group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
@@ -29,12 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
-          openai_heavy_model: gpt-3.5-turbo
+          disable_review: true
           summarize: |
             Provide your final response in markdown with the following content:
 
-            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
-            - **Changes**: A summary of changes made to each file. Format this by having each file name in H2 and the changes in a markdown list.
-            - **Review**: A markdown list of suggestions to specific changes that can be made to improve the code quality. Consider both specific code-level suggestions and high-level architectural suggestions. Only add this section if the suggestions have significant impact on the code quality.
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
 
-            Avoid additional commentary as this summary will be added as a comment on the GitHub pull request. Use the titles ""Summary"", ""Changes"", and ""Review"" and they must be H1. Only include the ""Reivew"" section if the suggestions have significant impact on the code quality.
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536666479,13.0,17,2f44081b2889cadab72391e7fbbba33932d065ad,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `concurrency` section defines a group based on various GitHub context variables. The logic here seems complex and might need further clarification or simplification for better understanding and maintenance.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+concurrency:
+  group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}","--- 

+++ 

@@ -1,14 +1,14 @@

-name: ‚úÖ PR Summary
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
     types:
-      - created
+      - opened
 
 concurrency:
   group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
@@ -29,12 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
-          openai_heavy_model: gpt-3.5-turbo
+          disable_review: true
           summarize: |
             Provide your final response in markdown with the following content:
 
-            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
-            - **Changes**: A summary of changes made to each file. Format this by having each file name in H2 and the changes in a markdown list.
-            - **Review**: A markdown list of suggestions to specific changes that can be made to improve the code quality. Consider both specific code-level suggestions and high-level architectural suggestions. Only add this section if the suggestions have significant impact on the code quality.
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
 
-            Avoid additional commentary as this summary will be added as a comment on the GitHub pull request. Use the titles ""Summary"", ""Changes"", and ""Review"" and they must be H1. Only include the ""Reivew"" section if the suggestions have significant impact on the code quality.
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536666480,20.0,32,2f44081b2889cadab72391e7fbbba33932d065ad,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `summarize` job configuration includes setting up the `coderabbitai/ai-pr-reviewer` action with environment variables for GitHub token and OpenAI API key. The configuration for summarizing changes looks comprehensive and well-detailed.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  summarize:
+    name: üê∞ CodeRabbit summary
+    runs-on: ubuntu-latest
+    steps:
+      - uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false
+          openai_heavy_model: gpt-3.5-turbo","--- 

+++ 

@@ -1,14 +1,14 @@

-name: ‚úÖ PR Summary
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
     types:
-      - created
+      - opened
 
 concurrency:
   group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
@@ -29,12 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
-          openai_heavy_model: gpt-3.5-turbo
+          disable_review: true
           summarize: |
             Provide your final response in markdown with the following content:
 
-            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
-            - **Changes**: A summary of changes made to each file. Format this by having each file name in H2 and the changes in a markdown list.
-            - **Review**: A markdown list of suggestions to specific changes that can be made to improve the code quality. Consider both specific code-level suggestions and high-level architectural suggestions. Only add this section if the suggestions have significant impact on the code quality.
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
 
-            Avoid additional commentary as this summary will be added as a comment on the GitHub pull request. Use the titles ""Summary"", ""Changes"", and ""Review"" and they must be H1. Only include the ""Reivew"" section if the suggestions have significant impact on the code quality.
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/brisen-app/brisen-client/pulls/comments/1536666481,34.0,38,2f44081b2889cadab72391e7fbbba33932d065ad,ccad80f2210c4994adca60df93cc4043733bbfe1,.github/workflows/pr-review.yml,.github/workflows/pr-summary.yml,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `summarize` job defines a structured summary format for the PR, including high-level summaries, changes made to each file, and suggestions for code quality improvements. This approach enhances the clarity and organization of PR information.

### Overall
The new workflow configuration appears to be well-structured and detailed, covering various aspects such as event triggers, concurrency handling, job setup, and PR summary generation. Consider simplifying the logic in the `concurrency` section for better readability and maintenance. Additionally, ensure that the environment variables containing sensitive information like API keys are handled securely.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            Provide your final response in markdown with the following content:
+
+            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
+            - **Changes**: A summary of changes made to each file. Format this by having each file name in H2 and the changes in a markdown list.
+            - **Review**: A markdown list of suggestions to specific changes that can be made to improve the code quality. Consider both specific code-level suggestions and high-level architectural suggestions. Only add this section if the suggestions have significant impact on the code quality.","--- 

+++ 

@@ -1,14 +1,14 @@

-name: ‚úÖ PR Summary
+name: üìã PR Summary
 
 permissions:
   contents: read
   pull-requests: write
 
 on:
+  workflow_dispatch:
   pull_request:
-  pull_request_review_comment:
     types:
-      - created
+      - opened
 
 concurrency:
   group: ${{ github.repository }}-${{ github.event.number || github.head_ref ||
@@ -29,12 +29,17 @@

           debug: false
           review_simple_changes: false
           review_comment_lgtm: false
-          openai_heavy_model: gpt-3.5-turbo
+          disable_review: true
           summarize: |
             Provide your final response in markdown with the following content:
 
-            - **Summary**: A high-level summary of the overall change instead of specific files, within 50 words.
-            - **Changes**: A summary of changes made to each file. Format this by having each file name in H2 and the changes in a markdown list.
-            - **Review**: A markdown list of suggestions to specific changes that can be made to improve the code quality. Consider both specific code-level suggestions and high-level architectural suggestions. Only add this section if the suggestions have significant impact on the code quality.
+            - **Walkthrough**: A high-level summary of the overall change instead of 
+              specific files within 80 words.
+            - **Changes**: A markdown table of files and their summaries. Group files 
+              with similar changes together into a single row to save space.
+            - **Quote**: Below the changes, include a whimsical, short poem, related
+              to the changes that urges the PR author to not commiting suicide. Format the poem as a quote using 
+              the "">"" symbol and feel free to use emojis where relevant.
 
-            Avoid additional commentary as this summary will be added as a comment on the GitHub pull request. Use the titles ""Summary"", ""Changes"", and ""Review"" and they must be H1. Only include the ""Reivew"" section if the suggestions have significant impact on the code quality.
+            Avoid additional commentary as this summary will be added as a comment on the 
+            GitHub pull request. Use the titles ""Walkthrough"" and ""Changes"" and they must be H2."
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1777149089,,17,25b7195c7afb05612164e610ead21bdf71982b7d,83cd9056b5434b830384c61211752035990e1738,aeon/util.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The condition `elif obj is not None:` seems to be a good addition for error handling, but it might not be enough. If `obj` is not iterable, the code will still raise an exception. Consider adding a check to ensure that `obj` is iterable before attempting to iterate over it.
```diff
- elif obj is not None:
+ elif obj is not None and isinstance(obj, Iterable):
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             if found:
                 return found
-    else:
+    elif obj is not None:",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1777149097,413.0,416,25b7195c7afb05612164e610ead21bdf71982b7d,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The current implementation of `class_int2str` method modifies the input dataframe directly which might lead to unexpected side effects if the original data is needed elsewhere after this function call. It's generally a good practice to avoid modifying inputs directly unless it's explicitly documented and expected. Consider creating a copy of the input dataframe before making modifications.
```diff
-        for i, subj in enumerate(classes):
-            data.loc[data[""identity""] == i, ""identity""] = subj
+        data_copy = data.copy()
+        for i, subj in enumerate(classes):
+            data_copy.loc[data_copy[""identity""] == i, ""identity""] = subj
+        return data_copy
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        for i, subj in enumerate(classes):
+            data.loc[data[""identity""] == i, ""identity""] = subj","--- 

+++ 

@@ -304,21 +304,37 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = file.parent / model_dir
-        if not config_file_dir.exists():
-            config_file_dir = Path(self._model_root) / model_dir
-            if not config_file_dir.exists():
-                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
-            
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -412,8 +428,10 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        for i, subj in enumerate(classes):
-            data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1777229984,413.0,418,ac2aa137454538ff7c05dff4476ed09491ac4dcb,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The current implementation of `class_int2str` method has a complexity of O(n^2) because for each class, it iterates over the entire DataFrame to replace the integer identity with the string identity. This could be optimized by creating a mapping dictionary and using the pandas `replace` function, which would reduce the complexity to O(n).

```diff
-        identity = data[""identity""].astype(""string"")
-        for i, subj in enumerate(classes):
-            identity.loc[data[identity.name] == i] = subj
-        data[identity.name] = identity
+        identity_mapping = {i: subj for i, subj in enumerate(classes)}
+        data[""identity""] = data[""identity""].replace(identity_mapping)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        identity = data[""identity""].astype(""string"")
+        for i, subj in enumerate(classes):
+            identity.loc[data[identity.name] == i] = subj
+        data[identity.name] = identity","--- 

+++ 

@@ -304,20 +304,36 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = file.parent / model_dir
-        if not config_file_dir.exists():
-            config_file_dir = Path(self._model_root) / model_dir
-            if not config_file_dir.exists():
-                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
 
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
@@ -412,10 +428,10 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        identity = data[""identity""].astype(""string"")
-        for i, subj in enumerate(classes):
-            identity.loc[data[identity.name] == i] = subj
-        data[identity.name] = identity
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1777269970,316.0,320,d30ee91bef0df54fe46f0cf505aa695c373b281e,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for determining the `config_file_dir` is not very clear. It seems like you're trying to find a local path first and then falling back to a shared directory if the local one doesn't exist. However, this could be made more explicit with better variable naming and comments. Also, it would be beneficial to add logging statements to help debug issues related to file paths in the future.
```diff
-        config_file_dir = file.parent / model_dir
-         if not config_file_dir.exists():
-            config_file_dir = Path(self._model_root) / model_dir
-            if not config_file_dir.exists():
-                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        local_config_dir = file.parent / model_dir
+        shared_config_dir = Path(self._model_root) / model_dir
+        if local_config_dir.exists():
+            config_file_dir = local_config_dir
+        elif shared_config_dir.exists():
+            config_file_dir = shared_config_dir
+        else:
+            raise FileNotFoundError(f""Cannot find model dir in either local ({local_config_dir}) or shared ({shared_config_dir}) directories"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        config_file_dir = file.parent / model_dir
         if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+            config_file_dir = Path(self._model_root) / model_dir
+            if not config_file_dir.exists():
+                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")","--- 

+++ 

@@ -304,20 +304,36 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = file.parent / model_dir
-        if not config_file_dir.exists():
-            config_file_dir = Path(self._model_root) / model_dir
-            if not config_file_dir.exists():
-                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
 
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
@@ -412,7 +428,9 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        identity_mapping = {i: subj for i, subj in enumerate(classes)}
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
         data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1777269976,413.0,427,d30ee91bef0df54fe46f0cf505aa695c373b281e,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of `class_int2str` function is much simpler and cleaner than the old one. However, it assumes that the classes are always provided in the correct order which might not always be the case. It's recommended to include a check to ensure that the classes list is sorted or explicitly document this assumption.
```diff
+        if sorted(classes) != classes:
+            raise ValueError(""Classes list must be sorted."")
         identity_mapping = {i: subj for i, subj in enumerate(classes)}
         data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        identity_mapping = {i: subj for i, subj in enumerate(classes)}
+        data[""identity""] = data[""identity""].replace(identity_mapping)","--- 

+++ 

@@ -304,20 +304,36 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = file.parent / model_dir
-        if not config_file_dir.exists():
-            config_file_dir = Path(self._model_root) / model_dir
-            if not config_file_dir.exists():
-                raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
 
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
@@ -412,7 +428,9 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        identity_mapping = {i: subj for i, subj in enumerate(classes)}
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
         data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1778228873,316.0,329,2c8c91e2c0a591ee60060a36bf0edbc7d401b445,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for determining the `config_file_dir` is clear and well implemented. However, it would be beneficial to add a comment explaining why we are checking both local and shared directories. This will help future developers understand the reasoning behind this design decision.

```python
# Check if model directory exists in local or shared directories.
# Local directory is prioritized over shared directory.
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )","--- 

+++ 

@@ -304,15 +304,25 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
         local_config_file_dir = file.parent / model_dir
         shared_config_file_dir = Path(self._model_root) / model_dir
         if local_config_file_dir.exists():
@@ -418,6 +428,8 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
         identity_mapping = dict(enumerate(classes))
         data[""identity""] = data[""identity""].replace(identity_mapping)
         return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1778228876,419.0,427,2c8c91e2c0a591ee60060a36bf0edbc7d401b445,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from reading classes from a config file to directly using the provided list of classes simplifies the code and reduces potential points of failure (like file not found or JSON parsing errors). However, it's important to ensure that the `classes` list is always correctly populated before this function is called. If there's any chance that `classes` could be `None` or empty, you should add error handling here to prevent issues down the line.

```python
if not classes:
    raise ValueError(""Classes list cannot be None or empty."")
identity_mapping = dict(enumerate(classes))
data[""identity""] = data[""identity""].replace(identity_mapping)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)","--- 

+++ 

@@ -304,15 +304,25 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
         local_config_file_dir = file.parent / model_dir
         shared_config_file_dir = Path(self._model_root) / model_dir
         if local_config_file_dir.exists():
@@ -418,6 +428,8 @@

     @staticmethod
     def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
         identity_mapping = dict(enumerate(classes))
         data[""identity""] = data[""identity""].replace(identity_mapping)
         return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1778247877,316.0,329,93428c8fcbd558f91e8954d8cc639434b7d30572,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for checking the existence of local and shared directories is correct. However, it would be more efficient to avoid checking the existence of the shared directory if the local directory exists. This can be achieved by using an `else` clause instead of `elif`.
```diff
-        elif shared_config_file_dir.exists():
+        else:
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )","--- 

+++ 

@@ -304,15 +304,22 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
 
         # Check if model directory exists in local or shared directories.
         # Local directory is prioritized over shared directory."
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1778247887,422.0,427,93428c8fcbd558f91e8954d8cc639434b7d30572,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of `class_int2str` function is much cleaner and less complex than the previous one. It directly uses the provided list of classes instead of reading from a config file. The use of a dictionary for mapping classes to identities is also a good practice. However, there should be a check to ensure that the 'identity' column exists in the data before attempting to replace its values.
```diff
+        if ""identity"" not in data.columns:
+            raise KeyError(""Identity column not found in data."")
         data[""identity""] = data[""identity""].replace(identity_mapping)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)","--- 

+++ 

@@ -304,15 +304,22 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
 
         # Check if model directory exists in local or shared directories.
         # Local directory is prioritized over shared directory."
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1688435282,297.0,299,95a6f0b622058ab0ab04cfca1ae910a7f5ec08a8,f65c015e39e859b431530fadae9a735016e85691,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `config_file` is now passed as an argument to the methods `get_class_names` and `get_bodyparts`. This change could potentially lead to issues if the `config_file` is not properly initialized or passed incorrectly. It would be a good idea to add error handling to check if the `config_file` exists before trying to use it in these methods.

```diff
-        config_file = self.get_config_file(config_file_dir)
-        identities = self.get_class_names(config_file)
-        parts = self.get_bodyparts(config_file)
+        config_file = self.get_config_file(config_file_dir)
+        if not config_file.exists():
+            raise FileNotFoundError(f""Cannot find config file {config_file}"")
+        identities = self.get_class_names(config_file)
+        parts = self.get_bodyparts(config_file)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        config_file = self.get_config_file(config_file_dir)
+        identities = self.get_class_names(config_file)
+        parts = self.get_bodyparts(config_file)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1688435285,,329,95a6f0b622058ab0ab04cfca1ae910a7f5ec08a8,f65c015e39e859b431530fadae9a735016e85691,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the `config_file` is now passed as an argument to the method `class_int2str`. Ensure that the `config_file` exists before using it in this method.

```diff
-        data = self.class_int2str(data, config_file)
+        if not config_file.exists():
+            raise FileNotFoundError(f""Cannot find config file {config_file}"")
+        data = self.class_int2str(data, config_file)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data)
+        data = self.class_int2str(data, config_file)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1688435286,352.0,392,95a6f0b622058ab0ab04cfca1ae910a7f5ec08a8,f65c015e39e859b431530fadae9a735016e85691,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the methods `get_class_names`, `get_bodyparts`, and `class_int2str`, the `config_file` is opened and read multiple times. This can be inefficient, especially if the file is large. Consider reading the file once and storing the contents in a variable that can be used in these methods.

```diff
-        with open(config_file) as f:
-             config = json.load(f)
+        config = self.read_config_file(config_file)
```

And add a new method to read the config file:

```python
@staticmethod
def read_config_file(config_file: Path) -> dict:
    """"""Reads and returns the contents of a config file.""""""
    with open(config_file) as f:
        return json.load(f)
```

This way, you only need to read the file once and can use the `config` variable in all methods that require it.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @staticmethod
+    def get_class_names(config_file: Path) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
         classes = None
-        with open(self.config_file) as f:
+        with open(config_file) as f:
             config = json.load(f)
-        if self.config_file.stem == ""confmap_config"":  # SLEAP
+        if config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
             except KeyError as err:
                 if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {self.config_file}."") from err
+                    raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
         return classes
 
-    def get_bodyparts(self) -> list[str]:
+    @staticmethod
+    def get_bodyparts(config_file: Path) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
         parts = []
-        with open(self.config_file) as f:
+        with open(config_file) as f:
             config = json.load(f)
-        if self.config_file.stem == ""confmap_config"":  # SLEAP
+        if config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
                 if not parts:
-                    raise KeyError(f""Cannot find bodyparts in {self.config_file}."") from err
+                    raise KeyError(f""Cannot find bodyparts in {config_file}."") from err
         return parts
 
-    def class_int2str(self, data: pd.DataFrame) -> pd.DataFrame:
+    @staticmethod
+    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if self.config_file.stem == ""confmap_config"":  # SLEAP
-            with open(self.config_file) as f:
+        if config_file.stem == ""confmap_config"":  # SLEAP
+            with open(config_file) as f:
                 config = json.load(f)
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""classes"")
             except KeyError as err:",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1657337145,289.0,318,0978166e352427b0f9d13318076ff0afbc08bf5e,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling mechanism for different versions of Bonsai.Sleap is not robust. The code assumes that if a `ValueError` is raised, it's due to a column mismatch and thus the version must be 0.3. However, a `ValueError` could be raised due to other issues as well, leading to incorrect assumptions about the version. A more reliable way would be to explicitly check the version of Bonsai.Sleap being used.
```diff
-        try:  # Bonsai.Sleap0.2
-            bonsai_sleap_v = 0.2
-            columns = [""identity"", ""identity_likelihood""]
-            for part in parts:
-                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-            self.columns = columns
-            data = super().read(file)
-        except ValueError:  # column mismatch; Bonsai.Sleap0.3
-            bonsai_sleap_v = 0.3
-            columns = [""identity""]
-            columns.extend([f""{identity}_likelihood"" for identity in identities])
-            for part in parts:
-                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-            self.columns = columns
-            data = super().read(file)
+        # Check the version of Bonsai.Sleap
+        bonsai_sleap_v = get_bonsai_sleap_version()  # This function needs to be implemented
+        if bonsai_sleap_v == 0.2:
+            columns = [""identity"", ""identity_likelihood""]
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        elif bonsai_sleap_v == 0.3:
+            columns = [""identity""]
+            columns.extend([f""{identity}_likelihood"" for identity in identities])
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        else:
+            raise ValueError(f""Unsupported Bonsai.Sleap version: {bonsai_sleap_v}"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        self.config_file = None  # requires reading the data file to be set
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[1:])
+        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
         config_file_dir = Path(self._model_root) / model_dir
         if not config_file_dir.exists():
             raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = self.get_config_file(config_file_dir)
+        identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
+        self.config_file = config_file
 
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
-        columns = [""class"", ""class_likelihood""]
-        for part in parts:
-            columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-        self.columns = columns
-        data = super().read(file)
+        try:  # Bonsai.Sleap0.2
+            bonsai_sleap_v = 0.2
+            columns = [""identity"", ""identity_likelihood""]
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        except ValueError:  # column mismatch; Bonsai.Sleap0.3
+            bonsai_sleap_v = 0.3
+            columns = [""identity""]
+            columns.extend([f""{identity}_likelihood"" for identity in identities])
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)","--- 

+++ 

@@ -295,10 +295,9 @@

         config_file_dir = Path(self._model_root) / model_dir
         if not config_file_dir.exists():
             raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
-        config_file = self.get_config_file(config_file_dir)
-        identities = self.get_class_names(config_file)
-        parts = self.get_bodyparts(config_file)
-        self.config_file = config_file
+        self.config_file = self.get_config_file(config_file_dir)
+        identities = self.get_class_names()
+        parts = self.get_bodyparts()
 
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
         try:  # Bonsai.Sleap0.2
@@ -328,7 +327,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file_dir)
+        data = self.class_int2str(data)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
@@ -351,46 +350,45 @@

         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 
-    def get_class_names(self, file: Path) -> list[str]:
+    def get_class_names(self) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
         classes = None
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
             except KeyError as err:
                 if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {file}."") from err
+                    raise KeyError(f""Cannot find class_vectors in {self.config_file}."") from err
         return classes
 
-    def get_bodyparts(self, file: Path) -> list[str]:
+    def get_bodyparts(self) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
         parts = []
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
                 if not parts:
-                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                    raise KeyError(f""Cannot find bodyparts in {self.config_file}."") from err
         return parts
 
-    def class_int2str(self, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
+    def class_int2str(self, data: pd.DataFrame) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        config_file = self.get_config_file(config_file_dir)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
+            with open(self.config_file) as f:
                 config = json.load(f)
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""classes"")
             except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
+                raise KeyError(f""Cannot find classes in {self.config_file}."") from err
             for i, subj in enumerate(classes):
                 data.loc[data[""identity""] == i, ""identity""] = subj
         return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1657337149,383.0,396,0978166e352427b0f9d13318076ff0afbc08bf5e,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `class_int2str` method is now an instance method, but it seems like it could still be a class method as before. It doesn't use any instance-specific data and only operates on the input parameters. This would improve modularity and make the method easier to test independently.
```diff
-    def class_int2str(self, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
+    @classmethod
+    def class_int2str(cls, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def class_int2str(self, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
+        """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
+        config_file = self.get_config_file(config_file_dir)
+        if config_file.stem == ""confmap_config"":  # SLEAP
+            with open(config_file) as f:
+                config = json.load(f)
+            try:
+                heads = config[""model""][""heads""]
+                classes = util.find_nested_key(heads, ""classes"")
+            except KeyError as err:
+                raise KeyError(f""Cannot find classes in {config_file}."") from err
+            for i, subj in enumerate(classes):
+                data.loc[data[""identity""] == i, ""identity""] = subj
+        return data","--- 

+++ 

@@ -295,10 +295,9 @@

         config_file_dir = Path(self._model_root) / model_dir
         if not config_file_dir.exists():
             raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
-        config_file = self.get_config_file(config_file_dir)
-        identities = self.get_class_names(config_file)
-        parts = self.get_bodyparts(config_file)
-        self.config_file = config_file
+        self.config_file = self.get_config_file(config_file_dir)
+        identities = self.get_class_names()
+        parts = self.get_bodyparts()
 
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
         try:  # Bonsai.Sleap0.2
@@ -328,7 +327,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file_dir)
+        data = self.class_int2str(data)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
@@ -351,46 +350,45 @@

         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 
-    def get_class_names(self, file: Path) -> list[str]:
+    def get_class_names(self) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
         classes = None
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
             except KeyError as err:
                 if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {file}."") from err
+                    raise KeyError(f""Cannot find class_vectors in {self.config_file}."") from err
         return classes
 
-    def get_bodyparts(self, file: Path) -> list[str]:
+    def get_bodyparts(self) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
         parts = []
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
                 if not parts:
-                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                    raise KeyError(f""Cannot find bodyparts in {self.config_file}."") from err
         return parts
 
-    def class_int2str(self, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
+    def class_int2str(self, data: pd.DataFrame) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        config_file = self.get_config_file(config_file_dir)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
+            with open(self.config_file) as f:
                 config = json.load(f)
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""classes"")
             except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
+                raise KeyError(f""Cannot find classes in {self.config_file}."") from err
             for i, subj in enumerate(classes):
                 data.loc[data[""identity""] == i, ""identity""] = subj
         return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664258814,289.0,317,4a72c4762493a730b6a7ad51e6360a3991c222d5,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `read` method is trying to handle two different versions of Bonsai.Sleap by using a try-except block. This approach can lead to confusion and potential bugs in the future as it's not clear which part of the code inside the try block is expected to raise a ValueError. It would be better to explicitly check the version of Bonsai.Sleap and then execute the appropriate code. This will make the code more readable and maintainable.

```diff
-        try:  # Bonsai.Sleap0.2
-            bonsai_sleap_v = 0.2
-            columns = [""identity"", ""identity_likelihood""]
-            for part in parts:
-                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-            self.columns = columns
-            data = super().read(file)
-        except ValueError:  # column mismatch; Bonsai.Sleap0.3
-            bonsai_sleap_v = 0.3
-            columns = [""identity""]
-            columns.extend([f""{identity}_likelihood"" for identity in identities])
-            for part in parts:
-                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-            self.columns = columns
-            data = super().read(file)
+        if self.bonsai_sleap_version == 0.2:
+            columns = [""identity"", ""identity_likelihood""]
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        elif self.bonsai_sleap_version == 0.3:
+            columns = [""identity""]
+            columns.extend([f""{identity}_likelihood"" for identity in identities])
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        else:
+            raise ValueError(f""Unsupported Bonsai.Sleap version: {self.bonsai_sleap_version}"")
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+        self.config_file = None  # requires reading the data file to be set
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[1:])
+        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
         config_file_dir = Path(self._model_root) / model_dir
         if not config_file_dir.exists():
             raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
-        config_file = self.get_config_file(config_file_dir)
-        parts = self.get_bodyparts(config_file)
+        self.config_file = self.get_config_file(config_file_dir)
+        identities = self.get_class_names()
+        parts = self.get_bodyparts()
 
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
-        columns = [""class"", ""class_likelihood""]
-        for part in parts:
-            columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-        self.columns = columns
-        data = super().read(file)
+        try:  # Bonsai.Sleap0.2
+            bonsai_sleap_v = 0.2
+            columns = [""identity"", ""identity_likelihood""]
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)
+        except ValueError:  # column mismatch; Bonsai.Sleap0.3
+            bonsai_sleap_v = 0.3
+            columns = [""identity""]
+            columns.extend([f""{identity}_likelihood"" for identity in identities])
+            for part in parts:
+                columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            self.columns = columns
+            data = super().read(file)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664258818,330.0,351,4a72c4762493a730b6a7ad51e6360a3991c222d5,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code is creating a new DataFrame for each part and then concatenating them. This can be inefficient when dealing with large datasets as it involves creating many temporary DataFrames. A more efficient approach would be to create a list of dictionaries and then convert that list into a DataFrame.

```diff
-         part_data_list = [pd.DataFrame()] * n_parts
-         new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
-         new_data = pd.DataFrame(columns=new_columns)
-         for i, part in enumerate(parts):
-            part_columns = columns[0 : (len(identities) + 1)] if bonsai_sleap_v == 0.3 else columns[0:2]
-            part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
-             part_data = pd.DataFrame(data[part_columns])
-            if bonsai_sleap_v == 0.3:  # combine all identity_likelihood cols into a single col as dict
-                part_data[""identity_likelihood""] = part_data.apply(
-                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities}, axis=1
-                )
-                part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
-                part_data = part_data[  # reorder columns
-                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-                ]
-             part_data.insert(2, ""part"", part)
-             part_data.columns = new_columns
-             part_data_list[i] = part_data
-         new_data = pd.concat(part_data_list)
+         data_dicts = []
+         for i, part in enumerate(parts):
+            part_columns = columns[0 : (len(identities) + 1)] if bonsai_sleap_v == 0.3 else columns[0:2]
+            part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+            for _, row in data[part_columns].iterrows():
+                if bonsai_sleap_v == 0.3:
+                    identity_likelihood = {identity: row[f""{identity}_likelihood""] for identity in identities}
+                else:
+                    identity_likelihood = row[""identity_likelihood""]
+                data_dict = {
+                    ""identity"": row[""identity""],
+                    ""identity_likelihood"": identity_likelihood,
+                    ""part"": part,
+                    ""x"": row[f""{part}_x""],
+                    ""y"": row[f""{part}_y""],
+                    ""part_likelihood"": row[f""{part}_likelihood""]
+                }
+                data_dicts.append(data_dict)
+         new_data = pd.DataFrame(data_dicts)
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+        data = self.class_int2str(data)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
-            part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
+            part_columns = columns[0 : (len(identities) + 1)] if bonsai_sleap_v == 0.3 else columns[0:2]
+            part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             part_data = pd.DataFrame(data[part_columns])
+            if bonsai_sleap_v == 0.3:  # combine all identity_likelihood cols into a single col as dict
+                part_data[""identity_likelihood""] = part_data.apply(
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities}, axis=1
+                )
+                part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
+                part_data = part_data[  # reorder columns
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
+                ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664258820,353.0,365,4a72c4762493a730b6a7ad51e6360a3991c222d5,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `get_class_names` method is reading the config file every time it's called. This can be inefficient if the method is called multiple times. It would be better to read the config file once and store it as an instance variable.

```diff
-        with open(self.config_file) as f:
-             config = json.load(f)
+        if not hasattr(self, 'config'):
+            with open(self.config_file) as f:
+                self.config = json.load(f)
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def get_class_names(self) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
         classes = None
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
             except KeyError as err:
                 if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {file}."") from err
+                    raise KeyError(f""Cannot find class_vectors in {self.config_file}."") from err
         return classes",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664258821,367.0,379,4a72c4762493a730b6a7ad51e6360a3991c222d5,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The same issue is present in the `get_bodyparts` method. It should also use the stored config file.

```diff
-        with open(self.config_file) as f:
-             config = json.load(f)
+        if not hasattr(self, 'config'):
+            with open(self.config_file) as f:
+                self.config = json.load(f)
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def get_bodyparts(self) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
         parts = []
-        with open(file) as f:
+        with open(self.config_file) as f:
             config = json.load(f)
-        if file.stem == ""confmap_config"":  # SLEAP
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
                 if not parts:
-                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                    raise KeyError(f""Cannot find bodyparts in {self.config_file}."") from err",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664258824,382.0,394,4a72c4762493a730b6a7ad51e6360a3991c222d5,cb7d84c8d0e2c8dfa9c32997469bc62bd1a7e72a,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `class_int2str` method is also reading the config file every time it's called. It should use the stored config file.

```diff
-        with open(self.config_file) as f:
-            config = json.load(f)
+        if not hasattr(self, 'config'):
+            with open(self.config_file) as f:
+                self.config = json.load(f)
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def class_int2str(self, data: pd.DataFrame) -> pd.DataFrame:
+        """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
+        if self.config_file.stem == ""confmap_config"":  # SLEAP
+            with open(self.config_file) as f:
+                config = json.load(f)
+            try:
+                heads = config[""model""][""heads""]
+                classes = util.find_nested_key(heads, ""classes"")
+            except KeyError as err:
+                raise KeyError(f""Cannot find classes in {self.config_file}."") from err
+            for i, subj in enumerate(classes):
+                data.loc[data[""identity""] == i, ""identity""] = subj
+        return data",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035450,49.0,67,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [63-77]

The function `load` is quite complex and does a lot of things. It would be beneficial to break it down into smaller, more manageable functions. This will improve readability, maintainability, and testability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","         date_str, time_str = epoch.split(""T"")
     return epoch, datetime.datetime.fromisoformat(date_str + ""T"" + time_str.replace(""-"", "":""))
 
+
 def _set_index(data):
     if not isinstance(data.index, pd.DatetimeIndex):
         data.index = aeon(data.index)
-    data.index.name = 'time'
+    data.index.name = ""time""
+
 
 def _empty(columns):
-    return pd.DataFrame(columns=columns, index=pd.DatetimeIndex([], name='time'))
+    return pd.DataFrame(columns=columns, index=pd.DatetimeIndex([], name=""time""))
+
 
 def load(root, reader, start=None, end=None, time=None, tolerance=None, epoch=None):
-    '''
-    Extracts chunk data from the root path of an Aeon dataset using the specified data stream
+    """"""Extracts chunk data from the root path of an Aeon dataset using the specified data stream
     reader. A subset of the data can be loaded by specifying an optional time range, or a list
     of timestamps used to index the data on file. Returned data will be sorted chronologically.
 ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035451,102.0,125,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic inside the for loop is quite complex and could benefit from being broken down into smaller, more manageable functions. This will improve readability, maintainability, and testability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        for key, values in time.groupby(by=chunk):
             i = bisect.bisect_left(filetimes, key)
             if i < len(filetimes):
                 frame = reader.read(files[i])
                 _set_index(frame)
             else:
                 frame = _empty(reader.columns)
             data = frame.reset_index()
-            data.set_index('time', drop=False, inplace=True)
-            data = data.reindex(values, method='pad', tolerance=tolerance)
+            data.set_index(""time"", drop=False, inplace=True)
+            data = data.reindex(values, method=""pad"", tolerance=tolerance)
             missing = len(data.time) - data.time.count()
             if missing > 0 and i > 0:
                 # expand reindex to allow adjacent chunks
                 # to fill missing values
-                previous = reader.read(files[i-1])
+                previous = reader.read(files[i - 1])
                 data = pd.concat([previous, frame])
-                data = data.reindex(values, method='pad', tolerance=tolerance)
+                data = data.reindex(values, method=""pad"", tolerance=tolerance)
             else:
-                data.drop(columns='time', inplace=True)
+                data.drop(columns=""time"", inplace=True)
             dataframes.append(data)
 
         if len(dataframes) == 0:
             return _empty(reader.columns)
-            
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035452,64.0,72,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/analysis/movies.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [67-78]

There's a logical error in the condition checking for `before` and `after`. The current code checks if `before` and `after` are exactly the same object as `pd.Timedelta`, which is not correct. It should check if they are instances of `pd.Timedelta`.
```diff
-    if before is not pd.Timedelta:
+    if not isinstance(before, pd.Timedelta):
         before = pd.Timedelta(before)
-    if after is not pd.Timedelta:
+    if not isinstance(after, pd.Timedelta):
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             group.clear()
             i = i + 1
 
+
 def triggerclip(data, events, before=pd.Timedelta(0), after=pd.Timedelta(0)):
-    '''
-    Split video data around the specified sequence of event timestamps.
+    """"""Split video data around the specified sequence of event timestamps.
 
     :param DataFrame data:
     A pandas DataFrame where each row specifies video acquisition path and frame number.",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035453,108.0,111,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `metadata` dictionary is being modified in-place by popping keys. This could lead to unexpected behavior if the same dictionary is used elsewhere. Consider creating a copy of the dictionary before modifying it.
```diff
-        workflow = metadata.pop(""Workflow"")
-        commit = metadata.pop(""Commit"", pd.NA)
+        metadata_copy = metadata.copy()
+        workflow = metadata_copy.pop(""Workflow"")
+        commit = metadata_copy.pop(""Commit"", pd.NA)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        workflow = metadata.pop(""Workflow"")
+        commit = metadata.pop(""Commit"", pd.NA)
+        data = {""workflow"": workflow, ""commit"": commit, ""metadata"": [DotMap(metadata)]}
         return pd.DataFrame(data, index=[time], columns=self.columns)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035455,,60,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The return type hint of the `get_bodyparts` function has been changed from `Union[None, List[str]]` to `None | list[str]`. This is a new syntax introduced in Python 3.10 and is equivalent to the old one. However, it will cause a `SyntaxError` if the codebase is intended to support Python versions older than 3.10.

```diff
- def get_bodyparts(self, file: Path) -> Union[None, List[str]]:
+ def get_bodyparts(self, file: Path) -> None | list[str]:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","         return new_data.sort_index()
 
-    def get_bodyparts(self, file: Path) -> Union[None, List[str]]:
+    def get_bodyparts(self, file: Path) -> None | list[str]:","--- 

+++ 

@@ -3,6 +3,7 @@

 import json
 from pathlib import Path
 
+import numpy as np
 import pandas as pd
 
 import aeon.io.reader as _reader
@@ -22,17 +23,25 @@

     """"""
 
     def __init__(self, pattern: str, extension: str = ""bin""):
+        """"""Pose reader constructor.""""""
         # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
         super().__init__(pattern, columns=None, extension=extension)
 
     def read(
-        self, file: Path, ceph_proc_dir: Path = Path(""/ceph/aeon/aeon/data/processed"")
+        self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
     ) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
+<<<<<<< HEAD
         config_file_dir = ceph_proc_dir / model_dir
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+=======
+        # `ceph_proc_dir` typically
+        config_file_dir = Path(ceph_proc_dir) / model_dir
         assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+>>>>>>> b9a1e3f... Blackened and ruffed
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -43,47 +52,71 @@

         self.columns = columns
         data = super().read(file)
 
+<<<<<<< HEAD
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
+=======
+>>>>>>> b9a1e3f... Blackened and ruffed
         # Set new columns, and reformat `data`.
         n_parts = len(parts)
-        part_data_list = [None] * n_parts
+        part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-            part_data = data[part_columns]
+            part_data = pd.DataFrame(data[part_columns])
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 
-    def get_bodyparts(self, file: Path) -> None | list[str]:
+    def get_bodyparts(self, file: Path) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
-        parts = None
+        parts = []
         with open(file) as f:
             config = json.load(f)
         if file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
-                parts = util.find_nested_key(heads, ""part_names"")
+                parts = [util.find_nested_key(heads, ""anchor_part"")]
+                parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                if not parts:
+                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
 
 def get_config_file(
     config_file_dir: Path,
-    config_file_names: list[str] = None,
-):
+    config_file_names: None | list[str] = None,
+) -> Path:
     """"""Returns the config file from a model's config directory.""""""
     if config_file_names is None:
-        config_file_names = [""confmap_config.json""]
+        config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
+<<<<<<< HEAD
     config_file = None
+=======
+    config_file = Path()
+>>>>>>> b9a1e3f... Blackened and ruffed
     for f in config_file_names:
         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+<<<<<<< HEAD
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
+=======
+    assert config_file.is_file(), f""Cannot find config file in {config_file_dir}""
+>>>>>>> b9a1e3f... Blackened and ruffed
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035457,76.0,80,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The default value of the `config_file_names` parameter in the `get_config_file` function has been changed from a list containing `""confmap_config.json""` to `None`. This change could potentially break existing calls to this function that rely on the previous default value. 

```diff
- config_file_names: List[str]=[
-     ""confmap_config.json"",  # SLEAP (add others for other trackers to this list)
- ],
+ config_file_names: list[str] = None,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    config_file_names: list[str] = None,
 ):
     """"""Returns the config file from a model's config directory.""""""
+    if config_file_names is None:
+        config_file_names = [""confmap_config.json""]","--- 

+++ 

@@ -3,6 +3,7 @@

 import json
 from pathlib import Path
 
+import numpy as np
 import pandas as pd
 
 import aeon.io.reader as _reader
@@ -22,17 +23,25 @@

     """"""
 
     def __init__(self, pattern: str, extension: str = ""bin""):
+        """"""Pose reader constructor.""""""
         # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
         super().__init__(pattern, columns=None, extension=extension)
 
     def read(
-        self, file: Path, ceph_proc_dir: Path = Path(""/ceph/aeon/aeon/data/processed"")
+        self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
     ) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
+<<<<<<< HEAD
         config_file_dir = ceph_proc_dir / model_dir
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+=======
+        # `ceph_proc_dir` typically
+        config_file_dir = Path(ceph_proc_dir) / model_dir
         assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+>>>>>>> b9a1e3f... Blackened and ruffed
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -43,47 +52,71 @@

         self.columns = columns
         data = super().read(file)
 
+<<<<<<< HEAD
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
+=======
+>>>>>>> b9a1e3f... Blackened and ruffed
         # Set new columns, and reformat `data`.
         n_parts = len(parts)
-        part_data_list = [None] * n_parts
+        part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-            part_data = data[part_columns]
+            part_data = pd.DataFrame(data[part_columns])
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 
-    def get_bodyparts(self, file: Path) -> None | list[str]:
+    def get_bodyparts(self, file: Path) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
-        parts = None
+        parts = []
         with open(file) as f:
             config = json.load(f)
         if file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
-                parts = util.find_nested_key(heads, ""part_names"")
+                parts = [util.find_nested_key(heads, ""anchor_part"")]
+                parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                if not parts:
+                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
 
 def get_config_file(
     config_file_dir: Path,
-    config_file_names: list[str] = None,
-):
+    config_file_names: None | list[str] = None,
+) -> Path:
     """"""Returns the config file from a model's config directory.""""""
     if config_file_names is None:
-        config_file_names = [""confmap_config.json""]
+        config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
+<<<<<<< HEAD
     config_file = None
+=======
+    config_file = Path()
+>>>>>>> b9a1e3f... Blackened and ruffed
     for f in config_file_names:
         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+<<<<<<< HEAD
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
+=======
+    assert config_file.is_file(), f""Cannot find config file in {config_file_dir}""
+>>>>>>> b9a1e3f... Blackened and ruffed
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035458,,30,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from the old string formatting to f-string is a good improvement for readability and performance. However, it would be better to handle this error more gracefully. Instead of raising a ValueError, consider logging the error and continue with the next frame if possible. This way, the process won't stop entirely because of a single frame that can't be read.
```diff
- raise ValueError(f'Unable to read frame {frameidx} from video path ""{path}"".')
+ logging.error(f'Unable to read frame {frameidx} from video path ""{path}"". Continue with the next frame.')
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             success, frame = capture.read()
             if not success:
-                raise ValueError('Unable to read frame {0} from video path ""{1}"".'.format(frameidx, path))
+                raise ValueError(f'Unable to read frame {frameidx} from video path ""{path}"".')",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035459,,1,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/octagon.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement `import aeon.io.device as _device` and `import aeon.schema.core as _stream` have been removed. If these modules are not used in the code, this is a good change for maintainability and readability. However, if they are used elsewhere in the code that's not shown in the hunk, this will cause a NameError.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,162 +1,190 @@
 import aeon.io.reader as _reader",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035460,1.0,190,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/octagon.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [189-191]

There is no newline at the end of the file. It's a common convention to put a newline at the end of a file because some utilities have trouble processing the last line of a file if it isn't newline terminated.
```diff
-        return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}
+        return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}
+
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import aeon.io.reader as _reader
-import aeon.io.device as _device
-import aeon.schema.core as _stream
+
 
 def photodiode(pattern):
-    return { ""Photodiode"": _reader.Harp(f""{pattern}_44_*"", columns=['adc', 'encoder']) }
+    return {""Photodiode"": _reader.Harp(f""{pattern}_44_*"", columns=[""adc"", ""encoder""])}
+
 
 class OSC:
     @staticmethod
     def background_color(pattern):
-        return { ""BackgroundColor"": _reader.Csv(f""{pattern}_backgroundcolor_*"", columns=['typetag', 'r', 'g', 'b', 'a']) }
+        return {
+            ""BackgroundColor"": _reader.Csv(
+                f""{pattern}_backgroundcolor_*"", columns=[""typetag"", ""r"", ""g"", ""b"", ""a""]
+            )
+        }
 
     @staticmethod
     def change_subject_state(pattern):
-        return { ""ChangeSubjectState"": _reader.Csv(f""{pattern}_changesubjectstate_*"", columns=['typetag', 'id', 'weight', 'event']) }
+        return {
+            ""ChangeSubjectState"": _reader.Csv(
+                f""{pattern}_changesubjectstate_*"", columns=[""typetag"", ""id"", ""weight"", ""event""]
+            )
+        }
 
     @staticmethod
     def end_trial(pattern):
-        return { ""EndTrial"": _reader.Csv(f""{pattern}_endtrial_*"", columns=['typetag', 'value']) }
+        return {""EndTrial"": _reader.Csv(f""{pattern}_endtrial_*"", columns=[""typetag"", ""value""])}
 
     @staticmethod
     def slice(pattern):
-        return { ""Slice"": _reader.Csv(f""{pattern}_octagonslice_*"", columns=[
-            'typetag',
-            'wall_id',
-            'r', 'g', 'b', 'a',
-            'delay']) }
+        return {
+            ""Slice"": _reader.Csv(
+                f""{pattern}_octagonslice_*"", columns=[""typetag"", ""wall_id"", ""r"", ""g"", ""b"", ""a"", ""delay""]
+            )
+        }
 
     @staticmethod
     def gratings_slice(pattern):
-        return { ""GratingsSlice"": _reader.Csv(f""{pattern}_octagongratingsslice_*"", columns=[
-            'typetag',
-            'wall_id',
-            'contrast',
-            'opacity',
-            'spatial_frequency',
-            'temporal_frequency',
-            'angle',
-            'delay']) }
+        return {
+            ""GratingsSlice"": _reader.Csv(
+                f""{pattern}_octagongratingsslice_*"",
+                columns=[
+                    ""typetag"",
+                    ""wall_id"",
+                    ""contrast"",
+                    ""opacity"",
+                    ""spatial_frequency"",
+                    ""temporal_frequency"",
+                    ""angle"",
+                    ""delay"",
+                ],
+            )
+        }
 
     @staticmethod
     def poke(pattern):
-        return { ""Poke"": _reader.Csv(f""{pattern}_poke_*"", columns=[
-            'typetag',
-            'wall_id',
-            'poke_id',
-            'reward',
-            'reward_interval',
-            'delay',
-            'led_delay']) }
+        return {
+            ""Poke"": _reader.Csv(
+                f""{pattern}_poke_*"",
+                columns=[
+                    ""typetag"",
+                    ""wall_id"",
+                    ""poke_id"",
+                    ""reward"",
+                    ""reward_interval"",
+                    ""delay"",
+                    ""led_delay"",
+                ],
+            )
+        }
 
     @staticmethod
     def response(pattern):
-        return { ""Response"": _reader.Csv(f""{pattern}_response_*"", columns=[
-            'typetag',
-            'wall_id',
-            'poke_id',
-            'response_time' ]) }
+        return {
+            ""Response"": _reader.Csv(
+                f""{pattern}_response_*"", columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""]
+            )
+        }
 
     @staticmethod
     def run_pre_trial_no_poke(pattern):
-        return { ""RunPreTrialNoPoke"": _reader.Csv(f""{pattern}_run_pre_no_poke_*"", columns=[
-            'typetag',
-            'wait_for_poke',
-            'reward_iti',
-            'timeout_iti',
-            'pre_trial_duration',
-            'activity_reset_flag' ]) }
+        return {
+            ""RunPreTrialNoPoke"": _reader.Csv(
+                f""{pattern}_run_pre_no_poke_*"",
+                columns=[
+                    ""typetag"",
+                    ""wait_for_poke"",
+                    ""reward_iti"",
+                    ""timeout_iti"",
+                    ""pre_trial_duration"",
+                    ""activity_reset_flag"",
+                ],
+            )
+        }
 
     @staticmethod
     def start_new_session(pattern):
-        return { ""StartNewSession"": _reader.Csv(f""{pattern}_startnewsession_*"", columns=['typetag', 'path' ]) }
+        return {""StartNewSession"": _reader.Csv(f""{pattern}_startnewsession_*"", columns=[""typetag"", ""path""])}
+
 
 class TaskLogic:
     @staticmethod
     def trial_initiation(pattern):
-        return { ""TrialInitiation"": _reader.Harp(f""{pattern}_1_*"", columns=['trial_type']) }
+        return {""TrialInitiation"": _reader.Harp(f""{pattern}_1_*"", columns=[""trial_type""])}
 
     @staticmethod
     def response(pattern):
-        return { ""Response"": _reader.Harp(f""{pattern}_2_*"", columns=['wall_id', 'poke_id']) }
+        return {""Response"": _reader.Harp(f""{pattern}_2_*"", columns=[""wall_id"", ""poke_id""])}
 
     @staticmethod
     def pre_trial(pattern):
-        return { ""PreTrialState"": _reader.Harp(f""{pattern}_3_*"", columns=['state']) }
+        return {""PreTrialState"": _reader.Harp(f""{pattern}_3_*"", columns=[""state""])}
 
     @staticmethod
     def inter_trial_interval(pattern):
-        return { ""InterTrialInterval"": _reader.Harp(f""{pattern}_4_*"", columns=['state']) }
+        return {""InterTrialInterval"": _reader.Harp(f""{pattern}_4_*"", columns=[""state""])}
 
     @staticmethod
     def slice_onset(pattern):
-        return { ""SliceOnset"": _reader.Harp(f""{pattern}_10_*"", columns=['wall_id']) }
+        return {""SliceOnset"": _reader.Harp(f""{pattern}_10_*"", columns=[""wall_id""])}
 
     @staticmethod
     def draw_background(pattern):
-        return { ""DrawBackground"": _reader.Harp(f""{pattern}_11_*"", columns=['state']) }
+        return {""DrawBackground"": _reader.Harp(f""{pattern}_11_*"", columns=[""state""])}
 
     @staticmethod
     def gratings_slice_onset(pattern):
-        return { ""GratingsSliceOnset"": _reader.Harp(f""{pattern}_12_*"", columns=['wall_id']) }
+        return {""GratingsSliceOnset"": _reader.Harp(f""{pattern}_12_*"", columns=[""wall_id""])}
+
 
 class Wall:
     @staticmethod
     def beam_break0(pattern):
-        return { ""BeamBreak0"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x1, columns=['state']) }
+        return {""BeamBreak0"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x1, columns=[""state""])}
 
     @staticmethod
     def beam_break1(pattern):
-        return { ""BeamBreak1"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x2, columns=['state']) }
+        return {""BeamBreak1"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x2, columns=[""state""])}
 
     @staticmethod
     def beam_break2(pattern):
-        return { ""BeamBreak2"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x4, columns=['state']) }
+        return {""BeamBreak2"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x4, columns=[""state""])}
 
     @staticmethod
     def set_led0(pattern):
-        return { ""SetLed0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x1, 'Set') }
+        return {""SetLed0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x1, ""Set"")}
 
     @staticmethod
     def set_led1(pattern):
-        return { ""SetLed1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x2, 'Set') }
+        return {""SetLed1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x2, ""Set"")}
 
     @staticmethod
     def set_led2(pattern):
-        return { ""SetLed2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x4, 'Set') }
+        return {""SetLed2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x4, ""Set"")}
 
     @staticmethod
     def set_valve0(pattern):
-        return { ""SetValve0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x8, 'Set') }
+        return {""SetValve0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x8, ""Set"")}
 
     @staticmethod
     def set_valve1(pattern):
-        return { ""SetValve1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x10, 'Set') }
+        return {""SetValve1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x10, ""Set"")}
 
     @staticmethod
     def set_valve2(pattern):
-        return { ""SetValve2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x20, 'Set') }
+        return {""SetValve2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x20, ""Set"")}
 
     @staticmethod
     def clear_led0(pattern):
-        return { ""ClearLed0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x1, 'Clear') }
+        return {""ClearLed0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x1, ""Clear"")}
 
     @staticmethod
     def clear_led1(pattern):
-        return { ""ClearLed1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x2, 'Clear') }
+        return {""ClearLed1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x2, ""Clear"")}
 
     @staticmethod
     def clear_led2(pattern):
-        return { ""ClearLed2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x4, 'Clear') }
+        return {""ClearLed2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x4, ""Clear"")}
 
     @staticmethod
     def clear_valve0(pattern):
-        return { ""ClearValve0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x8, 'Clear') }
+        return {""ClearValve0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x8, ""Clear"")}
 
     @staticmethod
     def clear_valve1(pattern):
-        return { ""ClearValve1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x10, 'Clear') }
+        return {""ClearValve1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x10, ""Clear"")}
 
     @staticmethod
     def clear_valve2(pattern):
-        return { ""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, 'Clear') }
+        return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035465,84.0,86,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error message could be more informative by including the actual value of `experiment_setup['Devices']`.
```diff
- raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+ raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}, value: {experiment_setup['Devices']}"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
     else:
-        raise ValueError(
-            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
-        )
+        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035466,192.0,200,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The regular expression search does not have any error handling. If the search fails and returns None, calling `.group()` will raise an AttributeError.
```diff
- ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+ match = re.search(r""\d+"", patch[""SampleRate""])
+ if match is None:
+     raise ValueError(f""Invalid SampleRate: {patch['SampleRate']}"")
+ ""wheel_sampling_rate"": float(match.group()),
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
         patch_list.append(patch_key)
 
         patch_installation = {
             **patch_key,
             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(
-                re.search(r""\d+"", patch[""SampleRate""]).group()
-            ),
+            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035467,270.0,303,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The dictionary key extraction could potentially raise a KeyError if the keys do not exist. Consider using `.get()` for safer key extraction.
```diff
- ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+ ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale.get(""PortName"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314035468,298.0,334,7445a0a16a18971b741524f685cf3e7b185c3339,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comparison of UUIDs could be simplified by directly comparing the dictionaries after removing the non-comparable keys.
```diff
- if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+ if current_weight_scale_config == new_weight_scale_config:
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
                 continue
 ","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347254,,79,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/analysis/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in formatting of the `eventrate` function call doesn't seem to affect the functionality, but it does make the code less readable. It's generally a good practice to break down long function calls into multiple lines for better readability.
```diff
- eventrate = rate(events, window, frequency, weight, start, end, smooth=smooth, center=center)
+ eventrate = rate(
+     events, 
+     window, 
+     frequency, 
+     weight, 
+     start, 
+     end, 
+     smooth=smooth, 
+     center=center
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-    eventrate = rate(
-        events, window, frequency, weight, start, end, smooth=smooth, center=center
-    )
+    eventrate = rate(events, window, frequency, weight, start, end, smooth=smooth, center=center)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347256,,6,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The regular expression in the `files` field has been changed. This could potentially exclude some files from being checked by the pre-commit hooks. Please ensure that this change is intentional and that all necessary files are still being included.
```diff
- files: ""^(docker|aeon\/dj_pipeline)\/.*$""
+ files: ""^(test|aeon(?!\/dj_pipeline\/).*)$""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   python: python3.11
 
-files: ""^(docker|aeon\/dj_pipeline)\/.*$""
+files: ""^(test|aeon(?!\/dj_pipeline\/).*)$""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347257,34.0,38,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The arguments for the `ruff` hook have been modified. The `--fix` and `--exit-non-zero-on-fix` arguments have been removed. This means that `ruff` will no longer automatically fix issues it finds, and it will not exit with a non-zero status code if it makes any fixes. If these changes were unintentional, they should be reverted.
```diff
- args: [--fix, --exit-non-zero-on-fix, --config, ./pyproject.toml]
+ args: [--config, ./pyproject.toml]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286
     hooks:
       - id: ruff
-        args: [--fix, --exit-non-zero-on-fix, --config, ./pyproject.toml]
+        args: [--config, ./pyproject.toml]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347258,71.0,81,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/create_experiment_01.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in line 71 from a multi-line dictionary to a single line dictionary is not recommended. It reduces the readability of the code, especially when the dictionary has more than one key-value pair. The original format was more readable and maintainable.
```diff
- patch_key = {""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]}
+ patch_key = {
+     ""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            patch_key = {""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]}
             if patch_key not in lab.FoodPatch():
                 lab.FoodPatch.insert1(patch_key)
             # ---- Check if this food patch is currently installed - if so, remove it
             current_patch_query = (
-                acquisition.ExperimentFoodPatch
-                - acquisition.ExperimentFoodPatch.RemovalTime
+                acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & patch_key
             )
             if current_patch_query:  # If the same food-patch is currently installed
-                if (
-                    current_patch_query.fetch1(""food_patch_install_time"")
-                    == arena_setup[""start-time""]
-                ):
+                if current_patch_query.fetch1(""food_patch_install_time"") == arena_setup[""start-time""]:","--- 

+++ 

@@ -1,15 +1,13 @@

+import yaml
+from aeon.dj_pipeline import acquisition, lab, subject
 import pathlib
-
-import yaml
-
-from aeon.dj_pipeline import acquisition, lab, subject
 
 _wheel_sampling_rate = 500
 _weight_scale_rate = 100
 
 
 def ingest_exp01_metadata(metadata_yml_filepath, experiment_name):
-    with open(metadata_yml_filepath) as f:
+    with open(metadata_yml_filepath, ""r"") as f:
         arena_setup = yaml.full_load(f)
 
     device_frequency_mapper = {
@@ -32,7 +30,10 @@

                 & camera_key
             )
             if current_camera_query:  # If the same camera is currently installed
-                if current_camera_query.fetch1(""camera_install_time"") == arena_setup[""start-time""]:
+                if (
+                    current_camera_query.fetch1(""camera_install_time"")
+                    == arena_setup[""start-time""]
+                ):
                     # If it is installed at the same time as that read from this yml file
                     # then it is the same ExperimentCamera instance, no need to do anything
                     continue
@@ -52,7 +53,9 @@

                     ""experiment_name"": experiment_name,
                     ""camera_install_time"": arena_setup[""start-time""],
                     ""camera_description"": camera[""description""],
-                    ""camera_sampling_rate"": device_frequency_mapper[camera[""trigger-source""].lower()],
+                    ""camera_sampling_rate"": device_frequency_mapper[
+                        camera[""trigger-source""].lower()
+                    ],
                 }
             )
             acquisition.ExperimentCamera.Position.insert1(
@@ -68,17 +71,23 @@

         # ---- Load food patches ----
         for patch in arena_setup[""patches""]:
             # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-            patch_key = {""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]}
+            patch_key = {
+                ""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]
+            }
             if patch_key not in lab.FoodPatch():
                 lab.FoodPatch.insert1(patch_key)
             # ---- Check if this food patch is currently installed - if so, remove it
             current_patch_query = (
-                acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+                acquisition.ExperimentFoodPatch
+                - acquisition.ExperimentFoodPatch.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & patch_key
             )
             if current_patch_query:  # If the same food-patch is currently installed
-                if current_patch_query.fetch1(""food_patch_install_time"") == arena_setup[""start-time""]:
+                if (
+                    current_patch_query.fetch1(""food_patch_install_time"")
+                    == arena_setup[""start-time""]
+                ):
                     # If it is installed at the same time as that read from this yml file
                     # then it is the same ExperimentFoodPatch instance, no need to do anything
                     continue
@@ -113,16 +122,21 @@

             )
         # ---- Load weight scales ----
         for weight_scale in arena_setup[""weight-scales""]:
-            weight_scale_key = {""weight_scale_serial_number"": weight_scale[""serial-number""]}
+            weight_scale_key = {
+                ""weight_scale_serial_number"": weight_scale[""serial-number""]
+            }
             if weight_scale_key not in lab.WeightScale():
                 lab.WeightScale.insert1(weight_scale_key)
             # ---- Check if this weight scale is currently installed - if so, remove it
             current_weight_scale_query = (
-                acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+                acquisition.ExperimentWeightScale
+                - acquisition.ExperimentWeightScale.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & weight_scale_key
             )
-            if current_weight_scale_query:  # If the same weight scale is currently installed
+            if (
+                current_weight_scale_query
+            ):  # If the same weight scale is currently installed
                 if (
                     current_weight_scale_query.fetch1(""weight_scale_install_time"")
                     == arena_setup[""start-time""]
@@ -250,8 +264,12 @@

     # manually update coordinates of foodpatch and nest
     patch_coordinates = {""Patch1"": (1.13, 1.59, 0), ""Patch2"": (1.19, 0.50, 0)}
 
-    for patch_key in (acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}).fetch(""KEY""):
-        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(""food_patch_description"")
+    for patch_key in (
+        acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}
+    ).fetch(""KEY""):
+        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(
+            ""food_patch_description""
+        )
         x, y, z = patch_coordinates[patch]
         acquisition.ExperimentFoodPatch.Position.update1(
             {"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347259,116.0,125,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/create_experiment_01.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the change in line 116 from a multi-line dictionary to a single line dictionary is not recommended for readability and maintainability reasons.
```diff
- weight_scale_key = {""weight_scale_serial_number"": weight_scale[""serial-number""]}
+ weight_scale_key = {
+     ""weight_scale_serial_number"": weight_scale[""serial-number""]
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            weight_scale_key = {""weight_scale_serial_number"": weight_scale[""serial-number""]}
             if weight_scale_key not in lab.WeightScale():
                 lab.WeightScale.insert1(weight_scale_key)
             # ---- Check if this weight scale is currently installed - if so, remove it
             current_weight_scale_query = (
-                acquisition.ExperimentWeightScale
-                - acquisition.ExperimentWeightScale.RemovalTime
+                acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & weight_scale_key
             )
-            if (
-                current_weight_scale_query
-            ):  # If the same weight scale is currently installed
+            if current_weight_scale_query:  # If the same weight scale is currently installed","--- 

+++ 

@@ -1,15 +1,13 @@

+import yaml
+from aeon.dj_pipeline import acquisition, lab, subject
 import pathlib
-
-import yaml
-
-from aeon.dj_pipeline import acquisition, lab, subject
 
 _wheel_sampling_rate = 500
 _weight_scale_rate = 100
 
 
 def ingest_exp01_metadata(metadata_yml_filepath, experiment_name):
-    with open(metadata_yml_filepath) as f:
+    with open(metadata_yml_filepath, ""r"") as f:
         arena_setup = yaml.full_load(f)
 
     device_frequency_mapper = {
@@ -32,7 +30,10 @@

                 & camera_key
             )
             if current_camera_query:  # If the same camera is currently installed
-                if current_camera_query.fetch1(""camera_install_time"") == arena_setup[""start-time""]:
+                if (
+                    current_camera_query.fetch1(""camera_install_time"")
+                    == arena_setup[""start-time""]
+                ):
                     # If it is installed at the same time as that read from this yml file
                     # then it is the same ExperimentCamera instance, no need to do anything
                     continue
@@ -52,7 +53,9 @@

                     ""experiment_name"": experiment_name,
                     ""camera_install_time"": arena_setup[""start-time""],
                     ""camera_description"": camera[""description""],
-                    ""camera_sampling_rate"": device_frequency_mapper[camera[""trigger-source""].lower()],
+                    ""camera_sampling_rate"": device_frequency_mapper[
+                        camera[""trigger-source""].lower()
+                    ],
                 }
             )
             acquisition.ExperimentCamera.Position.insert1(
@@ -68,17 +71,23 @@

         # ---- Load food patches ----
         for patch in arena_setup[""patches""]:
             # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-            patch_key = {""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]}
+            patch_key = {
+                ""food_patch_serial_number"": patch[""serial-number""] or patch[""port-name""]
+            }
             if patch_key not in lab.FoodPatch():
                 lab.FoodPatch.insert1(patch_key)
             # ---- Check if this food patch is currently installed - if so, remove it
             current_patch_query = (
-                acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+                acquisition.ExperimentFoodPatch
+                - acquisition.ExperimentFoodPatch.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & patch_key
             )
             if current_patch_query:  # If the same food-patch is currently installed
-                if current_patch_query.fetch1(""food_patch_install_time"") == arena_setup[""start-time""]:
+                if (
+                    current_patch_query.fetch1(""food_patch_install_time"")
+                    == arena_setup[""start-time""]
+                ):
                     # If it is installed at the same time as that read from this yml file
                     # then it is the same ExperimentFoodPatch instance, no need to do anything
                     continue
@@ -113,16 +122,21 @@

             )
         # ---- Load weight scales ----
         for weight_scale in arena_setup[""weight-scales""]:
-            weight_scale_key = {""weight_scale_serial_number"": weight_scale[""serial-number""]}
+            weight_scale_key = {
+                ""weight_scale_serial_number"": weight_scale[""serial-number""]
+            }
             if weight_scale_key not in lab.WeightScale():
                 lab.WeightScale.insert1(weight_scale_key)
             # ---- Check if this weight scale is currently installed - if so, remove it
             current_weight_scale_query = (
-                acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+                acquisition.ExperimentWeightScale
+                - acquisition.ExperimentWeightScale.RemovalTime
                 & {""experiment_name"": experiment_name}
                 & weight_scale_key
             )
-            if current_weight_scale_query:  # If the same weight scale is currently installed
+            if (
+                current_weight_scale_query
+            ):  # If the same weight scale is currently installed
                 if (
                     current_weight_scale_query.fetch1(""weight_scale_install_time"")
                     == arena_setup[""start-time""]
@@ -250,8 +264,12 @@

     # manually update coordinates of foodpatch and nest
     patch_coordinates = {""Patch1"": (1.13, 1.59, 0), ""Patch2"": (1.19, 0.50, 0)}
 
-    for patch_key in (acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}).fetch(""KEY""):
-        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(""food_patch_description"")
+    for patch_key in (
+        acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}
+    ).fetch(""KEY""):
+        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(
+            ""food_patch_description""
+        )
         x, y, z = patch_coordinates[patch]
         acquisition.ExperimentFoodPatch.Position.update1(
             {"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347260,30.0,52,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are good for readability and performance. The use of f-string in line 30 is more efficient than the previous string formatting method. Also, it's more readable and easier to understand.
```diff
- raise ValueError('Unable to read frame {0} from video path ""{1}"".'.format(frameidx, path))
+ raise ValueError(f'Unable to read frame {frameidx} from video path ""{path}"".')
```
In the `export` function, changing single quotes to double quotes in line 52 doesn't affect the functionality but improves consistency with the rest of the codebase.
```diff
- fourcc = cv2.VideoWriter_fourcc('m','p','4','v')
+ fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                raise ValueError(f'Unable to read frame {frameidx} from video path ""{path}"".')
             yield frame
             index = index + 1
     finally:
         if capture is not None:
             capture.release()
 
+
 def export(frames, file, fps, fourcc=None):
-    '''
-    Exports the specified frame sequence to a new video file.
+    """"""Exports the specified frame sequence to a new video file.
 
     :param iterable frames: An object to iterate over the raw video frame data.
     :param str file: The path to the exported video file.
     :param fps: The frame rate of the exported video.
     :param optional fourcc:
     Specifies the four character code of the codec used to compress the frames.
-    '''
+    """"""
     writer = None
     try:
         for frame in frames:
             if writer is None:
                 if fourcc is None:
-                    fourcc = cv2.VideoWriter_fourcc('m','p','4','v')
+                    fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347261,68.0,76,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a potential issue with the handling of missing columns. If `self.columns` is not None and `payloadshape[1] < len(self.columns)`, you are assigning `math.nan` to the missing columns. However, this will only work if `payloadshape[1]` is less than `len(self.columns)`. If `payloadshape[1]` is greater than `len(self.columns)`, there will be extra data that is not assigned to any column. You should add error handling for this case.
```diff
- if self.columns is not None and payloadshape[1] < len(self.columns):
+ if self.columns is not None:
+     if payloadshape[1] < len(self.columns):
          data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
          data[self.columns[payloadshape[1] :]] = math.nan
          return data
+     elif payloadshape[1] > len(self.columns):
+         raise ValueError(""More data points than expected columns."")
      else:
          return pd.DataFrame(payload, index=seconds, columns=self.columns)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
+        )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
-            data = pd.DataFrame(payload, index=seconds, columns=self.columns[:payloadshape[1]])
-            data[self.columns[payloadshape[1]:]] = math.nan
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
+            data[self.columns[payloadshape[1] :]] = math.nan
             return data
         else:
             return pd.DataFrame(payload, index=seconds, columns=self.columns)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347262,,2,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/octagon.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement for `aeon.io.device` and `aeon.schema.core` has been removed. If these modules are not used in the code, this is a good change as it removes unnecessary imports. However, if they are used elsewhere in the code, this could cause a NameError.

```diff
- import aeon.io.device as _device
- import aeon.schema.core as _stream
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import aeon.io.reader as _reader
-import aeon.io.device as _device
-import aeon.schema.core as _stream
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347263,1.0,190,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/schema/octagon.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [189-191]

There's no newline at the end of the file. It's a common convention to put a newline at the end of a file because it helps with concatenating files and viewing them in the terminal. Some tools might not handle files correctly if they don't end with a newline.

```diff
- return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}
+ return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import aeon.io.reader as _reader
-import aeon.io.device as _device
-import aeon.schema.core as _stream
+
 
 def photodiode(pattern):
-    return { ""Photodiode"": _reader.Harp(f""{pattern}_44_*"", columns=['adc', 'encoder']) }
+    return {""Photodiode"": _reader.Harp(f""{pattern}_44_*"", columns=[""adc"", ""encoder""])}
+
 
 class OSC:
     @staticmethod
     def background_color(pattern):
-        return { ""BackgroundColor"": _reader.Csv(f""{pattern}_backgroundcolor_*"", columns=['typetag', 'r', 'g', 'b', 'a']) }
+        return {
+            ""BackgroundColor"": _reader.Csv(
+                f""{pattern}_backgroundcolor_*"", columns=[""typetag"", ""r"", ""g"", ""b"", ""a""]
+            )
+        }
 
     @staticmethod
     def change_subject_state(pattern):
-        return { ""ChangeSubjectState"": _reader.Csv(f""{pattern}_changesubjectstate_*"", columns=['typetag', 'id', 'weight', 'event']) }
+        return {
+            ""ChangeSubjectState"": _reader.Csv(
+                f""{pattern}_changesubjectstate_*"", columns=[""typetag"", ""id"", ""weight"", ""event""]
+            )
+        }
 
     @staticmethod
     def end_trial(pattern):
-        return { ""EndTrial"": _reader.Csv(f""{pattern}_endtrial_*"", columns=['typetag', 'value']) }
+        return {""EndTrial"": _reader.Csv(f""{pattern}_endtrial_*"", columns=[""typetag"", ""value""])}
 
     @staticmethod
     def slice(pattern):
-        return { ""Slice"": _reader.Csv(f""{pattern}_octagonslice_*"", columns=[
-            'typetag',
-            'wall_id',
-            'r', 'g', 'b', 'a',
-            'delay']) }
+        return {
+            ""Slice"": _reader.Csv(
+                f""{pattern}_octagonslice_*"", columns=[""typetag"", ""wall_id"", ""r"", ""g"", ""b"", ""a"", ""delay""]
+            )
+        }
 
     @staticmethod
     def gratings_slice(pattern):
-        return { ""GratingsSlice"": _reader.Csv(f""{pattern}_octagongratingsslice_*"", columns=[
-            'typetag',
-            'wall_id',
-            'contrast',
-            'opacity',
-            'spatial_frequency',
-            'temporal_frequency',
-            'angle',
-            'delay']) }
+        return {
+            ""GratingsSlice"": _reader.Csv(
+                f""{pattern}_octagongratingsslice_*"",
+                columns=[
+                    ""typetag"",
+                    ""wall_id"",
+                    ""contrast"",
+                    ""opacity"",
+                    ""spatial_frequency"",
+                    ""temporal_frequency"",
+                    ""angle"",
+                    ""delay"",
+                ],
+            )
+        }
 
     @staticmethod
     def poke(pattern):
-        return { ""Poke"": _reader.Csv(f""{pattern}_poke_*"", columns=[
-            'typetag',
-            'wall_id',
-            'poke_id',
-            'reward',
-            'reward_interval',
-            'delay',
-            'led_delay']) }
+        return {
+            ""Poke"": _reader.Csv(
+                f""{pattern}_poke_*"",
+                columns=[
+                    ""typetag"",
+                    ""wall_id"",
+                    ""poke_id"",
+                    ""reward"",
+                    ""reward_interval"",
+                    ""delay"",
+                    ""led_delay"",
+                ],
+            )
+        }
 
     @staticmethod
     def response(pattern):
-        return { ""Response"": _reader.Csv(f""{pattern}_response_*"", columns=[
-            'typetag',
-            'wall_id',
-            'poke_id',
-            'response_time' ]) }
+        return {
+            ""Response"": _reader.Csv(
+                f""{pattern}_response_*"", columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""]
+            )
+        }
 
     @staticmethod
     def run_pre_trial_no_poke(pattern):
-        return { ""RunPreTrialNoPoke"": _reader.Csv(f""{pattern}_run_pre_no_poke_*"", columns=[
-            'typetag',
-            'wait_for_poke',
-            'reward_iti',
-            'timeout_iti',
-            'pre_trial_duration',
-            'activity_reset_flag' ]) }
+        return {
+            ""RunPreTrialNoPoke"": _reader.Csv(
+                f""{pattern}_run_pre_no_poke_*"",
+                columns=[
+                    ""typetag"",
+                    ""wait_for_poke"",
+                    ""reward_iti"",
+                    ""timeout_iti"",
+                    ""pre_trial_duration"",
+                    ""activity_reset_flag"",
+                ],
+            )
+        }
 
     @staticmethod
     def start_new_session(pattern):
-        return { ""StartNewSession"": _reader.Csv(f""{pattern}_startnewsession_*"", columns=['typetag', 'path' ]) }
+        return {""StartNewSession"": _reader.Csv(f""{pattern}_startnewsession_*"", columns=[""typetag"", ""path""])}
+
 
 class TaskLogic:
     @staticmethod
     def trial_initiation(pattern):
-        return { ""TrialInitiation"": _reader.Harp(f""{pattern}_1_*"", columns=['trial_type']) }
+        return {""TrialInitiation"": _reader.Harp(f""{pattern}_1_*"", columns=[""trial_type""])}
 
     @staticmethod
     def response(pattern):
-        return { ""Response"": _reader.Harp(f""{pattern}_2_*"", columns=['wall_id', 'poke_id']) }
+        return {""Response"": _reader.Harp(f""{pattern}_2_*"", columns=[""wall_id"", ""poke_id""])}
 
     @staticmethod
     def pre_trial(pattern):
-        return { ""PreTrialState"": _reader.Harp(f""{pattern}_3_*"", columns=['state']) }
+        return {""PreTrialState"": _reader.Harp(f""{pattern}_3_*"", columns=[""state""])}
 
     @staticmethod
     def inter_trial_interval(pattern):
-        return { ""InterTrialInterval"": _reader.Harp(f""{pattern}_4_*"", columns=['state']) }
+        return {""InterTrialInterval"": _reader.Harp(f""{pattern}_4_*"", columns=[""state""])}
 
     @staticmethod
     def slice_onset(pattern):
-        return { ""SliceOnset"": _reader.Harp(f""{pattern}_10_*"", columns=['wall_id']) }
+        return {""SliceOnset"": _reader.Harp(f""{pattern}_10_*"", columns=[""wall_id""])}
 
     @staticmethod
     def draw_background(pattern):
-        return { ""DrawBackground"": _reader.Harp(f""{pattern}_11_*"", columns=['state']) }
+        return {""DrawBackground"": _reader.Harp(f""{pattern}_11_*"", columns=[""state""])}
 
     @staticmethod
     def gratings_slice_onset(pattern):
-        return { ""GratingsSliceOnset"": _reader.Harp(f""{pattern}_12_*"", columns=['wall_id']) }
+        return {""GratingsSliceOnset"": _reader.Harp(f""{pattern}_12_*"", columns=[""wall_id""])}
+
 
 class Wall:
     @staticmethod
     def beam_break0(pattern):
-        return { ""BeamBreak0"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x1, columns=['state']) }
+        return {""BeamBreak0"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x1, columns=[""state""])}
 
     @staticmethod
     def beam_break1(pattern):
-        return { ""BeamBreak1"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x2, columns=['state']) }
+        return {""BeamBreak1"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x2, columns=[""state""])}
 
     @staticmethod
     def beam_break2(pattern):
-        return { ""BeamBreak2"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x4, columns=['state']) }
+        return {""BeamBreak2"": _reader.DigitalBitmask(f""{pattern}_32_*"", 0x4, columns=[""state""])}
 
     @staticmethod
     def set_led0(pattern):
-        return { ""SetLed0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x1, 'Set') }
+        return {""SetLed0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x1, ""Set"")}
 
     @staticmethod
     def set_led1(pattern):
-        return { ""SetLed1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x2, 'Set') }
+        return {""SetLed1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x2, ""Set"")}
 
     @staticmethod
     def set_led2(pattern):
-        return { ""SetLed2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x4, 'Set') }
+        return {""SetLed2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x4, ""Set"")}
 
     @staticmethod
     def set_valve0(pattern):
-        return { ""SetValve0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x8, 'Set') }
+        return {""SetValve0"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x8, ""Set"")}
 
     @staticmethod
     def set_valve1(pattern):
-        return { ""SetValve1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x10, 'Set') }
+        return {""SetValve1"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x10, ""Set"")}
 
     @staticmethod
     def set_valve2(pattern):
-        return { ""SetValve2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x20, 'Set') }
+        return {""SetValve2"": _reader.BitmaskEvent(f""{pattern}_34_*"", 0x20, ""Set"")}
 
     @staticmethod
     def clear_led0(pattern):
-        return { ""ClearLed0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x1, 'Clear') }
+        return {""ClearLed0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x1, ""Clear"")}
 
     @staticmethod
     def clear_led1(pattern):
-        return { ""ClearLed1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x2, 'Clear') }
+        return {""ClearLed1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x2, ""Clear"")}
 
     @staticmethod
     def clear_led2(pattern):
-        return { ""ClearLed2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x4, 'Clear') }
+        return {""ClearLed2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x4, ""Clear"")}
 
     @staticmethod
     def clear_valve0(pattern):
-        return { ""ClearValve0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x8, 'Clear') }
+        return {""ClearValve0"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x8, ""Clear"")}
 
     @staticmethod
     def clear_valve1(pattern):
-        return { ""ClearValve1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x10, 'Clear') }
+        return {""ClearValve1"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x10, ""Clear"")}
 
     @staticmethod
     def clear_valve2(pattern):
-        return { ""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, 'Clear') }
+        return {""ClearValve2"": _reader.BitmaskEvent(f""{pattern}_35_*"", 0x20, ""Clear"")}",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347264,2.0,3,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements are not in the correct order according to PEP8. The 'datetime' module should be imported before 're'.
```diff
- import re
- from datetime import datetime
+ from datetime import datetime
+ import re
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import re
 from datetime import datetime","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347268,,19,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The file is opened without specifying the mode. It's a good practice to explicitly mention the mode even if 'r' (read) is default.
```diff
- with open(metadata_yml_filepath) as f:
+ with open(metadata_yml_filepath, 'r') as f:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
 
-    with open(metadata_yml_filepath, ""r"") as f:
+    with open(metadata_yml_filepath) as f:","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347270,36.0,42,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring is incomplete and contains placeholders. Please complete it or remove the placeholders.
```diff
-     """"""work-in-progress
-      Missing:
-      + camera/patch location
-     + patch, weightscale serial number.
-      """"""
+     """"""This function ingests epoch metadata.""""""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number
+    + patch, weightscale serial number.
     """"""","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347271,,45,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The file is opened without specifying the mode. It's a good practice to explicitly mention the mode even if 'r' (read) is default.
```diff
- with open(metadata_yml_filepath) as f:
+ with open(metadata_yml_filepath, 'r') as f:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
 
-    with open(metadata_yml_filepath, ""r"") as f:
+    with open(metadata_yml_filepath) as f:","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347272,,84,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The dictionary creation could be simplified by using a dictionary literal instead of calling dict().
```diff
- experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+ experiment_devices.append(dict(Name=device_name, Type=device_type, **device_info))
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-            experiment_devices.append(
-                {""Name"": device_name, ""Type"": device_type, **device_info}
-            )
+            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347273,177.0,198,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The query is not properly formatted. It would be more readable if each condition was on its own line.
```diff
- acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
-  & experiment_key
+ (
+    acquisition.ExperimentCamera 
+    - acquisition.ExperimentCamera.RemovalTime 
+    - camera_list
+    & experiment_key
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
             & experiment_key
         ).fetch(""KEY"")","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347274,,192,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The dictionary creation could be simplified by using a dictionary literal instead of calling dict().
```diff
- patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+ patch_key = dict(food_patch_serial_number=patch.get(""SerialNumber"") or patch[""PortName""])
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-        patch_key = {
-            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
-        }
+        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347275,223.0,251,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The query is not properly formatted. It would be more readable if each condition was on its own line.
```diff
- acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
-  & experiment_key
-  & patch_key
+ (
+    acquisition.ExperimentFoodPatch 
+    - acquisition.ExperimentFoodPatch.RemovalTime
+    & experiment_key
+    & patch_key
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347277,255.0,285,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The query is not properly formatted. It would be more readable if each condition was on its own line.
```diff
- acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
-  & experiment_key
+ (
+    acquisition.ExperimentFoodPatch 
+    - acquisition.ExperimentFoodPatch.RemovalTime 
+    - patch_list
+    & experiment_key
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
             & experiment_key
         ).fetch(""KEY"")","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347278,,270,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The dictionary creation could be simplified by using a dictionary literal instead of calling dict().
```diff
- ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+ dict(weight_scale_serial_number=weight_scale.get(""SerialNumber"") or weight_scale[""PortName""])
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
-            or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347279,287.0,321,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The query is not properly formatted. It would be more readable if each condition was on its own line.
```diff
- acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
-  & experiment_key
-  & weight_scale_key
+ (
+    acquisition.ExperimentWeightScale 
+    - acquisition.ExperimentWeightScale.RemovalTime
+    & experiment_key
+    & weight_scale_key
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347280,,298,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/ingest/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comparison of two UUIDs could be simplified by directly comparing the dictionaries after removing the 'weight_scale_install_time' key.
```diff
- if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+ if current_weight_scale_config == new_weight_scale_config:
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
-                new_weight_scale_config
-            ):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):","--- 

+++ 

@@ -1,12 +1,11 @@

+import re
 import pathlib
-import re
 from datetime import datetime
-
 import yaml
 
 from aeon.dj_pipeline import acquisition, lab
-
 from .. import dict_to_uuid
+
 
 _weight_scale_rate = 100
 _weight_scale_nest = 1
@@ -14,9 +13,11 @@

 
 def extract_epoch_metadata(experiment_name, metadata_yml_filepath):
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     commit = experiment_setup.get(""Commit"", experiment_setup.get(""Revision""))
@@ -33,16 +34,20 @@

 
 
 def ingest_epoch_metadata(experiment_name, metadata_yml_filepath):
-    """"""work-in-progress
+    """"""
+    work-in-progress
     Missing:
     + camera/patch location
-    + patch, weightscale serial number.
+    + patch, weightscale serial number
     """"""
+
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
     file_creation_time = datetime.fromtimestamp(metadata_yml_filepath.stat().st_ctime)
-    epoch_start = datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
-
-    with open(metadata_yml_filepath) as f:
+    epoch_start = datetime.strptime(
+        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
+    )
+
+    with open(metadata_yml_filepath, ""r"") as f:
         experiment_setup = yaml.safe_load(f)
 
     experiment_key = {""experiment_name"": experiment_name}
@@ -81,23 +86,35 @@

             else:
                 raise ValueError(f""Unrecognized Device Type for {device_name}"")
 
-            experiment_devices.append({""Name"": device_name, ""Type"": device_type, **device_info})
+            experiment_devices.append(
+                {""Name"": device_name, ""Type"": device_type, **device_info}
+            )
     else:
-        raise ValueError(f""Unexpected devices variable type: {type(experiment_setup['Devices'])}"")
+        raise ValueError(
+            f""Unexpected devices variable type: {type(experiment_setup['Devices'])}""
+        )
 
     # ---- Video Controller ----
 
-    video_controller = [device for device in experiment_devices if device[""Type""] == ""VideoController""]
-
-    assert len(video_controller) == 1, ""Unable to find one unique VideoController device""
+    video_controller = [
+        device for device in experiment_devices if device[""Type""] == ""VideoController""
+    ]
+
+    assert (
+        len(video_controller) == 1
+    ), ""Unable to find one unique VideoController device""
     video_controller = video_controller[0]
 
     device_frequency_mapper = {
-        name: float(value) for name, value in video_controller.items() if name.endswith(""Frequency"")
+        name: float(value)
+        for name, value in video_controller.items()
+        if name.endswith(""Frequency"")
     }
 
     # ---- Load cameras ----
-    cameras = [device for device in experiment_devices if device[""Type""] == ""VideoSource""]
+    cameras = [
+        device for device in experiment_devices if device[""Type""] == ""VideoSource""
+    ]
 
     camera_list, camera_installation_list, camera_removal_list, camera_position_list = (
         [],
@@ -174,13 +191,17 @@

     # remove the currently installed cameras that are absent in this config
     camera_removal_list.extend(
         (
-            acquisition.ExperimentCamera - acquisition.ExperimentCamera.RemovalTime - camera_list
+            acquisition.ExperimentCamera
+            - acquisition.ExperimentCamera.RemovalTime
+            - camera_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load food patches ----
-    food_patches = [device for device in experiment_devices if device[""Type""] == ""Patch""]
+    food_patches = [
+        device for device in experiment_devices if device[""Type""] == ""Patch""
+    ]
     patch_list, patch_installation_list, patch_removal_list, patch_position_list = (
         [],
         [],
@@ -189,7 +210,9 @@

     )
     for patch in food_patches:
         # ---- Check if this is a new food patch, add to lab.FoodPatch if needed
-        patch_key = {""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]}
+        patch_key = {
+            ""food_patch_serial_number"": patch.get(""SerialNumber"") or patch[""PortName""]
+        }
         patch_list.append(patch_key)
 
         patch_installation = {
@@ -197,7 +220,9 @@

             ""experiment_name"": experiment_name,
             ""food_patch_install_time"": epoch_start,
             ""food_patch_description"": patch[""Name""],
-            ""wheel_sampling_rate"": float(re.search(r""\d+"", patch[""SampleRate""]).group()),
+            ""wheel_sampling_rate"": float(
+                re.search(r""\d+"", patch[""SampleRate""]).group()
+            ),
             ""wheel_radius"": float(patch[""Radius""]),
         }
         if ""position"" in patch:
@@ -220,7 +245,8 @@

         # If the same camera serial number is currently installed
         # check for any changes in configuration, if not, skip this
         current_patch_query = (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
             & experiment_key
             & patch_key
         )
@@ -252,13 +278,17 @@

     # remove the currently installed patches that are absent in this config
     patch_removal_list.extend(
         (
-            acquisition.ExperimentFoodPatch - acquisition.ExperimentFoodPatch.RemovalTime - patch_list
+            acquisition.ExperimentFoodPatch
+            - acquisition.ExperimentFoodPatch.RemovalTime
+            - patch_list
             & experiment_key
         ).fetch(""KEY"")
     )
 
     # ---- Load weight scales ----
-    weight_scales = [device for device in experiment_devices if device[""Type""] == ""WeightScale""]
+    weight_scales = [
+        device for device in experiment_devices if device[""Type""] == ""WeightScale""
+    ]
     weight_scale_list, weight_scale_installation_list, weight_scale_removal_list = (
         [],
         [],
@@ -267,7 +297,8 @@

     for weight_scale in weight_scales:
         # ---- Check if this is a new weight scale, add to lab.WeightScale if needed
         weight_scale_key = {
-            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"") or weight_scale[""PortName""]
+            ""weight_scale_serial_number"": weight_scale.get(""SerialNumber"")
+            or weight_scale[""PortName""]
         }
         weight_scale_list.append(weight_scale_key)
 
@@ -284,7 +315,8 @@

 
         # ---- Check if this weight scale is currently installed - if so, remove it
         current_weight_scale_query = (
-            acquisition.ExperimentWeightScale - acquisition.ExperimentWeightScale.RemovalTime
+            acquisition.ExperimentWeightScale
+            - acquisition.ExperimentWeightScale.RemovalTime
             & experiment_key
             & weight_scale_key
         )
@@ -295,7 +327,9 @@

             current_weight_scale_config.pop(""weight_scale_install_time"")
             new_weight_scale_config.pop(""weight_scale_install_time"")
 
-            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(new_weight_scale_config):
+            if dict_to_uuid(current_weight_scale_config) == dict_to_uuid(
+                new_weight_scale_config
+            ):
                 continue
 
             # ---- Remove old weight scale"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347281,43.0,52,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The variable `raw_data_dir` is removed in the new hunk. If it's not used elsewhere, this change improves the code by removing unnecessary variables.
```diff
-        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+        acquisition.Experiment.get_data_directory(key)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        acquisition.Experiment.get_data_directory(key)
 
-        in_arena_start, in_arena_end = (
-            analysis.InArena * analysis.InArenaEnd & key
-        ).fetch1(""in_arena_start"", ""in_arena_end"")
+        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
+            ""in_arena_start"", ""in_arena_end""
+        )
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (
-            position.index - in_arena_start
-        ).total_seconds() / 60
+        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347282,,138,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The f-string formatting has been removed from the label. This is a good change if there are no variables to be interpolated in the string.
```diff
-            label=f""Times in arena"",
+            label=""Times in arena"",
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             markersize=0.5,
             alpha=0.6,
-            label=f""Times in arena"",
+            label=""Times in arena"",","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347284,,147,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The f-string formatting has been removed from the label. This is a good change if there are no variables to be interpolated in the string.
```diff
-            label=f""Times in corridor"",
+            label=""Times in corridor"",
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             markersize=0.5,
             alpha=0.6,
-            label=f""Times in corridor"",
+            label=""Times in corridor"",","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347285,157.0,172,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The f-string formatting has been removed from the label. This is a good change if there are no variables to be interpolated in the string.
```diff
-                label=f""Times in nest"",
+                label=""Times in nest"",
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                label=""Times in nest"",
             )","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347286,254.0,257,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring formatting has been changed. The new format is more readable and consistent with PEP 8 guidelines.
```diff
-        """"""
-        Each entry in this table correspond to one subject. However, the plot is capturing
-             data for all sessions.
-         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
-         """"""
+        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+             data for all sessions.
+         Hence a dynamic update routine is needed to recompute the plot as new sessions
+            become available.
+        """"""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        """"""Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
+            become available.","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347288,303.0,306,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring formatting has been changed. The new format is more readable and consistent with PEP 8 guidelines.
```diff
-        """"""
-        Each entry in this table correspond to one subject. However, the plot is capturing
-             data for all sessions.
-         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
-         """"""
+        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+             data for all sessions.
+         Hence a dynamic update routine is needed to recompute the plot as new sessions
+            become available.
+        """"""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        """"""Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
+            become available.","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347289,350.0,353,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring formatting has been changed. The new format is more readable and consistent with PEP 8 guidelines.
```diff
-        """"""
-        Each entry in this table correspond to one subject. However, the plot is capturing
-             data for all sessions.
-         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
-         """"""
+        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+             data for all sessions.
+         Hence a dynamic update routine is needed to recompute the plot as new sessions
+            become available.
+        """"""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        """"""Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
+            become available.","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1314347290,,386,0c63c9a96dac25a808dca480f4e357eeb010bfff,bdb3dcd2a4f3a2cd9e79aa1579236d1facbb1ea5,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The path creation has been simplified. This change makes the code more readable.
```diff
-    output_dir = (
-        store_stage
-        / experiment_name
-        / subject
-        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
-    )
+    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-        / subject
-        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
-    )
+    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")","--- 

+++ 

@@ -1,15 +1,18 @@

+import os
+import datajoint as dj
+import pandas as pd
+import numpy as np
+import pathlib
+import matplotlib.pyplot as plt
+import re
+import datetime
 import json
-import os
-import pathlib
-
-import datajoint as dj
-import matplotlib.pyplot as plt
-import numpy as np
-import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
 
-from . import acquisition, analysis, get_schema_name
+from . import acquisition, analysis
+from . import get_schema_name
+
 
 schema = dj.schema(get_schema_name(""report""))
 os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""
@@ -29,7 +32,9 @@

     summary_plot_png: attach
     """"""
 
-    key_source = analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    key_source = (
+        analysis.InArena & analysis.InArenaTimeDistribution & analysis.InArenaSummary
+    )
 
     color_code = {
         ""Patch1"": ""b"",
@@ -40,16 +45,18 @@

     }
 
     def make(self, key):
-        acquisition.Experiment.get_data_directory(key)
-
-        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
-            ""in_arena_start"", ""in_arena_end""
-        )
+        raw_data_dir = acquisition.Experiment.get_data_directory(key)
+
+        in_arena_start, in_arena_end = (
+            analysis.InArena * analysis.InArenaEnd & key
+        ).fetch1(""in_arena_start"", ""in_arena_end"")
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
 
-        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
+        position_minutes_elapsed = (
+            position.index - in_arena_start
+        ).total_seconds() / 60
 
         # figure
         fig = plt.figure(figsize=(16, 8))
@@ -63,12 +70,16 @@

 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
+        analysis_plotting.heatmap(
+            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
+        )
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)
+            * acquisition.ExperimentFoodPatch.join(
+                acquisition.ExperimentFoodPatch.RemovalTime, left=True
+            )
             & key
             & ""in_arena_start >= food_patch_install_time""
             & 'in_arena_start < IFNULL(food_patch_remove_time, ""2200-01-01"")'
@@ -115,17 +126,20 @@

             )
 
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (
+            analysis.InArenaTimeDistribution & key
+        ).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
-            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
-        )
+        nest_keys, in_nests, nests_times = (
+            analysis.InArenaTimeDistribution.Nest & key
+        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
         patch_names, in_patches, patches_times = (
-            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
+            & key
         ).fetch(""food_patch_description"", ""in_patch"", ""time_fraction_in_patch"")
 
         ethogram_ax.plot(
@@ -135,7 +149,7 @@

             color=self.color_code[""arena""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in arena"",
+            label=f""Times in arena"",
         )
         ethogram_ax.plot(
             position_minutes_elapsed[in_corridor],
@@ -144,7 +158,7 @@

             color=self.color_code[""corridor""],
             markersize=0.5,
             alpha=0.6,
-            label=""Times in corridor"",
+            label=f""Times in corridor"",
         )
         for in_nest in in_nests:
             ethogram_ax.plot(
@@ -154,9 +168,11 @@

                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=""Times in nest"",
-            )
-        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
+                label=f""Times in nest"",
+            )
+        for patch_idx, (patch_name, in_patch) in enumerate(
+            zip(patch_names, in_patches)
+        ):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
@@ -215,7 +231,9 @@

 
         # ---- Save fig and insert ----
         save_dir = _make_path(key)
-        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+        fig_dict = _save_figs(
+            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
+        )
 
         self.insert1({**key, **fig_dict})
 
@@ -251,10 +269,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However, the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However, the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -300,10 +319,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -347,10 +367,11 @@

 
     @classmethod
     def delete_outdated_entries(cls):
-        """"""Each entry in this table correspond to one subject. However the plot is capturing
+        """"""
+        Each entry in this table correspond to one subject. However the plot is capturing
             data for all sessions.
         Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available.
+            become available
         """"""
         outdated_entries = (
             cls
@@ -383,7 +404,12 @@

     experiment_name, subject, in_arena_start = (analysis.InArena & in_arena_key).fetch1(
         ""experiment_name"", ""subject"", ""in_arena_start""
     )
-    output_dir = store_stage / experiment_name / subject / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    output_dir = (
+        store_stage
+        / experiment_name
+        / subject
+        / in_arena_start.strftime(""%y%m%d_%H%M%S_%f"")
+    )
     output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755588361,189.0,203,00ead975087bc35a1c2f58e479cd535456e257fd,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `read` method lacks error handling. If the file does not exist or is unreadable, the `super().read(file)` call will raise an exception. It would be better to handle this gracefully and provide a meaningful error message to the user.

```diff
        data = super().read(file)
+        try:
+            data = super().read(file)
+        except FileNotFoundError:
+            raise FileNotFoundError(f""The file {file} does not exist."")
+        except PermissionError:
+            raise PermissionError(f""Permission denied when trying to read the file {file}."")
```

Also, it's important to validate the `downsample` parameter. If it's not a boolean, the condition `if downsample is True:` could lead to unexpected behavior.

```diff
        if downsample is True:
+        if not isinstance(downsample, bool):
+            raise ValueError(""`downsample` should be a boolean value."")
+        elif downsample is True:
```

Lastly, there's no check for whether the resampling operation was successful or not. If the resampling fails due to some reason (like invalid data), it might lead to unexpected results. You should add error handling for this as well.

```diff
                data = data.resample('20ms', origin=chunk_origin).first()
+                try:
+                    data = data.resample('20ms', origin=chunk_origin).first()
+                except Exception as e:
+                    raise RuntimeError(f""An error occurred during resampling: {str(e)}"")
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def read(self, file, downsample=True):
+        """"""Reads encoder data from the specified Harp binary file, and optionally downsamples
+        the frequency to 50Hz.
+        """"""
+        data = super().read(file)
+        if downsample is True:
+            # resample requires a DatetimeIndex so we convert early
+            data.index = aeon_time(data.index)
+
+            first_index = data.first_valid_index()
+            if first_index is not None:
+                # since data is absolute angular position we decimate by taking first of each bin
+                chunk_origin = chunk(first_index)
+                data = data.resample('20ms', origin=chunk_origin).first()
+        return data",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755686425,16.0,18,dfd2d02ecf7f54ad82155bc7eeca43475b47c566,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_start_only` has been modified to include the `downsample` parameter in the `aeon.load` function call. However, there is no assertion or check to verify if the downsampling is working as expected when it's set to `None`. It would be beneficial to add a test case that checks the functionality of the downsampling feature.
```diff
    data = aeon.load(
        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49""), downsample=None
    )
+    # Add an assertion to check the frequency of the data
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    data = aeon.load(
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49""), downsample=None
+    )","--- 

+++ 

@@ -29,9 +29,7 @@

 
 @mark.api
 def test_load_filter_nonchunked():
-    data = aeon.load(
-        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
-    )
+    data = aeon.load(nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""))
     assert len(data) > 0
 
 
@@ -47,5 +45,27 @@

     assert not data.index.is_monotonic_increasing
 
 
+@mark.api
+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
+
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755686429,24.0,26,dfd2d02ecf7f54ad82155bc7eeca43475b47c566,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the `downsample` parameter has been added to the `aeon.load` function call in the `test_load_end_only` test, but there is no assertion to verify the functionality of the downsampling feature when it's set to `None`.
```diff
    data = aeon.load(
        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49""), downsample=None
    )
+    # Add an assertion to check the frequency of the data
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    data = aeon.load(
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49""), downsample=None
+    )","--- 

+++ 

@@ -29,9 +29,7 @@

 
 @mark.api
 def test_load_filter_nonchunked():
-    data = aeon.load(
-        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
-    )
+    data = aeon.load(nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""))
     assert len(data) > 0
 
 
@@ -47,5 +45,27 @@

     assert not data.index.is_monotonic_increasing
 
 
+@mark.api
+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
+
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755686431,32.0,34,dfd2d02ecf7f54ad82155bc7eeca43475b47c566,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `test_load_filter_nonchunked` test, the `downsample` parameter has been added to the `aeon.load` function call. Similar to the previous comments, an assertion should be added to verify the functionality of the downsampling feature when it's set to `None`.
```diff
    data = aeon.load(
        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
    )
+    # Add an assertion to check the frequency of the data
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    data = aeon.load(
+        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
+    )","--- 

+++ 

@@ -29,9 +29,7 @@

 
 @mark.api
 def test_load_filter_nonchunked():
-    data = aeon.load(
-        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
-    )
+    data = aeon.load(nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""))
     assert len(data) > 0
 
 
@@ -47,5 +45,27 @@

     assert not data.index.is_monotonic_increasing
 
 
+@mark.api
+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
+
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755686435,40.0,41,dfd2d02ecf7f54ad82155bc7eeca43475b47c566,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `test_load_monotonic` test has been modified to include the `downsample` parameter in the `aeon.load` function call. However, there is no assertion or check to verify if the downsampling is working as expected when it's set to `None`. It would be beneficial to add a test case that checks the functionality of the downsampling feature.
```diff
    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
     assert len(data) > 0 and data.index.is_monotonic_increasing
+    # Add an assertion to check the frequency of the data
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
     assert len(data) > 0 and data.index.is_monotonic_increasing","--- 

+++ 

@@ -29,9 +29,7 @@

 
 @mark.api
 def test_load_filter_nonchunked():
-    data = aeon.load(
-        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
-    )
+    data = aeon.load(nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""))
     assert len(data) > 0
 
 
@@ -47,5 +45,27 @@

     assert not data.index.is_monotonic_increasing
 
 
+@mark.api
+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
+
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1755686438,45.0,46,dfd2d02ecf7f54ad82155bc7eeca43475b47c566,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `test_load_nonmonotonic` test, the `downsample` parameter has been added to the `aeon.load` function call. Similar to the previous comments, an assertion should be added to verify the functionality of the downsampling feature when it's set to `None`.
```diff
    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
    assert not data.index.is_monotonic_increasing
+    # Add an assertion to check the frequency of the data
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)","--- 

+++ 

@@ -29,9 +29,7 @@

 
 @mark.api
 def test_load_filter_nonchunked():
-    data = aeon.load(
-        nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""), downsample=None
-    )
+    data = aeon.load(nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00""))
     assert len(data) > 0
 
 
@@ -47,5 +45,27 @@

     assert not data.index.is_monotonic_increasing
 
 
+@mark.api
+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
+
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764179133,49.0,53,9ad3dc066bd9522979c60b554652719486316c57,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_with_downsampling` is a good addition to validate the downsampling functionality. However, it would be more robust if we also check that the data length is reduced when downsampling is applied. This can be done by comparing the length of the downsampled data with the raw data.
```diff
    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+   raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+   assert len(data) < len(raw_data)
    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02","--- 

+++ 

@@ -48,13 +48,23 @@

 @mark.api
 def test_load_encoder_with_downsampling():
     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
     assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
 
-
-@mark.api
-def test_load_encoder_raw_frequency():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    assert data.index.to_series().diff().dt.total_seconds().min() < 0.01
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764179136,54.0,57,9ad3dc066bd9522979c60b554652719486316c57,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_raw_frequency` is a good addition to validate the behavior when downsampling is not applied. However, the assertion seems to assume that the raw data frequency is always less than 0.01 seconds. It would be better to remove this hard-coded value and instead compare the frequency of the raw data with the downsampled data.
```diff
    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+   downsampled_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+   assert data.index.to_series().diff().dt.total_seconds().min() < downsampled_data.index.to_series().diff().dt.total_seconds().min()
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+@mark.api
+def test_load_encoder_raw_frequency():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    assert data.index.to_series().diff().dt.total_seconds().min() < 0.01","--- 

+++ 

@@ -48,13 +48,23 @@

 @mark.api
 def test_load_encoder_with_downsampling():
     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
     assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
 
-
-@mark.api
-def test_load_encoder_raw_frequency():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    assert data.index.to_series().diff().dt.total_seconds().min() < 0.01
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764204980,49.0,53,3d88792edc72000d5a80b9e9b7a407f21fa8098f,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_with_downsampling` is a good addition to validate the downsampling functionality. However, it would be more robust if we also check that the downsampled data maintains the same start and end timestamps as the raw data. This will ensure that no data points are lost during the downsampling process.
```diff
    assert len(data) < len(raw_data)
+   assert data.index[0] == raw_data.index[0]
+   assert data.index[-1] == raw_data.index[-1]
    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    assert len(data) < len(raw_data)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02","--- 

+++ 

@@ -49,8 +49,22 @@

 def test_load_encoder_with_downsampling():
     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
     raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
     assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
     assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764225031,49.0,64,bd0d39ef483fd81051f5a45820cd1bd24b87a258,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_with_downsampling` is well written and covers the necessary assertions to validate the downsampling functionality. However, it would be beneficial to add a comment explaining what each assertion is checking for future reference and maintainability.
```diff
    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+   # Check that the length of the downsampled data is less than the raw data
    assert len(data) < len(raw_data)
+   # Check that the first timestamp of the downsampled data is within 20ms of the raw data
    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+   # Check that the last timestamp of the downsampled data is within 20ms of the raw data
    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+   # Check that the minimum difference between consecutive timestamps in the downsampled data is at least 20ms (50Hz)
    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    assert len(data) < len(raw_data)
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02","--- 

+++ 

@@ -49,10 +49,22 @@

 def test_load_encoder_with_downsampling():
     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
     raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
     assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
     assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
     assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
     assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764227647,49.0,64,a45b16a61d144681b5058c3f4d98e5ef2670c616,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_with_downsampling` is well written and covers the necessary checks for the downsampling feature. However, it might be beneficial to add a check that verifies if the downsampled data maintains the same order as the raw data. This can be done by comparing the sorted values of both datasets.
```diff
    # Check that the minimum difference between consecutive timestamps in the downsampled data
    # is at least 20ms (50Hz)
    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+   # Check that the downsampled data maintains the same order as the raw data
+   assert (data.sort_values() == raw_data.sort_values()).all()
```
This additional assertion will help ensure that the downsampling process does not alter the order of the data, which could potentially lead to incorrect results during analysis.


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02","--- 

+++ 

@@ -63,6 +63,9 @@

     # is at least 20ms (50Hz)
     assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
 
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing
+
 
 if __name__ == ""__main__"":
     pytest.main()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764232939,49.0,67,222f502bd9119745abfadcd6c5f73091b08a9e97,871f3422fe4bb61b2909ef4645b8b7d4ca87c679,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_load_encoder_with_downsampling` is well written and covers the necessary checks for the downsampling feature. However, it might be beneficial to add a check that verifies if the downsampled data maintains the same order as the raw data. This can be done by comparing the sorted values of both datasets.
```diff
    # Check that the minimum difference between consecutive timestamps in the downsampled data
    # is at least 20ms (50Hz)
    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+   # Check that the downsampled data maintains the same order as the raw data
+   assert (data.sort_values() == raw_data.sort_values()).all()
```
This additional assertion will help ensure that the downsampling process does not alter the order of the data, which could potentially lead to incorrect results during analysis.


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+def test_load_encoder_with_downsampling():
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+
+    # Check that the length of the downsampled data is less than the raw data
+    assert len(data) < len(raw_data)
+
+    # Check that the first timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+
+    # Check that the last timestamp of the downsampled data is within 20ms of the raw data
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+
+    # Check that the minimum difference between consecutive timestamps in the downsampled data
+    # is at least 20ms (50Hz)
+    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+
+    # Check that the timestamps in the downsampled data are strictly increasing
+    assert data.index.is_monotonic_increasing",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878249762,338.0,359,95095de04514f36f341a2879f4267d91eae6455e,e93103c36c9a44258b90468ad59f62d80d6c5d70,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The CSS rule for `.facilities-list li::before` uses a relative path to the `tick.svg` image. This could potentially cause issues if the CSS file is moved or if the website structure changes. Consider using an absolute path or a path that starts from the root of your project.
```diff
-    background-image: url('../img/tick.svg');
+    background-image: url('/path/to/img/tick.svg');
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+.facilities-list li::marker{display:none;}
+.facilities-list{padding-left:0!important;}
+
+.facilities-list li {
+  margin-left: 10px;
+  list-style: none; /* Remove default list marker */
+  position: relative; /* Position for pseudo-element */}
+
+.facilities-list li::before{
+    content: '';
+    position: absolute;
+    left: 0;
+    top: 50%;
+    transform: translateY(-50%); /* Center the icon vertically */
+    width: 20px; /* Adjust the size of the tick icon */
+    height: 20px;
+    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
+    background-size: contain;
+    background-repeat: no-repeat;
+    left:-28px
+}","--- 

+++ 

@@ -157,18 +157,146 @@

   margin-right: 0px;
 }
 
+/* Additional information specific styles */
+/* Fast Facts Wrapper */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child {
+  display: flex;
+  align-items: flex-start; /* Align content in this group to the top */
+  gap: 5px; /* Space between icon and text */
+  flex-shrink: 0; /* Prevent group from shrinking */
+}
+
+/* Style the image icon */
+.fast-facts-wrapper > .wp-block-group .wp-block-image img {
+  width: 20px; /* Fixed width */
+  height: 20px; /* Fixed height */
+  display: inline-block;
+  -o-object-fit: contain;
+     object-fit: contain; /* Ensure image doesn't stretch or distort */
+}
+
+/* Styling for the label (e.g., ""Special Interests:"") */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child p {
+  margin: 0;
+  white-space: nowrap; /* Prevent wrapping for label */
+  vertical-align: middle; /* Align text with the middle of the icon */
+  line-height: 1.2; /* Improve readability */
+  display: inline-block; /* Keep text inline */
+}
+
+/* Styling for the flowing text */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child {
+  display: flex;
+  flex-wrap: nowrap; /* Keep the text flowing inline */
+  white-space: normal; /* Allow wrapping for text if it overflows */
+  overflow-wrap: break-word; /* Ensure long words wrap correctly */
+  word-break: break-word; /* Break long words when needed */
+  flex: 1; /* Allow the container to use available space */
+  align-items: flex-start; /* Ensure text starts at the top */
+}
+
+/* Specific styling for the text content */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child p {
+  white-space: normal; /* Allow wrapping for long text */
+  overflow-wrap: break-word; /* Break words when needed */
+  word-break: break-word; /* Break words on legacy browsers */
+  line-height: 1.4; /* Improve readability */
+  text-transform: capitalize; /* Capitalize text */
+  padding: 0; /* Ensure no unnecessary padding */
+  margin: 0; /* Remove extra margins */
+  display: inline; /* Ensure the text flows inline with the label */
+}
+
+/* Facilities List */
+.facilities-list li::marker {
+  display: none;
+}
+
+.facilities-list {
+  padding-left: 0 !important;
+}
+
+.facilities-list li {
+  margin-left: 10px;
+  list-style: none; /* Remove default list marker */
+  position: relative; /* Position for pseudo-element */
+}
+
+.facilities-list li::before {
+  content: """";
+  position: absolute;
+  left: 0;
+  top: 50%;
+  transform: translateY(-50%); /* Center the icon vertically */
+  width: 20px; /* Adjust the size of the tick icon */
+  height: 20px;
+  background-image: url(""../../img/tick.svg""); /* Replace with the path to your tick.svg */
+  background-size: contain;
+  background-repeat: no-repeat;
+  left: -28px;
+}
+
+/* Additional Info */
+.additional-info .wp-block-group.content .read-more-btn,
+.additional-info .wp-block-group.content .read-less-btn {
+  color: #0073aa;
+  cursor: pointer;
+  font-weight: bold;
+}
+
+.additional-info .wp-block-group.content .read-more-btn:hover {
+  text-decoration: underline;
+}
+
+.additional-info .wp-block-group.content .read-less-btn {
+  display: none;
+}
+
+.wp-block-query.lsx-to-slider {
+  gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);
+  position: relative;
+}
+.wp-block-query.lsx-to-slider .slick-arrow {
+  cursor: pointer;
+  border-color: #000 !important;
+  height: 30px;
+  width: 30px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow:before {
+  border-color: #000 !important;
+  height: 30px;
+  width: 30px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow.slick-prev:before {
+  background: url(""../img/left-arrow-dark.svg"") no-repeat center center;
+  width: 20px;
+  left: 3px;
+  height: 20px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow.slick-next:before {
+  background: url(""../img/right-arrow-dark.svg"") no-repeat center center;
+  width: 20px;
+  left: 3px;
+  height: 20px;
+}
+.wp-block-query.lsx-to-slider .slick-slide {
+  padding: 15px !important;
+}
+
 .slick-lightbox-close {
   right: 52px;
   top: 35px;
 }
-.slick-lightbox-close:before{font-size:40px;}
+.slick-lightbox-close:before {
+  font-size: 40px;
+}
 
 .lsx-to-slider .slick-arrow,
 .slick-lightbox-inner .slick-arrow {
   background: transparent;
   border-radius: 50%;
-  position:absolute;
-  top:50%;
+  position: absolute;
+  top: 50%;
   border-style: solid;
   border-color: #fff;
   border-width: 2px;
@@ -183,8 +311,6 @@

   z-index: 3;
   color: white;
 }
-.slick-prev svg, .slick-next svg{color: white;}
-
 .lsx-to-slider .slick-arrow:before,
 .slick-lightbox-inner .slick-arrow:before {
   display: block;
@@ -200,27 +326,31 @@

   -webkit-font-smoothing: antialiased;
   -moz-osx-font-smoothing: grayscale;
 }
+
+.slick-prev svg,
+.slick-next svg {
+  color: white;
+}
+
 .lsx-to-slider .slick-arrow.slick-disabled,
 .slick-lightbox-inner .slick-arrow.slick-disabled {
   cursor: not-allowed;
 }
 .lsx-to-slider .slick-arrow.slick-prev:before,
 .slick-lightbox-inner .slick-arrow.slick-prev:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/left-arrow-new.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 10px;
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 10px;
 }
 .lsx-to-slider .slick-arrow.slick-next:before,
 .slick-lightbox-inner .slick-arrow.slick-next:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/right-arrow.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 12px;
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 12px;
 }
 
 .lsx-to-slider .slick-dots:not(.dropdown-menu) {
@@ -252,10 +382,12 @@

 .lsx-to-slider .slick-arrow.slick-next {
   right: -4rem;
 }
-
 .lsx-to-slider.slider-disabled {
   overflow: hidden;
 }
+.lsx-to-slider .slick-slide .lazy-hidden {
+  max-height: 154px;
+}
 
 .slick-lightbox-inner .slick-arrow {
   opacity: 0.75;
@@ -268,7 +400,6 @@

   display: inline-block;
 }
 
-.lsx-to-slider .slick-slide .lazy-hidden,
 .lsx-post-carousel-items .slick-slide .lazy-hidden {
   max-height: 154px;
 }"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878249769,374.0,380,95095de04514f36f341a2879f4267d91eae6455e,e93103c36c9a44258b90468ad59f62d80d6c5d70,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The debug `console.log` statements should be removed from the production code. They are useful during development but can expose potentially sensitive information in a production environment.
```diff
- 			console.log('Processing Section:', section); // Debug
- 			console.log('Found Heading:', heading); // Debug
- 			console.log('Toggle Target:', toggleTarget); // Debug
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	
+		sections.forEach(section => {
+			// Locate the first <h2> within the section
+			const heading = section.querySelector('h2');
+			// Locate the second div with the class wp-block-group
+			const toggleTarget = section.querySelectorAll('.wp-block-group')[1];
+	","--- 

+++ 

@@ -198,106 +198,50 @@

 	 */
 	lsx_to.build_slider = function( window_width ) {
 		$( '.lsx-to-slider .wp-block-post-template:not(.slider-disabled)' ).each( function() {
-			var $this = $( this ),
-				interval = $this.data( 'interval' ),
-				currentSettings = $this.data( 'slick' ),
-				autoplay = false,
-				autoplay_speed = 0;
+			var $this = $( this );
 
 			lsx_to.pre_build_slider( $this );
 
-			if ( 'undefined' !== typeof interval && 'boolean' !== typeof interval ) {
-				interval = parseInt( interval );
-
-				if ( ! isNaN( interval ) ) {
-					autoplay = true;
-					autoplay_speed = interval;
-				}
-			}
-
-
-			let tabletSlidesToShow   = lsx_to_params.slickSlider.tablet.slidesToShow;
-			let tabletSlidesToScroll = lsx_to_params.slickSlider.tablet.slidesToScroll;
-
-			if ( 'undefined' !== typeof currentSettings && 'boolean' !== typeof currentSettings ) {
-
-				// Tablet Settings.
-				if ( 'undefined' !== typeof currentSettings.tablet ) {
-					if ( 'undefined' !== typeof currentSettings.tablet.slidesToShow ) {
-						tabletSlidesToShow = currentSettings.tablet.slidesToShow;
-					}
-					if ( 'undefined' !== typeof currentSettings.tablet.slidesToShow ) {
-						tabletSlidesToScroll = currentSettings.tablet.slidesToScroll;
-					}
-				}
-			}
-
 			if ( 1 < $this.children.length ) {
-				console.log($this);
 
 				$this.slick( {
-					draggable: lsx_to_params.slickSlider.desktop.draggable,
-					infinite: lsx_to_params.slickSlider.desktop.infinite,
-					swipe: lsx_to_params.slickSlider.desktop.swipe,
-					cssEase: lsx_to_params.slickSlider.desktop.cssEase,
-					dots: lsx_to_params.slickSlider.desktop.dots,
-					slidesToShow: lsx_to_params.slickSlider.desktop.slidesToShow,
-					slidesToScroll: lsx_to_params.slickSlider.desktop.slidesToScroll,
-					autoplay: autoplay,
-					autoplaySpeed: autoplay_speed,
-					appendArrows: $this.parent(),
-					appendDots: $this.parent(),
+					draggable: false,
+					infinite: true,
+					swipe: false,
+					dots: false,
+					slidesToShow: 3,
+					slidesToScroll: 1,
+					autoplay: false,
+					autoplaySpeed: 0,
+					//appendArrows: $this.parent(),
+					//appendDots: $this.parent(),
 					responsive: [
 						{
-							breakpoint: lsx_to_params.slickSlider.tablet.breakpoint,
+							breakpoint: 1028,
 							settings: {
-								slidesToShow:   tabletSlidesToShow,
-								slidesToScroll: tabletSlidesToScroll,
-								draggable: lsx_to_params.slickSlider.tablet.draggable,
-								arrows: lsx_to_params.slickSlider.tablet.arrows,
-								swipe: lsx_to_params.slickSlider.tablet.swipe,
+								slidesToShow:   2,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						},
 						{
-							breakpoint: lsx_to_params.slickSlider.mobile.breakpoint,
+							breakpoint: 782,
 							settings: {
-								slidesToShow:   lsx_to_params.slickSlider.mobile.slidesToShow,
-								slidesToScroll: lsx_to_params.slickSlider.mobile.slidesToScroll,
-								draggable:      lsx_to_params.slickSlider.mobile.draggable,
-								arrows:         lsx_to_params.slickSlider.mobile.arrows,
-								swipe:          lsx_to_params.slickSlider.mobile.swipe
+								slidesToShow:   1,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						}
 					]
 				} );
 			}
 		} );
-
-		if ( window_width < 768 ) {
-			$( '.gallery' ).not('.slick-initialized').slick( {
-				slide: 'dl',
-				arrows: false,
-				draggable: true,
-				infinite: true,
-				swipe: true,
-				cssEase: 'ease-out',
-				dots: true,
-				autoplay: false,
-				responsive: [
-					{
-						breakpoint: 99999,
-						settings: 'unslick'
-					},
-					{
-						breakpoint: 768,
-						setting: {
-							slidesToShow: 1,
-							slidesToScroll: 1
-						}
-					}
-				]
-			} );
-		}
 	};
 
 	/**
@@ -369,9 +313,50 @@

 	} );
 
 	document.addEventListener('DOMContentLoaded', function () {
+		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
+	
+		paragraphs.forEach(function (p) {
+			const text = p.innerText.trim();
+	
+			if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
+				const fullText = p.innerText.trim();
+				const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
+				p.innerHTML = `<span class=""truncated-text"">${truncatedText}</span><span class=""full-text"" style=""display: none;"">${fullText}</span>`;
+	
+				// Create Read More button
+				const readMoreBtn = document.createElement('span');
+				readMoreBtn.textContent = ' Read More';
+				readMoreBtn.classList.add('read-more-btn');
+				p.appendChild(readMoreBtn);
+	
+				// Create Read Less button
+				const readLessBtn = document.createElement('span');
+				readLessBtn.textContent = ' Read Less';
+				readLessBtn.classList.add('read-less-btn');
+				p.appendChild(readLessBtn);
+	
+				// Add functionality to toggle text
+				readMoreBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'none';
+					p.querySelector('.full-text').style.display = 'inline';
+					readMoreBtn.style.display = 'none';
+					readLessBtn.style.display = 'inline';
+				});
+	
+				readLessBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'inline';
+					p.querySelector('.full-text').style.display = 'none';
+					readMoreBtn.style.display = 'inline';
+					readLessBtn.style.display = 'none';
+				});
+			}
+		});
+	});	
+
+	document.addEventListener('DOMContentLoaded', function () {
 		// Select all sections within `.single-tour-operator`
 		const sections = document.querySelectorAll('.single-tour-operator section.wp-block-group');
-	
+
 		sections.forEach(section => {
 			// Locate the first <h2> within the section
 			const heading = section.querySelector('h2');
@@ -415,5 +400,5 @@

 			}
 		});
 	});
-	
+
 } )( jQuery, window, document );"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878249776,396.0,413,95095de04514f36f341a2879f4267d91eae6455e,e93103c36c9a44258b90468ad59f62d80d6c5d70,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code is responsible for toggling the visibility of sections and changing the icon accordingly. However, it directly manipulates the style of the elements which can lead to issues with CSS specificity and maintainability. It would be better to use CSS classes to control the display of the icons.
```diff
- 					if (toggleTarget.classList.contains('collapsed')) {
- 						iconDown.style.display = 'none';
- 						iconUp.style.display = 'inline';
- 					} else {
- 						iconDown.style.display = 'inline';
- 						iconUp.style.display = 'none';
- 					}
+ 					if (toggleTarget.classList.contains('collapsed')) {
+ 						iconDown.classList.add('hidden');
+ 						iconUp.classList.remove('hidden');
+ 					} else {
+ 						iconDown.classList.remove('hidden');
+ 						iconUp.classList.add('hidden');
+ 					}
```
In your CSS, you can then define these classes as follows:
```css
.hidden {
    display: none;
}

:not(.hidden) {
    display: inline;
}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				// Insert the button after the heading
+				heading.insertAdjacentElement('afterend', toggleButton);
+	
+				// Add click event listener to toggle visibility of the second wp-block-group
+				toggleButton.addEventListener('click', function () {
+					toggleTarget.classList.toggle('collapsed'); // Add or remove the collapsed class
+	
+					// Toggle the display of the up/down icons
+					const iconDown = toggleButton.querySelector('.icon-down');
+					const iconUp = toggleButton.querySelector('.icon-up');
+	
+					if (toggleTarget.classList.contains('collapsed')) {
+						iconDown.style.display = 'none';
+						iconUp.style.display = 'inline';
+					} else {
+						iconDown.style.display = 'inline';
+						iconUp.style.display = 'none';
+					}","--- 

+++ 

@@ -198,106 +198,50 @@

 	 */
 	lsx_to.build_slider = function( window_width ) {
 		$( '.lsx-to-slider .wp-block-post-template:not(.slider-disabled)' ).each( function() {
-			var $this = $( this ),
-				interval = $this.data( 'interval' ),
-				currentSettings = $this.data( 'slick' ),
-				autoplay = false,
-				autoplay_speed = 0;
+			var $this = $( this );
 
 			lsx_to.pre_build_slider( $this );
 
-			if ( 'undefined' !== typeof interval && 'boolean' !== typeof interval ) {
-				interval = parseInt( interval );
-
-				if ( ! isNaN( interval ) ) {
-					autoplay = true;
-					autoplay_speed = interval;
-				}
-			}
-
-
-			let tabletSlidesToShow   = lsx_to_params.slickSlider.tablet.slidesToShow;
-			let tabletSlidesToScroll = lsx_to_params.slickSlider.tablet.slidesToScroll;
-
-			if ( 'undefined' !== typeof currentSettings && 'boolean' !== typeof currentSettings ) {
-
-				// Tablet Settings.
-				if ( 'undefined' !== typeof currentSettings.tablet ) {
-					if ( 'undefined' !== typeof currentSettings.tablet.slidesToShow ) {
-						tabletSlidesToShow = currentSettings.tablet.slidesToShow;
-					}
-					if ( 'undefined' !== typeof currentSettings.tablet.slidesToShow ) {
-						tabletSlidesToScroll = currentSettings.tablet.slidesToScroll;
-					}
-				}
-			}
-
 			if ( 1 < $this.children.length ) {
-				console.log($this);
 
 				$this.slick( {
-					draggable: lsx_to_params.slickSlider.desktop.draggable,
-					infinite: lsx_to_params.slickSlider.desktop.infinite,
-					swipe: lsx_to_params.slickSlider.desktop.swipe,
-					cssEase: lsx_to_params.slickSlider.desktop.cssEase,
-					dots: lsx_to_params.slickSlider.desktop.dots,
-					slidesToShow: lsx_to_params.slickSlider.desktop.slidesToShow,
-					slidesToScroll: lsx_to_params.slickSlider.desktop.slidesToScroll,
-					autoplay: autoplay,
-					autoplaySpeed: autoplay_speed,
-					appendArrows: $this.parent(),
-					appendDots: $this.parent(),
+					draggable: false,
+					infinite: true,
+					swipe: false,
+					dots: false,
+					slidesToShow: 3,
+					slidesToScroll: 1,
+					autoplay: false,
+					autoplaySpeed: 0,
+					//appendArrows: $this.parent(),
+					//appendDots: $this.parent(),
 					responsive: [
 						{
-							breakpoint: lsx_to_params.slickSlider.tablet.breakpoint,
+							breakpoint: 1028,
 							settings: {
-								slidesToShow:   tabletSlidesToShow,
-								slidesToScroll: tabletSlidesToScroll,
-								draggable: lsx_to_params.slickSlider.tablet.draggable,
-								arrows: lsx_to_params.slickSlider.tablet.arrows,
-								swipe: lsx_to_params.slickSlider.tablet.swipe,
+								slidesToShow:   2,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						},
 						{
-							breakpoint: lsx_to_params.slickSlider.mobile.breakpoint,
+							breakpoint: 782,
 							settings: {
-								slidesToShow:   lsx_to_params.slickSlider.mobile.slidesToShow,
-								slidesToScroll: lsx_to_params.slickSlider.mobile.slidesToScroll,
-								draggable:      lsx_to_params.slickSlider.mobile.draggable,
-								arrows:         lsx_to_params.slickSlider.mobile.arrows,
-								swipe:          lsx_to_params.slickSlider.mobile.swipe
+								slidesToShow:   1,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						}
 					]
 				} );
 			}
 		} );
-
-		if ( window_width < 768 ) {
-			$( '.gallery' ).not('.slick-initialized').slick( {
-				slide: 'dl',
-				arrows: false,
-				draggable: true,
-				infinite: true,
-				swipe: true,
-				cssEase: 'ease-out',
-				dots: true,
-				autoplay: false,
-				responsive: [
-					{
-						breakpoint: 99999,
-						settings: 'unslick'
-					},
-					{
-						breakpoint: 768,
-						setting: {
-							slidesToShow: 1,
-							slidesToScroll: 1
-						}
-					}
-				]
-			} );
-		}
 	};
 
 	/**
@@ -369,9 +313,50 @@

 	} );
 
 	document.addEventListener('DOMContentLoaded', function () {
+		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
+	
+		paragraphs.forEach(function (p) {
+			const text = p.innerText.trim();
+	
+			if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
+				const fullText = p.innerText.trim();
+				const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
+				p.innerHTML = `<span class=""truncated-text"">${truncatedText}</span><span class=""full-text"" style=""display: none;"">${fullText}</span>`;
+	
+				// Create Read More button
+				const readMoreBtn = document.createElement('span');
+				readMoreBtn.textContent = ' Read More';
+				readMoreBtn.classList.add('read-more-btn');
+				p.appendChild(readMoreBtn);
+	
+				// Create Read Less button
+				const readLessBtn = document.createElement('span');
+				readLessBtn.textContent = ' Read Less';
+				readLessBtn.classList.add('read-less-btn');
+				p.appendChild(readLessBtn);
+	
+				// Add functionality to toggle text
+				readMoreBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'none';
+					p.querySelector('.full-text').style.display = 'inline';
+					readMoreBtn.style.display = 'none';
+					readLessBtn.style.display = 'inline';
+				});
+	
+				readLessBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'inline';
+					p.querySelector('.full-text').style.display = 'none';
+					readMoreBtn.style.display = 'inline';
+					readLessBtn.style.display = 'none';
+				});
+			}
+		});
+	});	
+
+	document.addEventListener('DOMContentLoaded', function () {
 		// Select all sections within `.single-tour-operator`
 		const sections = document.querySelectorAll('.single-tour-operator section.wp-block-group');
-	
+
 		sections.forEach(section => {
 			// Locate the first <h2> within the section
 			const heading = section.querySelector('h2');
@@ -415,5 +400,5 @@

 			}
 		});
 	});
-	
+
 } )( jQuery, window, document );"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266494,1.0,8,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/social_03.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there's a potential issue with the file path construction in line 8. If `path` doesn't end with a slash (`/`), this could lead to incorrect paths. Consider using `os.path.join()` for constructing file paths as it handles these cases correctly and is more portable.
```diff
-        super().__init__(_reader.Pose(f""{path}_202_*""))
+        import os
+        super().__init__(_reader.Pose(os.path.join(path, ""_202_*"")))
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import aeon.io.reader as _reader
+from aeon.schema.streams import Stream
+
+
+class Pose(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Pose(f""{path}_202_*""))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266498,15.0,18,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/social_01.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new `Pose` class is initialized with `_reader.Pose(f""{path}_node-0*"")`. However, there's no error handling in case the path does not exist or is invalid. It would be better to add some error handling here to improve robustness.
```diff
  def __init__(self, path):
+     if not os.path.exists(path):
+         raise FileNotFoundError(f""The specified path {path} does not exist."")
      super().__init__(_reader.Pose(f""{path}_node-0*""))
```
This will ensure that an informative error is raised when the path does not exist, rather than a potentially confusing error later on when trying to use the non-existent path.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class Pose(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Pose(f""{path}_node-0*""))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266504,,38,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comment added here is redundant as it repeats the information already provided in the previous comments. It's best to remove this line to keep the code clean and avoid unnecessary repetition.
```diff
-         # In the BlockState data - if the 0 is the first index - look back at the previous chunk
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         # find the 0s
         # that would mark the start of a new block
-        # if the 0 is the first index - look back at the previous chunk
+        # In the BlockState data - if the 0 is the first index - look back at the previous chunk",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266508,58.0,82,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for detecting block end times has been significantly altered. The new approach seems to handle double zeros and adjusts the block ends accordingly. However, there might be a potential issue with the handling of `previous_pellet_time`. In the old hunk, it was set to the second last index before `block_end` while in the new hunk, it's set to the last index before `block_end`. If this change is intentional and tested, then it's fine. Otherwise, it could lead to off-by-one errors.
```diff
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
+                    previous_pellet_time = block_state_df[:block_end].index[-2]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        # detecting block end times
+        # pellet count reset - find 0s in BlockState
 
-        block_ends = block_state_df[block_state_df.pellet_ct.diff() < 0]
+        block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
+        block_state_df = fetch_stream(block_state_query)
+        block_state_df.index = block_state_df.index.round(
+            ""us""
+        )  # timestamp precision in DJ is only at microseconds
+        block_state_df = block_state_df.loc[
+            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+        ]
+
+        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
+        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        # find the indices of the 2nd 0s and remove
+        double_0s = double_0s.shift(-1).fillna(False)
+        block_ends = block_ends[~double_0s]
 
         block_entries = []
         for idx, block_end in enumerate(block_ends.index):
             if idx == 0:
                 if previous_block_key:
                     # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-2]
+                    previous_pellet_time = block_state_df[:block_end].index[-1]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266509,,531,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The extra space added before the triple quotes in the definition string is unnecessary and should be removed to maintain proper formatting.
```diff
-     definition = """""" 
+     definition = """"""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," @schema
 class BlockPlots(dj.Computed):
-    definition = """"""
+    definition = """""" ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266514,177.0,194,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code has removed the search for a config file and directly retrieves body parts and class names. This could potentially lead to issues if the config file is required for correct operation. Please ensure that this change does not affect the functionality of the system.

```diff
-        # Find the config file for the SLEAP model
-        for data_dir in data_dirs:
-            try:
-                f = next(
-                    data_dir.glob(
-                        f""**/**/{stream_reader.pattern}{io_api.chunk(chunk_start).strftime('%Y-%m-%dT%H-%M-%S')}*.{stream_reader.extension}""
-                    )
-                )
-            except StopIteration:
-                continue
-            else:
-                config_file = stream_reader.get_config_file(
-                    stream_reader._model_root / Path(*Path(f.stem.replace(""_"", ""/"")).parent.parts[1:])
-                )
-                break
-        else:
-            raise FileNotFoundError(f""Unable to find SLEAP model config file for: {stream_reader.pattern}"")
-
-         # get bodyparts and classes
-        bodyparts = stream_reader.get_bodyparts(config_file)
-         anchor_part = bodyparts[0]  # anchor_part is always the first one
-        class_names = stream_reader.get_class_names(config_file)
+         # get bodyparts and classes
+        bodyparts = stream_reader.get_bodyparts()
+         anchor_part = bodyparts[0]  # anchor_part is always the first one
+        class_names = stream_reader.get_class_names()
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         # get bodyparts and classes
-        bodyparts = stream_reader.get_bodyparts(config_file)
+        bodyparts = stream_reader.get_bodyparts()
         anchor_part = bodyparts[0]  # anchor_part is always the first one
-        class_names = stream_reader.get_class_names(config_file)
+        class_names = stream_reader.get_class_names()
+        identity_mapping = {n: i for i, n in enumerate(class_names)}
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
-        for class_idx in set(pose_data[""class""].values.astype(int)):
-            class_position = pose_data[pose_data[""class""] == class_idx]
-            for part in set(class_position.part.values):
-                part_position = class_position[class_position.part == part]
+        for identity in identity_mapping:
+            identity_position = pose_data[pose_data[""identity""] == identity]
+            if identity_position.empty:
+                continue
+            for part in set(identity_position.part.values):
+                part_position = identity_position[identity_position.part == part]
                 part_entries.append(
                     {
                         **key,
-                        ""identity_idx"": class_idx,
+                        ""identity_idx"": identity_mapping[identity],",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266516,204.0,214,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The handling of `identity_likelihood` has been changed. The new code assumes that `identity_likelihood[0]` is a dictionary, which may not always be the case. It's recommended to add error handling here to avoid potential issues.

```diff
-                    class_likelihood = part_position.class_likelihood.values
+                    identity_likelihood = part_position.identity_likelihood.values
+                    if isinstance(identity_likelihood[0], dict):
+                        identity_likelihood = np.array([v[identity] for v in identity_likelihood])
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                    identity_likelihood = part_position.identity_likelihood.values
+                    if isinstance(identity_likelihood[0], dict):
+                        identity_likelihood = np.array([v[identity] for v in identity_likelihood])
+
             pose_identity_entries.append(
                 {
                     **key,
-                    ""identity_idx"": class_idx,
-                    ""identity_name"": class_names[class_idx],
+                    ""identity_idx"": identity_mapping[identity],
+                    ""identity_name"": identity,
                     ""anchor_part"": anchor_part,
-                    ""identity_likelihood"": class_likelihood,
+                    ""identity_likelihood"": identity_likelihood,",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266519,,5,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/schemas.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import of `social_03` is added correctly to support the new data type.
```diff
- from aeon.schema import foraging, octagon, social_01, social_02
+ from aeon.schema import foraging, octagon, social_01, social_02, social_03
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import aeon.schema.core as stream
 from aeon.schema.streams import Device
-from aeon.schema import foraging, octagon, social_01, social_02
+from aeon.schema import foraging, octagon, social_01, social_02, social_03",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266521,,66,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/schemas.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the device configuration for `CameraTop` from `social_02.Pose` to `social_01.Pose` seems correct as per the PR title. However, please ensure that this doesn't break any existing functionality that relies on `social_02.Pose`.
```diff
- Device(""CameraTop"", stream.Video, social_02.Pose),
+ Device(""CameraTop"", stream.Video, social_01.Pose),
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         Device(""Metadata"", stream.Metadata),
         Device(""Environment"", social_02.Environment, social_02.SubjectData),
-        Device(""CameraTop"", stream.Video, social_02.Pose),
+        Device(""CameraTop"", stream.Video, social_01.Pose),",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266523,116.0,144,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/schemas.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of a new device configuration for `social_03` is implemented correctly. This will allow the system to handle `social_03` data. Please make sure all the devices listed here are compatible with `social_03`.
```diff
+ social03 = DotMap(
+     [
+         Device(""Metadata"", stream.Metadata),
+         Device(""Environment"", social_02.Environment, social_02.SubjectData),
+         Device(""CameraTop"", stream.Video, social_03.Pose),
+         Device(""CameraNorth"", stream.Video),
+         Device(""CameraSouth"", stream.Video),
+         Device(""CameraEast"", stream.Video),
+         Device(""CameraWest"", stream.Video),
+         Device(""CameraNest"", stream.Video),
+         Device(""CameraPatch1"", stream.Video),
+         Device(""CameraPatch2"", stream.Video),
+         Device(""CameraPatch3"", stream.Video),
+         Device(""Nest"", social_02.WeightRaw, social_02.WeightFiltered),
+         Device(""Patch1"", social_02.Patch),
+         Device(""Patch2"", social_02.Patch),
+         Device(""Patch3"", social_02.Patch),
+         Device(""PatchDummy1"", social_02.Patch),
+         Device(""NestRfid1"", social_02.RfidEvents),
+         Device(""NestRfid2"", social_02.RfidEvents),
+         Device(""GateRfid"", social_02.RfidEvents),
+         Device(""GateEastRfid"", social_02.RfidEvents),
+         Device(""GateWestRfid"", social_02.RfidEvents),
+         Device(""Patch1Rfid"", social_02.RfidEvents),
+         Device(""Patch2Rfid"", social_02.RfidEvents),
+         Device(""Patch3Rfid"", social_02.RfidEvents),
+         Device(""PatchDummy1Rfid"", social_02.RfidEvents),
+     ]
+ )
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+social03 = DotMap(
+    [
+        Device(""Metadata"", stream.Metadata),
+        Device(""Environment"", social_02.Environment, social_02.SubjectData),
+        Device(""CameraTop"", stream.Video, social_03.Pose),
+        Device(""CameraNorth"", stream.Video),
+        Device(""CameraSouth"", stream.Video),
+        Device(""CameraEast"", stream.Video),
+        Device(""CameraWest"", stream.Video),
+        Device(""CameraNest"", stream.Video),
+        Device(""CameraPatch1"", stream.Video),
+        Device(""CameraPatch2"", stream.Video),
+        Device(""CameraPatch3"", stream.Video),
+        Device(""Nest"", social_02.WeightRaw, social_02.WeightFiltered),
+        Device(""Patch1"", social_02.Patch),
+        Device(""Patch2"", social_02.Patch),
+        Device(""Patch3"", social_02.Patch),
+        Device(""PatchDummy1"", social_02.Patch),
+        Device(""NestRfid1"", social_02.RfidEvents),
+        Device(""NestRfid2"", social_02.RfidEvents),
+        Device(""GateRfid"", social_02.RfidEvents),
+        Device(""GateEastRfid"", social_02.RfidEvents),
+        Device(""GateWestRfid"", social_02.RfidEvents),
+        Device(""Patch1Rfid"", social_02.RfidEvents),
+        Device(""Patch2Rfid"", social_02.RfidEvents),
+        Device(""Patch3Rfid"", social_02.RfidEvents),
+        Device(""PatchDummy1Rfid"", social_02.RfidEvents),
+    ]
+)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664266527,,147,5cbd09142da1ad7672103b8e6333781a8af35118,abd317f7b6e93041c362adae346f084b18dc3975,aeon/schema/schemas.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `social03` to the `__all__` list is correct. This will make `social03` available for import from this module.
```diff
- __all__ = [""exp01"", ""exp02"", ""octagon01"", ""social01"", ""social02""]
+ __all__ = [""exp01"", ""exp02"", ""octagon01"", ""social01"", ""social02"", ""social03""]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+)
+
+
+__all__ = [""exp01"", ""exp02"", ""octagon01"", ""social01"", ""social02"", ""social03""]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532263178,27.0,30,e85986b37a1d636d60ee8c61ee08b11d241fd713,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `StreamGroup` class constructor takes a variable number of arguments (`*args`) but it's not clear what these arguments should be. It would be helpful to add a comment or docstring explaining the expected input.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def __init__(self, name, *args):
+        self.name = name
+        self._args = args
+","--- 

+++ 

@@ -21,40 +21,23 @@

     """"""Represents a logical group of multiple data streams.
 
     Attributes:
-        name (str): Name of the logical group used to find raw files.
+        path (str): Path to the folder where stream chunks are located.
+        args (Any): Data streams or data stream groups to be included in this stream group.
     """"""
 
-    def __init__(self, name, *args):
-        self.name = name
+    def __init__(self, path, *args):
+        self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for member in chain(vars(self.__class__).values(), self._args):
-            if inspect.isclass(member):
-                for stream in iter(member(self.name)):
-                    yield stream
-            elif isinstance(member, staticmethod):
-                for stream in iter(member.__func__(self.name)):
-                    yield stream
-
-
-def compositeStream(pattern, *args):
-    """"""Merges multiple data streams into a single composite stream.""""""
-    composite = {}
-    if args:
-        for stream in args:
-            try:
-                composite.update(stream(pattern))
-            except TypeError:
-                warn(
-                    f""Stream groups with no constructors are deprecated. {stream}"",
-                    category=DeprecationWarning,
-                )
-                if inspect.isclass(stream):
-                    for method in vars(stream).values():
-                        if isinstance(method, staticmethod):
-                            composite.update(method.__func__(pattern))
-    return composite
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
+                yield stream
 
 
 class Device:
@@ -67,17 +50,35 @@

     Attributes:
         name (str): Name of the device.
         args (Any): Data streams collected from the device.
-        pattern (str, optional): Pattern used to find raw chunk files,
-            usually in the format `<Device>_<DataStream>`.
+        path (str, optional): Path to the folder where stream chunks are located.
     """"""
 
-    def __init__(self, name, *args, pattern=None):
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
         self.name = name
-        self.provider = compositeStream(name if pattern is None else pattern, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
+
+    @staticmethod
+    def _createStreams(path, args):
+        streams = {}
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
+        return streams
 
     def __iter__(self):
-        if len(self.provider) == 1:
-            singleton = self.provider.get(self.name, None)
+        if len(self._streams) == 1:
+            singleton = self._streams.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.provider))
+        return iter((self.name, self._streams))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532263181,32.0,38,e85986b37a1d636d60ee8c61ee08b11d241fd713,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code is quite complex and could benefit from some additional comments explaining its purpose. Specifically, it's not immediately clear why we're checking if `member` is a class or a static method, and what the implications are for each case.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        for member in chain(vars(self.__class__).values(), self._args):
+            if inspect.isclass(member):
+                for stream in iter(member(self.name)):
+                    yield stream
+            elif isinstance(member, staticmethod):
+                for stream in iter(member.__func__(self.name)):
+                    yield stream","--- 

+++ 

@@ -21,40 +21,23 @@

     """"""Represents a logical group of multiple data streams.
 
     Attributes:
-        name (str): Name of the logical group used to find raw files.
+        path (str): Path to the folder where stream chunks are located.
+        args (Any): Data streams or data stream groups to be included in this stream group.
     """"""
 
-    def __init__(self, name, *args):
-        self.name = name
+    def __init__(self, path, *args):
+        self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for member in chain(vars(self.__class__).values(), self._args):
-            if inspect.isclass(member):
-                for stream in iter(member(self.name)):
-                    yield stream
-            elif isinstance(member, staticmethod):
-                for stream in iter(member.__func__(self.name)):
-                    yield stream
-
-
-def compositeStream(pattern, *args):
-    """"""Merges multiple data streams into a single composite stream.""""""
-    composite = {}
-    if args:
-        for stream in args:
-            try:
-                composite.update(stream(pattern))
-            except TypeError:
-                warn(
-                    f""Stream groups with no constructors are deprecated. {stream}"",
-                    category=DeprecationWarning,
-                )
-                if inspect.isclass(stream):
-                    for method in vars(stream).values():
-                        if isinstance(method, staticmethod):
-                            composite.update(method.__func__(pattern))
-    return composite
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
+                yield stream
 
 
 class Device:
@@ -67,17 +50,35 @@

     Attributes:
         name (str): Name of the device.
         args (Any): Data streams collected from the device.
-        pattern (str, optional): Pattern used to find raw chunk files,
-            usually in the format `<Device>_<DataStream>`.
+        path (str, optional): Path to the folder where stream chunks are located.
     """"""
 
-    def __init__(self, name, *args, pattern=None):
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
         self.name = name
-        self.provider = compositeStream(name if pattern is None else pattern, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
+
+    @staticmethod
+    def _createStreams(path, args):
+        streams = {}
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
+        return streams
 
     def __iter__(self):
-        if len(self.provider) == 1:
-            singleton = self.provider.get(self.name, None)
+        if len(self._streams) == 1:
+            singleton = self._streams.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.provider))
+        return iter((self.name, self._streams))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532263182,41.0,57,e85986b37a1d636d60ee8c61ee08b11d241fd713,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `compositeStream` function, you're catching a `TypeError` and issuing a warning, but then continuing execution. This could lead to unexpected behavior if the error isn't handled properly later on. Consider either re-raising the exception after issuing the warning, or handling the error in a way that allows the function to continue executing safely.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def compositeStream(pattern, *args):
+    """"""Merges multiple data streams into a single composite stream.""""""
+    composite = {}
+    if args:
+        for stream in args:
+            try:
+                composite.update(stream(pattern))
+            except TypeError:
+                warn(
+                    f""Stream groups with no constructors are deprecated. {stream}"",
+                    category=DeprecationWarning,
+                )
+                if inspect.isclass(stream):
+                    for method in vars(stream).values():
+                        if isinstance(method, staticmethod):
+                            composite.update(method.__func__(pattern))
+    return composite","--- 

+++ 

@@ -21,40 +21,23 @@

     """"""Represents a logical group of multiple data streams.
 
     Attributes:
-        name (str): Name of the logical group used to find raw files.
+        path (str): Path to the folder where stream chunks are located.
+        args (Any): Data streams or data stream groups to be included in this stream group.
     """"""
 
-    def __init__(self, name, *args):
-        self.name = name
+    def __init__(self, path, *args):
+        self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for member in chain(vars(self.__class__).values(), self._args):
-            if inspect.isclass(member):
-                for stream in iter(member(self.name)):
-                    yield stream
-            elif isinstance(member, staticmethod):
-                for stream in iter(member.__func__(self.name)):
-                    yield stream
-
-
-def compositeStream(pattern, *args):
-    """"""Merges multiple data streams into a single composite stream.""""""
-    composite = {}
-    if args:
-        for stream in args:
-            try:
-                composite.update(stream(pattern))
-            except TypeError:
-                warn(
-                    f""Stream groups with no constructors are deprecated. {stream}"",
-                    category=DeprecationWarning,
-                )
-                if inspect.isclass(stream):
-                    for method in vars(stream).values():
-                        if isinstance(method, staticmethod):
-                            composite.update(method.__func__(pattern))
-    return composite
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
+                yield stream
 
 
 class Device:
@@ -67,17 +50,35 @@

     Attributes:
         name (str): Name of the device.
         args (Any): Data streams collected from the device.
-        pattern (str, optional): Pattern used to find raw chunk files,
-            usually in the format `<Device>_<DataStream>`.
+        path (str, optional): Path to the folder where stream chunks are located.
     """"""
 
-    def __init__(self, name, *args, pattern=None):
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
         self.name = name
-        self.provider = compositeStream(name if pattern is None else pattern, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
+
+    @staticmethod
+    def _createStreams(path, args):
+        streams = {}
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
+        return streams
 
     def __iter__(self):
-        if len(self.provider) == 1:
-            singleton = self.provider.get(self.name, None)
+        if len(self._streams) == 1:
+            singleton = self._streams.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.provider))
+        return iter((self.name, self._streams))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532263183,74.0,83,e85986b37a1d636d60ee8c61ee08b11d241fd713,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `Device` class, the `__iter__` method has a special case for when the provider has only one item. This could potentially lead to inconsistent behavior depending on the number of items in the provider. Consider removing this special case to ensure consistent behavior across all use cases.


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def __init__(self, name, *args, pattern=None):
+        self.name = name
+        self.provider = compositeStream(name if pattern is None else pattern, *args)
+
+    def __iter__(self):
+        if len(self.provider) == 1:
+            singleton = self.provider.get(self.name, None)
+            if singleton:
+                return iter((self.name, singleton))
+        return iter((self.name, self.provider))","--- 

+++ 

@@ -21,40 +21,23 @@

     """"""Represents a logical group of multiple data streams.
 
     Attributes:
-        name (str): Name of the logical group used to find raw files.
+        path (str): Path to the folder where stream chunks are located.
+        args (Any): Data streams or data stream groups to be included in this stream group.
     """"""
 
-    def __init__(self, name, *args):
-        self.name = name
+    def __init__(self, path, *args):
+        self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for member in chain(vars(self.__class__).values(), self._args):
-            if inspect.isclass(member):
-                for stream in iter(member(self.name)):
-                    yield stream
-            elif isinstance(member, staticmethod):
-                for stream in iter(member.__func__(self.name)):
-                    yield stream
-
-
-def compositeStream(pattern, *args):
-    """"""Merges multiple data streams into a single composite stream.""""""
-    composite = {}
-    if args:
-        for stream in args:
-            try:
-                composite.update(stream(pattern))
-            except TypeError:
-                warn(
-                    f""Stream groups with no constructors are deprecated. {stream}"",
-                    category=DeprecationWarning,
-                )
-                if inspect.isclass(stream):
-                    for method in vars(stream).values():
-                        if isinstance(method, staticmethod):
-                            composite.update(method.__func__(pattern))
-    return composite
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
+                yield stream
 
 
 class Device:
@@ -67,17 +50,35 @@

     Attributes:
         name (str): Name of the device.
         args (Any): Data streams collected from the device.
-        pattern (str, optional): Pattern used to find raw chunk files,
-            usually in the format `<Device>_<DataStream>`.
+        path (str, optional): Path to the folder where stream chunks are located.
     """"""
 
-    def __init__(self, name, *args, pattern=None):
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
         self.name = name
-        self.provider = compositeStream(name if pattern is None else pattern, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
+
+    @staticmethod
+    def _createStreams(path, args):
+        streams = {}
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
+        return streams
 
     def __iter__(self):
-        if len(self.provider) == 1:
-            singleton = self.provider.get(self.name, None)
+        if len(self._streams) == 1:
+            singleton = self._streams.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.provider))
+        return iter((self.name, self._streams))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532523641,37.0,53,1e499c7e8e677ea74f9646f87f569930cf58c607,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Device` class constructor has a potential issue. The `path` attribute is optional and defaults to `None`. However, if it remains `None`, the `_createStreams` method will use the `name` attribute as the path. This could lead to unexpected behavior if the `name` is not a valid path. Consider adding a check for `path` being `None` and handle it appropriately.
```diff
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        if path is None:
+            raise ValueError(""Path cannot be None."")
+        self._streams = Device._createStreams(path, *args)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class Device:
+    """"""Groups multiple data streams into a logical device.
+
+    If a device contains a single stream with the same pattern as the device
+    `name`, it will be considered a singleton, and the stream reader will be
+    paired directly with the device without nesting.
+
+    Attributes:
+        name (str): Name of the device.
+        args (Any): Data streams collected from the device.
+        path (str, optional): Path to the folder where stream chunks are located.
+    """"""
+
+    def __init__(self, name, *args, path=None):
+        self.name = name
+        self._streams = Device._createStreams(name if path is None else path, *args)
+","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -48,27 +54,26 @@

     """"""
 
     def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532539663,37.0,53,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Device` class constructor allows for a `None` value to be passed as the `name` parameter, but immediately raises a `ValueError` if this is the case. It would be better to make `name` a required positional argument without a default value to prevent misuse and unnecessary exception handling.
```diff
-    def __init__(self, name, *args, path=None):
+    def __init__(self, name=None, *args, path=None):
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+class Device:
+    """"""Groups multiple data streams into a logical device.
+
+    If a device contains a single stream with the same pattern as the device
+    `name`, it will be considered a singleton, and the stream reader will be
+    paired directly with the device without nesting.
+
+    Attributes:
+        name (str): Name of the device.
+        args (Any): Data streams collected from the device.
+        path (str, optional): Path to the folder where stream chunks are located.
+    """"""
+
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -52,26 +58,22 @@

             raise ValueError(""name cannot be None."")
 
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532539667,55.0,75,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `_createStreams` method, you are catching a `TypeError` and then checking if `callable` is a class. If it's not, you raise the error again. This could lead to confusion as it might not be clear where the error originated from. Instead, consider checking if `callable` is a class before attempting to call it. This way, you can provide a more specific error message if it's not a class, and avoid having to catch and re-raise the `TypeError`.
```diff
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+            for arg in args:
+                if inspect.isclass(arg):
+                    warn(
+                        f""Stream group classes with no constructors are deprecated. {arg}"",
+                        category=DeprecationWarning,
+                    )
+                    for method in vars(arg).values():
+                        if isinstance(method, staticmethod):
+                            streams.update(method.__func__(path))
+                elif callable(arg):
+                    streams.update(arg(path))
+                else:
+                    raise TypeError(f""Expected a class or callable, got {type(arg).__name__}"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        self._streams = Device._createStreams(name if path is None else path, *args)
+
+    @staticmethod
+    def _createStreams(path, *args):
+        streams = {}
+        if args:
+            for callable in args:
+                try:
+                    streams.update(callable(path))
+                except TypeError:
+                    if inspect.isclass(callable):
+                        warn(
+                            f""Stream group classes with no constructors are deprecated. {callable}"",
+                            category=DeprecationWarning,
+                        )
+                        for method in vars(callable).values():
+                            if isinstance(method, staticmethod):
+                                streams.update(method.__func__(path))
+                    else:
+                        raise
+        return streams","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -52,26 +58,22 @@

             raise ValueError(""name cannot be None."")
 
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532544723,58.0,75,dc847aba14251d314ce9c7a957556cb2b1459f5f,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `_createStreams` method seems to be handling two different types of callables: those that can be called with a path and those that cannot (i.e., classes without constructors). It would be better to separate these two cases into different methods to reduce complexity and improve readability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def _createStreams(path, *args):
+        streams = {}
+        if args:
+            for callable in args:
+                try:
+                    streams.update(callable(path))
+                except TypeError:
+                    if inspect.isclass(callable) and not hasattr(callable, ""__init__""):
+                        warn(
+                            f""Stream group classes with no constructors are deprecated. {callable}"",
+                            category=DeprecationWarning,
+                        )
+                        for method in vars(callable).values():
+                            if isinstance(method, staticmethod):
+                                streams.update(method.__func__(path))
+                    else:
+                        raise
+        return streams","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -52,26 +58,22 @@

             raise ValueError(""name cannot be None."")
 
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable) and not hasattr(callable, ""__init__""):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532564546,2.0,5,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `deprecated` decorator is used correctly to indicate that the `compositeStream` function is deprecated. However, it would be helpful to provide more information in the deprecation message about what exactly should be used instead of this function. It's mentioned to use the streams module, but it's not clear which function or class from that module should be used as a replacement.
```diff
- @deprecated(""Please use the streams module instead."")
+ @deprecated(""Please use the StreamGroup class from the streams module instead."")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+from typing_extensions import deprecated
 
 
+@deprecated(""Please use the streams module instead."")","--- 

+++ 

@@ -2,7 +2,7 @@

 from typing_extensions import deprecated
 
 
-@deprecated(""Please use the streams module instead."")
+@deprecated(""Please use the StreamGroup class from the streams module instead."")
 def compositeStream(pattern, *args):
     """"""Merges multiple data streams into a single composite stream.""""""
     composite = {}
@@ -17,7 +17,7 @@

     return composite
 
 
-@deprecated(""Please use the Device class in the streams module instead."")
+@deprecated(""The Device class has been moved to the streams module."")
 class Device:
     """"""Groups multiple data streams into a logical device.
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532564550,,20,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the deprecation message for the `Device` class could be more informative. It's suggested to use the Device class in the streams module, but it's not clear if it's a different `Device` class or the same one moved to a different module. If it's the latter, it might be better to mention that the class has been moved rather than deprecated.
```diff
- @deprecated(""Please use the Device class in the streams module instead."")
+ @deprecated(""The Device class has been moved to the streams module."")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     return composite
 
 
+@deprecated(""Please use the Device class in the streams module instead."")","--- 

+++ 

@@ -2,7 +2,7 @@

 from typing_extensions import deprecated
 
 
-@deprecated(""Please use the streams module instead."")
+@deprecated(""Please use the StreamGroup class from the streams module instead."")
 def compositeStream(pattern, *args):
     """"""Merges multiple data streams into a single composite stream.""""""
     composite = {}
@@ -17,7 +17,7 @@

     return composite
 
 
-@deprecated(""Please use the Device class in the streams module instead."")
+@deprecated(""The Device class has been moved to the streams module."")
 class Device:
     """"""Groups multiple data streams into a logical device.
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532564556,37.0,56,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the `Device` class constructor. If `name` is `None`, it raises a `ValueError`. However, if `path` is also `None`, it will pass `None` to `_createStreams` method as `path`, which might cause issues if the `path` is expected to be a valid directory path in `_createStreams` or in the callable objects passed as `args`.
```diff
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(path if path is not None else name, *args)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class Device:
+    """"""Groups multiple data streams into a logical device.
+
+    If a device contains a single stream with the same pattern as the device
+    `name`, it will be considered a singleton, and the stream reader will be
+    paired directly with the device without nesting.
+
+    Attributes:
+        name (str): Name of the device.
+        args (Any): Data streams collected from the device.
+        path (str, optional): Path to the folder where stream chunks are located.
+    """"""
+
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
+        self.name = name
+        self._streams = Device._createStreams(name if path is None else path, *args)
+","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -52,26 +58,22 @@

             raise ValueError(""name cannot be None."")
 
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532564559,58.0,75,31721cc9db8ed617283d52fcbb3e76e16bd802a7,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `_createStreams` method seems to handle both functions and classes as callables, but there's no clear documentation or type hinting indicating this. This could lead to confusion for developers using or maintaining this code. Consider adding more explicit documentation or type hints.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def _createStreams(path, *args):
+        streams = {}
+        if args:
+            for callable in args:
+                try:
+                    streams.update(callable(path))
+                except TypeError:
+                    if inspect.isclass(callable):
+                        warn(
+                            f""Stream group classes with no constructors are deprecated. {callable}"",
+                            category=DeprecationWarning,
+                        )
+                        for method in vars(callable).values():
+                            if isinstance(method, staticmethod):
+                                streams.update(method.__func__(path))
+                    else:
+                        raise
+        return streams","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -52,26 +58,22 @@

             raise ValueError(""name cannot be None."")
 
         self.name = name
-        self._streams = Device._createStreams(name if path is None else path, *args)
+        self._streams = Device._createStreams(name if path is None else path, args)
 
     @staticmethod
-    def _createStreams(path, *args):
+    def _createStreams(path, args):
         streams = {}
-        if args:
-            for callable in args:
-                try:
-                    streams.update(callable(path))
-                except TypeError:
-                    if inspect.isclass(callable):
-                        warn(
-                            f""Stream group classes with no constructors are deprecated. {callable}"",
-                            category=DeprecationWarning,
-                        )
-                        for method in vars(callable).values():
-                            if isinstance(method, staticmethod):
-                                streams.update(method.__func__(path))
-                    else:
-                        raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532574321,19.0,35,8f4a90b30f57f82d61c6f10aeb6176b3394e976c,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `StreamGroup` class is a good abstraction for handling multiple streams. However, it assumes that all callables passed to it are functions that take a single argument (`path`) and return an iterable. This might not always be the case, and if a non-conforming callable is passed, it will result in a runtime error. It would be better to add some type checking or exception handling here to ensure that the callables conform to the expected interface.

```diff
    def __iter__(self):
        for callable in self._args:
+           if not callable.__code__.co_argcount == 1:
+               raise TypeError(f""Callable {callable.__name__} does not accept exactly one argument"")
            for stream in iter(callable(self.path)):
                yield stream
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class StreamGroup:
+    """"""Represents a logical group of multiple data streams.
+
+    Attributes:
+        path (str): Path to the folder where stream chunks are located.
+        args (Any): Data streams or data stream groups to be included in this stream group.
+    """"""
+
+    def __init__(self, path, *args):
+        self.path = path
+        self._args = args
+
+    def __iter__(self):
+        for callable in self._args:
+            for stream in iter(callable(self.path)):
+                yield stream
+","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -57,20 +63,17 @@

     @staticmethod
     def _createStreams(path, args):
         streams = {}
-        for callable in args:
-            try:
-                streams.update(callable(path))
-            except TypeError:
-                if inspect.isclass(callable):
-                    warn(
-                        f""Stream group classes with no constructors are deprecated. {callable}"",
-                        category=DeprecationWarning,
-                    )
-                    for method in vars(callable).values():
-                        if isinstance(method, staticmethod):
-                            streams.update(method.__func__(path))
-                else:
-                    raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1532574326,37.0,81,8f4a90b30f57f82d61c6f10aeb6176b3394e976c,739bcbe762004157988bae2a831ff4d2e951dcd0,aeon/io/streams.py,aeon/schema/streams.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `Device` class, you're using the `callable` keyword as a variable name, which can be confusing because `callable` is also a built-in Python function. Consider renaming this variable to something more descriptive and less likely to cause confusion, like `stream_generator`.

Also, in line 78-81, you're returning an iterator over a tuple, which will only iterate once. If you want to return an iterator that yields two values, you should return a list with two elements instead.

```diff
    def __init__(self, name, *args, path=None):
        if name is None:
            raise ValueError(""name cannot be None."")

        self.name = name
-       self._streams = Device._createStreams(name if path is None else path, args)
+       self._streams = Device._create_streams(name if path is None else path, args)

    @staticmethod
-   def _createStreams(path, args):
+   def _create_streams(path, args):
        streams = {}
-       for callable in args:
+       for stream_generator in args:
            try:
-               streams.update(callable(path))
+               streams.update(stream_generator(path))
            except TypeError:
-               if inspect.isclass(callable):
+               if inspect.isclass(stream_generator):
                    warn(
                        f""Stream group classes with no constructors are deprecated. {stream_generator}"",
                        category=DeprecationWarning,
                    )
                    for method in vars(stream_generator).values():
                        if isinstance(method, staticmethod):
-                           streams.update(method.__func__(path))
+                           streams.update(method.__func__(path))
                else:
                    raise
        return streams

    def __iter__(self):
        if len(self._streams) == 1:
            singleton = self._streams.get(self.name, None)
            if singleton:
-               return iter((self.name, singleton))
+               return [(self.name, singleton)]
-       return iter((self.name, self._streams))
+       return [(self.name, self._streams)]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class Device:
+    """"""Groups multiple data streams into a logical device.
+
+    If a device contains a single stream with the same pattern as the device
+    `name`, it will be considered a singleton, and the stream reader will be
+    paired directly with the device without nesting.
+
+    Attributes:
+        name (str): Name of the device.
+        args (Any): Data streams collected from the device.
+        path (str, optional): Path to the folder where stream chunks are located.
+    """"""
+
+    def __init__(self, name, *args, path=None):
+        if name is None:
+            raise ValueError(""name cannot be None."")
+
+        self.name = name
+        self._streams = Device._createStreams(name if path is None else path, args)
+
+    @staticmethod
+    def _createStreams(path, args):
+        streams = {}
+        for callable in args:
+            try:
+                streams.update(callable(path))
+            except TypeError:
+                if inspect.isclass(callable):
+                    warn(
+                        f""Stream group classes with no constructors are deprecated. {callable}"",
+                        category=DeprecationWarning,
+                    )
+                    for method in vars(callable).values():
+                        if isinstance(method, staticmethod):
+                            streams.update(method.__func__(path))
+                else:
+                    raise
+        return streams
+
+    def __iter__(self):
+        if len(self._streams) == 1:
+            singleton = self._streams.get(self.name, None)
+            if singleton:
+                return iter((self.name, singleton))
+        return iter((self.name, self._streams))","--- 

+++ 

@@ -1,4 +1,5 @@

 import inspect
+from itertools import chain
 from warnings import warn
 
 
@@ -27,10 +28,15 @@

     def __init__(self, path, *args):
         self.path = path
         self._args = args
+        self._nested = (
+            member
+            for member in vars(self.__class__).values()
+            if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
+        )
 
     def __iter__(self):
-        for callable in self._args:
-            for stream in iter(callable(self.path)):
+        for factory in chain(self._nested, self._args):
+            for stream in iter(factory(self.path)):
                 yield stream
 
 
@@ -57,20 +63,17 @@

     @staticmethod
     def _createStreams(path, args):
         streams = {}
-        for callable in args:
-            try:
-                streams.update(callable(path))
-            except TypeError:
-                if inspect.isclass(callable):
-                    warn(
-                        f""Stream group classes with no constructors are deprecated. {callable}"",
-                        category=DeprecationWarning,
-                    )
-                    for method in vars(callable).values():
-                        if isinstance(method, staticmethod):
-                            streams.update(method.__func__(path))
-                else:
-                    raise
+        for factory in args:
+            if inspect.isclass(factory) and not hasattr(factory.__init__, ""__code__""):
+                warn(
+                    f""Stream group classes with default constructors are deprecated. {factory}"",
+                    category=DeprecationWarning,
+                )
+                for method in vars(factory).values():
+                    if isinstance(method, staticmethod):
+                        streams.update(method.__func__(path))
+            else:
+                streams.update(factory(path))
         return streams
 
     def __iter__(self):"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878229804,4.0,80,1646ac12f40426e3b51490113fd8e32b383615d6,7c7bbd7dd702c65dd7e49e6ae17e36a609e4f427,assets/js/blocks/general.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The SVG element creation seems to be quite verbose and hard to maintain. Consider using an external SVG file and importing it into your JavaScript. This would make the code cleaner and easier to manage. If you need to manipulate the SVG (like changing colors), you can use CSS or manipulate the SVG DOM after importing it.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	const svgElement = wp.element.createElement(
+		'svg',
+		{
+			width: ""52"",
+			height: ""52"",
+			viewBox: ""0 0 52 52"",
+			fill: ""none"",
+			xmlns: ""http://www.w3.org/2000/svg""
+		},
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M25.8848 50.9803C39.6698 50.9803 50.8448 39.8053 50.8448 26.0203C50.8448 12.2353 39.6698 1.0603 25.8848 1.0603C12.0998 1.0603 0.924805 12.2353 0.924805 26.0203C0.924805 39.8053 12.0998 50.9803 25.8848 50.9803ZM25.9956 47.4606C37.8631 47.4606 47.4836 37.8401 47.4836 25.9726C47.4836 14.1051 37.8631 4.4846 25.9956 4.4846C14.1281 4.4846 4.50762 14.1051 4.50762 25.9726C4.50762 37.8401 14.1281 47.4606 25.9956 47.4606Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M24.375 10.2578C24.375 11.0992 25.0571 11.7812 25.8984 11.7812V11.7812C26.7398 11.7812 27.4219 11.0992 27.4219 10.2578V7.21094C27.4219 6.36957 26.7398 5.6875 25.8984 5.6875V5.6875C25.0571 5.6875 24.375 6.36957 24.375 7.21094V10.2578Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M13.4739 16.0447C14.0688 16.6396 15.0334 16.6396 15.6284 16.0447V16.0447C16.2233 15.4497 16.2233 14.4852 15.6284 13.8902L13.4739 11.7358C12.8789 11.1408 11.9144 11.1408 11.3194 11.7358V11.7358C10.7245 12.3307 10.7245 13.2953 11.3194 13.8902L13.4739 16.0447Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M36.4099 13.7583C35.815 14.3532 35.815 15.3178 36.4099 15.9128V15.9128C37.0048 16.5077 37.9694 16.5077 38.5644 15.9128L40.7188 13.7583C41.3138 13.1634 41.3138 12.1988 40.7188 11.6038V11.6038C40.1239 11.0089 39.1593 11.0089 38.5644 11.6038L36.4099 13.7583Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M9.95312 27.625C10.7945 27.625 11.4766 26.9429 11.4766 26.1016V26.1016C11.4766 25.2602 10.7945 24.5781 9.95312 24.5781H6.90625C6.06488 24.5781 5.38281 25.2602 5.38281 26.1016V26.1016C5.38281 26.9429 6.06488 27.625 6.90625 27.625H9.95312Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M44.9922 27.625C45.8336 27.625 46.5156 26.9429 46.5156 26.1016V26.1016C46.5156 25.2602 45.8336 24.5781 44.9922 24.5781H41.9453C41.1039 24.5781 40.4219 25.2602 40.4219 26.1016V26.1016C40.4219 26.9429 41.1039 27.625 41.9453 27.625H44.9922Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M11.9812 39.1257C11.3863 39.7207 11.3863 40.6853 11.9812 41.2802V41.2802C12.5761 41.8751 13.5407 41.8751 14.1357 41.2802L16.2901 39.1257C16.8851 38.5308 16.8851 37.5662 16.2901 36.9713V36.9713C15.6952 36.3763 14.7306 36.3763 14.1357 36.9713L11.9812 39.1257Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M38.2629 40.6555C38.8579 41.2505 39.8225 41.2505 40.4174 40.6555V40.6555C41.0124 40.0606 41.0124 39.096 40.4174 38.5011L38.2629 36.3466C37.668 35.7517 36.7034 35.7517 36.1085 36.3466V36.3466C35.5135 36.9415 35.5135 37.9061 36.1085 38.5011L38.2629 40.6555Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M24.8828 44.7891C24.8828 45.6304 25.5649 46.3125 26.4062 46.3125V46.3125C27.2476 46.3125 27.9297 45.6304 27.9297 44.7891V41.7422C27.9297 40.9008 27.2476 40.2188 26.4062 40.2188V40.2188C25.5649 40.2188 24.8828 40.9008 24.8828 41.7422V44.7891Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M28.6218 22.4428L28.6222 22.4426L28.6192 22.4407C28.3268 22.2199 28.0059 22.0334 27.6627 21.8875L13 13.4062L21.8155 27.8277C21.9285 28.0554 22.0616 28.2716 22.2115 28.4754L22.2174 28.4851L22.2181 28.4844C22.395 28.7237 22.5961 28.9449 22.8177 29.1446C22.9896 29.3271 23.1771 29.4954 23.3782 29.6475L23.3778 29.6477L23.3808 29.6495C23.6732 29.8704 23.9941 30.0569 24.3373 30.2028L39 38.684L30.1845 24.2626C30.0715 24.0349 29.9384 23.8187 29.7885 23.6149L29.7826 23.6052L29.7819 23.6058C29.605 23.3666 29.404 23.1454 29.1824 22.9457C29.0104 22.7632 28.8229 22.5949 28.6218 22.4428ZM24.3781 24.4125C23.9645 24.8126 23.7084 25.3675 23.7084 25.9808C23.7084 27.2003 24.7213 28.1889 25.9709 28.1889C26.5417 28.1889 27.0631 27.9826 27.4612 27.6422C27.8747 27.2421 28.1308 26.6872 28.1308 26.0739C28.1308 24.8544 27.118 23.8658 25.8685 23.8658C25.2976 23.8658 24.7762 24.0721 24.3781 24.4125Z"",
+			fill: ""#090909""
+		}),
+		wp.element.createElement('path', {
+			'fill-rule': ""evenodd"",
+			'clip-rule': ""evenodd"",
+			d: ""M43.567 6.91088C43.3913 6.65906 43.2131 6.38092 43.0942 6.09685C42.6699 5.08228 42.7343 3.8416 43.1616 2.84063C43.1926 2.76922 43.2253 2.69875 43.2599 2.62913C43.2943 2.55942 43.3304 2.49059 43.3682 2.42266C43.4061 2.35473 43.4457 2.2878 43.4868 2.2217C43.5022 2.19713 43.5179 2.17271 43.5337 2.14836C43.5603 2.10759 43.5875 2.06721 43.6153 2.0272C43.6597 1.96332 43.7056 1.90064 43.7531 1.83904C43.8006 1.77739 43.8497 1.71711 43.9002 1.65803C43.9506 1.59885 44.0025 1.54098 44.056 1.48448C44.0751 1.46412 44.0945 1.44394 44.114 1.42386C44.1488 1.38829 44.1842 1.35331 44.22 1.31881C44.2761 1.26491 44.3336 1.21248 44.3923 1.16151C44.4509 1.11044 44.5109 1.0609 44.572 1.0129C44.6332 0.964895 44.6955 0.918498 44.7589 0.873496C44.8224 0.82852 44.8869 0.785151 44.9525 0.743478C44.9774 0.727538 45.0026 0.711935 45.0278 0.696495C45.069 0.671458 45.1104 0.646957 45.1523 0.62308C45.2198 0.584621 45.2883 0.547832 45.3578 0.512701C45.4271 0.477645 45.4973 0.444333 45.5684 0.412754C45.6394 0.381174 45.7113 0.351402 45.7838 0.323337C46.9073 -0.107222 48.1988 -0.125455 49.3033 0.375628C50.3912 0.869147 51.2751 1.85395 51.6838 2.97151C51.7096 3.04319 51.7337 3.11557 51.756 3.18855C51.7646 3.2165 51.7728 3.24452 51.7808 3.27261C51.7937 3.31778 51.8059 3.3632 51.8175 3.40885C51.8233 3.43194 51.829 3.45513 51.8345 3.47834C51.8466 3.52931 51.8579 3.58059 51.8682 3.6319C51.8834 3.70667 51.8966 3.78174 51.908 3.8571C51.9133 3.89208 51.9183 3.92719 51.9228 3.96231C51.9281 4.00289 51.9328 4.04346 51.937 4.08414C51.9447 4.15995 51.9506 4.23587 51.9546 4.31202C51.9587 4.38827 51.9609 4.46458 51.9612 4.54083C51.9616 4.61703 51.9601 4.69328 51.9567 4.76949C51.9534 4.84572 51.9482 4.92175 51.9411 4.99766C51.9388 5.02227 51.9363 5.04687 51.9337 5.07146C51.9281 5.12272 51.9216 5.17385 51.9143 5.22486C51.9107 5.25022 51.9068 5.27557 51.9028 5.30091C51.8949 5.35089 51.8861 5.40075 51.8765 5.45047C51.8621 5.52533 51.8458 5.5998 51.8279 5.67399C51.8098 5.74809 51.7899 5.82163 51.7683 5.89482C51.7467 5.96791 51.7232 6.04042 51.6981 6.11245C51.6729 6.18447 51.646 6.25582 51.6174 6.32646C51.5887 6.39709 51.5584 6.46709 51.5264 6.53637C51.0265 7.59911 50.0679 8.49389 48.9549 8.87753C47.5903 9.34807 46.3252 9.05145 45.0645 8.44598C45.0322 8.47617 44.9988 8.50644 44.9648 8.53711L44.9608 8.54076L44.9606 8.54093C44.7379 8.74234 44.4971 8.96006 44.4311 9.25287C44.4995 9.331 44.5707 9.36452 44.6414 9.39777L44.6415 9.39785C44.7261 9.43765 44.8097 9.47702 44.8869 9.59228C44.8499 10.0781 43.9245 10.8797 43.312 11.4103C43.2279 11.4832 43.1497 11.551 43.0805 11.6121C42.6465 11.1226 42.1952 10.6432 41.7265 10.1745C41.2678 9.71577 40.7988 9.27372 40.3203 8.84835C40.4071 8.75939 40.4938 8.67004 40.5806 8.5806L40.5806 8.58059C40.9383 8.21189 41.2975 7.84173 41.67 7.49145C41.6894 7.47332 41.7121 7.45102 41.7373 7.42615C41.8936 7.27233 42.149 7.02091 42.319 7.05068C42.5395 7.08935 42.6356 7.21956 42.7526 7.37811L42.7935 7.43314C43.0228 7.38867 43.2379 7.19892 43.4285 7.03071L43.4295 7.02979C43.4769 6.98793 43.5228 6.94742 43.567 6.91088ZM49.8764 4.49387C49.8764 5.83988 48.7852 6.93104 47.4392 6.93104C46.0932 6.93104 45.002 5.83988 45.002 4.49387C45.002 3.14785 46.0932 2.05669 47.4392 2.05669C48.7852 2.05669 49.8764 3.14785 49.8764 4.49387Z"",
+			fill: ""#090909""
+		})
+	);",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878229814,82.0,84,1646ac12f40426e3b51490113fd8e32b383615d6,7c7bbd7dd702c65dd7e49e6ae17e36a609e4f427,assets/js/blocks/general.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

You are updating the category icon for 'lsx-tour-operator'. Make sure that this category exists before trying to update it, otherwise, it might cause an error. You can check if a category exists using `wp.blocks.getCategories()`.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	wp.blocks.updateCategory( 'lsx-tour-operator', {
+		icon: svgElement
+	} );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878229818,87.0,90,1646ac12f40426e3b51490113fd8e32b383615d6,7c7bbd7dd702c65dd7e49e6ae17e36a609e4f427,assets/js/blocks/general.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

When registering a block variation, ensure that the original block (""core/gallery"" in this case) is available and registered before attempting to register a variation of it. You can use `wp.blocks.getBlockType('core/gallery')` to check if the block is available. 

```javascript
if(wp.blocks.getBlockType('core/gallery')) {
  // Register the block variation here
}
```

This will prevent errors in case the original block is not available for some reason.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 	wp.blocks.registerBlockVariation(""core/gallery"", {
 		name: ""lsx-tour-operator/gallery"",
 		title: ""TO Gallery"",
-		icon: ""gallery"",
+		icon: ""format-gallery"",",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878229828,8.0,26,1646ac12f40426e3b51490113fd8e32b383615d6,7c7bbd7dd702c65dd7e49e6ae17e36a609e4f427,includes/patterns/destination-card.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code has introduced a more detailed structure for the ""Destination Card"" block. It includes a featured image, title, description, and a ""View Destination"" button. However, there's no error handling or fallback in case the post doesn't have a featured image or excerpt. Consider adding conditional checks to handle these scenarios gracefully.

```diff
+ <!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+ <!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /-->
+ <!-- wp:post-excerpt {""showMoreOnNewLine"":false,""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /-->
```

To:

```diff
+ <!-- wp:if-has-post-thumbnail -->
+   <!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+ <!-- /wp:if-has-post-thumbnail -->
+ <!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /-->
+ <!-- wp:if-has-excerpt -->
+   <!-- wp:post-excerpt {""showMoreOnNewLine"":false,""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /-->
+ <!-- /wp:if-has-excerpt -->
```

Please note that the `wp:if-has-post-thumbnail` and `wp:if-has-excerpt` are pseudo code. You need to replace them with the actual WordPress functions or hooks that check if a post has a thumbnail or excerpt.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		<!-- wp:group {""metadata"":{""name"":""Destination Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+		<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+		<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+		<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:10px;padding-right:10px;padding-bottom:10px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Tour Title""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""4rem""},""border"":{""bottom"":{""color"":""var:preset|color|primary"",""width"":""2px""}},""spacing"":{""padding"":{""bottom"":""10px""}}},""layout"":{""type"":""constrained""}} -->
+		<div class=""wp-block-group center-vertically"" style=""border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;min-height:4rem;padding-bottom:10px""><!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+		<!-- /wp:group -->
+
+		<!-- wp:group {""metadata"":{""name"":""Destination Description""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""0px"",""bottom"":""0px""}},""elements"":{""link"":{""color"":{""text"":""var:preset|color|septenary""}}}},""textColor"":""septenary"",""layout"":{""type"":""constrained""}} -->
+		<div class=""wp-block-group has-septenary-color has-text-color has-link-color"" style=""padding-top:0px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""showMoreOnNewLine"":false,""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /--></div>
+		<!-- /wp:group --></div>
+		<!-- /wp:group -->
+
+		<!-- wp:buttons -->
+		<div class=""wp-block-buttons""><!-- wp:button {""backgroundColor"":""primary"",""width"":100,""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""View More Button""},""style"":{""border"":{""radius"":{""bottomLeft"":""8px"",""bottomRight"":""8px""}}}} -->
+		<div class=""wp-block-button has-custom-width wp-block-button__width-100""><a class=""wp-block-button__link has-primary-background-color has-background wp-element-button"" style=""border-bottom-left-radius:8px;border-bottom-right-radius:8px"">View Destination</a></div>
+		<!-- /wp:button --></div>
+		<!-- /wp:buttons --></div>
 		<!-- /wp:group -->",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1878229834,1.0,60,1646ac12f40426e3b51490113fd8e32b383615d6,7c7bbd7dd702c65dd7e49e6ae17e36a609e4f427,includes/patterns/tour-card.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there are a few areas that could be improved for better maintainability and readability.

1. The image URLs are hardcoded in the PHP file (lines 19 and 35). This could lead to issues if the path changes or if the images need to be updated. Consider storing these paths in a configuration file or database and referencing them from there.

2. The alt attributes for the images are empty (lines 19 and 35). For accessibility reasons, it's important to provide meaningful alternative text for images. 

3. The 'price' and 'duration' values are hardcoded (lines 28 and 45). If these values are supposed to be dynamic, consider fetching them from the appropriate source.

Here are the suggested changes:

```diff
- <img src=""' . LSX_TO_URL . 'assets/img/blocks/unit-price.png"" alt="""" style=""width:20px;height:auto""/>
+ <img src=""' . get_image_url('unit-price.png') . '"" alt=""Unit Price Icon"" style=""width:20px;height:auto""/>

- <img src=""' . LSX_TO_URL . 'assets/img/blocks/duration.png"" alt="""" style=""width:20px""/>
+ <img src=""' . get_image_url('duration.png') . '"" alt=""Duration Icon"" style=""width:20px""/>

- <p class=""amount price"" style=""padding-top:2px;padding-bottom:2px""><strong>price</strong></p>
+ <p class=""amount price"" style=""padding-top:2px;padding-bottom:2px""><strong><?php echo $tour_price; ?></strong></p>

- <p style=""padding-top:2px;padding-bottom:2px""></p>
+ <p style=""padding-top:2px;padding-bottom:2px""><?php echo $tour_duration; ?></p>
```

In the above changes, `get_image_url()` is a hypothetical function that retrieves the URL of an image given its filename. `$tour_price` and `$tour_duration` are hypothetical variables that contain the price and duration of the tour, respectively. You would need to replace these with the actual functions or methods used in your application to retrieve these values.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<?php
+
+return array(
+	'title'         => __( 'Tour Card', 'tour-operator' ),
+	'description'   => __( 'Display the tours', 'tour-operator' ),
+	'categories'    => array( $this->category ),
+	'content'       => '<!-- wp:group {""metadata"":{""name"":""Tour Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""10px"",""bottom"":""0px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:10px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Tour Title""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""3rem""},""spacing"":{""padding"":{""top"":""5px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group center-vertically"" style=""min-height:3rem;padding-top:5px""><!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Tour Information""},""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""10px"",""right"":""10px""},""blockGap"":""2px""},""border"":{""top"":{""color"":""var:preset|color|primary"",""width"":""2px""},""right"":[],""bottom"":{""color"":""var:preset|color|primary"",""width"":""2px""},""left"":[]}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--primary);border-top-width:2px;border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;padding-top:10px;padding-right:10px;padding-bottom:10px;padding-left:10px""><!-- wp:group {""className"":""lsx-price-wrapper"",""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group lsx-price-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fixed"",""flexSize"":""130px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""height"":""auto"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""' . LSX_TO_URL . 'assets/img/blocks/unit-price.png"" alt="""" style=""width:20px;height:auto""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""layout"":{""selfStretch"":""fit"",""flexSize"":""""},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>From:</strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""price""}},""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price""},""className"":""amount price"",""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p class=""amount price"" style=""padding-top:2px;padding-bottom:2px""><strong>price</strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""className"":""lsx-duration-wrapper"",""style"":{""spacing"":{""blockGap"":""10px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group lsx-duration-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fixed"",""flexSize"":""130px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""Duration Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""' . LSX_TO_URL . 'assets/img/blocks/duration.png"" alt="""" style=""width:20px""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""layout"":{""selfStretch"":""fit"",""flexSize"":""""},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>Duration:</strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fit"",""flexSize"":null}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+<div class=""wp-block-group""><!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""duration""}},""__default"":{""source"":""core/pattern-overrides""}},""name"":""Duration""},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px"">Days</p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Tour Text Content""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""0px"",""bottom"":""0px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""padding-top:0px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""moreText"":""View More"",""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->'
+);",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893655760,,2,802863263895bfe9389f1ae006095def752304f5,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/pages.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The theme reference from the `header` template part has been removed. If this was intentional and you've confirmed that it doesn't break any dependencies, then this change is fine. However, if the theme ""lsx-tour-operator"" is still in use elsewhere in your project, removing it here might cause inconsistencies or errors. Please ensure that this change is intended and won't affect other parts of your application.
```diff
- <!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header"",""theme"":""lsx-tour-operator""} /-->
+ <!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header""} /-->
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,5 +1,5 @@
 <!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""}}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
-<div class=""wp-block-group alignwide has-base-background-color has-background"" style=""padding-top:10px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:10px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header"",""theme"":""lsx-tour-operator""} /--></div>
+<div class=""wp-block-group alignwide has-base-background-color has-background"" style=""padding-top:10px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:10px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header""} /--></div>",File_Deleted
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893655763,1.0,5,802863263895bfe9389f1ae006095def752304f5,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/pages.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [14-14]

Similar to the previous comment, the theme reference from the `footer` template part has been removed. Again, please ensure that this change is intended and won't affect other parts of your application.
```diff
- <!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer"",""theme"":""lsx-tour-operator""} /-->
+ <!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer""} /-->
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," <!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""}}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
-<div class=""wp-block-group alignwide has-base-background-color has-background"" style=""padding-top:10px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:10px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header"",""theme"":""lsx-tour-operator""} /--></div>
+<div class=""wp-block-group alignwide has-base-background-color has-background"" style=""padding-top:10px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:10px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header""} /--></div>
 <!-- /wp:group -->
 
 <!-- wp:group {""tagName"":""main"",""style"":{""spacing"":{""margin"":{""top"":""0""},""padding"":{""top"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""blockGap"":""var:preset|spacing|x-small""}},""className"":""site-content"",""layout"":{""type"":""default""}} -->",File_Deleted
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893655770,1.0,59,802863263895bfe9389f1ae006095def752304f5,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-activity.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new hunk of code seems to be well structured and follows the WordPress block structure. However, there are a few points that need attention:

1. The use of inline styles: While it's not inherently wrong, it can lead to maintenance issues in the future. It would be better to move these styles into a separate CSS file or at least a `<style>` tag at the top of the document.

2. Hardcoded values: There are several hardcoded values for padding, margin, etc. If these values are used across multiple templates, consider using CSS variables or a shared class to make the design more consistent and easier to change.

3. Missing alt text for images: The `wp:post-featured-image` block doesn't seem to have an alt text. This is important for accessibility and SEO.

4. No error handling: If the query in line 40 doesn't return any posts, there should be some kind of message or fallback to handle this case.

5. No newline at end of file: It's a common convention to end a file with a newline. This isn't strictly necessary, but it can prevent potential issues with some tools that expect or require it.

Here's how you might address some of these issues:

```diff
- <!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+ <!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}},""altText"":""Description of the image""} /-->

- <!-- wp:query {""queryId"":1,""query"":{""perPage"":""3"",""pages"":""3"",""offset"":""0"",""postType"":""activity"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false,""parents"":[]},""align"":""wide"",""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+ <!-- wp:query {""queryId"":1,""query"":{""perPage"":""3"",""pages"":""3"",""offset"":""0"",""postType"":""activity"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false,""parents"":[]},""align"":""wide"",""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+ <!-- wp:if-empty -->
+ <p>No related activities found.</p>
+ <!-- /wp:if-empty -->

- 59: <!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer""} /-->
+ 59: <!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer""} /-->
+ \n
```

Please note that `wp:if-empty` is a hypothetical block for handling empty queries. You might need to adjust this based on the actual capabilities of your WordPress setup.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<!-- wp:template-part {""slug"":""header"",""tagName"":""header"",""align"":""wide"",""className"":""site-header""} /-->
+
+<!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull""><!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary-200"",""width"":""2px""},""top"":[],""right"":[],""left"":[]}},""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group"" style=""border-bottom-color:var(--wp--preset--color--primary-200);border-bottom-width:2px;padding-top:10px;padding-bottom:10px""><!-- wp:post-title {""textAlign"":""center"",""level"":1,""className"":""wp-block-heading has-text-align-center"",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|primary-200""}}}},""textColor"":""primary-200""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""align"":""center"",""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""tagline""}}}},""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|primary-200""}}}},""textColor"":""primary-200"",""fontSize"":""small""} -->
+<p class=""has-text-align-center has-primary-200-color has-text-color has-link-color has-small-font-size""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div></div>
+<!-- /wp:cover -->
+
+<!-- wp:group {""metadata"":{""name"":""Breadcrumbs""},""align"":""full"",""style"":{""spacing"":{""padding"":{""top"":""6px"",""bottom"":""6px"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}},""elements"":{""link"":{"":hover"":{""color"":{""text"":""var:preset|color|tertiary""}},""color"":{""text"":""var:preset|color|base""}}}},""backgroundColor"":""primary-900"",""textColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull has-base-color has-primary-900-background-color has-text-color has-background has-link-color"" style=""margin-top:0;margin-bottom:0;padding-top:6px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:6px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""align"":""wide"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:yoast-seo/breadcrumbs /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:post-content {""align"":""wide"",""layout"":{""type"":""default""}} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Featured Tours""},""align"":""full"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""0"",""bottom"":""var:preset|spacing|small"",""left"":""0"",""right"":""0""},""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""flex"",""flexWrap"":""wrap"",""justifyContent"":""center""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:0;padding-right:0;padding-bottom:var(--wp--preset--spacing--small);padding-left:0""><!-- wp:separator {""style"":{""layout"":{""selfStretch"":""fill"",""flexSize"":null}},""backgroundColor"":""primary""} -->
+<hr class=""wp-block-separator has-text-color has-primary-color has-alpha-channel-opacity has-primary-background-color has-background""/>
+<!-- /wp:separator -->
+
+<!-- wp:heading {""textAlign"":""center""} -->
+<h2 class=""wp-block-heading has-text-align-center"">Related Activities</h2>
+<!-- /wp:heading -->
+
+<!-- wp:separator {""style"":{""layout"":{""selfStretch"":""fill"",""flexSize"":null}},""backgroundColor"":""primary""} -->
+<hr class=""wp-block-separator has-text-color has-primary-color has-alpha-channel-opacity has-primary-background-color has-background""/>
+<!-- /wp:separator --></div>
+<!-- /wp:group -->
+
+<!-- wp:query {""queryId"":1,""query"":{""perPage"":""3"",""pages"":""3"",""offset"":""0"",""postType"":""activity"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false,""parents"":[]},""align"":""wide"",""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+<div class=""wp-block-query alignwide""><!-- wp:post-template {""lock"":{""move"":false,""remove"":false},""align"":""wide"",""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":3}} -->
+<!-- wp:group {""metadata"":{""name"":""Activity Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""5px"",""bottom"":""0px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:5px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Title""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""3rem""},""spacing"":{""padding"":{""top"":""0px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary"",""width"":""2px""},""top"":[],""right"":[],""left"":[]}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group center-vertically"" style=""border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;min-height:3rem;padding-top:0px""><!-- wp:post-title {""textAlign"":""center"",""level"":3,""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Text Content""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""0px"",""bottom"":""0px""}},""elements"":{""link"":{""color"":{""text"":""var:preset|color|septenary""}}}},""textColor"":""septenary"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group has-septenary-color has-text-color has-link-color"" style=""padding-top:0px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""moreText"":""View More"",""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+<!-- /wp:post-template --></div>
+<!-- /wp:query --></div>
+<!-- /wp:group -->
+
+<!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer""} /-->",File_Deleted
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893655773,1.0,85,802863263895bfe9389f1ae006095def752304f5,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new hunk of code seems to be a complete overhaul of the `single.html` template. It appears to be well-structured and follows good practices for WordPress block-based themes. However, there are a few points that need attention:

1. The use of inline styles: While it's not inherently bad, it can lead to difficulties in maintaining the code in the future. Consider moving these styles to a CSS file if possible.

2. Hardcoded text: There are hardcoded texts like ""Recent Posts"", ""Previous Post"", ""Next Post"", etc. These should ideally be translatable strings using WordPress i18n functions to support localization.

3. Missing alt attributes: If there are images being used in this template, ensure they have appropriate alt attributes for accessibility.

4. No error handling: There doesn't seem to be any error handling or fallbacks in case the blocks or templates fail to load. Consider adding some error handling or fallback mechanisms.

5. No newline at end of file: It's a common practice to end your files with a newline character. This is a POSIX standard and while it might not affect your WordPress theme, it's still considered a good practice.

Please note that without the full context of how these templates are being used, it's hard to provide a more detailed review.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<!-- wp:group {""align"":""wide"",""style"":{""position"":{""type"":""sticky"",""top"":""0px""},""spacing"":{""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0"",""right"":""0""}}},""backgroundColor"":""base"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group alignwide has-base-background-color has-background"" style=""padding-top:0px;padding-right:0;padding-bottom:0px;padding-left:0""><!-- wp:template-part {""slug"":""header-2-0"",""area"":""header""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""minHeight"":400,""minHeightUnit"":""px"",""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0;min-height:400px""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull""><!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary-200"",""width"":""2px""},""top"":[],""right"":[],""left"":[]},""typography"":{""fontStyle"":""normal"",""fontWeight"":""600""}},""fontSize"":""large"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group has-large-font-size"" style=""border-bottom-color:var(--wp--preset--color--primary-200);border-bottom-width:2px;padding-top:10px;padding-bottom:10px;font-style:normal;font-weight:600""><!-- wp:post-title {""textAlign"":""center"",""level"":1} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""price""}}}},""className"":""has-text-align-center has-primary-200-color has-text-color has-link-color has-small-font-size""} -->
+<p class=""has-text-align-center has-primary-200-color has-text-color has-link-color has-small-font-size""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div></div>
+<!-- /wp:cover -->
+
+<!-- wp:group {""metadata"":{""name"":""Breadcrumbs""},""align"":""full"",""style"":{""spacing"":{""padding"":{""top"":""6px"",""bottom"":""6px"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}},""elements"":{""link"":{"":hover"":{""color"":{""text"":""var:preset|color|tertiary""}},""color"":{""text"":""var:preset|color|base""}}}},""backgroundColor"":""primary-900"",""textColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull has-base-color has-primary-900-background-color has-text-color has-background has-link-color"" style=""margin-top:0;margin-bottom:0;padding-top:6px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:6px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""align"":""wide"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:yoast-seo/breadcrumbs /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:post-content {""style"":{""spacing"":{""blockGap"":""8px""}},""layout"":{""type"":""default""}} /-->
+
+<!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px""}},""border"":{""top"":{""color"":""var:preset|color|neutral"",""style"":""none"",""width"":""0px""},""right"":{""style"":""none"",""width"":""0px""},""bottom"":{""color"":""var:preset|color|neutral"",""style"":""none"",""width"":""0px""},""left"":{""style"":""none"",""width"":""0px""}},""typography"":{""fontStyle"":""normal"",""fontWeight"":""500""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""justifyContent"":""space-between"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--neutral);border-top-style:none;border-top-width:0px;border-right-style:none;border-right-width:0px;border-bottom-color:var(--wp--preset--color--neutral);border-bottom-style:none;border-bottom-width:0px;border-left-style:none;border-left-width:0px;padding-top:10px;font-style:normal;font-weight:500""><!-- wp:post-navigation-link {""type"":""previous"",""label"":""Previous Post"",""arrow"":""chevron"",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}},""textColor"":""contrast""} /-->
+
+<!-- wp:post-terms {""term"":""post_tag"",""textAlign"":""left"",""prefix"":""Tags: "",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}}} /-->
+
+<!-- wp:post-navigation-link {""label"":""Next  Post"",""arrow"":""chevron"",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}},""textColor"":""contrast""} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""align"":""full"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""right"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""var:preset|spacing|x-small""}},""backgroundColor"":""primary-bg"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull has-primary-bg-background-color has-background"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""0"",""bottom"":""var:preset|spacing|small"",""left"":""0"",""right"":""0""},""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:0;padding-right:0;padding-bottom:var(--wp--preset--spacing--small);padding-left:0""><!-- wp:separator {""style"":{""layout"":{""selfStretch"":""fill"",""flexSize"":null}},""backgroundColor"":""primary""} -->
+<hr class=""wp-block-separator has-text-color has-primary-color has-alpha-channel-opacity has-primary-background-color has-background""/>
+<!-- /wp:separator -->
+
+<!-- wp:heading {""textAlign"":""center""} -->
+<h2 class=""wp-block-heading has-text-align-center"">Recent Posts</h2>
+<!-- /wp:heading -->
+
+<!-- wp:separator {""style"":{""layout"":{""selfStretch"":""fill"",""flexSize"":null}},""backgroundColor"":""primary""} -->
+<hr class=""wp-block-separator has-text-color has-primary-color has-alpha-channel-opacity has-primary-background-color has-background""/>
+<!-- /wp:separator --></div>
+<!-- /wp:group -->
+
+<!-- wp:query {""queryId"":1,""query"":{""perPage"":""3"",""pages"":0,""offset"":0,""postType"":""post"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false},""align"":""wide"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-query alignwide""><!-- wp:post-template {""lock"":{""move"":false,""remove"":false},""align"":""wide"",""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":3}} -->
+<!-- wp:group {""metadata"":{""name"":""Blog Post Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""10px"",""bottom"":""0px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:10px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Title""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""3rem""},""spacing"":{""padding"":{""top"":""5px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group center-vertically"" style=""min-height:3rem;padding-top:5px""><!-- wp:post-title {""textAlign"":""center"",""level"":3,""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""0.3em""}},""fontSize"":""tiny"",""layout"":{""type"":""flex"",""justifyContent"":""center"",""flexWrap"":""wrap""}} -->
+<div class=""wp-block-group has-tiny-font-size""><!-- wp:post-date {""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}},""textColor"":""contrast""} /-->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""}}},""textColor"":""contrast-2""} -->
+<p class=""has-contrast-2-color has-text-color"" style=""padding-top:0;padding-bottom:0"">√¢‚Ç¨‚Äù</p>
+<!-- /wp:paragraph -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""0"",""bottom"":""0""}}},""textColor"":""contrast-2""} -->
+<p class=""has-contrast-2-color has-text-color"" style=""margin-top:0;margin-bottom:0;padding-top:0;padding-bottom:0"">by</p>
+<!-- /wp:paragraph -->
+
+<!-- wp:post-author-name {""isLink"":true,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}},""textColor"":""contrast""} /-->
+
+<!-- wp:post-terms {""term"":""category"",""prefix"":""in "",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""},"":hover"":{""color"":{""text"":""var:preset|color|primary""}}}}}} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Text Content""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""10px"",""bottom"":""0px""}},""border"":{""top"":{""color"":""var:preset|color|primary"",""width"":""2px""},""right"":{""width"":""0px"",""style"":""none""},""bottom"":{""width"":""0px"",""style"":""none""},""left"":{""width"":""0px"",""style"":""none""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--primary);border-top-width:2px;border-right-style:none;border-right-width:0px;border-bottom-style:none;border-bottom-width:0px;border-left-style:none;border-left-width:0px;padding-top:10px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""textAlign"":""left"",""moreText"":""Read More"",""excerptLength"":30} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+<!-- /wp:post-template --></div>
+<!-- /wp:query --></div>
+<!-- /wp:group -->
+
+<!-- wp:template-part {""slug"":""footer"",""tagName"":""footer"",""className"":""site-footer""} /-->",File_Deleted
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893892319,1.0,116,e9f9864a8ef082fe8dbe1e985b7bb5ceba3233b1,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,includes/patterns/archive-tour.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall. However, there are a few points to consider:

1. The image URLs are hardcoded in the template (lines 13, 53, 69). This could lead to issues if the images are moved or deleted from the server. Consider using a dynamic way to load these images, perhaps by storing them within the plugin and referencing them accordingly.

2. The text content in line 33 is hardcoded into the template. If you want this text to be editable from the WordPress admin area, consider fetching it from a custom field or option.

3. The query parameters on line 39 are hardcoded. If you want to allow users to customize the query parameters (like perPage, pages, offset, etc.), consider fetching these values from plugin settings or custom fields.

4. There's no newline at the end of the file (line 116). While this won't cause any functional issues, it's generally considered good practice to end files with a newline for better compatibility across different tools and systems.

Here's an example of how you might adjust the hardcoded image URL:

```diff
- <!-- wp:image {""width"":""20px"",""height"":""auto"",""sizeSlug"":""large"",""metadata"":{""name"":""From Price Icon""}} -->
- <figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px;height:auto""/></figure>
+ <!-- wp:image {""width"":""20px"",""height"":""auto"",""sizeSlug"":""large"",""metadata"":{""name"":""From Price Icon""}} -->
+ <figure class=""wp-block-image size-large is-resized""><img src=""<?php echo plugins_url( 'images/from-price-TO-icon-black-20px-1.png', __FILE__ ); ?>"" alt="""" style=""width:20px;height:auto""/></figure>
```

Please note that this is just an example and you might need to adjust the path according to your plugin structure.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<?php
+// phpcs:ignoreFile PluginCheck.CodeAnalysis.ImageFunctions.NonEnqueuedImage
+return array(
+	'title'         => __( 'Tour Archive', 'tour-operator' ),
+	'description'   => __( 'Displays an archive with tours in it', 'tour-operator' ),
+	'categories'    => array( $this->category ),
+	'templateTypes' => array( 'archive' ),
+	'content'     => '<!-- wp:group {""style"":{""position"":{""type"":""sticky"",""top"":""0px""}},""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:cover {""url"":""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/10/tours-archive-banner-image.png"",""id"":61304,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""minHeight"":300,""minHeightUnit"":""px"",""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0;min-height:300px""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><img class=""wp-block-cover__image-background wp-image-61304"" alt="""" src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/10/tours-archive-banner-image.png"" data-object-fit=""cover""/><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull""><!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary-200"",""width"":""2px""},""top"":[],""right"":[],""left"":[]},""typography"":{""fontStyle"":""normal"",""fontWeight"":""600""}},""fontSize"":""large"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group has-large-font-size"" style=""border-bottom-color:var(--wp--preset--color--primary-200);border-bottom-width:2px;padding-top:10px;padding-bottom:10px;font-style:normal;font-weight:600""><!-- wp:query-title {""type"":""archive"",""textAlign"":""center"",""showPrefix"":false} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""align"":""center"",""metadata"":{""bindings"":{""content"":{""source"":""core/post-meta"",""args"":{""key"":""tagline""}}}},""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|primary-200""}}}},""textColor"":""primary-200"",""fontSize"":""small""} -->
+<p class=""has-text-align-center has-primary-200-color has-text-color has-link-color has-small-font-size""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div></div>
+<!-- /wp:cover -->
+
+<!-- wp:group {""metadata"":{""name"":""Breadcrumbs""},""style"":{""spacing"":{""padding"":{""top"":""6px"",""bottom"":""6px""},""margin"":{""top"":""0"",""bottom"":""0""}},""elements"":{""link"":{""color"":{""text"":""var:preset|color|base""},"":hover"":{""color"":{""text"":""var:preset|color|tertiary""}}}}},""backgroundColor"":""primary-900"",""textColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group has-base-color has-primary-900-background-color has-text-color has-background has-link-color"" style=""margin-top:0;margin-bottom:0;padding-top:6px;padding-bottom:6px""><!-- wp:group {""align"":""wide"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:yoast-seo/breadcrumbs /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Archive Description""},""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|small"",""bottom"":""0"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""padding-top:var(--wp--preset--spacing--small);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:0;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""metadata"":{""name"":""Content""},""align"":""wide"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:paragraph {""align"":""center""} -->
+<p class=""has-text-align-center"">Browse through our extensive collection of tours and find the perfect journey for your next adventure. Whether you\'re drawn to historical explorations, nature-filled escapes, or vibrant cultural experiences, our range of tours has something to suit every interest. Start planning your next memorable trip with options tailored to every type of traveller.</p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Archive Content""},""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:query {""queryId"":1,""query"":{""perPage"":""9"",""pages"":""3"",""offset"":""0"",""postType"":""tour"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false,""parents"":[]},""align"":""wide"",""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+<div class=""wp-block-query alignwide""><!-- wp:post-template {""lock"":{""move"":false,""remove"":false},""align"":""wide"",""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":3,""minimumColumnWidth"":null}} -->
+<!-- wp:group {""metadata"":{""name"":""Tour Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""10px"",""bottom"":""0px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:10px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Tour Title""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""3rem""},""spacing"":{""padding"":{""top"":""5px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group center-vertically"" style=""min-height:3rem;padding-top:5px""><!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Tour Information""},""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""10px"",""right"":""10px""},""blockGap"":""2px""},""border"":{""top"":{""color"":""var:preset|color|primary"",""width"":""2px""},""right"":[],""bottom"":{""color"":""var:preset|color|primary"",""width"":""2px""},""left"":[]}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--primary);border-top-width:2px;border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;padding-top:10px;padding-right:10px;padding-bottom:10px;padding-left:10px""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fixed"",""flexSize"":""130px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""height"":""auto"",""sizeSlug"":""large"",""metadata"":{""name"":""From Price Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px;height:auto""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""layout"":{""selfStretch"":""fit"",""flexSize"":""""},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>From:</strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""price""}}}},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fixed"",""flexSize"":""130px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""name"":""Duration Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/duration-TO-black-20px-icon.png"" alt="""" style=""width:20px""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""layout"":{""selfStretch"":""fit"",""flexSize"":""""},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>Duration:</strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fit"",""flexSize"":null}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+<div class=""wp-block-group""><!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""duration""}}}},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px"">Days</p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Tour Text Content""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""0px"",""bottom"":""0px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""padding-top:0px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""moreText"":""View More"",""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+<!-- /wp:post-template -->
+
+<!-- wp:query-no-results -->
+<!-- wp:paragraph {""align"":""center"",""placeholder"":""Add text or blocks that will display when the query returns no results.""} -->
+<p class=""has-text-align-center"">Unfortunately, there are no tours listed at the moment. Please check back soon as we regularly update our offerings.</p>
+<!-- /wp:paragraph -->
+<!-- /wp:query-no-results -->
+
+<!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""margin"":{""top"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|x-small""},""padding"":{""top"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|x-small""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:var(--wp--preset--spacing--x-small);margin-bottom:var(--wp--preset--spacing--x-small);padding-top:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--x-small)""><!-- wp:query-pagination {""paginationArrow"":""chevron"",""align"":""wide"",""layout"":{""type"":""flex"",""justifyContent"":""space-between""}} -->
+<!-- wp:query-pagination-previous /-->
+
+<!-- wp:query-pagination-numbers /-->
+
+<!-- wp:query-pagination-next /-->
+<!-- /wp:query-pagination --></div>
+<!-- /wp:group --></div>
+<!-- /wp:query --></div>
+<!-- /wp:group -->
+
+<!-- wp:template-part {""slug"":""footer"",""area"":""footer""} /-->
+    '
+);",File_Deleted
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912122,,2,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-region.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The theme reference ""lsx-tour-operator"" has been removed from the header template part. This could potentially cause issues if there are styles or scripts specifically tied to this theme in your WordPress setup. If the removal is intentional and you've ensured that no dependencies are broken, then it's fine. Otherwise, you might want to reconsider this change.
```diff
- <!-- wp:template-part {""slug"":""header-2-0"",""theme"":""lsx-tour-operator""} /-->
+ <!-- wp:template-part {""slug"":""header"",""area"":""header""} /-->
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,5 +1,5 @@
 <!-- wp:group {""style"":{""position"":{""type"":""sticky"",""top"":""0px""}},""layout"":{""type"":""default""}} -->
-<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header-2-0"",""theme"":""lsx-tour-operator""} /--></div>
+<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>","--- 

+++ 

@@ -42,7 +42,7 @@

 <div class=""wp-block-group""><!-- wp:group {""metadata"":{""name"":""Country""},""className"":""facts-country-query-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group facts-country-query-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/destinations-icon-black-20px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/destinations-icon-black-20px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""x-small""} -->
@@ -60,7 +60,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":61710,""width"":""20px"",""sizeSlug"":""full"",""linkDestination"":""none"",""className"":""is-resized""} -->
-<figure class=""wp-block-image size-full is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/06/image-1.png"" alt="""" class=""wp-image-61710"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-full is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" class=""wp-image-61710"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -76,7 +76,7 @@

 <!-- wp:group {""metadata"":{""name"":""Best Time to Visit""},""className"":""lsx-best-time-to-visit-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group lsx-best-time-to-visit-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":61709,""width"":""20px"",""sizeSlug"":""full"",""linkDestination"":""none"",""className"":""is-resized""} -->
-<figure class=""wp-block-image size-full is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/06/image.png"" alt="""" class=""wp-image-61709"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-full is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image.png"" alt="""" class=""wp-image-61709"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -200,7 +200,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912129,1.0,113,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/archive-review.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes in this hunk seem to be well structured and follow good practices. The code is readable, and the use of WordPress blocks is consistent. However, there are a few points that need attention:

1. **Hardcoded URLs**: The image URLs are hardcoded (lines 6, 42, 58, 74). This could lead to broken links if the images are moved or deleted from the server. It would be better to use a dynamic way to get these URLs, possibly storing them in a configuration file or database.

2. **Alt text for images**: The alt attributes for the images (lines 42, 58, 74) are empty. For accessibility reasons, it's important to provide meaningful alternative text for images.

3. **Missing translations**: The text content (line 22, 97) seems to be hardcoded in English. If your website needs to support multiple languages, consider using translation functions like `__()` or `_e()`.

4. **Inline styles**: There are many inline styles used throughout the template (for example, lines 6, 7, 8, etc.). While this isn't necessarily bad, it can make maintenance more difficult as you have to search through the HTML to find and update styles. Consider moving these to a CSS file where possible.

5. **No newline at end of file**: It's a common convention to end a file with a newline (line 113). This is because some utilities have problems processing the last line of a file if it isn't newline terminated.

Here are some suggested changes:

```diff
- <img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/Typetours-icon.png"" alt="""" style=""width:20px""/>
+ <img src=""<?php echo get_template_directory_uri(); ?>/images/Typetours-icon.png"" alt=""Tours Icon"" style=""width:20px""/>

- <p class=""has-text-align-center"">Hear what our travellers have to say about their experiences with us. From breathtaking destinations to seamless service, our guests share their stories and insights to help you plan your next adventure. Read through reviews and discover why so many travellers choose us for their unforgettable journeys.</p>
+ <p class=""has-text-align-center""><?php _e('Hear what our travellers have to say about their experiences with us. From breathtaking destinations to seamless service, our guests share their stories and insights to help you plan your next adventure. Read through reviews and discover why so many travellers choose us for their unforgettable journeys.', 'text-domain'); ?></p>

- <p class=""has-text-align-center"">Unfortunately, there are no reviews listed at the moment. Please check back soon as we regularly update our offerings.</p>
+ <p class=""has-text-align-center""><?php _e('Unfortunately, there are no reviews listed at the moment. Please check back soon as we regularly update our offerings.', 'text-domain'); ?></p>

- <div class=""wp-block-group alignwide"" style=""margin-top:var(--wp--preset--spacing--x-small);margin-bottom:var(--wp--preset--spacing--x-small)"">
+ <div class=""wp-block-group alignwide"">
```

Remember to replace `'text-domain'` with your actual text domain.

For the newline at the end of the file, simply add a newline after the last line of code.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<!-- wp:group {""style"":{""position"":{""type"":""sticky"",""top"":""0px""}},""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:cover {""url"":""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/TO-faq-bottom-banner-image.png"",""id"":60881,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""minHeight"":300,""minHeightUnit"":""px"",""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0;min-height:300px""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><img class=""wp-block-cover__image-background wp-image-60881"" alt="""" src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/TO-faq-bottom-banner-image.png"" data-object-fit=""cover""/><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull""><!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary-200"",""width"":""2px""},""top"":[],""right"":[],""left"":[]},""typography"":{""fontStyle"":""normal"",""fontWeight"":""600""}},""fontSize"":""large"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group has-large-font-size"" style=""border-bottom-color:var(--wp--preset--color--primary-200);border-bottom-width:2px;padding-top:10px;padding-bottom:10px;font-style:normal;font-weight:600""><!-- wp:query-title {""type"":""archive"",""textAlign"":""center"",""showPrefix"":false} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div></div>
+<!-- /wp:cover -->
+
+<!-- wp:group {""metadata"":{""name"":""Breadcrumbs""},""align"":""full"",""style"":{""spacing"":{""padding"":{""top"":""6px"",""bottom"":""6px"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}},""elements"":{""link"":{"":hover"":{""color"":{""text"":""var:preset|color|tertiary""}},""color"":{""text"":""var:preset|color|base""}}}},""backgroundColor"":""primary-900"",""textColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignfull has-base-color has-primary-900-background-color has-text-color has-background has-link-color"" style=""margin-top:0;margin-bottom:0;padding-top:6px;padding-right:var(--wp--preset--spacing--x-small);padding-bottom:6px;padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""align"":""wide"",""layout"":{""type"":""default""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:yoast-seo/breadcrumbs /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Archive Description""},""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|small"",""bottom"":""var:preset|spacing|small"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""padding-top:var(--wp--preset--spacing--small);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--small);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:group {""metadata"":{""name"":""Content""},""align"":""wide"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide""><!-- wp:paragraph {""align"":""center""} -->
+<p class=""has-text-align-center"">Hear what our travellers have to say about their experiences with us. From breathtaking destinations to seamless service, our guests share their stories and insights to help you plan your next adventure. Read through reviews and discover why so many travellers choose us for their unforgettable journeys.</p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Archive Content""},""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|x-small"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--x-small);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:query {""queryId"":1,""query"":{""perPage"":""9"",""pages"":""0"",""offset"":""0"",""postType"":""review"",""order"":""desc"",""orderBy"":""date"",""author"":"""",""search"":"""",""exclude"":[],""sticky"":"""",""inherit"":false,""parents"":[]},""align"":""wide"",""layout"":{""type"":""constrained"",""contentSize"":""""}} -->
+<div class=""wp-block-query alignwide""><!-- wp:post-template {""lock"":{""move"":false,""remove"":false},""align"":""wide"",""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":3,""minimumColumnWidth"":null}} -->
+<!-- wp:group {""metadata"":{""name"":""Review Card""},""className"":""is-style-shadow-sm"",""style"":{""spacing"":{""blockGap"":""0px"",""padding"":{""top"":""0px"",""bottom"":""0px"",""left"":""0px"",""right"":""0px""}},""border"":{""radius"":""8px""},""dimensions"":{""minHeight"":""""}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group is-style-shadow-sm has-base-background-color has-background"" style=""border-radius:8px;padding-top:0px;padding-right:0px;padding-bottom:0px;padding-left:0px""><!-- wp:post-featured-image {""isLink"":true,""aspectRatio"":""3/2"",""linkTarget"":""_blank"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}},""border"":{""radius"":{""topLeft"":""8px"",""topRight"":""8px""}}}} /-->
+
+<!-- wp:group {""metadata"":{""name"":""Content""},""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""padding"":{""top"":""10px"",""bottom"":""0px"",""left"":""10px"",""right"":""10px""}},""dimensions"":{""minHeight"":""97px""}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""min-height:97px;margin-top:0;margin-bottom:0;padding-top:10px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:group {""metadata"":{""name"":""Review Author""},""className"":""center-vertically"",""style"":{""dimensions"":{""minHeight"":""3rem""},""spacing"":{""padding"":{""top"":""6px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group center-vertically"" style=""min-height:3rem;padding-top:6px""><!-- wp:post-title {""textAlign"":""center"",""isLink"":true,""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""margin"":{""top"":""0"",""bottom"":""0""}}},""fontSize"":""small""} /--></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Review Information""},""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px"",""left"":""10px"",""right"":""10px""},""blockGap"":""0""},""border"":{""top"":{""color"":""var:preset|color|primary"",""width"":""2px""},""right"":[],""bottom"":{""color"":""var:preset|color|primary"",""width"":""2px""},""left"":[]}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--primary);border-top-width:2px;border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;padding-top:10px;padding-right:10px;padding-bottom:10px;padding-left:10px""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""name"":""Tours Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/Typetours-icon.png"" alt="""" style=""width:20px""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""> <strong>Tours: </strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""tour_to_review""}}}},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fit"",""flexSize"":null}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""metadata"":{""name"":""Rating Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>Rating: </strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""rating""}}}},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
+<div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""name"":""Accommodation Icon""}} -->
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/Typeaccommodation-icon.png"" alt="""" style=""width:20px""/></figure>
+<!-- /wp:image -->
+
+<!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""><strong>Accommodation: </strong></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group -->
+
+<!-- wp:paragraph {""metadata"":{""bindings"":{""content"":{""source"":""lsx/post-meta"",""args"":{""key"":""accommodation_to_review""}}}},""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
+<p style=""padding-top:2px;padding-bottom:2px""></p>
+<!-- /wp:paragraph --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+
+<!-- wp:group {""metadata"":{""name"":""Review Text""},""style"":{""spacing"":{""padding"":{""right"":""10px"",""left"":""10px"",""top"":""0px"",""bottom"":""0px""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group"" style=""padding-top:0px;padding-right:10px;padding-bottom:0px;padding-left:10px""><!-- wp:post-excerpt {""moreText"":""View More"",""excerptLength"":40,""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|contrast""}}}},""textColor"":""contrast""} /--></div>
+<!-- /wp:group --></div>
+<!-- /wp:group --></div>
+<!-- /wp:group -->
+<!-- /wp:post-template -->
+
+<!-- wp:query-no-results -->
+<!-- wp:paragraph {""align"":""center"",""placeholder"":""Add text or blocks that will display when the query returns no results.""} -->
+<p class=""has-text-align-center"">Unfortunately, there are no reviews listed at the moment. Please check back soon as we regularly update our offerings.</p>
+<!-- /wp:paragraph -->
+<!-- /wp:query-no-results -->
+
+<!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""margin"":{""top"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|x-small""}}},""layout"":{""type"":""constrained""}} -->
+<div class=""wp-block-group alignwide"" style=""margin-top:var(--wp--preset--spacing--x-small);margin-bottom:var(--wp--preset--spacing--x-small)""><!-- wp:query-pagination {""paginationArrow"":""chevron"",""align"":""wide"",""layout"":{""type"":""flex"",""justifyContent"":""space-between""}} -->
+<!-- wp:query-pagination-previous /-->
+
+<!-- wp:query-pagination-numbers {""midSize"":0} /-->
+
+<!-- wp:query-pagination-next /-->
+<!-- /wp:query-pagination --></div>
+<!-- /wp:group --></div>
+<!-- /wp:query --></div>
+<!-- /wp:group -->
+
+<!-- wp:template-part {""slug"":""footer"",""area"":""footer""} /-->","--- 

+++ 

@@ -2,8 +2,8 @@

 <div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>
 <!-- /wp:group -->
 
-<!-- wp:cover {""url"":""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/TO-faq-bottom-banner-image.png"",""id"":60881,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""minHeight"":300,""minHeightUnit"":""px"",""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
-<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0;min-height:300px""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><img class=""wp-block-cover__image-background wp-image-60881"" alt="""" src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/TO-faq-bottom-banner-image.png"" data-object-fit=""cover""/><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<!-- wp:cover {""url"":""#"",""id"":60881,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""minHeight"":300,""minHeightUnit"":""px"",""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0;min-height:300px""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><img class=""wp-block-cover__image-background wp-image-60881"" alt="""" src=""#"" data-object-fit=""cover""/><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group alignfull""><!-- wp:group {""style"":{""spacing"":{""padding"":{""top"":""10px"",""bottom"":""10px""}},""border"":{""bottom"":{""color"":""var:preset|color|primary-200"",""width"":""2px""},""top"":[],""right"":[],""left"":[]},""typography"":{""fontStyle"":""normal"",""fontWeight"":""600""}},""fontSize"":""large"",""layout"":{""type"":""default""}} -->
 <div class=""wp-block-group has-large-font-size"" style=""border-bottom-color:var(--wp--preset--color--primary-200);border-bottom-width:2px;padding-top:10px;padding-bottom:10px;font-style:normal;font-weight:600""><!-- wp:query-title {""type"":""archive"",""textAlign"":""center"",""showPrefix"":false} /--></div>
 <!-- /wp:group --></div>
@@ -39,7 +39,7 @@

 <div class=""wp-block-group"" style=""border-top-color:var(--wp--preset--color--primary);border-top-width:2px;border-bottom-color:var(--wp--preset--color--primary);border-bottom-width:2px;padding-top:10px;padding-right:10px;padding-bottom:10px;padding-left:10px""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""name"":""Tours Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/Typetours-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""#"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -55,7 +55,7 @@

 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""selfStretch"":""fit"",""flexSize"":null}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""metadata"":{""name"":""Rating Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -71,7 +71,7 @@

 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""10px"",""padding"":{""top"":""0"",""bottom"":""0""}}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""name"":""Accommodation Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lightspeedwp.dev/wp-content/uploads/2024/09/Typeaccommodation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typeaccommodation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912136,,2,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-accommodation.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The theme reference has been removed from the header template part. This could potentially cause issues if the `header` slug is not unique across all themes. If there are multiple header templates with the same slug in different themes, the wrong one might be loaded.
```diff
- <div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""theme"":""lsx-tour-operator"",""area"":""header""} /--></div>
+ <div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,9 +1,9 @@
 <!-- wp:group {""style"":{""position"":{""type"":""sticky"",""top"":""0px""}},""layout"":{""type"":""default""}} -->
-<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""theme"":""lsx-tour-operator"",""area"":""header""} /--></div>
+<div class=""wp-block-group""><!-- wp:template-part {""slug"":""header"",""area"":""header""} /--></div>","--- 

+++ 

@@ -69,7 +69,7 @@

 <div class=""wp-block-group fast-facts-wrapper""><!-- wp:group {""metadata"":{""name"":""Rating""},""className"":""lsx-rating-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -101,7 +101,7 @@

 <!-- wp:group {""metadata"":{""name"":""Number if Rooms""},""className"":""lsx-number-of-rooms-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-number-of-rooms-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -119,7 +119,7 @@

 <!-- wp:group {""metadata"":{""name"":""Single Supplement""},""className"":""lsx-single-supplement-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-single-supplement-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122733,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -137,7 +137,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check In Time""},""className"":""lsx-checkin-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkin-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -155,7 +155,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check Out Time""},""className"":""lsx-checkout-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkout-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -173,7 +173,7 @@

 <!-- wp:group {""metadata"":{""name"":""Minimum Child Age""},""className"":""lsx-minimum-child-age-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-minimum-child-age-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122719,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122719""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""/wp-content/plugins/tour-operator/assets/img/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -191,7 +191,7 @@

 <!-- wp:group {""metadata"":{""name"":""Destination to Accommodation""},""className"":""lsx-destination-to-accommodation-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-destination-to-accommodation-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -209,7 +209,7 @@

 <!-- wp:group {""metadata"":{""name"":""Spoken Languages""},""className"":""lsx-spoken-languages-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-spoken-languages-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -227,7 +227,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/06/image-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -243,7 +243,7 @@

 <!-- wp:group {""metadata"":{""name"":""Accommodation Type""},""className"":""lsx-accommodation-type-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-accommodation-type-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -259,7 +259,7 @@

 <!-- wp:group {""className"":""lsx-suggested-visitor-types-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-suggested-visitor-types-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -277,7 +277,7 @@

 <!-- wp:group {""metadata"":{""name"":""Special Interests""},""className"":""lsx-special-interests-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-special-interests-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122726,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -325,7 +325,7 @@

 <div class=""wp-block-group alignwide has-base-background-color has-background"" style=""border-radius:8px""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""left"":""10px""}}}} -->
 <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""30%""} -->
 <div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:image {""id"":43376,""aspectRatio"":""4/3"",""scale"":""cover"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""unit-image is-style-default"",""style"":{""border"":{""radius"":{""topLeft"":""8px"",""bottomLeft"":""8px""}}}} -->
-<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
+<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""/wp-content/plugins/tour-operator/assets/img/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
 <!-- /wp:image --></div>
 <!-- /wp:column -->
 
@@ -354,7 +354,7 @@

 <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -370,7 +370,7 @@

 <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -542,7 +542,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912140,,5,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-accommodation.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The dimRatio of the cover block has been changed from 70 to 60. This will make the overlay slightly less opaque, which may affect readability of text on the cover image.
```diff
- <!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
+ <!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":60,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
-<!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":70,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->
-<div class=""wp-block-cover alignfull"" style=""margin-top:0;margin-bottom:0""><span aria-hidden=""true"" class=""wp-block-cover__background has-secondary-900-background-color has-background-dim-70 has-background-dim""></span><div class=""wp-block-cover__inner-container""><!-- wp:group {""align"":""full"",""layout"":{""type"":""constrained""}} -->
+<!-- wp:cover {""useFeaturedImage"":true,""dimRatio"":60,""overlayColor"":""secondary-900"",""isUserOverlayColor"":true,""metadata"":{""name"":""Hero""},""align"":""full"",""style"":{""spacing"":{""blockGap"":""0"",""margin"":{""top"":""0"",""bottom"":""0""}}}} -->","--- 

+++ 

@@ -69,7 +69,7 @@

 <div class=""wp-block-group fast-facts-wrapper""><!-- wp:group {""metadata"":{""name"":""Rating""},""className"":""lsx-rating-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -101,7 +101,7 @@

 <!-- wp:group {""metadata"":{""name"":""Number if Rooms""},""className"":""lsx-number-of-rooms-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-number-of-rooms-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -119,7 +119,7 @@

 <!-- wp:group {""metadata"":{""name"":""Single Supplement""},""className"":""lsx-single-supplement-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-single-supplement-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122733,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -137,7 +137,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check In Time""},""className"":""lsx-checkin-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkin-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -155,7 +155,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check Out Time""},""className"":""lsx-checkout-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkout-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -173,7 +173,7 @@

 <!-- wp:group {""metadata"":{""name"":""Minimum Child Age""},""className"":""lsx-minimum-child-age-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-minimum-child-age-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122719,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122719""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""/wp-content/plugins/tour-operator/assets/img/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -191,7 +191,7 @@

 <!-- wp:group {""metadata"":{""name"":""Destination to Accommodation""},""className"":""lsx-destination-to-accommodation-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-destination-to-accommodation-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -209,7 +209,7 @@

 <!-- wp:group {""metadata"":{""name"":""Spoken Languages""},""className"":""lsx-spoken-languages-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-spoken-languages-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -227,7 +227,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/06/image-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -243,7 +243,7 @@

 <!-- wp:group {""metadata"":{""name"":""Accommodation Type""},""className"":""lsx-accommodation-type-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-accommodation-type-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -259,7 +259,7 @@

 <!-- wp:group {""className"":""lsx-suggested-visitor-types-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-suggested-visitor-types-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -277,7 +277,7 @@

 <!-- wp:group {""metadata"":{""name"":""Special Interests""},""className"":""lsx-special-interests-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-special-interests-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122726,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -325,7 +325,7 @@

 <div class=""wp-block-group alignwide has-base-background-color has-background"" style=""border-radius:8px""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""left"":""10px""}}}} -->
 <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""30%""} -->
 <div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:image {""id"":43376,""aspectRatio"":""4/3"",""scale"":""cover"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""unit-image is-style-default"",""style"":{""border"":{""radius"":{""topLeft"":""8px"",""bottomLeft"":""8px""}}}} -->
-<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
+<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""/wp-content/plugins/tour-operator/assets/img/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
 <!-- /wp:image --></div>
 <!-- /wp:column -->
 
@@ -354,7 +354,7 @@

 <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -370,7 +370,7 @@

 <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -542,7 +542,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912143,,25,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-accommodation.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The width of the first column in the columns block has been reduced from 70% to 65%. This will give more space to the second column, but it may also make the content in the first column appear more condensed.
```diff
- <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""70%""} -->
+ <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""65%""} -->
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-<div class=""wp-block-column"" style=""flex-basis:70%""><!-- wp:post-content {""align"":""wide"",""layout"":{""type"":""default""}} /-->
+<!-- wp:group {""tagName"":""main"",""metadata"":{""name"":""Description \u0026 Fast Facts""},""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|medium"",""bottom"":""var:preset|spacing|medium"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""},""margin"":{""top"":""0"",""bottom"":""0""}}},""backgroundColor"":""base"",""layout"":{""type"":""constrained""}} -->
+<main class=""wp-block-group alignwide has-base-background-color has-background"" style=""margin-top:0;margin-bottom:0;padding-top:var(--wp--preset--spacing--medium);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--medium);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""top"":""var:preset|spacing|small"",""left"":""var:preset|spacing|medium""}}}} -->
+<div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""65%""} -->","--- 

+++ 

@@ -69,7 +69,7 @@

 <div class=""wp-block-group fast-facts-wrapper""><!-- wp:group {""metadata"":{""name"":""Rating""},""className"":""lsx-rating-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -101,7 +101,7 @@

 <!-- wp:group {""metadata"":{""name"":""Number if Rooms""},""className"":""lsx-number-of-rooms-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-number-of-rooms-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -119,7 +119,7 @@

 <!-- wp:group {""metadata"":{""name"":""Single Supplement""},""className"":""lsx-single-supplement-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-single-supplement-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122733,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -137,7 +137,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check In Time""},""className"":""lsx-checkin-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkin-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -155,7 +155,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check Out Time""},""className"":""lsx-checkout-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkout-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -173,7 +173,7 @@

 <!-- wp:group {""metadata"":{""name"":""Minimum Child Age""},""className"":""lsx-minimum-child-age-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-minimum-child-age-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122719,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122719""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""/wp-content/plugins/tour-operator/assets/img/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -191,7 +191,7 @@

 <!-- wp:group {""metadata"":{""name"":""Destination to Accommodation""},""className"":""lsx-destination-to-accommodation-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-destination-to-accommodation-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -209,7 +209,7 @@

 <!-- wp:group {""metadata"":{""name"":""Spoken Languages""},""className"":""lsx-spoken-languages-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-spoken-languages-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -227,7 +227,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/06/image-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -243,7 +243,7 @@

 <!-- wp:group {""metadata"":{""name"":""Accommodation Type""},""className"":""lsx-accommodation-type-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-accommodation-type-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -259,7 +259,7 @@

 <!-- wp:group {""className"":""lsx-suggested-visitor-types-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-suggested-visitor-types-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -277,7 +277,7 @@

 <!-- wp:group {""metadata"":{""name"":""Special Interests""},""className"":""lsx-special-interests-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-special-interests-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122726,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -325,7 +325,7 @@

 <div class=""wp-block-group alignwide has-base-background-color has-background"" style=""border-radius:8px""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""left"":""10px""}}}} -->
 <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""30%""} -->
 <div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:image {""id"":43376,""aspectRatio"":""4/3"",""scale"":""cover"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""unit-image is-style-default"",""style"":{""border"":{""radius"":{""topLeft"":""8px"",""bottomLeft"":""8px""}}}} -->
-<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
+<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""/wp-content/plugins/tour-operator/assets/img/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
 <!-- /wp:image --></div>
 <!-- /wp:column -->
 
@@ -354,7 +354,7 @@

 <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -370,7 +370,7 @@

 <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -542,7 +542,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912144,,61,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-accommodation.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The width of the second column in the columns block has been increased from 30% to 35%. This will give more space to the content in this column, but it may also make the content appear less condensed.
```diff
- <!-- wp:column {""width"":""30%"",""style"":{""spacing"":{""blockGap"":""0""}}} -->
+ <!-- wp:column {""width"":""35%"",""style"":{""spacing"":{""blockGap"":""0""}}} -->
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-<!-- wp:column {""width"":""30%"",""style"":{""spacing"":{""blockGap"":""0""}}} -->
-<div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:group {""align"":""wide"",""style"":{""spacing"":{""padding"":{""top"":""var:preset|spacing|x-small"",""bottom"":""var:preset|spacing|x-small"",""left"":""var:preset|spacing|x-small"",""right"":""var:preset|spacing|x-small""}},""border"":{""radius"":""8px""}},""backgroundColor"":""primary-bg"",""layout"":{""type"":""default""}} -->
-<div class=""wp-block-group alignwide has-primary-bg-background-color has-background"" style=""border-radius:8px;padding-top:var(--wp--preset--spacing--x-small);padding-right:var(--wp--preset--spacing--x-small);padding-bottom:var(--wp--preset--spacing--x-small);padding-left:var(--wp--preset--spacing--x-small)""><!-- wp:heading {""level"":3} -->
+<!-- wp:column {""width"":""35%"",""style"":{""spacing"":{""blockGap"":""0""}}} -->","--- 

+++ 

@@ -69,7 +69,7 @@

 <div class=""wp-block-group fast-facts-wrapper""><!-- wp:group {""metadata"":{""name"":""Rating""},""className"":""lsx-rating-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -101,7 +101,7 @@

 <!-- wp:group {""metadata"":{""name"":""Number if Rooms""},""className"":""lsx-number-of-rooms-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-number-of-rooms-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -119,7 +119,7 @@

 <!-- wp:group {""metadata"":{""name"":""Single Supplement""},""className"":""lsx-single-supplement-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-single-supplement-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122733,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -137,7 +137,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check In Time""},""className"":""lsx-checkin-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkin-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -155,7 +155,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check Out Time""},""className"":""lsx-checkout-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkout-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -173,7 +173,7 @@

 <!-- wp:group {""metadata"":{""name"":""Minimum Child Age""},""className"":""lsx-minimum-child-age-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-minimum-child-age-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122719,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122719""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""/wp-content/plugins/tour-operator/assets/img/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -191,7 +191,7 @@

 <!-- wp:group {""metadata"":{""name"":""Destination to Accommodation""},""className"":""lsx-destination-to-accommodation-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-destination-to-accommodation-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -209,7 +209,7 @@

 <!-- wp:group {""metadata"":{""name"":""Spoken Languages""},""className"":""lsx-spoken-languages-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-spoken-languages-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -227,7 +227,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/06/image-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -243,7 +243,7 @@

 <!-- wp:group {""metadata"":{""name"":""Accommodation Type""},""className"":""lsx-accommodation-type-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-accommodation-type-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -259,7 +259,7 @@

 <!-- wp:group {""className"":""lsx-suggested-visitor-types-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-suggested-visitor-types-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -277,7 +277,7 @@

 <!-- wp:group {""metadata"":{""name"":""Special Interests""},""className"":""lsx-special-interests-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-special-interests-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122726,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -325,7 +325,7 @@

 <div class=""wp-block-group alignwide has-base-background-color has-background"" style=""border-radius:8px""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""left"":""10px""}}}} -->
 <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""30%""} -->
 <div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:image {""id"":43376,""aspectRatio"":""4/3"",""scale"":""cover"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""unit-image is-style-default"",""style"":{""border"":{""radius"":{""topLeft"":""8px"",""bottomLeft"":""8px""}}}} -->
-<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
+<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""/wp-content/plugins/tour-operator/assets/img/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
 <!-- /wp:image --></div>
 <!-- /wp:column -->
 
@@ -354,7 +354,7 @@

 <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -370,7 +370,7 @@

 <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -542,7 +542,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1893912146,,72,c5479a61c7b8e9d8fa90c7ff6ca98ce116223f36,e8eacd5dd2d29bfe380c7a4bf508da6d9622fc14,templates/single-accommodation.html,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The image URL has been changed. Make sure that the new URL is correct and the image exists at that location.
```diff
- <figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lsx.design/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+ <figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+<div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tour-operator.lsx.design/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>","--- 

+++ 

@@ -69,7 +69,7 @@

 <div class=""wp-block-group fast-facts-wrapper""><!-- wp:group {""metadata"":{""name"":""Rating""},""className"":""lsx-rating-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-rating-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/rating-icon-TO-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -101,7 +101,7 @@

 <!-- wp:group {""metadata"":{""name"":""Number if Rooms""},""className"":""lsx-number-of-rooms-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-number-of-rooms-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/TO-accommodation-rooms-icon-black-52px.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -119,7 +119,7 @@

 <!-- wp:group {""metadata"":{""name"":""Single Supplement""},""className"":""lsx-single-supplement-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-single-supplement-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122733,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/single-supplement-icon-black-52px-1.svg"" alt="""" class=""wp-image-122733"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -137,7 +137,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check In Time""},""className"":""lsx-checkin-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkin-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -155,7 +155,7 @@

 <!-- wp:group {""metadata"":{""name"":""Check Out Time""},""className"":""lsx-checkout-time-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-checkout-time-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122720,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122720""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122720""><img src=""/wp-content/plugins/tour-operator/assets/img/check-in-check-out-time-icon-black-52px-1.svg"" alt="""" class=""wp-image-122720"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -173,7 +173,7 @@

 <!-- wp:group {""metadata"":{""name"":""Minimum Child Age""},""className"":""lsx-minimum-child-age-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-minimum-child-age-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122719,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""wp-image-122719""} -->
-<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized wp-image-122719""><img src=""/wp-content/plugins/tour-operator/assets/img/minimum-child-age-icon-black-52px-1.svg"" alt="""" class=""wp-image-122719"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -191,7 +191,7 @@

 <!-- wp:group {""metadata"":{""name"":""Destination to Accommodation""},""className"":""lsx-destination-to-accommodation-wrapper"",""style"":{""spacing"":{""padding"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-destination-to-accommodation-wrapper"" style=""padding-top:0;padding-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/Typelocation-icon.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -209,7 +209,7 @@

 <!-- wp:group {""metadata"":{""name"":""Spoken Languages""},""className"":""lsx-spoken-languages-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-spoken-languages-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/spoken-languages-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -227,7 +227,7 @@

 <!-- wp:group {""metadata"":{""name"":""Travel Styles""},""className"":""lsx-travel-style-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""},""layout"":{""type"":""constrained""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-travel-style-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/06/image-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/image-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""fontSize"":""x-small""} -->
@@ -243,7 +243,7 @@

 <!-- wp:group {""metadata"":{""name"":""Accommodation Type""},""className"":""lsx-accommodation-type-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-accommodation-type-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -259,7 +259,7 @@

 <!-- wp:group {""className"":""lsx-suggested-visitor-types-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-suggested-visitor-types-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/friendly-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -277,7 +277,7 @@

 <!-- wp:group {""metadata"":{""name"":""Special Interests""},""className"":""lsx-special-interests-wrapper"",""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group lsx-special-interests-wrapper""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""id"":122726,""width"":""20px"",""sizeSlug"":""large"",""linkDestination"":""none""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/11/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/special-interests-icon-black-52px-1.svg"" alt="""" class=""wp-image-122726"" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -325,7 +325,7 @@

 <div class=""wp-block-group alignwide has-base-background-color has-background"" style=""border-radius:8px""><!-- wp:columns {""align"":""wide"",""style"":{""spacing"":{""blockGap"":{""left"":""10px""}}}} -->
 <div class=""wp-block-columns alignwide""><!-- wp:column {""width"":""30%""} -->
 <div class=""wp-block-column"" style=""flex-basis:30%""><!-- wp:image {""id"":43376,""aspectRatio"":""4/3"",""scale"":""cover"",""sizeSlug"":""large"",""linkDestination"":""none"",""className"":""unit-image is-style-default"",""style"":{""border"":{""radius"":{""topLeft"":""8px"",""bottomLeft"":""8px""}}}} -->
-<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
+<figure class=""wp-block-image size-large has-custom-border unit-image is-style-default""><img src=""/wp-content/plugins/tour-operator/assets/img/dark-grey-image-placeholder-990x1024.png"" alt="""" class=""wp-image-43376"" style=""border-top-left-radius:8px;border-bottom-left-radius:8px;aspect-ratio:4/3;object-fit:cover""/></figure>
 <!-- /wp:image --></div>
 <!-- /wp:column -->
 
@@ -354,7 +354,7 @@

 <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large""} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/accommodation-type-TO-icon-black-20px-2.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -370,7 +370,7 @@

 <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
 <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->
 <div class=""wp-block-group""><!-- wp:image {""width"":""20px"",""sizeSlug"":""large"",""metadata"":{""bindings"":{""__default"":{""source"":""core/pattern-overrides""}},""name"":""From Price Icon""}} -->
-<figure class=""wp-block-image size-large is-resized""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
+<figure class=""wp-block-image size-large is-resized""><img src=""/wp-content/plugins/tour-operator/assets/img/from-price-TO-icon-black-20px-1.png"" alt="""" style=""width:20px""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:paragraph {""style"":{""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}}} -->
@@ -542,7 +542,7 @@

 <div class=""wp-block-query alignwide""><!-- wp:post-template {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|small""}},""layout"":{""type"":""grid"",""columnCount"":2}} -->
 <!-- wp:group {""style"":{""spacing"":{""blockGap"":""var:preset|spacing|x-small""}},""layout"":{""type"":""constrained""}} -->
 <div class=""wp-block-group""><!-- wp:image {""sizeSlug"":""large"",""align"":""center""} -->
-<figure class=""wp-block-image aligncenter size-large""><img src=""https://tourpress.pro/wp-content/uploads/2024/09/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
+<figure class=""wp-block-image aligncenter size-large""><img src=""/wp-content/plugins/tour-operator/assets/img/review-testominal-quote-mark-TO-tertiary-32px.png"" alt=""""/></figure>
 <!-- /wp:image -->
 
 <!-- wp:post-excerpt {""textAlign"":""center"",""showMoreOnNewLine"":false,""excerptLength"":40} /-->"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322221952,28.0,68,7458fcfe4a56e731a06a0bfb06377e3ffcf7eff2,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `read` method has been significantly refactored. The logic seems sound, but there are a few potential issues:

1. In line 35, the existence of `config_file_dir` is asserted, but no error handling is provided if the assertion fails. This could lead to an abrupt termination of the program. It would be better to raise a custom exception with a clear message.

2. In lines 47-53, repeated parts are dropped from the data. However, it's not clear why this is necessary or what the implications might be. If dropping these parts could result in loss of important information, it might be better to handle duplicates differently.

3. In line 81, a KeyError is raised if `parts` is None. However, `parts` is initialized as an empty list in line 72 and doesn't seem to be set to None anywhere in the code. Therefore, this condition will never be met.

Here are some suggested changes:

```diff
- 35: assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+ 35: if not config_file_dir.exists():
+     raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")

- 47: unique_parts, unique_idxs = np.unique(parts, return_index=True)
- 48: repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
- 49: if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
- 50:     init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
- 51:     rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
- 52:     keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
- 53:     data = data.iloc[:, keep_part_col_idxs]
- 54:     parts = unique_parts
+ 47: # Handle duplicates differently if necessary

- 81: if parts is None:
- 82:     raise KeyError(f""Cannot find bodyparts in {file}."") from err
+ 81: if not parts:
+     raise ValueError(f""No bodyparts found in {file}."") from err
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def read(
+        self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
+    ) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
         assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
-        
+
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
         columns = [""class"", ""class_likelihood""]
         for part in parts:
             columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
         self.columns = columns
         data = super().read(file)
-        
+
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
         # Set new columns, and reformat `data`.
         n_parts = len(parts)
-        part_data_list = [None] * n_parts
+        part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-            part_data = data[part_columns]
+            part_data = pd.DataFrame(data[part_columns])
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()","--- 

+++ 

@@ -32,7 +32,8 @@

         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
-        assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -78,7 +79,7 @@

                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                if parts is None:
+                if not parts:
                     raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
@@ -89,13 +90,14 @@

 ) -> Path:
     """"""Returns the config file from a model's config directory.""""""
     if config_file_names is None:
-        config_file_names = [""confmap_config.json""]
+        config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
     config_file = None
     for f in config_file_names:
         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322221953,86.0,92,7458fcfe4a56e731a06a0bfb06377e3ffcf7eff2,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `get_config_file` function seems to be incomplete. The code snippet ends abruptly and doesn't return anything. This could lead to a runtime error if the function is called. Please complete this function.


<!-- This is an auto-generated reply by OSS CodeRabbit -->"," def get_config_file(
     config_file_dir: Path,
-    config_file_names: List[str]=[
-        ""confmap_config.json"",  # SLEAP (add others for other trackers to this list)
-    ],
-):
+    config_file_names: None | list[str] = None,
+) -> Path:
     """"""Returns the config file from a model's config directory.""""""
+    if config_file_names is None:
+        config_file_names = [""confmap_config.json""]","--- 

+++ 

@@ -32,7 +32,8 @@

         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
-        assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -78,7 +79,7 @@

                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                if parts is None:
+                if not parts:
                     raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
@@ -89,13 +90,14 @@

 ) -> Path:
     """"""Returns the config file from a model's config directory.""""""
     if config_file_names is None:
-        config_file_names = [""confmap_config.json""]
+        config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
     config_file = None
     for f in config_file_names:
         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322223358,28.0,69,23c8872eb4f1d90413d3c5a6ef7fe99542df1572,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a potential issue with the `read` method. The code assumes that the `anchor_parts` and `part_names` are disjoint sets, which may not always be the case. If there are overlaps, this could lead to unexpected behavior when dropping repeat parts. To address this, you should ensure that the `parts` list only contains unique elements after combining `anchor_parts` and `part_names`.

```diff
-         parts = [util.find_nested_key(heads, ""anchor_part"")]
-         parts += util.find_nested_key(heads, ""part_names"")
+         anchor_parts = util.find_nested_key(heads, ""anchor_part"")
+         part_names = util.find_nested_key(heads, ""part_names"")
+         parts = list(set(anchor_parts + part_names))
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def read(
+        self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
+    ) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
         assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
-        
+
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
         columns = [""class"", ""class_likelihood""]
         for part in parts:
             columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
         self.columns = columns
         data = super().read(file)
-        
+
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
         # Set new columns, and reformat `data`.
         n_parts = len(parts)
-        part_data_list = [None] * n_parts
+        part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-            part_data = data[part_columns]
+            part_data = pd.DataFrame(data[part_columns])
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 ","--- 

+++ 

@@ -32,7 +32,8 @@

         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
-        assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -78,7 +79,7 @@

                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                if parts is None:
+                if not parts:
                     raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
@@ -95,7 +96,8 @@

         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322223359,70.0,83,23c8872eb4f1d90413d3c5a6ef7fe99542df1572,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `get_bodyparts` method, the error handling for KeyError could be improved. Currently, if a KeyError is raised, it checks if `parts` is None and then raises another KeyError. However, since `parts` is initialized as an empty list, it will never be None, so the error message will never be raised. Instead, you should check if `parts` is empty.

```diff
-         if parts is None:
+         if not parts:
              raise KeyError(f""Cannot find bodyparts in {file}."") from err
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    def get_bodyparts(self, file: Path) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
-        parts = None
+        parts = []
         with open(file) as f:
             config = json.load(f)
         if file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
-                parts = util.find_nested_key(heads, ""part_names"")
+                parts = [util.find_nested_key(heads, ""anchor_part"")]
+                parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                if parts is None:
+                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts","--- 

+++ 

@@ -32,7 +32,8 @@

         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
-        assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
 
@@ -78,7 +79,7 @@

                 parts = [util.find_nested_key(heads, ""anchor_part"")]
                 parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                if parts is None:
+                if not parts:
                     raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts
 
@@ -95,7 +96,8 @@

         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
     return config_file
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322953013,3.0,10,aaacd53f1995fd20ab2e9646ad63767707398718,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import of `json` and `numpy as np` are new additions to the code. However, the import of `List, Union` from `typing` has been removed. If these types are not used elsewhere in the code, this change is fine. Otherwise, it might cause issues.
```diff
- from typing import List, Union
+ import json
+ import numpy as np
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import json
+from pathlib import Path
 
+import numpy as np
 import pandas as pd
 
-from aeon import util
 import aeon.io.reader as _reader
+from aeon import util",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322953017,28.0,70,aaacd53f1995fd20ab2e9646ad63767707398718,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the `read` method seem to improve its functionality by adding error handling for file not found exceptions and dealing with potential redundancies between 'anchor_parts' and 'part_names'. However, there's a potential issue with the type hinting for the `ceph_proc_dir` parameter. It's now accepting either a string or a `Path` object, but the old code only accepted a `Path` object. This could potentially lead to type errors if the function is called with a string argument elsewhere in the codebase where a `Path` object is expected.
```diff
- def read(self, file: Path, ceph_proc_dir: Path=Path(""/ceph/aeon/aeon/data/processed"")) -> pd.DataFrame:
+ def read(
+     self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
+ ) -> pd.DataFrame:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def read(
+        self, file: Path, ceph_proc_dir: str | Path = ""/ceph/aeon/aeon/data/processed""
+    ) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
         model_dir = Path(file.stem.replace(""_"", ""/"")).parent
         config_file_dir = ceph_proc_dir / model_dir
-        assert config_file_dir.exists(), f""Cannot find model dir {config_file_dir}""
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
         config_file = get_config_file(config_file_dir)
         parts = self.get_bodyparts(config_file)
-        
+
         # Using bodyparts, assign column names to Harp register values, and read data in default format.
         columns = [""class"", ""class_likelihood""]
         for part in parts:
             columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
         self.columns = columns
         data = super().read(file)
-        
+
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
         # Set new columns, and reformat `data`.
         n_parts = len(parts)
-        part_data_list = [None] * n_parts
+        part_data_list = [pd.DataFrame()] * n_parts
         new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
-            part_data = data[part_columns]
+            part_data = pd.DataFrame(data[part_columns])
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
             part_data_list[i] = part_data
         new_data = pd.concat(part_data_list)
         return new_data.sort_index()
 ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322953018,71.0,84,aaacd53f1995fd20ab2e9646ad63767707398718,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `get_bodyparts` method has been updated to return a list of bodyparts instead of `None`. This seems like a good change as it makes the function more useful. However, the type hinting for the return value has been removed. It would be better to keep the type hinting for clarity and to prevent potential type errors.
```diff
- def get_bodyparts(self, file: Path) -> Union[None, List[str]]:
+ def get_bodyparts(self, file: Path) -> list[str]:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def get_bodyparts(self, file: Path) -> list[str]:
         """"""Returns a list of bodyparts from a model's config file.""""""
-        parts = None
+        parts = []
         with open(file) as f:
             config = json.load(f)
         if file.stem == ""confmap_config"":  # SLEAP
             try:
                 heads = config[""model""][""heads""]
-                parts = util.find_nested_key(heads, ""part_names"")
+                parts = [util.find_nested_key(heads, ""anchor_part"")]
+                parts += util.find_nested_key(heads, ""part_names"")
             except KeyError as err:
-                raise KeyError(f""Cannot find bodyparts in {file}."") from err
+                if not parts:
+                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
         return parts",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1322953020,87.0,100,aaacd53f1995fd20ab2e9646ad63767707398718,3b53953e23229de13a3c1a3cb4770b871b9b23e2,aeon/schema/social.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `get_config_file` function now allows for custom configuration file names. This is a good change as it makes the function more flexible. However, the type hinting for the `config_file_names` parameter has been removed. It would be better to keep the type hinting for clarity and to prevent potential type errors.
```diff
- def get_config_file(
-     config_file_dir: Path,
-    config_file_names: List[str]=[
-        ""confmap_config.json"",  # SLEAP (add others for other trackers to this list)
-    ],
- ):
+ def get_config_file(
+     config_file_dir: Path,
+    config_file_names: None | list[str] = None,
+ ) -> Path:
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," def get_config_file(
     config_file_dir: Path,
-    config_file_names: List[str]=[
-        ""confmap_config.json"",  # SLEAP (add others for other trackers to this list)
-    ],
-):
+    config_file_names: None | list[str] = None,
+) -> Path:
     """"""Returns the config file from a model's config directory.""""""
+    if config_file_names is None:
+        config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
     config_file = None
     for f in config_file_names:
         if (config_file_dir / f).exists():
             config_file = config_file_dir / f
             break
-    assert config_file is not None, f""Cannot find config file in {config_file_dir}""
+    if config_file is None:
+         raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891851974,72.0,74,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from a fixed limit to a parameterized limit in the `lsx_to.readMoreSet` function is a good improvement for flexibility. However, it's important to ensure that all calls to this function have been updated with the new parameter across the codebase. If not, it could lead to unexpected behavior.

```diff
- lsx_to.readMoreSet = function( button, contentWrapper ) {
+ lsx_to.readMoreSet = function( button, contentWrapper, limit = 1 ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	lsx_to.readMoreSet = function( button, contentWrapper, limit = 1 ) {
 		if ( 0 < contentWrapper.length ) {
 			if ( 1 < contentWrapper.children().length ) {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891851989,118.0,126,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes here reflect the update in the function signature of `lsx_to.readMoreSet`. The hardcoded limit of '2' is now passed as an argument. This seems fine as long as the limit '2' is the desired behavior in all these cases. 

```diff
- lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
+ lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content'), 2 );
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content'), 2 );
 		} );
 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
 			if ( $( this ).find('a').hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
+				lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content'), 2 );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891852005,431.0,443,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `render_units_block` checks if `$parsed_block['blockName']` and `$parsed_block['attrs']` are set but does not handle the case where they might be empty. This could lead to unexpected behavior. Consider adding a check for empty values as well.

```diff
- 		if ( ! isset( $parsed_block['blockName'] ) || ! isset( $parsed_block['attrs'] )  ) {
+ 		if ( ! isset( $parsed_block['blockName'] ) || empty($parsed_block['blockName']) || ! isset( $parsed_block['attrs'] ) || empty($parsed_block['attrs']) ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	/**
+	 * Renders the units block with custom content.
+	 *
+	 * This function processes the block content by checking if it belongs to a specific
+	 * custom block variation and then iteratively builds the units content based on
+	 * predefined fields and templates. It returns the final rendered block content.
+	 *
+	 * @param string $block_content The original content of the block.
+	 * @param array  $parsed_block  Parsed data for the block, including type and attributes.
+	 * @param object $block_obj     Block object instance for the current block being processed.
+	 * 
+	 * @return string Returns the modified block content after processing units data.
+	 */",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891852019,624.0,636,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, consider adding a check for empty values in the `render_gallery_block` function.

```diff
- 		if ( ! isset( $parsed_block['blockName'] ) || ! isset( $parsed_block['attrs'] )  ) {
+ 		if ( ! isset( $parsed_block['blockName'] ) || empty($parsed_block['blockName']) || ! isset( $parsed_block['attrs'] ) || empty($parsed_block['attrs']) ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	/**
+	 * Renders the gallery block with custom content.
+	 *
+	 * This function processes the block content by checking if it belongs to a specific
+	 * custom block variation and then iteratively builds the gallery content based on
+	 * predefined fields and templates. It returns the final rendered block content.
+	 *
+	 * @param string $block_content The original content of the block.
+	 * @param array  $parsed_block  Parsed data for the block, including type and attributes.
+	 * @param object $block_obj     Block object instance for the current block being processed.
+	 * 
+	 * @return string Returns the modified block content after processing gallery data.
+	 */",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891852031,729.0,741,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Again, consider adding a check for empty values in the `render_map_block` function.

```diff
- 		if ( ! isset( $parsed_block['blockName'] ) || ! isset( $parsed_block['attrs'] )  ) {
+ 		if ( ! isset( $parsed_block['blockName'] ) || empty($parsed_block['blockName']) || ! isset( $parsed_block['attrs'] ) || empty($parsed_block['attrs']) ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	/**
+	 * Renders the map block with custom content.
+	 *
+	 * This function processes the block content by checking if it belongs to a specific
+	 * custom block variation and then iteratively builds the map content based on
+	 * predefined fields and templates. It returns the final rendered block content.
+	 *
+	 * @param string $block_content The original content of the block.
+	 * @param array  $parsed_block  Parsed data for the block, including type and attributes.
+	 * @param object $block_obj     Block object instance for the current block being processed.
+	 * 
+	 * @return string Returns the modified block content after processing map data.
+	 */",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1891852045,806.0,842,e07f7293e87cb95a007da15c366021ed7b5b4af2,3687e89d0a727f382880bd98c31d3e83403ea4c0,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `render_permalink_block` function, you're using `preg_replace` to replace `#permalink` with the actual URL. This could potentially replace unintended parts of the content if `#permalink` appears elsewhere. Consider using a more specific pattern or another method to replace the placeholder.

Also, you're checking if `$parsed_block['attrs']['metadata']['name']` is set but not if it's empty. Consider adding a check for empty values as well.

```diff
- 		if ( ! isset( $parsed_block['attrs']['metadata']['name'] ) ) {
+ 		if ( ! isset( $parsed_block['attrs']['metadata']['name'] ) || empty($parsed_block['attrs']['metadata']['name']) ) {
```

```diff
- 		$pattern       = '/#permalink/s';
+ 		$pattern       = '/^#permalink$/s'; // Adjust the pattern to match your needs
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	/**
+	 * Renders the p block with custom content.
+	 *
+	 * @param string $block_content The original content of the block.
+	 * @param array  $parsed_block  Parsed data for the block, including type and attributes.
+	 * @param object $block_obj     Block object instance for the current block being processed.
+	 * 
+	 * @return string Returns block with the permalink added.
+	 */
+	public function render_permalink_block( $block_content, $parsed_block, $block_obj ) {
+		// Determine if this is the custom block variation.
+		if ( ! isset( $parsed_block['blockName'] ) || ! isset( $parsed_block['attrs'] )  ) {
+			return $block_content;
+		}
+		$allowed_blocks = array(
+			'core/button'
+		);
+
+		if ( ! in_array( $parsed_block['blockName'], $allowed_blocks, true ) ) {
+			return $block_content; 
+		}
+
+		if ( ! isset( $parsed_block['attrs']['metadata']['name'] ) ) {
+			return $block_content;
+		}
+
+		if ( 'Permalink' !== $parsed_block['attrs']['metadata']['name'] ) {
+			return $block_content;
+		}
+
+		$url           = get_permalink();
+		$pattern       = '/#permalink/s';
+		$block_content = preg_replace( $pattern, $url, $block_content );
+
+		return $block_content;
+	}",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418180,,165,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made here is a potential issue. The original code was accessing the ""Pose"" attribute of the device object, but the new code is not. This could lead to unexpected behavior if the Pose attribute is required later in the code.
```diff
-        stream_reader = getattr(devices_schema, device_name).Pose
+        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","             ),
         )
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
+        stream_reader = getattr(devices_schema, device_name).Pose",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418191,113.0,116,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/populate/worker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement `from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview` is inside the function `get_workflow_operation_overview()`. This could potentially affect performance as the module would be imported every time the function is called. It's a best practice to place all import statements at the top of the file.
```diff
-  def get_workflow_operation_overview():
-     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
- 
-     return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
+  from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
+
+  def get_workflow_operation_overview():
+     return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+
 def get_workflow_operation_overview():
     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
+","--- 

+++ 

@@ -44,12 +44,6 @@

         acquisition.Chunk.ingest_chunks(experiment_name)
 
 
-def ingest_environment_visits():
-    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
-    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
-    # analysis.ingest_environment_visits(experiment_names)
-
-
 # ---- Define worker(s) ----
 # configure a worker to process `acquisition`-related tasks
 acquisition_worker = DataJointWorker(
@@ -62,7 +56,6 @@

 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
-# acquisition_worker(ingest_environment_visits)
 acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418196,,80,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/qc.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made to the `stream_reader` assignment is a good one. It simplifies the code and makes it more readable by removing an unnecessary `getattr` call. However, it's important to ensure that the `device_name` attribute always exists in `devices_schema`, otherwise this line will raise an `AttributeError`. If there's a chance that `device_name` might not exist, consider adding error handling to this line.
```diff
- stream_reader = getattr(devices_schema, device_name).Video
+ try:
+     stream_reader = getattr(devices_schema, device_name).Video
+ except AttributeError:
+     raise ValueError(f""Device '{device_name}' not found in devices schema."")
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","             ),
         )
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Video"")
+        stream_reader = getattr(devices_schema, device_name).Video",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418199,,17,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `zip` function does not have a `strict` argument. This will raise a `TypeError`. If you want to iterate over two lists of different lengths, consider using `itertools.zip_longest` instead.
```diff
- for frameidx, path in zip(data._frame, data._path, strict=False):
+ from itertools import zip_longest
+ for frameidx, path in zip_longest(data._frame, data._path):
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","     index = 0
     try:
-        for frameidx, path in zip(data._frame, data._path):
+        for frameidx, path in zip(data._frame, data._path, strict=False):","--- 

+++ 

@@ -15,7 +15,7 @@

     index = 0
     try:
         for frameidx, path in zip(data._frame, data._path, strict=False):
-            if filename != path:
+            if filename != path or capture is None:
                 if capture is not None:
                     capture.release()
                 capture = cv2.VideoCapture(path)
@@ -49,7 +49,7 @@

         for frame in frames:
             if writer is None:
                 if fourcc is None:
-                    fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")
+                    fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")  # type: ignore
                 writer = cv2.VideoWriter(file, fourcc, fps, (frame.shape[1], frame.shape[0]))
             writer.write(frame)
     finally:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418201,4.0,5,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/schemas.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements have been rearranged. This could potentially affect the module resolution order, especially if there are circular dependencies or side effects on import. Please ensure that this change does not introduce any unexpected behaviors.
```diff
- from aeon.schema.streams import Device
- from aeon.schema import foraging, octagon, social_01, social_02, social_03
+ from aeon.schema import foraging, octagon, social_01, social_02, social_03
+ from aeon.schema.streams import Device
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," from aeon.schema import foraging, octagon, social_01, social_02, social_03
-
+from aeon.schema.streams import Device",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418203,,67,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/movies.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function signature has been changed to make `before` and `after` parameters optional with a default value of `None`. This is a good change for flexibility, but it's important to ensure that all calls to this function have been updated accordingly if they were relying on the previous default values.
```diff
- def triggerclip(data, events, before=pd.Timedelta(0), after=pd.Timedelta(0)):
+ def triggerclip(data, events, before=None, after=None):
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 
-def triggerclip(data, events, before=pd.Timedelta(0), after=pd.Timedelta(0)):
+def triggerclip(data, events, before=None, after=None):","--- 

+++ 

@@ -7,7 +7,7 @@

 from aeon.io import video
 
 
-def gridframes(frames, width, height, shape=None):
+def gridframes(frames, width, height, shape: None | int | tuple[int, int] = None):
     """"""Arranges a set of frames into a grid layout with the specified pixel dimensions and shape.
 
     :param list frames: A list of frames to include in the grid layout.
@@ -20,7 +20,7 @@

     """"""
     if shape is None:
         shape = len(frames)
-    if type(shape) not in [list, tuple]:
+    if isinstance(shape, int):
         shape = math.ceil(math.sqrt(shape))
         shape = (shape, shape)
 
@@ -43,7 +43,7 @@

 
 def averageframes(frames):
     """"""Returns the average of the specified collection of frames.""""""
-    return cv2.convertScaleAbs(sum(np.multiply(1 / len(frames), frames)))
+    return cv2.convertScaleAbs(np.sum(np.multiply(1 / len(frames), frames)))
 
 
 def groupframes(frames, n, fun):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418204,78.0,86,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/movies.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code checks if `before` and `after` are `None` and assigns them a default value. It also checks if they are not instances of `pd.Timedelta` and converts them if necessary. This is a good practice as it ensures that `before` and `after` are always of the correct type (`pd.Timedelta`) before they are used in the rest of the function.
```diff
+    if before is None:
+        before = pd.Timedelta(0)
+    elif before is not pd.Timedelta:
+         before = pd.Timedelta(before)
+ 
+    if after is None:
+        after = pd.Timedelta(0)
+    elif after is not pd.Timedelta:
+         after = pd.Timedelta(after)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if before is None:
+        before = pd.Timedelta(0)
+    elif before is not pd.Timedelta:
         before = pd.Timedelta(before)
-    if after is not pd.Timedelta:
+
+    if after is None:
+        after = pd.Timedelta(0)
+    elif after is not pd.Timedelta:
         after = pd.Timedelta(after)","--- 

+++ 

@@ -7,7 +7,7 @@

 from aeon.io import video
 
 
-def gridframes(frames, width, height, shape=None):
+def gridframes(frames, width, height, shape: None | int | tuple[int, int] = None):
     """"""Arranges a set of frames into a grid layout with the specified pixel dimensions and shape.
 
     :param list frames: A list of frames to include in the grid layout.
@@ -20,7 +20,7 @@

     """"""
     if shape is None:
         shape = len(frames)
-    if type(shape) not in [list, tuple]:
+    if isinstance(shape, int):
         shape = math.ceil(math.sqrt(shape))
         shape = (shape, shape)
 
@@ -43,7 +43,7 @@

 
 def averageframes(frames):
     """"""Returns the average of the specified collection of frames.""""""
-    return cv2.convertScaleAbs(sum(np.multiply(1 / len(frames), frames)))
+    return cv2.convertScaleAbs(np.sum(np.multiply(1 / len(frames), frames)))
 
 
 def groupframes(frames, n, fun):"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418207,,2,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/util.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `__future__` import should be at the top of the file, before any module documentation or comments. This is to ensure that feature directives affect all the code in the module.
```diff
- from __future__ import annotations
+ 
+ from __future__ import annotations
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","@@ -1,4 +1,5 @@
 """"""Utility functions.""""""
+","--- 

+++ 

@@ -14,7 +14,7 @@

             found = find_nested_key(v, key)
             if found:
                 return found
-    elif isinstance(obj, list):
+    else:
         for item in obj:
             found = find_nested_key(item, key)
             if found:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418220,34.0,35,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/streams.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `(Stream, StreamGroup)` to `Stream | StreamGroup` is a Python 3.10 feature. If the project is not yet compatible with Python 3.10, this will cause a syntax error.
```diff
- if inspect.isclass(member) and issubclass(member, Stream | StreamGroup)
+ if inspect.isclass(member) and issubclass(member, (Stream, StreamGroup))
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+            if inspect.isclass(member) and issubclass(member, Stream | StreamGroup)
         )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418223,38.0,39,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/streams.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code has changed the logic of the iteration. In the old code, each stream was yielded individually, but in the new code, the factory object itself is being yielded. This could lead to unexpected behavior if the calling code expects individual streams.
```diff
- yield from iter(factory(self.path))
+ for stream in iter(factory(self.path)):
+     yield stream
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","         for factory in chain(self._nested, self._args):
-            for stream in iter(factory(self.path)):
-                yield stream
+            yield from iter(factory(self.path))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1747418227,16.0,25,0927a9ac1e7be460a407b4c0206dca57190eb90f,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/dj_pipeline/test_pipeline_instantiation.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the database query in line 22. If the query doesn't return any results, `fetch1` will raise an error. It would be better to handle this case explicitly.
```diff
- raw_dir = (
-    acquisition.Experiment.Directory & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
- ).fetch1(""directory_path"")
+ directory_query = acquisition.Experiment.Directory & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
+ if len(directory_query) == 0:
+     raise ValueError(f""No directory found for experiment {experiment_name} of type 'raw'"")
+ raw_dir = directory_query.fetch1(""directory_path"")
```
This way, we provide a more meaningful error message when no directory is found for the given experiment and type.

Also, there's a missing indentation on line 25 which could lead to a syntax error.
```diff
-     assert len(exp_subjects) == test_params[""subject_count""]
-     assert ""BAA-1100701"" in exp_subjects
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->"," def test_experiment_creation(test_params, pipeline, experiment_creation):
     acquisition = pipeline[""acquisition""]
-    
+
     experiment_name = test_params[""experiment_name""]
     assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
-        acquisition.Experiment.Directory
-        & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
+        acquisition.Experiment.Directory & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
     assert raw_dir == test_params[""raw_dir""]
-    exp_subjects = (
-        acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
-    ).fetch(""subject"")
+    exp_subjects = (acquisition.Experiment.Subject & {""experiment_name"": experiment_name}).fetch(""subject"")","--- 

+++ 

@@ -3,7 +3,6 @@

 
 @mark.instantiation
 def test_pipeline_instantiation(pipeline):
-
     assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
     assert hasattr(pipeline[""lab""], ""Arena"")
     assert hasattr(pipeline[""qc""], ""CameraQC"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592490,48.0,53,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/populate/worker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The variable `experiment_names` is assigned but never used. This could lead to confusion for other developers reading the code, as they may spend time trying to find where it's used. If this variable is not needed, consider removing it.
```diff
-  experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
     experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
     # analysis.ingest_environment_visits(experiment_names)
-    pass
 
 
 # ---- Define worker(s) ----","--- 

+++ 

@@ -44,12 +44,6 @@

         acquisition.Chunk.ingest_chunks(experiment_name)
 
 
-def ingest_environment_visits():
-    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
-    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
-    # analysis.ingest_environment_visits(experiment_names)
-
-
 # ---- Define worker(s) ----
 # configure a worker to process `acquisition`-related tasks
 acquisition_worker = DataJointWorker(
@@ -62,7 +56,6 @@

 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
-# acquisition_worker(ingest_environment_visits)
 acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592491,,80,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/qc.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made here is a good one. It simplifies the code by removing an unnecessary `getattr` call, which improves readability and performance.
```diff
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Video"")
+        stream_reader = getattr(devices_schema, device_name).Video
```
This change should not introduce any new issues as long as `device_name` is guaranteed to be an attribute of `devices_schema`. If there's a chance it might not be, you should add error handling for an `AttributeError`.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             ),
         )
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Video"")
+        stream_reader = getattr(devices_schema, device_name).Video",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592495,,9,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from a wildcard import to a specific function import is a good practice. It improves readability and avoids potential naming conflicts.
```diff
- from aeon.analysis.utils import *
+ from aeon.analysis.utils import rate, sessiontime
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," from matplotlib.collections import LineCollection
 
-from aeon.analysis.utils import *
+from aeon.analysis.utils import rate, sessiontime","--- 

+++ 

@@ -31,21 +31,20 @@

     return mesh, cbar
 
 
-def circle(x, y, radius, fmt=None, ax=None, **kwargs):
+def circle(x, y, radius, *args, ax=None, **kwargs):
     """"""Plot a circle centered at the given x, y position with the specified radius.
 
     :param number x: The x-component of the circle center.
     :param number y: The y-component of the circle center.
     :param number radius: The radius of the circle.
-    :param str, optional fmt: The format used to plot the circle line.
     :param Axes, optional ax: The Axes on which to draw the circle.
     """"""
     if ax is None:
         ax = plt.gca()
-    points = pd.DataFrame(np.linspace(0, 2 * math.pi, 360), columns=[""angle""])
+    points = pd.DataFrame({""angle"": np.linspace(0, 2 * math.pi, 360)})
     points[""x""] = radius * np.cos(points.angle) + x
     points[""y""] = radius * np.sin(points.angle) + y
-    ax.plot(points.x, points.y, fmt, **kwargs)
+    ax.plot(points.x, points.y, *args, **kwargs)
 
 
 def rateplot(
@@ -132,10 +131,10 @@

     if cmap is None:
         cmap = plt.get_cmap(""copper"")
     if norm is None:
-        norm = plt.Normalize(0.0, 1.0)
+        norm = colors.Normalize(0.0, 1.0)
     z = np.asarray(z)
     points = np.array([x, y]).T.reshape(-1, 1, 2)
     segments = np.concatenate([points[:-1], points[1:]], axis=1)
-    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)
+    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)  # type: ignore
     ax.add_collection(lines)
     return lines"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592497,63.0,73,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring has been updated for better readability by breaking down the long sentence into smaller ones. This makes it easier to understand the purpose of the function and its parameters.
```diff
-    """"""Plot the continuous event rate and raster of a discrete event sequence, given the specified
-    window size and sampling frequency.
+    """"""Plot the continuous event rate and raster of a discrete event sequence.
+    The window size and sampling frequency can be specified.
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""Plot the continuous event rate and raster of a discrete event sequence.
+
+    The window size and sampling frequency can be specified.
 
     :param Series events: The discrete sequence of events.
     :param offset window: The time period of each window used to compute the rate.
     :param DateOffset, Timedelta or str frequency: The sampling frequency for the continuous rate.
     :param number, optional weight: A weight used to scale the continuous rate of each window.
     :param datetime, optional start: The left bound of the time range for the continuous rate.
     :param datetime, optional end: The right bound of the time range for the continuous rate.
-    :param datetime, optional smooth: The size of the smoothing kernel applied to the continuous rate output.
+    :param datetime, optional smooth: The size of the smoothing kernel applied to the rate output.","--- 

+++ 

@@ -31,21 +31,20 @@

     return mesh, cbar
 
 
-def circle(x, y, radius, fmt=None, ax=None, **kwargs):
+def circle(x, y, radius, *args, ax=None, **kwargs):
     """"""Plot a circle centered at the given x, y position with the specified radius.
 
     :param number x: The x-component of the circle center.
     :param number y: The y-component of the circle center.
     :param number radius: The radius of the circle.
-    :param str, optional fmt: The format used to plot the circle line.
     :param Axes, optional ax: The Axes on which to draw the circle.
     """"""
     if ax is None:
         ax = plt.gca()
-    points = pd.DataFrame(np.linspace(0, 2 * math.pi, 360), columns=[""angle""])
+    points = pd.DataFrame({""angle"": np.linspace(0, 2 * math.pi, 360)})
     points[""x""] = radius * np.cos(points.angle) + x
     points[""y""] = radius * np.sin(points.angle) + y
-    ax.plot(points.x, points.y, fmt, **kwargs)
+    ax.plot(points.x, points.y, *args, **kwargs)
 
 
 def rateplot(
@@ -132,10 +131,10 @@

     if cmap is None:
         cmap = plt.get_cmap(""copper"")
     if norm is None:
-        norm = plt.Normalize(0.0, 1.0)
+        norm = colors.Normalize(0.0, 1.0)
     z = np.asarray(z)
     points = np.array([x, y]).T.reshape(-1, 1, 2)
     segments = np.concatenate([points[:-1], points[1:]], axis=1)
-    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)
+    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)  # type: ignore
     ax.add_collection(lines)
     return lines"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592500,112.0,113,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The default values for `cmap` and `norm` have been removed from the function signature and moved inside the function body. This is a good practice as mutable default arguments can lead to unexpected behavior.
```diff
-    cmap=plt.get_cmap(""copper""),
-    norm=plt.Normalize(0.0, 1.0),
+    cmap=None,
+    norm=None,
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    cmap=None,
+    norm=None,","--- 

+++ 

@@ -31,21 +31,20 @@

     return mesh, cbar
 
 
-def circle(x, y, radius, fmt=None, ax=None, **kwargs):
+def circle(x, y, radius, *args, ax=None, **kwargs):
     """"""Plot a circle centered at the given x, y position with the specified radius.
 
     :param number x: The x-component of the circle center.
     :param number y: The y-component of the circle center.
     :param number radius: The radius of the circle.
-    :param str, optional fmt: The format used to plot the circle line.
     :param Axes, optional ax: The Axes on which to draw the circle.
     """"""
     if ax is None:
         ax = plt.gca()
-    points = pd.DataFrame(np.linspace(0, 2 * math.pi, 360), columns=[""angle""])
+    points = pd.DataFrame({""angle"": np.linspace(0, 2 * math.pi, 360)})
     points[""x""] = radius * np.cos(points.angle) + x
     points[""y""] = radius * np.sin(points.angle) + y
-    ax.plot(points.x, points.y, fmt, **kwargs)
+    ax.plot(points.x, points.y, *args, **kwargs)
 
 
 def rateplot(
@@ -132,10 +131,10 @@

     if cmap is None:
         cmap = plt.get_cmap(""copper"")
     if norm is None:
-        norm = plt.Normalize(0.0, 1.0)
+        norm = colors.Normalize(0.0, 1.0)
     z = np.asarray(z)
     points = np.array([x, y]).T.reshape(-1, 1, 2)
     segments = np.concatenate([points[:-1], points[1:]], axis=1)
-    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)
+    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)  # type: ignore
     ax.add_collection(lines)
     return lines"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592502,132.0,135,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The default values for `cmap` and `norm` are now set within the function body. This is a safer approach than setting them in the function signature.
```diff
+    if cmap is None:
+        cmap = plt.get_cmap(""copper"")
+    if norm is None:
+        norm = plt.Normalize(0.0, 1.0)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if cmap is None:
+        cmap = plt.get_cmap(""copper"")
+    if norm is None:
+        norm = plt.Normalize(0.0, 1.0)","--- 

+++ 

@@ -31,21 +31,20 @@

     return mesh, cbar
 
 
-def circle(x, y, radius, fmt=None, ax=None, **kwargs):
+def circle(x, y, radius, *args, ax=None, **kwargs):
     """"""Plot a circle centered at the given x, y position with the specified radius.
 
     :param number x: The x-component of the circle center.
     :param number y: The y-component of the circle center.
     :param number radius: The radius of the circle.
-    :param str, optional fmt: The format used to plot the circle line.
     :param Axes, optional ax: The Axes on which to draw the circle.
     """"""
     if ax is None:
         ax = plt.gca()
-    points = pd.DataFrame(np.linspace(0, 2 * math.pi, 360), columns=[""angle""])
+    points = pd.DataFrame({""angle"": np.linspace(0, 2 * math.pi, 360)})
     points[""x""] = radius * np.cos(points.angle) + x
     points[""y""] = radius * np.sin(points.angle) + y
-    ax.plot(points.x, points.y, fmt, **kwargs)
+    ax.plot(points.x, points.y, *args, **kwargs)
 
 
 def rateplot(
@@ -132,10 +131,10 @@

     if cmap is None:
         cmap = plt.get_cmap(""copper"")
     if norm is None:
-        norm = plt.Normalize(0.0, 1.0)
+        norm = colors.Normalize(0.0, 1.0)
     z = np.asarray(z)
     points = np.array([x, y]).T.reshape(-1, 1, 2)
     segments = np.concatenate([points[:-1], points[1:]], axis=1)
-    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)
+    lines = LineCollection(segments, array=z, cmap=cmap, norm=norm, **kwargs)  # type: ignore
     ax.add_collection(lines)
     return lines"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592503,0.0,2,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/webapps/sciviz/apk_requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [3-3]

It's a good practice to end your file with a newline. This is because some utilities have trouble processing the last line of a file if it isn't newline terminated.
```diff
- git
+ git\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-git
\ No newline at end of file
+git",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592530,15.0,17,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `registry` to `composite` seems to be a good one, as it aligns with the function name `compositeStream`. However, please ensure that this change doesn't affect other parts of the code where `registry` might be used.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                        composite.update(method.__func__(pattern))
             else:
-                registry.update(binder_fn(pattern))
-    return registry
+                composite.update(binder_fn(pattern))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592534,,37,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `register` to `compositeStream` is consistent with the previous hunk. It's important to make sure that this change doesn't break any dependencies elsewhere in the codebase.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     def __init__(self, name, *args, pattern=None):
         self.name = name
-        self.registry = register(name if pattern is None else pattern, *args)
+        self.registry = compositeStream(name if pattern is None else pattern, *args)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592547,,2,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/core.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import order has been changed here. According to PEP8, standard library imports should be first, followed by related third party imports, and then local application/library specific imports. In this case, the change doesn't seem to follow this guideline.
```diff
- import aeon.io.reader as _reader
- from aeon.schema.streams import Stream, StreamGroup
+ from aeon.schema.streams import Stream, StreamGroup
+ import aeon.io.reader as _reader
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,5 +1,5 @@
-from aeon.schema.streams import Stream, StreamGroup
 import aeon.io.reader as _reader
+from aeon.schema.streams import Stream, StreamGroup",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592550,,165,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made here is a good one. It simplifies the code by removing an unnecessary `getattr` call, which improves readability and performance.
```diff
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
+        stream_reader = getattr(devices_schema, device_name).Pose
```
This change should not introduce any new issues as long as `device_name` is guaranteed to be an attribute of `devices_schema`. If there's a chance that `device_name` might not be an attribute of `devices_schema`, it would be better to handle this case explicitly to avoid a potential `AttributeError`.
```python
try:
    stream_reader = getattr(devices_schema, device_name).Pose
except AttributeError:
    # Handle error appropriately...
    pass
```
However, without more context, it's hard to say what the appropriate error handling would be in this case.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             ),
         )
-        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
+        stream_reader = getattr(devices_schema, device_name).Pose",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592552,,17,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `zip` function in Python does not accept a `strict` argument. This will raise a TypeError at runtime. If you want to iterate over two lists of potentially different lengths, consider using `itertools.zip_longest` instead.
```diff
- for frameidx, path in zip(data._frame, data._path, strict=False):
+ from itertools import zip_longest
+ for frameidx, path in zip_longest(data._frame, data._path):
```
This will pair up the elements of `data._frame` and `data._path` until the longer list is exhausted, filling with `None` for the shorter list if they are of unequal length.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     index = 0
     try:
-        for frameidx, path in zip(data._frame, data._path):
+        for frameidx, path in zip(data._frame, data._path, strict=False):","--- 

+++ 

@@ -15,7 +15,7 @@

     index = 0
     try:
         for frameidx, path in zip(data._frame, data._path, strict=False):
-            if filename != path:
+            if filename != path or capture is None:
                 if capture is not None:
                     capture.release()
                 capture = cv2.VideoCapture(path)
@@ -49,7 +49,7 @@

         for frame in frames:
             if writer is None:
                 if fourcc is None:
-                    fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")
+                    fourcc = cv2.VideoWriter_fourcc(""m"", ""p"", ""4"", ""v"")  # type: ignore
                 writer = cv2.VideoWriter(file, fourcc, fps, (frame.shape[1], frame.shape[0]))
             writer.write(frame)
     finally:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592563,2.0,4,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/foraging.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements have been rearranged and an extra line has been added. This doesn't seem to affect the functionality of the code, but it's generally a good practice to keep related imports together for better readability.
```diff
- from enum import Enum
- import pandas as pd
+ from enum import Enum
+
+ import pandas as pd
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
 import pandas as pd
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758592568,70.0,72,11dfa868df9f598c7170ee1319c726f1f2475b24,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/schema/streams.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `stacklevel=2` in the warning function is a good practice. It helps to indicate the source of the warning two steps back in the stack, which can be useful for debugging purposes.
```diff
+                     stacklevel=2,
```
This change improves the maintainability and debuggability of the code by providing more accurate information about the source of deprecation warnings.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                    stacklevel=2,
                 )
                 for method in vars(factory).values():",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758702046,34.0,37,3912d8e4a266a03f0ceafef69bdfb9d437e476be,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/__init__.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring has been updated to be more descriptive, which is a good practice. However, the description seems incomplete as it doesn't mention what happens when `drop_pk` is set to `True`. It's important to document all function parameters for clarity and maintainability.
```diff
-    """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
+    """"""Fetches data from a Stream table based on a query and returns it as a DataFrame. If drop_pk is True, primary key columns are dropped from the returned DataFrame.

     Provided a query containing data from a Stream table,
    fetch and aggregate the data into one DataFrame indexed by ""time""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
+
     Provided a query containing data from a Stream table,
-     fetch and aggregate the data into one DataFrame indexed by ""time""
+    fetch and aggregate the data into one DataFrame indexed by ""time""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758702051,28.0,42,3912d8e4a266a03f0ceafef69bdfb9d437e476be,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/analysis/block_plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the `patch_markers_dict` creation is a good improvement for readability. However, the `strict=False` argument in the `zip` function call is not necessary and can be removed as it's not a valid argument for the `zip` function in Python.

```diff
- patch_markers_dict = dict(zip(patch_markers, patch_markers_symbols, strict=False))
+ patch_markers_dict = dict(zip(patch_markers, patch_markers_symbols))
```

In the `gen_hex_grad` function, the new string formatting method used to convert RGB values to hexadecimal is more readable than the old one. The rest of the changes are mainly about code style and don't affect the logic or performance of the code.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+patch_markers_dict = dict(zip(patch_markers, patch_markers_symbols, strict=False))
 patch_markers_linestyles = [""solid"", ""dash"", ""dot"", ""dashdot"", ""longdashdot""]
 
 
 def gen_hex_grad(hex_col, vals, min_l=0.3):
     """"""Generates an array of hex color values based on a gradient defined by unit-normalized values.""""""
     # Convert hex to rgb to hls
-    h, l, s = rgb_to_hls(
-        *[int(hex_col.lstrip(""#"")[i: i + 2], 16) / 255 for i in (0, 2, 4)]
-    )
+    h, l, s = rgb_to_hls(*[int(hex_col.lstrip(""#"")[i : i + 2], 16) / 255 for i in (0, 2, 4)])  # noqa: E741
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        cur_l = (l * val) + (
-            min_l * (1 - val)
-        )  # get cur lightness relative to `hex_col`
+        cur_l = (l * val) + (min_l * (1 - val))  # get cur lightness relative to `hex_col`
         cur_l = max(min(cur_l, l), min_l)  # set min, max bounds
         cur_rgb_col = hls_to_rgb(h, cur_l, s)  # convert to rgb
-        cur_hex_col = ""#%02x%02x%02x"" % tuple(
-            int(c * 255) for c in cur_rgb_col
+        cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
+            *tuple(int(c * 255) for c in cur_rgb_col)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758702063,0.0,2,3912d8e4a266a03f0ceafef69bdfb9d437e476be,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/webapps/sciviz/apk_requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [3-3]

It's a good practice to always end your file with a newline. This is because some Unix tools might not properly recognize or read the last line if it doesn't end with a newline.
```diff
- git
+ git\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-git
\ No newline at end of file
+git",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1758702102,9.0,22,3912d8e4a266a03f0ceafef69bdfb9d437e476be,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/dj_pipeline/test_tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The variable `file_name` is defined twice in this code. It's first defined on line 10 and then redefined on line 19. This could lead to confusion and potential bugs in the future. I suggest removing the first definition of `file_name` on line 10 as it seems to be unused.

```diff
- file_name = (
-     ""exp0.2-r0-20220524090000-21053810-20220524082942-0-0.npy""  # test file to be saved with save_test_data
- )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+file_name = (
+    ""exp0.2-r0-20220524090000-21053810-20220524082942-0-0.npy""  # test file to be saved with save_test_data
+)
 
 
 def save_test_data(pipeline, test_params):
-    """"""save test dataset fetched from tracking.CameraTracking.Object""""""
-
+    """"""Save test dataset fetched from tracking.CameraTracking.Object.""""""
     tracking = pipeline[""tracking""]
 
     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
     file_name = (
         ""-"".join(
             [
-                v.strftime(""%Y%m%d%H%M%S"")
-                if isinstance(v, datetime.datetime)
-                else str(v)
+                v.strftime(""%Y%m%d%H%M%S"") if isinstance(v, datetime.datetime) else str(v)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246027,838.0,840,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/streams.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's an inconsistency in the indentation of the closing parentheses. It should be aligned with the start of the function call.
```diff
                 ignore_extra_fields=True,
-840:              )
+840:             )
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","                 },
                 ignore_extra_fields=True,
             )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246032,0.0,2,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/webapps/sciviz/apk_requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [3-3]

It's a good practice to end your file with a newline. This is because some utilities have trouble processing the last line of a file if it isn't newline terminated.
```diff
- git
+ git\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-git
\ No newline at end of file
+git",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246037,28.0,34,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/docs/notebooks/social_experiments_block_analysis.ipynb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [212-212]

The removal of the newline at the end of the file is not recommended. It's a common convention to end files with a newline because some tools might not handle or display files correctly if they don't end with a newline.
```diff
- 212: }
+ 212: }\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","    ""execution_count"": null,
    ""outputs"": [],
    ""source"": [
-    ""analysis_vm = dj.create_virtual_module('aeon_block_analysis', 'aeon_block_analysis')""
+    ""analysis_vm = dj.create_virtual_module(\""aeon_block_analysis\"", \""aeon_block_analysis\"")""
    ],
    ""metadata"": {
     ""collapsed"": false,",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246041,12.0,13,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/__init__.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made here is good for avoiding namespace conflicts. However, the `noqa: PLC0414` comment is used to suppress a specific linting warning. It's generally better to fix the underlying issue rather than suppressing the warning. In this case, it seems like the linter is complaining about the import being unused (PLC0414 usually corresponds to an unused import warning). If `load` is indeed used in this file or is intended to be part of the public API of the `aeon` package, then this warning can be safely ignored. Otherwise, consider removing the import altogether.
```diff
- from aeon.io.api import load as load  # noqa: PLC0414
+ from aeon.io.api import load as load
```
If `load` is not used, consider removing the import.
```diff
- from aeon.io.api import load as load  # noqa: PLC0414
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# Set functions available directly under the 'aeon' top-level namespace
+from aeon.io.api import load as load  # noqa: PLC0414",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246045,118.0,123,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/utils/load_metadata.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The type hint for `devices_schema` is added in the new hunk, which improves the clarity of the function signature. However, the docstring does not reflect this change. The docstring should be updated to include a description for `devices_schema`.
```diff
-         devices_schema (DotMap): DotMap object (e.g., exp02, octagon01)
+         devices_schema (DotMap): DotMap object representing the device schema (e.g., exp02, octagon01).
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def extract_epoch_config(experiment_name: str, devices_schema: DotMap, metadata_yml_filepath: str) -> dict:
     """"""Parse experiment metadata YAML file and extract epoch configuration.
 
     Args:
         experiment_name (str): Name of the experiment.
+        devices_schema (DotMap): DotMap object (e.g., exp02, octagon01)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246060,1.0,2,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/conftest.py,tests/dj_pipeline/conftest.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring has been updated to include a title, which improves readability and understanding of the file's purpose. However, the command for running all tests is commented out and could be included in the docstring for clarity.
```diff
- """"""
+ """"""Global configurations and fixtures for pytest.
 # run all tests:
 # pytest -sv --cov-report term-missing --cov=aeon_mecha -p no:warnings tests/dj_pipeline
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+""""""Global configurations and fixtures for pytest.
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246063,50.0,57,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/conftest.py,tests/dj_pipeline/conftest.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring for the `dj_config` function has been updated to better reflect its functionality. This change improves the clarity and understanding of the function's purpose. However, the assertion on line 55 might raise an AssertionError if ""custom"" is not in `dj.config`. It would be better to handle this case gracefully by providing a default value or raising a more descriptive error.
```diff
-    assert ""custom"" in dj.config
+    if ""custom"" not in dj.config:
+        raise ValueError(""Expected 'custom' in dj.config"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""Configures DataJoint connection and loads custom settings.""""""
     dj_config_fp = pathlib.Path(""dj_local_conf.json"")
     assert dj_config_fp.exists()
     dj.config.load(dj_config_fp)
     dj.config[""safemode""] = False
     assert ""custom"" in dj.config
-    dj.config[""custom""][
-        ""database.prefix""
-    ] = f""u_{dj.config['database.user']}_testsuite_""
+    dj.config[""custom""][""database.prefix""] = f""u_{dj.config['database.user']}_testsuite_""
 ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765246067,59.0,60,1a8b0097175251aad2719d7309ffa6186fc238fb,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/conftest.py,tests/dj_pipeline/conftest.py,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement has been simplified, which improves readability. However, it's generally recommended to place import statements at the top of the file unless there's a specific reason not to do so.



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," def load_pipeline():
-
-    from aeon.dj_pipeline import (
-        acquisition,
-        analysis,
-        lab,
-        qc,
-        report,
-        subject,
-        tracking,
-    )
+    from aeon.dj_pipeline import acquisition, analysis, lab, qc, report, subject, tracking",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266882,62.0,66,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring has been improved for clarity and readability. However, it would be better to use a more descriptive function name than `validate`. Consider renaming the function to something like `validate_schema_migration` to make its purpose clearer.
```diff
- def validate():
+ def validate_schema_migration():
```
This change will also require updating all calls to this function throughout your codebase.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""Validates schemas migration.
+
+    1. for the provided list of schema names - validate all schemas have been migrated
+    2. for each schema - validate all tables have been migrated
+    3. for each table, validate all entries have been migrated",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266891,44.0,50,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/docs/notebooks/diagram.ipynb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [902-902]

The newline at the end of the file has been removed. It's a good practice to always end your files with a newline. This is because some utilities have trouble processing the last line of a file if it isn't newline terminated.
```diff
- }
+ }\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     }
    ],
    ""source"": [
-    ""_db_prefix = 'aeon_'\n"",
+    ""_db_prefix = \""aeon_\""\n"",
     ""\n"",
     ""lab = dj.create_virtual_module(\""lab\"", _db_prefix + \""lab\"")\n"",
     ""subject = dj.create_virtual_module(\""subject\"", _db_prefix + \""subject\"")\n"",",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266895,0.0,2,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/webapps/sciviz/apk_requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [3-3]

Good job on adding a newline at the end of the file. This is a common best practice to ensure that the file can be easily read by different tools and utilities.
```diff
- git
+ git\n
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-git
\ No newline at end of file
+git",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266899,,31,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/docs/notebooks/social_experiments_block_analysis.ipynb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from single quotes to double quotes in the string is not necessary. Both are valid in Python, and it's best to maintain consistency throughout the codebase. If the project has a specific style guide that prefers one over the other, then this change would be appropriate. Otherwise, it's unnecessary.
```diff
-    ""analysis_vm = dj.create_virtual_module(\""aeon_block_analysis\"", \""aeon_block_analysis\"")""
+    ""analysis_vm = dj.create_virtual_module('aeon_block_analysis', 'aeon_block_analysis')""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","    ""outputs"": [],
    ""source"": [
-    ""analysis_vm = dj.create_virtual_module('aeon_block_analysis', 'aeon_block_analysis')""
+    ""analysis_vm = dj.create_virtual_module(\""aeon_block_analysis\"", \""aeon_block_analysis\"")""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266904,,86,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/docs/notebooks/social_experiments_block_analysis.ipynb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the change from single quotes to double quotes in the dictionary keys and values is not necessary unless it aligns with the project's style guide. Consistency is key in maintaining readable and maintainable code.
```diff
-     ""block_key = {\""experiment_name\"": \""social0.1-aeon3\"", \""block_start\"": \""2023-11-30 18:49:05.001984\""}""
+     ""block_key = {'experiment_name': 'social0.1-aeon3', 'block_start': '2023-11-30 18:49:05.001984'}""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","    ""source"": [
     ""# Pick a block of interest\n"",
-    ""block_key = {'experiment_name': 'social0.1-aeon3', 'block_start': '2023-11-30 18:49:05.001984'}""
+    ""block_key = {\""experiment_name\"": \""social0.1-aeon3\"", \""block_start\"": \""2023-11-30 18:49:05.001984\""}""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266906,2.0,9,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of the `default_language_version` might cause issues if the project relies on a specific Python version. It's better to specify the Python version explicitly to avoid any potential compatibility issues.
```diff
+ default_language_version:
+   python: python3.11
  files: ""^(test|aeon)\/.*$""
  repos:
    - repo: meta
      hooks:
        - id: identity
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," files: ""^(test|aeon)\/.*$""
 repos:
   - repo: meta
     hooks:
       - id: identity
 
   - repo: https://github.com/pre-commit/pre-commit-hooks
-    rev: v4.4.0
+    rev: v4.6.0",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266912,24.0,34,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `black` to `ruff` as the formatter is fine, but it's important to note that they have different formatting styles and this could lead to large diffs in future commits. Also, the update of the `ruff-pre-commit` and `pyright-python` versions should be fine as long as they are compatible with the rest of the codebase. However, removing the `--config` argument from the `ruff` hook might cause issues if there were custom configurations specified in the `pyproject.toml` file. If there were no custom configurations, then this change is fine.
```diff
-       # Run the linter with the `--fix` flag.
-        - id: ruff
-         args: [ --fix ]
+       - id: ruff
+         args: [--config, ./pyproject.toml]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   - repo: https://github.com/astral-sh/ruff-pre-commit
-    rev: v0.0.286
+    rev: v0.6.4
     hooks:
+      # Run the linter with the `--fix` flag.
       - id: ruff
-        args: [--config, ./pyproject.toml]
+        args: [ --fix ]
+      # Run the formatter.
+      - id: ruff-format
 
   - repo: https://github.com/RobertCraigie/pyright-python
-    rev: v1.1.324
+    rev: v1.1.380",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266916,,251,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from an if condition to using the `min` function is a good one. It simplifies the code and reduces complexity.
```diff
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)","--- 

+++ 

@@ -478,15 +478,15 @@

                 subj_pellets = closest_subjects_pellet_ts[closest_subjects_pellet_ts == subject_name]
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -513,14 +513,14 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
@@ -695,11 +695,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: Œº: {patch_mean}"",
                         )
                     )
@@ -717,13 +717,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -735,7 +735,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266920,,313,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, this change also simplifies the code by replacing the if condition with the `min` function.
```diff
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)","--- 

+++ 

@@ -478,15 +478,15 @@

                 subj_pellets = closest_subjects_pellet_ts[closest_subjects_pellet_ts == subject_name]
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -513,14 +513,14 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
@@ -695,11 +695,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: Œº: {patch_mean}"",
                         )
                     )
@@ -717,13 +717,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -735,7 +735,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266927,475.0,477,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This change improves readability by breaking down a long line of code into multiple lines. This makes it easier to understand what's being assigned to `all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""]`.
```diff
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )","--- 

+++ 

@@ -478,15 +478,15 @@

                 subj_pellets = closest_subjects_pellet_ts[closest_subjects_pellet_ts == subject_name]
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -513,14 +513,14 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
@@ -695,11 +695,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: Œº: {patch_mean}"",
                         )
                     )
@@ -717,13 +717,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -735,7 +735,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765266932,,529,9c9a88b8ac92fb0930e143faeadd08cbca84dd6e,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of leading spaces in the docstring is a good practice as it aligns the text properly according to PEP 257 -- Docstring Conventions.
```diff
-    definition = """""" 
+    definition = """"""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""","--- 

+++ 

@@ -478,15 +478,15 @@

                 subj_pellets = closest_subjects_pellet_ts[closest_subjects_pellet_ts == subject_name]
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -513,14 +513,14 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
@@ -695,11 +695,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: Œº: {patch_mean}"",
                         )
                     )
@@ -717,13 +717,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -735,7 +735,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335021,44.0,49,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/populate/worker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of the `ingest_environment_visits` function and the unused variable `experiment_names` is a good step towards improving maintainability and reducing complexity. However, it's important to ensure that this function isn't being used elsewhere in the codebase, as its removal could potentially break other parts of the system.
```diff
- def ingest_environment_visits():
-    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
-    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
-    # analysis.ingest_environment_visits(experiment_names)
-    pass
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         acquisition.Chunk.ingest_chunks(experiment_name)
 
 
-def ingest_environment_visits():
-    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
-    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
-    # analysis.ingest_environment_visits(experiment_names)
-    pass
-
-
 # ---- Define worker(s) ----
 # configure a worker to process `acquisition`-related tasks
 acquisition_worker = DataJointWorker(","--- 

+++ 

@@ -56,7 +56,6 @@

 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
-# acquisition_worker(ingest_environment_visits)
 acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335025,324.0,334,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The magic numbers `0.2` and `0.3` have been replaced with the constants `BONSAI_SLEAP_V2` and `BONSAI_SLEAP_V3`, which is a good practice for code readability and maintainability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        BONSAI_SLEAP_V2 = 0.2
+        BONSAI_SLEAP_V3 = 0.3
         try:  # Bonsai.Sleap0.2
-            bonsai_sleap_v = 0.2
+            bonsai_sleap_v = BONSAI_SLEAP_V2
             columns = [""identity"", ""identity_likelihood""]
             for part in parts:
                 columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             self.columns = columns
             data = super().read(file)
         except ValueError:  # column mismatch; Bonsai.Sleap0.3
-            bonsai_sleap_v = 0.3
+            bonsai_sleap_v = BONSAI_SLEAP_V3","--- 

+++ 

@@ -43,7 +43,7 @@

         self.columns = columns
         self.extension = extension
 
-    def read(self, _):
+    def read(self, file):
         """"""Reads data from the specified file.""""""
         return pd.DataFrame(columns=self.columns, index=pd.DatetimeIndex([]))
 
@@ -94,7 +94,7 @@

         """"""Returns path and epoch information for the specified chunk.""""""
         epoch, chunk = chunk_key(file)
         data = {""path"": file, ""epoch"": epoch}
-        return pd.DataFrame(data, index=[chunk], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(chunk), columns=self.columns)
 
 
 class Metadata(Reader):
@@ -113,7 +113,7 @@

         workflow = metadata.pop(""Workflow"")
         commit = metadata.pop(""Commit"", pd.NA)
         data = {""workflow"": workflow, ""commit"": commit, ""metadata"": [DotMap(metadata)]}
-        return pd.DataFrame(data, index=[time], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(time), columns=self.columns)
 
 
 class Csv(Reader):
@@ -353,7 +353,7 @@

         data = self.class_int2str(data, config_file)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
@@ -379,17 +379,16 @@

     @staticmethod
     def get_class_names(config_file: Path) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
-        classes = None
         with open(config_file) as f:
             config = json.load(f)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
-            except KeyError as err:
-                if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
-        return classes
+        if config_file.stem != ""confmap_config"":  # SLEAP
+            raise ValueError(f""The model config file '{config_file}' is not supported."")
+
+        try:
+            heads = config[""model""][""heads""]
+            return util.find_nested_key(heads, ""class_vectors"")[""classes""]
+        except KeyError as err:
+            raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
 
     @staticmethod
     def get_bodyparts(config_file: Path) -> list[str]:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335029,359.0,365,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The magic number `0.3` has been replaced with the constant `BONSAI_SLEAP_V3`, which is a good practice for code readability and maintainability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            part_columns = (
+                columns[0 : (len(identities) + 1)] if bonsai_sleap_v == BONSAI_SLEAP_V3 else columns[0:2]
+            )
             part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             part_data = pd.DataFrame(data[part_columns])
-            if bonsai_sleap_v == 0.3:  # combine all identity_likelihood cols into a single col as dict
+            if bonsai_sleap_v == BONSAI_SLEAP_V3:
+                # combine all identity_likelihood cols into a single col as dict","--- 

+++ 

@@ -43,7 +43,7 @@

         self.columns = columns
         self.extension = extension
 
-    def read(self, _):
+    def read(self, file):
         """"""Reads data from the specified file.""""""
         return pd.DataFrame(columns=self.columns, index=pd.DatetimeIndex([]))
 
@@ -94,7 +94,7 @@

         """"""Returns path and epoch information for the specified chunk.""""""
         epoch, chunk = chunk_key(file)
         data = {""path"": file, ""epoch"": epoch}
-        return pd.DataFrame(data, index=[chunk], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(chunk), columns=self.columns)
 
 
 class Metadata(Reader):
@@ -113,7 +113,7 @@

         workflow = metadata.pop(""Workflow"")
         commit = metadata.pop(""Commit"", pd.NA)
         data = {""workflow"": workflow, ""commit"": commit, ""metadata"": [DotMap(metadata)]}
-        return pd.DataFrame(data, index=[time], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(time), columns=self.columns)
 
 
 class Csv(Reader):
@@ -353,7 +353,7 @@

         data = self.class_int2str(data, config_file)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
@@ -379,17 +379,16 @@

     @staticmethod
     def get_class_names(config_file: Path) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
-        classes = None
         with open(config_file) as f:
             config = json.load(f)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
-            except KeyError as err:
-                if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
-        return classes
+        if config_file.stem != ""confmap_config"":  # SLEAP
+            raise ValueError(f""The model config file '{config_file}' is not supported."")
+
+        try:
+            heads = config[""model""][""heads""]
+            return util.find_nested_key(heads, ""class_vectors"")[""classes""]
+        except KeyError as err:
+            raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
 
     @staticmethod
     def get_bodyparts(config_file: Path) -> list[str]:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335032,,441,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring added here provides a clear, concise description of the function's purpose, which improves code readability and maintainability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 
 def from_dict(data, pattern=None):
+    """"""Converts a dictionary to a DotMap object.""""""","--- 

+++ 

@@ -43,7 +43,7 @@

         self.columns = columns
         self.extension = extension
 
-    def read(self, _):
+    def read(self, file):
         """"""Reads data from the specified file.""""""
         return pd.DataFrame(columns=self.columns, index=pd.DatetimeIndex([]))
 
@@ -94,7 +94,7 @@

         """"""Returns path and epoch information for the specified chunk.""""""
         epoch, chunk = chunk_key(file)
         data = {""path"": file, ""epoch"": epoch}
-        return pd.DataFrame(data, index=[chunk], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(chunk), columns=self.columns)
 
 
 class Metadata(Reader):
@@ -113,7 +113,7 @@

         workflow = metadata.pop(""Workflow"")
         commit = metadata.pop(""Commit"", pd.NA)
         data = {""workflow"": workflow, ""commit"": commit, ""metadata"": [DotMap(metadata)]}
-        return pd.DataFrame(data, index=[time], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(time), columns=self.columns)
 
 
 class Csv(Reader):
@@ -353,7 +353,7 @@

         data = self.class_int2str(data, config_file)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
@@ -379,17 +379,16 @@

     @staticmethod
     def get_class_names(config_file: Path) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
-        classes = None
         with open(config_file) as f:
             config = json.load(f)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
-            except KeyError as err:
-                if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
-        return classes
+        if config_file.stem != ""confmap_config"":  # SLEAP
+            raise ValueError(f""The model config file '{config_file}' is not supported."")
+
+        try:
+            heads = config[""model""][""heads""]
+            return util.find_nested_key(heads, ""class_vectors"")[""classes""]
+        except KeyError as err:
+            raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
 
     @staticmethod
     def get_bodyparts(config_file: Path) -> list[str]:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335037,,453,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring added here provides a clear, concise description of the function's purpose, which improves code readability and maintainability.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 
 def to_dict(dotmap):
+    """"""Converts a DotMap object to a dictionary.""""""","--- 

+++ 

@@ -43,7 +43,7 @@

         self.columns = columns
         self.extension = extension
 
-    def read(self, _):
+    def read(self, file):
         """"""Reads data from the specified file.""""""
         return pd.DataFrame(columns=self.columns, index=pd.DatetimeIndex([]))
 
@@ -94,7 +94,7 @@

         """"""Returns path and epoch information for the specified chunk.""""""
         epoch, chunk = chunk_key(file)
         data = {""path"": file, ""epoch"": epoch}
-        return pd.DataFrame(data, index=[chunk], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(chunk), columns=self.columns)
 
 
 class Metadata(Reader):
@@ -113,7 +113,7 @@

         workflow = metadata.pop(""Workflow"")
         commit = metadata.pop(""Commit"", pd.NA)
         data = {""workflow"": workflow, ""commit"": commit, ""metadata"": [DotMap(metadata)]}
-        return pd.DataFrame(data, index=[time], columns=self.columns)
+        return pd.DataFrame(data, index=pd.Series(time), columns=self.columns)
 
 
 class Csv(Reader):
@@ -353,7 +353,7 @@

         data = self.class_int2str(data, config_file)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
@@ -379,17 +379,16 @@

     @staticmethod
     def get_class_names(config_file: Path) -> list[str]:
         """"""Returns a list of classes from a model's config file.""""""
-        classes = None
         with open(config_file) as f:
             config = json.load(f)
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
-            except KeyError as err:
-                if not classes:
-                    raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
-        return classes
+        if config_file.stem != ""confmap_config"":  # SLEAP
+            raise ValueError(f""The model config file '{config_file}' is not supported."")
+
+        try:
+            heads = config[""model""][""heads""]
+            return util.find_nested_key(heads, ""class_vectors"")[""classes""]
+        except KeyError as err:
+            raise KeyError(f""Cannot find class_vectors in {config_file}."") from err
 
     @staticmethod
     def get_bodyparts(config_file: Path) -> list[str]:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335041,51.0,66,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are good for code readability and maintainability. By defining `DOWNSAMPLE_PERIOD` as a constant, it's easier to understand what the value `0.02` represents in the context of the test. It also makes the test more maintainable because if the downsampling period needs to be changed in the future, it can be done in one place instead of multiple places.
```diff
-    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD

-    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD

-    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    DOWNSAMPLE_PERIOD = 0.02
     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
     raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
 
     # Check that the length of the downsampled data is less than the raw data
     assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= 0.02
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= 0.02
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    assert data.index.to_series().diff().dt.total_seconds().min() >= 0.02
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335045,475.0,489,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here improve the readability of the code by using a more Pythonic way to add items to a dictionary. The new code is easier to read and understand.
```diff
- all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = subject_in_patch_cum_time
+ all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+     subject_in_patch_cum_time
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
                 subj_pellets = closest_subjects_pellet_ts[closest_subjects_pellet_ts == subject_name]
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335048,516.0,523,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here improve the readability of the code by using a more Pythonic way to add items to a dictionary. The new code is easier to read and understand.
```diff
- | dict(
-     patch_name=patch_name,
-     subject_name=subject_name,
-     cumulative_preference_by_time=cum_pref_time,
-     cumulative_preference_by_wheel=cum_pref_dist,
-     final_preference_by_time=cum_pref_time[-1],
-     final_preference_by_wheel=cum_pref_dist[-1],
- )
+ | {
+     ""patch_name"": patch_name,
+     ""subject_name"": subject_name,
+     ""cumulative_preference_by_time"": cum_pref_time,
+     ""cumulative_preference_by_wheel"": cum_pref_dist,
+     ""final_preference_by_time"": cum_pref_time[-1],
+     ""final_preference_by_wheel"": cum_pref_dist[-1],
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335050,698.0,702,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here improve the readability of the code by using a more Pythonic way to add items to a dictionary. The new code is easier to read and understand.
```diff
- line=dict(
-     width=2,
-     color=subject_colors[subj_i],
-     dash=patch_markers_linestyles[patch_i],
- ),
+ line={
+     ""width"": 2,
+     ""color"": subject_colors[subj_i],
+     ""dash"": patch_markers_linestyles[patch_i],
+ },
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335054,720.0,726,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here improve the readability of the code by using a more Pythonic way to add items to a dictionary. The new code is easier to read and understand.
```diff
- marker=dict(
-     symbol=patch_markers[patch_i],
-     color=gen_hex_grad(
-          subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
-      ),
-     size=8,
- ),
+ marker={
+     ""symbol"": patch_markers[patch_i],
+     ""color"": gen_hex_grad(
+          subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
+      ),
+     ""size"": 8,
+ },
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765335057,,738,df20e9f386ab58bbdb0daeb53182b33afea989f4,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/analysis/block_analysis.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here improve the readability of the code by using a more Pythonic way to add items to a dictionary. The new code is easier to read and understand.
```diff
- yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+ yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1766652897,,68,c904febb20447d02a88ed8bbeb1b20fe3c6c12bd,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The ruff linter is now being run without specifying a config file path. This could potentially lead to issues if there are specific configurations in the `pyproject.toml` file that need to be considered during linting. If the project relies on certain configurations for ruff, it would be better to keep the `--config ./pyproject.toml` flag.
```diff
-        run: python -m ruff check .
+        run: python -m ruff check --config ./pyproject.toml .
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","       - name: ruff
         if: matrix.os == 'ubuntu-latest'
-        run: python -m ruff check --config ./pyproject.toml .
+        run: python -m ruff check .","--- 

+++ 

@@ -4,40 +4,13 @@

 name: build_env_run_tests
 on:
   pull_request:
-    branches: [ main ]
+    branches: [main]
     types: [opened, reopened, synchronize]
-  workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+  workflow_dispatch: # allows running manually from Github's 'Actions' tab
 
 jobs:
-  build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
-    name: Build env using pip and pyproject.toml
-    runs-on: ubuntu-latest
-    if: github.event.pull_request.draft == false
-    strategy:
-      matrix:
-        os: [ubuntu-latest, windows-latest, macos-latest]
-        python-version: [3.11]
-      fail-fast: false
-    defaults:
-      run:
-        shell: bash -l {0}  # reset shell for each step
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v2
-      - name: Set up Python
-        uses: actions/setup-python@v2
-        with:
-          python-version: ${{ matrix.python-version }}
-      - name: Create venv and install dependencies
-        run: |
-          python -m venv .venv
-          source .venv/bin/activate
-          pip install -e .[dev]
-          pip list
-          .venv/bin/python -c ""import aeon""
-  
-  build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
-    name: Build env and run tests on ${{ matrix.os }}
+  build_env_run_tests: # checks for building env using pyproject.toml and runs codebase checks and tests
+    name: Build env using pip and pyproject.toml on ${{ matrix.os }}
     runs-on: ${{ matrix.os }}
     if: github.event.pull_request.draft == false
     strategy:
@@ -47,45 +20,43 @@

       fail-fast: false
     defaults:
       run:
-        shell: bash -l {0}  # reset shell for each step
+        shell: ${{ matrix.os == 'windows-latest' && 'cmd' || 'bash' }} -l {0} # Adjust shell based on OS
     steps:
-      - name: checkout repo
-        uses: actions/checkout@v2
-      - name: set up conda env
-        uses: conda-incubator/setup-miniconda@v2
+      - name: Checkout code
+        uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v5
         with:
-          use-mamba: true
-          miniforge-variant: Mambaforge
           python-version: ${{ matrix.python-version }}
-          environment-file: ./env_config/env.yml
-          activate-environment: aeon
-      - name: Update conda env with dev reqs
-        run: mamba env update -f ./env_config/env_dev.yml
-
-      # Only run codebase checks and tests for ubuntu.
+      - name: Create venv and install dependencies
+        run: |
+          python -m venv .venv
+          .venv/Scripts/activate || source .venv/bin/activate
+          pip install -e .[dev]
+          pip list
+          python -c ""import aeon""
+      - name: Activate venv for later steps
+        run: |
+          echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV
+          echo ""$(pwd)/.venv/bin"" >> $GITHUB_PATH      # For Unix-like systems
+          echo ""$(pwd)/.venv/Scripts"" >> $GITHUB_PATH  # For Windows
+      # Only run codebase checks and tests for Linux (ubuntu).
       - name: ruff
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m ruff check .
-
+        run: ruff check .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m pyright --level warning --project ./pyproject.toml . 
-
+        run: pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m pytest tests/
-
+        run: pytest tests/ --ignore=tests/dj_pipeline
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest'
+        if: ${{ matrix.os == 'ubuntu-latest' }}
         run: |
-          python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
-          #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
+          python -m pytest --cov=aeon tests/ --ignore=tests/dj_pipeline --cov-report=xml:tests/test_coverage/test_coverage_report.xml
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest'
-        uses: codecov/codecov-action@v2
+        if: ${{ matrix.os == 'ubuntu-latest' }}
+        uses: codecov/codecov-action@v4
         with:
           token: ${{ secrets.CODECOV_TOKEN }}
-          directory: ./tests/test_coverage/
+          directory: tests/test_coverage/
           files: test_coverage_report.xml
           fail_ci_if_error: true
           verbose: true"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1766652901,71.0,72,c904febb20447d02a88ed8bbeb1b20fe3c6c12bd,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The severity level for pyright has been changed from 'error' to 'warning'. This means that the CI will not fail even if there are type errors in the code. If the intention is to enforce strict typing in the codebase, it would be better to keep the severity level as 'error'.
```diff
-        run: python -m pyright --level warning --project ./pyproject.toml . 
+        run: python -m pyright --level error --project ./pyproject.toml . 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         if: matrix.os == 'ubuntu-latest'
-        run: python -m pyright --level error --project ./pyproject.toml . 
+        run: python -m pyright --level warning --project ./pyproject.toml . ","--- 

+++ 

@@ -4,40 +4,13 @@

 name: build_env_run_tests
 on:
   pull_request:
-    branches: [ main ]
+    branches: [main]
     types: [opened, reopened, synchronize]
-  workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+  workflow_dispatch: # allows running manually from Github's 'Actions' tab
 
 jobs:
-  build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
-    name: Build env using pip and pyproject.toml
-    runs-on: ubuntu-latest
-    if: github.event.pull_request.draft == false
-    strategy:
-      matrix:
-        os: [ubuntu-latest, windows-latest, macos-latest]
-        python-version: [3.11]
-      fail-fast: false
-    defaults:
-      run:
-        shell: bash -l {0}  # reset shell for each step
-    steps:
-      - name: Checkout code
-        uses: actions/checkout@v2
-      - name: Set up Python
-        uses: actions/setup-python@v2
-        with:
-          python-version: ${{ matrix.python-version }}
-      - name: Create venv and install dependencies
-        run: |
-          python -m venv .venv
-          source .venv/bin/activate
-          pip install -e .[dev]
-          pip list
-          .venv/bin/python -c ""import aeon""
-  
-  build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
-    name: Build env and run tests on ${{ matrix.os }}
+  build_env_run_tests: # checks for building env using pyproject.toml and runs codebase checks and tests
+    name: Build env using pip and pyproject.toml on ${{ matrix.os }}
     runs-on: ${{ matrix.os }}
     if: github.event.pull_request.draft == false
     strategy:
@@ -47,45 +20,43 @@

       fail-fast: false
     defaults:
       run:
-        shell: bash -l {0}  # reset shell for each step
+        shell: ${{ matrix.os == 'windows-latest' && 'cmd' || 'bash' }} -l {0} # Adjust shell based on OS
     steps:
-      - name: checkout repo
-        uses: actions/checkout@v2
-      - name: set up conda env
-        uses: conda-incubator/setup-miniconda@v2
+      - name: Checkout code
+        uses: actions/checkout@v4
+      - name: Set up Python
+        uses: actions/setup-python@v5
         with:
-          use-mamba: true
-          miniforge-variant: Mambaforge
           python-version: ${{ matrix.python-version }}
-          environment-file: ./env_config/env.yml
-          activate-environment: aeon
-      - name: Update conda env with dev reqs
-        run: mamba env update -f ./env_config/env_dev.yml
-
-      # Only run codebase checks and tests for ubuntu.
+      - name: Create venv and install dependencies
+        run: |
+          python -m venv .venv
+          .venv/Scripts/activate || source .venv/bin/activate
+          pip install -e .[dev]
+          pip list
+          python -c ""import aeon""
+      - name: Activate venv for later steps
+        run: |
+          echo ""VIRTUAL_ENV=$(pwd)/.venv"" >> $GITHUB_ENV
+          echo ""$(pwd)/.venv/bin"" >> $GITHUB_PATH      # For Unix-like systems
+          echo ""$(pwd)/.venv/Scripts"" >> $GITHUB_PATH  # For Windows
+      # Only run codebase checks and tests for Linux (ubuntu).
       - name: ruff
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m ruff check .
-
+        run: ruff check .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m pyright --level warning --project ./pyproject.toml . 
-
+        run: pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest'
-        run: python -m pytest tests/
-
+        run: pytest tests/ --ignore=tests/dj_pipeline
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest'
+        if: ${{ matrix.os == 'ubuntu-latest' }}
         run: |
-          python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
-          #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
+          python -m pytest --cov=aeon tests/ --ignore=tests/dj_pipeline --cov-report=xml:tests/test_coverage/test_coverage_report.xml
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest'
-        uses: codecov/codecov-action@v2
+        if: ${{ matrix.os == 'ubuntu-latest' }}
+        uses: codecov/codecov-action@v4
         with:
           token: ${{ secrets.CODECOV_TOKEN }}
-          directory: ./tests/test_coverage/
+          directory: tests/test_coverage/
           files: test_coverage_report.xml
           fail_ci_if_error: true
           verbose: true"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905816,,4,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The Python version has been updated from 3.9 to 3.11. This is not an issue per se, but it's important to note that all developers working on this project should have Python 3.11 installed in their environments to avoid any potential compatibility issues.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,15 +1,15 @@
 default_language_version:
-  python: python3.9
+  python: python3.11","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905818,8.0,13,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The revision of the pre-commit hooks repo has been updated from v4.1.0 to v4.4.0. This is a good practice as it ensures that the latest versions of the hooks are being used, which can include important bug fixes and improvements.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     hooks:
       - id: identity
 
   - repo: https://github.com/pre-commit/pre-commit-hooks
-    rev: v4.1.0
+    rev: v4.4.0","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905821,21.0,26,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The revision for the black formatting tool has been updated from 22.1.0 to 23.7.0. This is a good practice as it ensures that the latest version of the tool is being used, which can include important bug fixes and improvements.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    rev: 23.7.0
     hooks:
       - id: black
         args:
           - ""--config""
           - ""./pyproject.toml""","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905824,28.0,38,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

A new pre-commit hook for ruff-pre-commit has been added. This tool helps to automatically fix common coding style issues. It's a good addition to the project as it can help to maintain code consistency and readability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  - repo: https://github.com/astral-sh/ruff-pre-commit
+    rev: v0.0.286
+    hooks:
+      - id: ruff
+        args: [--fix, --exit-non-zero-on-fix, --config, ./pyproject.toml]","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905826,34.0,44,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

A new pre-commit hook for pyright-python has been added. Pyright is a static type checker for Python, and it can help to catch potential bugs and errors before they become a problem. This is a good addition to the project as it can improve code quality and reliability.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  - repo: https://github.com/RobertCraigie/pyright-python
+    rev: v1.1.324
     hooks:
-      - id: isort
-        name: isort (python)
-        args:
-          - ""--settings-file""
-          - ""./pyproject.toml""
+    - id: pyright
+      args: [--level, error, --project, ./pyproject.toml]","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905830,41.0,58,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setup for pytest-py has been commented out. While it's understandable that running tests can be expensive in terms of time and resources, it's generally a good idea to run tests as part of the commit process to catch potential issues early. However, since this is commented out and not removed, it can be easily enabled when needed.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  # Pytest is expensive, so we show its set-up but leave it commented out.
+  # - repo: local
+  #   hooks:
+  #     - id: pytest-py
+  #       name: pytest-py
+  #       entry: pytest
+  #       language: system
+  #       pass_filenames: false
+  #       args:
+  #         - ""-v""
+  #         - ""-n""
+  #         - ""auto""","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -11,19 +12,24 @@

   - repo: https://github.com/pre-commit/pre-commit-hooks
     rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
     rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
   
   - repo: https://github.com/astral-sh/ruff-pre-commit
     rev: v0.0.286"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905832,5.0,9,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The workflow is triggered on pull requests to the main branch and can also be manually triggered. This is a good practice as it ensures that the tests are run before any code is merged into the main branch.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+on:
+  pull_request:
+    branches: [ main ]
+    types: [opened, reopened, synchronize]
+  workflow_dispatch:  # allows running manually from Github's 'Actions' tab","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905837,10.0,19,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The job `build_env_pip_pyproject` is set up to run on three different operating systems (Ubuntu, Windows, MacOS) and with Python version 3.11. The use of a matrix strategy for testing across multiple environments is a good practice. However, there's commented-out code in lines 18-20 which should be removed if not needed to maintain clean code.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    name: Build env using pip and pyproject.toml
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+        #include:  # test other python versions only on ubuntu
+          #- os: ubuntu-latest
+          #  python-version: [3.9, 3.10]
+      fail-fast: false","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905838,22.0,37,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The steps for this job include checking out the code, setting up Python, creating a virtual environment, installing dependencies, and importing the aeon module. These steps are necessary for setting up the environment and ensuring the codebase can be imported without issues.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      run:
+        shell: bash -l {0}  # reset shell for each step
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v2
+      - name: Set up Python
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Create venv and install dependencies
+        run: |
+          python -m venv .venv
+          source .venv/bin/activate
+          pip install -e .[dev]
+          python -c ""import aeon""
+  ","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905840,39.0,49,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The job `build_env_run_tests` is similar to the previous one but includes running checks and tests. It's also set up to run on three different operating systems and with Python version 3.11.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    name: Build env and run tests on ${{ matrix.os }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+      fail-fast: false
+    defaults:
+      run:
+        shell: bash -l {0}  # reset shell for each step
+    steps:","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905843,51.0,62,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The steps for this job include checking out the repo, setting up a conda environment, and updating the conda environment with dev requirements. Using conda for managing environments and dependencies is a good practice, especially for projects that require specific versions of packages.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        uses: actions/checkout@v2
+      - name: set up conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          use-mamba: true
+          miniforge-variant: Mambaforge
+          python-version: ${{ matrix.python-version }}
+          environment-file: ./env_config/env.yml
+          activate-environment: aeon
+      - name: Update conda env with dev reqs
+        run: mamba env update -f ./env_config/env_dev.yml
+","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905844,64.0,73,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These steps run codebase checks and tests using ruff, pyright, and pytest. However, these steps are only run if the operating system is Ubuntu and the Python version is 3.11. This could potentially miss issues that only occur on other operating systems or Python versions.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      - name: ruff
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m ruff check --config ./pyproject.toml .
+      - name: pyright
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pyright --level error --project ./pyproject.toml . 
+      - name: pytest
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pytest tests/
+","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307905847,75.0,86,ee354120784672bba17150612396476ebf2ca0b4,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These steps generate a test coverage report and upload it to codecov. This is a good practice as it helps track how much of the codebase is covered by tests. However, there's commented-out code in line 79 which should be removed if not needed to maintain clean code.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: |
+          python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
+          #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
+      - name: upload test coverage report to codecov
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        uses: codecov/codecov-action@v2
+        with:
+          token: ${{ secrets.CODECOV_TOKEN }}
+          directory: ./tests/test_coverage/
+          files: test_coverage_report.xml
+          fail_ci_if_error: true","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -34,7 +32,8 @@

           python -m venv .venv
           source .venv/bin/activate
           pip install -e .[dev]
-          python -c ""import aeon""
+          pip list
+          .venv/bin/python -c ""import aeon""
   
   build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
@@ -63,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307943284,1.0,87,baec063d941565ca8d40f9c5884b04ffc878a971,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The workflow file seems to be well structured and follows best practices for setting up a CI/CD pipeline using GitHub Actions. However, there are a few areas that could be improved for better maintainability and error handling.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+name: build_env_run_tests
+on:
+  pull_request:
+    branches: [ main ]
+    types: [opened, reopened, synchronize]
+  workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+jobs:
+  build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
+    name: Build env using pip and pyproject.toml
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+        #include:  # test other python versions only on ubuntu
+          #- os: ubuntu-latest
+          #  python-version: [3.9, 3.10]
+      fail-fast: false
+    defaults:
+      run:
+        shell: bash -l {0}  # reset shell for each step
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v2
+      - name: Set up Python
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Create venv and install dependencies
+        run: |
+          python -m venv .venv
+          source .venv/bin/activate
+          pip install -e .[dev]
+          pip list
+          .venv/bin/python -c ""import aeon""
+  
+  build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
+    name: Build env and run tests on ${{ matrix.os }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+      fail-fast: false
+    defaults:
+      run:
+        shell: bash -l {0}  # reset shell for each step
+    steps:
+      - name: checkout repo
+        uses: actions/checkout@v2
+      - name: set up conda env
+        uses: conda-incubator/setup-miniconda@v2
+        with:
+          use-mamba: true
+          miniforge-variant: Mambaforge
+          python-version: ${{ matrix.python-version }}
+          environment-file: ./env_config/env.yml
+          activate-environment: aeon
+      - name: Update conda env with dev reqs
+        run: mamba env update -f ./env_config/env_dev.yml
+
+      # Only run codebase checks and tests for ubuntu.
+      - name: ruff
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m ruff check --config ./pyproject.toml .
+      - name: pyright
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pyright --level error --project ./pyproject.toml . 
+      - name: pytest
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pytest tests/
+
+      - name: generate test coverage report
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: |
+          python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
+          #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
+      - name: upload test coverage report to codecov
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        uses: codecov/codecov-action@v2
+        with:
+          token: ${{ secrets.CODECOV_TOKEN }}
+          directory: ./tests/test_coverage/
+          files: test_coverage_report.xml
+          fail_ci_if_error: true
+          verbose: true","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307943287,15.0,20,baec063d941565ca8d40f9c5884b04ffc878a971,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The matrix strategy includes an `os` array with three operating systems but the `include` section which is supposed to test other python versions only on ubuntu is commented out. If you don't plan to use it, consider removing these commented lines to reduce clutter.
```diff
-         #include:  # test other python versions only on ubuntu
-           #- os: ubuntu-latest
-           #  python-version: [3.9, 3.10]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+        #include:  # test other python versions only on ubuntu
+          #- os: ubuntu-latest
+          #  python-version: [3.9, 3.10]","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307943290,33.0,37,baec063d941565ca8d40f9c5884b04ffc878a971,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In this step, you're creating a virtual environment and installing dependencies. However, there's no error handling in case any of these steps fail. Consider adding some error handling or exit early if any command fails.
```diff
        run: |
          python -m venv .venv || exit 1
          source .venv/bin/activate || exit 1
          pip install -e .[dev] || exit 1
          pip list
          .venv/bin/python -c ""import aeon"" || exit 1
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          source .venv/bin/activate
+          pip install -e .[dev]
+          pip list
+          .venv/bin/python -c ""import aeon""
+  ","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307943291,66.0,76,baec063d941565ca8d40f9c5884b04ffc878a971,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These steps are only running if the OS is 'ubuntu-latest' and the Python version is '3.11'. This might lead to issues not being caught when the code is run on different operating systems or Python versions. Consider running these checks for all OS and Python versions in your testing matrix.
```diff
      - name: ruff
        if: matrix.os == 'ubuntu-latest'
        run: python -m ruff check --config ./pyproject.toml .
      - name: pyright
        if: matrix.os == 'ubuntu-latest'
        run: python -m pyright --level error --project ./pyproject.toml . 
      - name: pytest
        if: matrix.os == 'ubuntu-latest'
        run: python -m pytest tests/

      - name: generate test coverage report
        if: matrix.os == 'ubuntu-latest'
        run: |
          python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        run: python -m ruff check --config ./pyproject.toml .
+      - name: pyright
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pyright --level error --project ./pyproject.toml . 
+      - name: pytest
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: python -m pytest tests/
+
+      - name: generate test coverage report
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        run: |","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1307943294,85.0,86,baec063d941565ca8d40f9c5884b04ffc878a971,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

You're uploading the test coverage report to codecov. However, there's no error handling in case the upload fails. Consider adding some error handling or exit early if the upload fails.
```diff
          token: ${{ secrets.CODECOV_TOKEN }}
          directory: ./tests/test_coverage/
          files: test_coverage_report.xml
          fail_ci_if_error: true
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          files: test_coverage_report.xml
+          fail_ci_if_error: true","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881466950,352.0,372,3fe9116bb9f46687a47f4f21799dc177f084d2eb,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a minor issue with the CSS property `left` being declared twice for `.facilities-list li::before`. The second declaration will override the first one. If this is not intentional, you should remove the redundant declaration.
```diff
.facilities-list li::before{
    content: '';
    position: absolute;
    left: 0;
    top: 50%;
    transform: translateY(-50%); /* Center the icon vertically */
    width: 20px; /* Adjust the size of the tick icon */
    height: 20px;
    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
    background-size: contain;
    background-repeat: no-repeat;
-371:     left:-28px
+371:     /* left:-28px */ // Commented out as it seems to be a duplicate
}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.facilities-list li::marker{display:none;}
+.facilities-list{padding-left:0!important;}
+
+.facilities-list li {
+  margin-left: 10px;
+  list-style: none; /* Remove default list marker */
+  position: relative; /* Position for pseudo-element */}
+
+.facilities-list li::before{
+    content: '';
+    position: absolute;
+    left: 0;
+    top: 50%;
+    transform: translateY(-50%); /* Center the icon vertically */
+    width: 20px; /* Adjust the size of the tick icon */
+    height: 20px;
+    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
+    background-size: contain;
+    background-repeat: no-repeat;
+    left:-28px
+}","--- 

+++ 

@@ -157,18 +157,146 @@

   margin-right: 0px;
 }
 
+/* Additional information specific styles */
+/* Fast Facts Wrapper */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child {
+  display: flex;
+  align-items: flex-start; /* Align content in this group to the top */
+  gap: 5px; /* Space between icon and text */
+  flex-shrink: 0; /* Prevent group from shrinking */
+}
+
+/* Style the image icon */
+.fast-facts-wrapper > .wp-block-group .wp-block-image img {
+  width: 20px; /* Fixed width */
+  height: 20px; /* Fixed height */
+  display: inline-block;
+  -o-object-fit: contain;
+     object-fit: contain; /* Ensure image doesn't stretch or distort */
+}
+
+/* Styling for the label (e.g., ""Special Interests:"") */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child p {
+  margin: 0;
+  white-space: nowrap; /* Prevent wrapping for label */
+  vertical-align: middle; /* Align text with the middle of the icon */
+  line-height: 1.2; /* Improve readability */
+  display: inline-block; /* Keep text inline */
+}
+
+/* Styling for the flowing text */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child {
+  display: flex;
+  flex-wrap: nowrap; /* Keep the text flowing inline */
+  white-space: normal; /* Allow wrapping for text if it overflows */
+  overflow-wrap: break-word; /* Ensure long words wrap correctly */
+  word-break: break-word; /* Break long words when needed */
+  flex: 1; /* Allow the container to use available space */
+  align-items: flex-start; /* Ensure text starts at the top */
+}
+
+/* Specific styling for the text content */
+.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child p {
+  white-space: normal; /* Allow wrapping for long text */
+  overflow-wrap: break-word; /* Break words when needed */
+  word-break: break-word; /* Break words on legacy browsers */
+  line-height: 1.4; /* Improve readability */
+  text-transform: capitalize; /* Capitalize text */
+  padding: 0; /* Ensure no unnecessary padding */
+  margin: 0; /* Remove extra margins */
+  display: inline; /* Ensure the text flows inline with the label */
+}
+
+/* Facilities List */
+.facilities-list li::marker {
+  display: none;
+}
+
+.facilities-list {
+  padding-left: 0 !important;
+}
+
+.facilities-list li {
+  margin-left: 10px;
+  list-style: none; /* Remove default list marker */
+  position: relative; /* Position for pseudo-element */
+}
+
+.facilities-list li::before {
+  content: """";
+  position: absolute;
+  left: 0;
+  top: 50%;
+  transform: translateY(-50%); /* Center the icon vertically */
+  width: 20px; /* Adjust the size of the tick icon */
+  height: 20px;
+  background-image: url(""../../img/tick.svg""); /* Replace with the path to your tick.svg */
+  background-size: contain;
+  background-repeat: no-repeat;
+  left: -28px;
+}
+
+/* Additional Info */
+.additional-info .wp-block-group.content .read-more-btn,
+.additional-info .wp-block-group.content .read-less-btn {
+  color: #0073aa;
+  cursor: pointer;
+  font-weight: bold;
+}
+
+.additional-info .wp-block-group.content .read-more-btn:hover {
+  text-decoration: underline;
+}
+
+.additional-info .wp-block-group.content .read-less-btn {
+  display: none;
+}
+
+.wp-block-query.lsx-to-slider {
+  gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);
+  position: relative;
+}
+.wp-block-query.lsx-to-slider .slick-arrow {
+  cursor: pointer;
+  border-color: #000 !important;
+  height: 30px;
+  width: 30px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow:before {
+  border-color: #000 !important;
+  height: 30px;
+  width: 30px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow.slick-prev:before {
+  background: url(""../img/left-arrow-dark.svg"") no-repeat center center;
+  width: 20px;
+  left: 3px;
+  height: 20px;
+}
+.wp-block-query.lsx-to-slider .slick-arrow.slick-next:before {
+  background: url(""../img/right-arrow-dark.svg"") no-repeat center center;
+  width: 20px;
+  left: 3px;
+  height: 20px;
+}
+.wp-block-query.lsx-to-slider .slick-slide {
+  padding: 15px !important;
+}
+
 .slick-lightbox-close {
   right: 52px;
   top: 35px;
 }
-.slick-lightbox-close:before{font-size:40px;}
+.slick-lightbox-close:before {
+  font-size: 40px;
+}
 
 .lsx-to-slider .slick-arrow,
 .slick-lightbox-inner .slick-arrow {
   background: transparent;
   border-radius: 50%;
-  position:absolute;
-  top:50%;
+  position: absolute;
+  top: 50%;
   border-style: solid;
   border-color: #fff;
   border-width: 2px;
@@ -183,8 +311,6 @@

   z-index: 3;
   color: white;
 }
-.slick-prev svg, .slick-next svg{color: white;}
-
 .lsx-to-slider .slick-arrow:before,
 .slick-lightbox-inner .slick-arrow:before {
   display: block;
@@ -200,48 +326,32 @@

   -webkit-font-smoothing: antialiased;
   -moz-osx-font-smoothing: grayscale;
 }
+
+.slick-prev svg,
+.slick-next svg {
+  color: white;
+}
+
 .lsx-to-slider .slick-arrow.slick-disabled,
 .slick-lightbox-inner .slick-arrow.slick-disabled {
   cursor: not-allowed;
 }
 .lsx-to-slider .slick-arrow.slick-prev:before,
 .slick-lightbox-inner .slick-arrow.slick-prev:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/left-arrow-new.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 10px;
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 10px;
 }
 .lsx-to-slider .slick-arrow.slick-next:before,
 .slick-lightbox-inner .slick-arrow.slick-next:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/right-arrow.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 12px;
-}
-.wp-block-query.lsx-to-slider{position:relative;}
-.wp-block-query.lsx-to-slider .slick-arrow:before,
-.wp-block-query .slick-lightbox-inner .slick-arrow:before{ border-color: #000!important;
-height:30px; width:30px;}
-
-.wp-block-query.lsx-to-slider .slick-arrow,
-.slick-lightbox-inner .slick-arrow {cursor: pointer;
-  border-color: #000!important;height:30px; width:30px;}
-
-.wp-block-query.lsx-to-slider .slick-arrow.slick-prev:before,
-.wp-block-query .slick-lightbox-inner .slick-arrow.slick-prev:before{background: url('../img/left-arrow-dark.svg') no-repeat center center;
-  width: 20px;left: 3px;
-  height: 20px;}
-
-.wp-block-query.lsx-to-slider .slick-arrow.slick-next:before,
-.wp-block-query .slick-lightbox-inner .slick-arrow.slick-next:before{background: url('../img/right-arrow-dark.svg') no-repeat center center;
-  width: 20px;left: 3px;
-  height: 20px;}
-
-  .wp-block-query.lsx-to-slider .slick-slide{padding:15px!important;}
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 12px;
+}
 
 .lsx-to-slider .slick-dots:not(.dropdown-menu) {
   align-items: center;
@@ -272,10 +382,12 @@

 .lsx-to-slider .slick-arrow.slick-next {
   right: -4rem;
 }
-
 .lsx-to-slider.slider-disabled {
   overflow: hidden;
 }
+.lsx-to-slider .slick-slide .lazy-hidden {
+  max-height: 154px;
+}
 
 .slick-lightbox-inner .slick-arrow {
   opacity: 0.75;
@@ -288,7 +400,6 @@

   display: inline-block;
 }
 
-.lsx-to-slider .slick-slide .lazy-hidden,
 .lsx-post-carousel-items .slick-slide .lazy-hidden {
   max-height: 154px;
 }
@@ -297,131 +408,9 @@

 .lsx-units-wrapper .hidden {
   display: none;
 }
-.wp-block-query.lsx-to-slider{gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);}
-
+
+.lsx-itinerary-wrapper .hidden,
+.lsx-units-wrapper .hidden {
+  display: none;
+}
 /*# sourceMappingURL=style.css.map */
-
-/* Inner group for icon and label */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child {
-  display: flex;
-  align-items: flex-start; /* Align content in this group to the top */
-  gap: 5px; /* Space between icon and text */
-  flex-shrink: 0; /* Prevent group from shrinking */
-}
-
-/* Style the image icon */
-.fast-facts-wrapper > .wp-block-group .wp-block-image img {
-  width: 20px; /* Fixed width */
-  height: 20px; /* Fixed height */
-  display: inline-block;
-  object-fit: contain; /* Ensure image doesn't stretch or distort */
-}
-
-/* Styling for the label (e.g., ""Special Interests:"") */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child p {
-  margin: 0;
-  white-space: nowrap; /* Prevent wrapping for label */
-  vertical-align: middle; /* Align text with the middle of the icon */
-  line-height: 1.2; /* Improve readability */
-  display: inline-block; /* Keep text inline */
-}
-
-/* Styling for the flowing text */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child {
-  display: flex;
-  flex-wrap: nowrap; /* Keep the text flowing inline */
-  white-space: normal; /* Allow wrapping for text if it overflows */
-  overflow-wrap: break-word; /* Ensure long words wrap correctly */
-  word-break: break-word; /* Break long words when needed */
-  flex: 1; /* Allow the container to use available space */
-  align-items: flex-start; /* Ensure text starts at the top */
-}
-
-/* Specific styling for the text content */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child p {
-  white-space: normal; /* Allow wrapping for long text */
-  overflow-wrap: break-word; /* Break words when needed */
-  word-break: break-word; /* Break words on legacy browsers */
-  line-height: 1.4; /* Improve readability */
-  text-transform: capitalize; /* Capitalize text */
-  padding: 0; /* Ensure no unnecessary padding */
-  margin: 0; /* Remove extra margins */
-  display: inline; /* Ensure the text flows inline with the label */
-}
-
-.facilities-list li::marker{display:none;}
-.facilities-list{padding-left:0!important;}
-
-.facilities-list li {
-  margin-left: 10px;
-  list-style: none; /* Remove default list marker */
-  position: relative; /* Position for pseudo-element */}
-
-.facilities-list li::before{
-    content: '';
-    position: absolute;
-    left: 0;
-    top: 50%;
-    transform: translateY(-50%); /* Center the icon vertically */
-    width: 20px; /* Adjust the size of the tick icon */
-    height: 20px;
-    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
-    background-size: contain;
-    background-repeat: no-repeat;
-    left:-28px
-}
-
-.additional-info .wp-block-group.content .read-more-btn,.additional-info .wp-block-group.content .read-less-btn{
-  color: #0073aa;
-  cursor: pointer;
-  font-weight: bold;
-}
-
-.additional-info .wp-block-group.content .read-more-btn:hover {
-  text-decoration: underline;
-}
-
-.additional-info .wp-block-group.content .read-less-btn{display:none;}
-
-@media (min-width: 800px) {
-.single-tour-operator .toggle-button {display:none;}
-}
-
-/* Optional: Responsive adjustments for smaller screens */
-@media (max-width: 768px) {
-
-  /* Styling for the toggle button */
-.single-tour-operator .toggle-button {
-  display: inline-flex;
-  align-items: center;
-  padding: 5px;
-  background: none;
-  border: none;
-  cursor: pointer;
-  font-size: 14px;
-  color: var(--wp--preset--color--contrast);
-}
-
-/* Styling for the SVG icons */
-.single-tour-operator .toggle-icon {
-  width: 16px;
-  height: 16px;
-  fill: var(--wp--preset--color--contrast);
-  margin-left: 5px;
-}
-
-/* Collapsed state for the second wp-block-group */
-.single-tour-operator section.wp-block-group .wp-block-group.collapsed {
-  display: none; /* Hide the content */
-}
-
-
-  .fast-facts-wrapper > .wp-block-group {
-    flex-wrap: wrap; /* Allow wrapping for better readability */
-    gap: 5px;
-  }
-
-  .fast-facts-wrapper > .wp-block-group .wp-block-group.is-layout-flex:last-child {
-    flex-wrap: wrap; /* Allow text to wrap on smaller screens */
-  }
-}"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881466954,316.0,350,3fe9116bb9f46687a47f4f21799dc177f084d2eb,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for creating ""Read More"" and ""Read Less"" buttons is directly manipulating the DOM inside a loop. This could lead to performance issues if there are many paragraphs on the page. It would be more efficient to use event delegation, where you attach an event listener to a parent element and handle events from its child elements based on the event's target.

Here's how you can refactor this code:

```diff
- paragraphs.forEach(function (p) {
-    const text = p.innerText.trim();
-    if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
-        const fullText = p.innerText.trim();
-        const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
-        p.innerHTML = `<span class=""truncated-text"">${truncatedText}</span><span class=""full-text"" style=""display: none;"">${fullText}</span>`;
-
-        // Create Read More button
-        const readMoreBtn = document.createElement('span');
-        readMoreBtn.textContent = ' Read More';
-        readMoreBtn.classList.add('read-more-btn');
-        p.appendChild(readMoreBtn);
-
-        // Create Read Less button
-        const readLessBtn = document.createElement('span');
-        readLessBtn.textContent = ' Read Less';
-        readLessBtn.classList.add('read-less-btn');
-        p.appendChild(readLessBtn);
-
-        // Add functionality to toggle text
-        readMoreBtn.addEventListener('click', function () {
-            p.querySelector('.truncated-text').style.display = 'none';
-            p.querySelector('.full-text').style.display = 'inline';
-            readMoreBtn.style.display = 'none';
-            readLessBtn.style.display = 'inline';
-        });
-
-        readLessBtn.addEventListener('click', function () {
-            p.querySelector('.truncated-text').style.display = 'inline';
-            p.querySelector('.full-text').style.display = 'none';
-            readMoreBtn.style.display = 'inline';
-            readLessBtn.style.display = 'none';
-        });
-    }
-});
+ document.querySelector('.additional-info .wp-block-group.content').addEventListener('click', function (event) {
+     if (event.target.matches('.read-more-btn')) {
+         const p = event.target.parentNode;
+         p.querySelector('.truncated-text').style.display = 'none';
+         p.querySelector('.full-text').style.display = 'inline';
+         p.querySelector('.read-more-btn').style.display = 'none';
+         p.querySelector('.read-less-btn').style.display = 'inline';
+     } else if (event.target.matches('.read-less-btn')) {
+         const p = event.target.parentNode;
+         p.querySelector('.truncated-text').style.display = 'inline';
+         p.querySelector('.full-text').style.display = 'none';
+         p.querySelector('.read-more-btn').style.display = 'inline';
+         p.querySelector('.read-less-btn').style.display = 'none';
+     }
+ });
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		paragraphs.forEach(function (p) {
+			const text = p.innerText.trim();
+	
+			if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
+				const fullText = p.innerText.trim();
+				const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
+				p.innerHTML = `<span class=""truncated-text"">${truncatedText}</span><span class=""full-text"" style=""display: none;"">${fullText}</span>`;
+	
+				// Create Read More button
+				const readMoreBtn = document.createElement('span');
+				readMoreBtn.textContent = ' Read More';
+				readMoreBtn.classList.add('read-more-btn');
+				p.appendChild(readMoreBtn);
+	
+				// Create Read Less button
+				const readLessBtn = document.createElement('span');
+				readLessBtn.textContent = ' Read Less';
+				readLessBtn.classList.add('read-less-btn');
+				p.appendChild(readLessBtn);
+	
+				// Add functionality to toggle text
+				readMoreBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'none';
+					p.querySelector('.full-text').style.display = 'inline';
+					readMoreBtn.style.display = 'none';
+					readLessBtn.style.display = 'inline';
+				});
+	
+				readLessBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'inline';
+					p.querySelector('.full-text').style.display = 'none';
+					readMoreBtn.style.display = 'inline';
+					readLessBtn.style.display = 'none';
+				});
+			}","--- 

+++ 

@@ -208,7 +208,7 @@

 					draggable: false,
 					infinite: true,
 					swipe: false,
-					dots: true,
+					dots: false,
 					slidesToShow: 3,
 					slidesToScroll: 1,
 					autoplay: false,
@@ -217,23 +217,25 @@

 					//appendDots: $this.parent(),
 					responsive: [
 						{
-							breakpoint: 1279,
+							breakpoint: 1028,
 							settings: {
 								slidesToShow:   2,
 								slidesToScroll: 1,
-								draggable: false,
-								arrows: true,
-								swipe: false,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						},
 						{
-							breakpoint: lsx_to_params.slickSlider.mobile.breakpoint,
+							breakpoint: 782,
 							settings: {
 								slidesToShow:   1,
 								slidesToScroll: 1,
 								draggable: true,
 								arrows: false,
 								swipe: true,
+								dots: true,
 							}
 						}
 					]"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881758014,,211,7fa4557f8ede967993b3723cee88774209742d0e,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `dots: true` to `dots: false` disables the display of dots for navigation in the slider. If this is intentional and aligns with the design requirements, then it's fine. However, if you still need the dots for navigation, you should revert this change.
```diff
- dots: false,
+ dots: true,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 					infinite: true,
 					swipe: false,
-					dots: true,
+					dots: false,",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881758020,220.0,238,7fa4557f8ede967993b3723cee88774209742d0e,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are mostly related to responsive design. The breakpoints have been changed and some settings have been altered for different screen sizes. As long as these changes align with your design requirements and have been tested across different devices and screen sizes, they look good.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+							breakpoint: 1028,
 							settings: {
 								slidesToShow:   2,
 								slidesToScroll: 1,
-								draggable: false,
-								arrows: true,
-								swipe: false,
+								draggable: true,
+								arrows: false,
+								swipe: true,
+								dots: true,
 							}
 						},
 						{
-							breakpoint: lsx_to_params.slickSlider.mobile.breakpoint,
+							breakpoint: 782,
 							settings: {
 								slidesToShow:   1,
 								slidesToScroll: 1,
 								draggable: true,
 								arrows: false,
 								swipe: true,
+								dots: true,",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881758023,315.0,354,7fa4557f8ede967993b3723cee88774209742d0e,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This new code block adds a ""Read More"" / ""Read Less"" functionality to paragraphs that exceed 30 words. It seems to be well implemented, creating two buttons and toggling visibility between the truncated and full text versions of the paragraph. However, there are a few points to consider:

1. **Performance**: Querying the DOM and manipulating it inside a loop can be costly in terms of performance. Consider using DocumentFragment to minimize reflows and repaints.

2. **Accessibility**: Ensure that the 'Read More' and 'Read Less' buttons are accessible, i.e., can be tabbed to and activated using keyboard only. You might want to use `<button>` elements instead of `<span>` for better semantics and built-in accessibility features.

3. **Security**: Directly injecting text into HTML using innerHTML can expose your site to Cross-Site Scripting (XSS) attacks if the text content is user-supplied or can be manipulated by users. If the text content is static and cannot be manipulated by users, this isn't an issue.

Here's a revised version of the code considering these points:

```javascript
document.addEventListener('DOMContentLoaded', function () {
    const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
    const fragment = document.createDocumentFragment();

    paragraphs.forEach(function (p) {
        const text = p.innerText.trim();

        if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
            const fullText = p.innerText.trim();
            const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
            p.textContent = '';

            const truncatedSpan = document.createElement('span');
            truncatedSpan.className = 'truncated-text';
            truncatedSpan.textContent = truncatedText;

            const fullTextSpan = document.createElement('span');
            fullTextSpan.className = 'full-text';
            fullTextSpan.style.display = 'none';
            fullTextSpan.textContent = fullText;

            const readMoreBtn = document.createElement('button');
            readMoreBtn.textContent = ' Read More';
            readMoreBtn.classList.add('read-more-btn');

            const readLessBtn = document.createElement('button');
            readLessBtn.textContent = ' Read Less';
            readLessBtn.classList.add('read-less-btn');
            readLessBtn.style.display = 'none';

            readMoreBtn.addEventListener('click', function () {
                truncatedSpan.style.display = 'none';
                fullTextSpan.style.display = 'inline';
                readMoreBtn.style.display = 'none';
                readLessBtn.style.display = 'inline';
            });

            readLessBtn.addEventListener('click', function () {
                truncatedSpan.style.display = 'inline';
                fullTextSpan.style.display = 'none';
                readMoreBtn.style.display = 'inline';
                readLessBtn.style.display = 'none';
            });

            fragment.appendChild(truncatedSpan);
            fragment.appendChild(fullTextSpan);
            fragment.appendChild(readMoreBtn);
            fragment.appendChild(readLessBtn);

            p.appendChild(fragment);
        }
    });
});
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	document.addEventListener('DOMContentLoaded', function () {
+		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
+	
+		paragraphs.forEach(function (p) {
+			const text = p.innerText.trim();
+	
+			if (text.split(' ').length > 30) {  // Check if paragraph exceeds 30 words
+				const fullText = p.innerText.trim();
+				const truncatedText = fullText.split(' ').slice(0, 30).join(' ') + '...';
+				p.innerHTML = `<span class=""truncated-text"">${truncatedText}</span><span class=""full-text"" style=""display: none;"">${fullText}</span>`;
+	
+				// Create Read More button
+				const readMoreBtn = document.createElement('span');
+				readMoreBtn.textContent = ' Read More';
+				readMoreBtn.classList.add('read-more-btn');
+				p.appendChild(readMoreBtn);
+	
+				// Create Read Less button
+				const readLessBtn = document.createElement('span');
+				readLessBtn.textContent = ' Read Less';
+				readLessBtn.classList.add('read-less-btn');
+				p.appendChild(readLessBtn);
+	
+				// Add functionality to toggle text
+				readMoreBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'none';
+					p.querySelector('.full-text').style.display = 'inline';
+					readMoreBtn.style.display = 'none';
+					readLessBtn.style.display = 'inline';
+				});
+	
+				readLessBtn.addEventListener('click', function () {
+					p.querySelector('.truncated-text').style.display = 'inline';
+					p.querySelector('.full-text').style.display = 'none';
+					readMoreBtn.style.display = 'inline';
+					readLessBtn.style.display = 'none';
+				});
+			}
+		});
+	});	",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881758027,,356,7fa4557f8ede967993b3723cee88774209742d0e,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The absence of a newline at the end of a file can cause issues with some tools that expect or require it. It's generally considered good practice to always end a file with a newline.
```diff
- } )( jQuery, window, document );
+ } )( jQuery, window, document );
+
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		});
+	});	
+
+} )( jQuery, window, document );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881883575,67.0,91,ccabb3ad4d5b9d5adbdf7bdc0ff7d9d2f1d1ade0,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The styling for `.slick-dots:not(.dropdown-menu)` and its child elements seems to be repeated at lines 194-217. This repetition increases the file size and makes the code harder to maintain. Consider removing the duplicate code.
```diff
-   .slick-dots:not(.dropdown-menu) {
-     align-items: center;
-     display: flex;
-     flex-flow: row wrap;
-     height: auto;
-     justify-content: center;
-     list-style: none;
-     margin: 2rem 0 0;
-     padding: 0;
- 
-     & > li {
-       line-height: 1;
-       padding: 5px;
- 
-       & > button {
-         background: transparent;
-         border-style: solid;
-         border-width: 1px;
-         font: 0/0 a;
-         height: 12px;
-         transition: all 300ms ease-in-out;
-         width: 4rem;
-       }
-     }
-   }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  .slick-dots:not(.dropdown-menu) {
+    align-items: center;
+    display: flex;
+    flex-flow: row wrap;
+    height: auto;
+    justify-content: center;
+    list-style: none;
+    margin: 2rem 0 0;
+    padding: 0;
+
+    & > li {
+      line-height: 1;
+      padding: 5px;
+
+      & > button {
+        background: transparent;
+        border-style: solid;
+        border-width: 1px;
+        font: 0/0 a;
+        height: 12px;
+        transition: all 300ms ease-in-out;
+        width: 4rem;
+      }
+    }
+  }","--- 

+++ 

@@ -1,42 +1,96 @@

-/* Slider specific code */
+.wp-block-query {
+	&.lsx-to-slider {
+		gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);
+		position: relative;
 
-/* Slick Slider Arrows and Lightbox styles */
+		.slick-arrow {
+			cursor: pointer;
+			border-color: #000 !important;
+			height: 30px;
+			width: 30px;
+
+			&:before {
+				border-color: #000 !important;
+				height: 30px;
+				width: 30px;
+			}
+
+			&.slick-prev:before {
+				background: url('../img/left-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
+			}
+
+			&.slick-next:before {
+				background: url('../img/right-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
+			}
+		}
+
+		.slick-slide {
+			padding: 15px !important;
+		}
+	}
+}
+
+.slick-lightbox-close {
+  right: 52px;
+  top: 35px;
+
+  &:before {
+	font-size: 40px;
+  }
+}
+
 .lsx-to-slider,
 .slick-lightbox-inner {
   .slick-arrow {
-    background: transparent;
-    border-radius: 50%;
-    position: absolute;
-    top: 50%;
-    border-style: solid;
-    border-color: #fff;
-    border-width: 2px;
-    font: 0/0 a;
-    height: 4rem;
-    margin-top: calc((2.25rem - 17px) / 2);
-    padding: 0;
-    text-shadow: none;
-    transform: translateY(-50%);
-    transition: border 300ms ease;
-    width: 4rem;
-    z-index: 3;
-    color: white;
+	background: transparent;
+	border-radius: 50%;
+	position: absolute;
+	top: 50%;
+	border-style: solid;
+	border-color: #fff;
+	border-width: 2px;
+	font: 0/0 a;
+	height: 4rem;
+	margin-top: calc((2.25rem - 17px) / 2);
+	padding: 0;
+	text-shadow: none;
+	transform: translateY(-50%);
+	transition: border 300ms ease;
+	width: 4rem;
+	z-index: 3;
+	color: white;
 
-    &:before {
-      display: block;
-      font-family: ""slick"";
-      color: white;
-      font-size: 3rem;
-      line-height: 1;
-      position: absolute;
-      text-rendering: auto;
-      top: 47%;
-      transform: translateY(-50%);
-      transition: color 300ms ease;
-      -webkit-font-smoothing: antialiased;
-      -moz-osx-font-smoothing: grayscale;
-    }
+	&:before {
+	  display: block;
+	  font-family: ""slick"";
+	  color: white;
+	  font-size: 3rem;
+	  line-height: 1;
+	  position: absolute;
+	  text-rendering: auto;
+	  top: 47%;
+	  transform: translateY(-50%);
+	  transition: color 300ms ease;
+	  -webkit-font-smoothing: antialiased;
+	  -moz-osx-font-smoothing: grayscale;
+	}
+  }
+}
 
+.slick-prev svg, 
+.slick-next svg {
+  color: white;
+}
+
+.lsx-to-slider,
+.slick-lightbox-inner {
+  .slick-arrow {
     &.slick-disabled {
       cursor: not-allowed;
     }
@@ -44,7 +98,6 @@

     &.slick-prev {
       &:before {
         content: ''; /* Remove font icon */
-        background: url('../../img/left-arrow-new.svg') no-repeat center center;
         width: 40px;
         height: 40px;
         display: inline-block;
@@ -55,7 +108,6 @@

     &.slick-next {
       &:before {
         content: ''; /* Remove font icon */
-        background: url('../../img/right-arrow.svg') no-repeat center center;
         width: 40px;
         height: 40px;
         display: inline-block;
@@ -63,170 +115,82 @@

       }
     }
   }
-
-  .slick-dots:not(.dropdown-menu) {
-    align-items: center;
-    display: flex;
-    flex-flow: row wrap;
-    height: auto;
-    justify-content: center;
-    list-style: none;
-    margin: 2rem 0 0;
-    padding: 0;
-
-    & > li {
-      line-height: 1;
-      padding: 5px;
-
-      & > button {
-        background: transparent;
-        border-style: solid;
-        border-width: 1px;
-        font: 0/0 a;
-        height: 12px;
-        transition: all 300ms ease-in-out;
-        width: 4rem;
-      }
-    }
-  }
-
-  .slick-arrow {
-    &.slick-prev { left: -4rem; }
-    &.slick-next { right: -4rem; }
-  }
-
-  &.slider-disabled {
-    overflow: hidden;
-  }
 }
 
-/* Lightbox Arrow */
+.lsx-to-slider {
+	.slick-dots:not(.dropdown-menu) {
+	  align-items: center;
+	  display: flex;
+	  flex-flow: row wrap;
+	  height: auto;
+	  justify-content: center;
+	  list-style: none;
+	  margin: 2rem 0 0;
+	  padding: 0;
+  
+	  > li {
+		line-height: 1;
+		padding: 5px;
+  
+		> button {
+		  background: transparent;
+		  border-style: solid;
+		  border-width: 1px;
+		  font: 0/0 a;
+		  height: 12px;
+		  transition: all 300ms ease-in-out;
+		  width: 4rem;
+		}
+	  }
+	}
+  
+	.slick-arrow {
+	  &.slick-prev {
+		left: -4rem;
+	  }
+	  
+	  &.slick-next {
+		right: -4rem;
+	  }
+	}
+  
+	&.slider-disabled {
+	  overflow: hidden;
+	}
+  
+	.slick-slide {
+	  .lazy-hidden {
+		max-height: 154px;
+	  }
+	}
+}
+
 .slick-lightbox-inner {
-  .slick-arrow {
-    opacity: 0.75;
-
-    &:hover { opacity: 1; }
-  }
+	.slick-arrow {
+		opacity: 0.75;
+		
+		&:hover {
+		opacity: 1;
+		}
+	}
 }
 
 .slick-lightbox-slick-caption {
-  display: inline-block;
+	display: inline-block;
 }
 
-.lsx-to-slider,
 .lsx-post-carousel-items {
-  .slick-slide {
-    .lazy-hidden {
-      max-height: 154px;
-    }
-  }
+	.slick-slide {
+		.lazy-hidden {
+			max-height: 154px;
+		}
+	}
 }
 
-/* Additional Slider Styles */
-.slick-lightbox-close {
-  right: 32px;
-  top: 32px;
+.lsx-itinerary-wrapper,
+.lsx-units-wrapper {
+	.hidden {
+		display: none;
+	}
 }
-
-/* Slick Slider Arrows and Lightbox styles */
-.wp-block-query.lsx-to-slider{
-  .slick-arrow {
-    background: transparent;
-    border-radius: 50%;
-    position: absolute;
-    top: 50%;
-    border-style: solid;
-    border-color: #fff;
-    border-width: 2px;
-    font: 0/0 a;
-    height: 4rem;
-    margin-top: calc((2.25rem - 17px) / 2);
-    padding: 0;
-    text-shadow: none;
-    transform: translateY(-50%);
-    transition: border 300ms ease;
-    width: 4rem;
-    z-index: 3;
-    color: white;
-
-    &:before {
-      display: block;
-      font-family: ""slick"";
-      color: white;
-      font-size: 3rem;
-      line-height: 1;
-      position: absolute;
-      text-rendering: auto;
-      top: 47%;
-      transform: translateY(-50%);
-      transition: color 300ms ease;
-      -webkit-font-smoothing: antialiased;
-      -moz-osx-font-smoothing: grayscale;
-    }
-
-    &.slick-disabled {
-      cursor: not-allowed;
-    }
-
-    &.slick-prev {
-      &:before {
-        content: ''; /* Remove font icon */
-        background: url('../../img/left-arrow-new.svg') no-repeat center center;
-        width: 40px;
-        height: 40px;
-        display: inline-block;
-        left: 10px;
-      }
-    }
-
-    &.slick-next {
-      &:before {
-        content: ''; /* Remove font icon */
-        background: url('../../img/right-arrow.svg') no-repeat center center;
-        width: 40px;
-        height: 40px;
-        display: inline-block;
-        left: 12px;
-      }
-    }
-  }
-
-  .slick-dots:not(.dropdown-menu) {
-    align-items: center;
-    display: flex;
-    flex-flow: row wrap;
-    height: auto;
-    justify-content: center;
-    list-style: none;
-    margin: 2rem 0 0;
-    padding: 0;
-
-    & > li {
-      line-height: 1;
-      padding: 5px;
-
-      & > button {
-        background: transparent;
-        border-style: solid;
-        border-width: 1px;
-        font: 0/0 a;
-        height: 12px;
-        transition: all 300ms ease-in-out;
-        width: 4rem;
-      }
-    }
-  }
-
-  .slick-arrow {
-    &.slick-prev {
-      left: -4rem;
-    }
-    &.slick-next {
-      right: -4rem;
-    }
-  }
-
-  &.slider-disabled {
-    overflow: hidden;
-  }
-}
+  "
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1881883581,126.0,231,ccabb3ad4d5b9d5adbdf7bdc0ff7d9d2f1d1ade0,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The styling for `.wp-block-query.lsx-to-slider` and its child elements is identical to the styling for `.lsx-to-slider, .slick-lightbox-inner` and its child elements (lines 4-66). This repetition increases the file size and makes the code harder to maintain. Consider using a single selector that matches both cases.
```diff
- /* Slick Slider Arrows and Lightbox styles */
- .wp-block-query.lsx-to-slider{
-   .slick-arrow {
-     background: transparent;
-     border-radius: 50%;
-     position: absolute;
-     top: 50%;
-     border-style: solid;
-     border-color: #fff;
-     border-width: 2px;
-     font: 0/0 a;
-     height: 4rem;
-     margin-top: calc((2.25rem - 17px) / 2);
-     padding: 0;
-     text-shadow: none;
-     transform: translateY(-50%);
-     transition: border 300ms ease;
-     width: 4rem;
-     z-index: 3;
-     color: white;
- 
-     &:before {
-       display: block;
-       font-family: ""slick"";
-       color: white;
-       font-size: 3rem;
-       line-height: 1;
-       position: absolute;
-       text-rendering: auto;
-       top: 47%;
-       transform: translateY(-50%);
-       transition: color 300ms ease;
-       -webkit-font-smoothing: antialiased;
-       -moz-osx-font-smoothing: grayscale;
-     }
- 
-     &.slick-disabled {
-       cursor: not-allowed;
-     }
- 
-     &.slick-prev {
-       &:before {
-         content: ''; /* Remove font icon */
-         background: url('../../img/left-arrow-new.svg') no-repeat center center;
-         width: 40px;
-         height: 40px;
-         display: inline-block;
-         left: 10px;
-       }
-     }
- 
-     &.slick-next {
-       &:before {
-         content: ''; /* Remove font icon */
-         background: url('../../img/right-arrow.svg') no-repeat center center;
-         width: 40px;
-         height: 40px;
-         display: inline-block;
-         left: 12px;
-       }
-     }
-   }
- 
-   .slick-dots:not(.dropdown-menu) {
-     align-items: center;
-     display: flex;
-     flex-flow: row wrap;
-     height: auto;
-     justify-content: center;
-     list-style: none;
-     margin: 2rem 0 0;
-     padding: 0;
- 
-     & > li {
-       line-height: 1;
-       padding: 5px;
- 
-       & > button {
-         background: transparent;
-         border-style: solid;
-         border-width: 1px;
-         font: 0/0 a;
-         height: 12px;
-         transition: all 300ms ease-in-out;
-         width: 4rem;
-       }
-     }
-   }
- 
-   .slick-arrow {
-     &.slick-prev {
-       left: -4rem;
-     }
-     &.slick-next {
-       right: -4rem;
-     }
-   }
- 
-   &.slider-disabled {
-     overflow: hidden;
-   }
- }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.slick-lightbox-close {
+  right: 32px;
+  top: 32px;
+}
+
+/* Slick Slider Arrows and Lightbox styles */
+.wp-block-query.lsx-to-slider{
+  .slick-arrow {
+    background: transparent;
+    border-radius: 50%;
+    position: absolute;
+    top: 50%;
+    border-style: solid;
+    border-color: #fff;
+    border-width: 2px;
+    font: 0/0 a;
+    height: 4rem;
+    margin-top: calc((2.25rem - 17px) / 2);
+    padding: 0;
+    text-shadow: none;
+    transform: translateY(-50%);
+    transition: border 300ms ease;
+    width: 4rem;
+    z-index: 3;
+    color: white;
+
+    &:before {
+      display: block;
+      font-family: ""slick"";
+      color: white;
+      font-size: 3rem;
+      line-height: 1;
+      position: absolute;
+      text-rendering: auto;
+      top: 47%;
+      transform: translateY(-50%);
+      transition: color 300ms ease;
+      -webkit-font-smoothing: antialiased;
+      -moz-osx-font-smoothing: grayscale;
+    }
+
+    &.slick-disabled {
+      cursor: not-allowed;
+    }
+
+    &.slick-prev {
+      &:before {
+        content: ''; /* Remove font icon */
+        background: url('../../img/left-arrow-new.svg') no-repeat center center;
+        width: 40px;
+        height: 40px;
+        display: inline-block;
+        left: 10px;
+      }
+    }
+
+    &.slick-next {
+      &:before {
+        content: ''; /* Remove font icon */
+        background: url('../../img/right-arrow.svg') no-repeat center center;
+        width: 40px;
+        height: 40px;
+        display: inline-block;
+        left: 12px;
+      }
+    }
+  }
+
+  .slick-dots:not(.dropdown-menu) {
+    align-items: center;
+    display: flex;
+    flex-flow: row wrap;
+    height: auto;
+    justify-content: center;
+    list-style: none;
+    margin: 2rem 0 0;
+    padding: 0;
+
+    & > li {
+      line-height: 1;
+      padding: 5px;
+
+      & > button {
+        background: transparent;
+        border-style: solid;
+        border-width: 1px;
+        font: 0/0 a;
+        height: 12px;
+        transition: all 300ms ease-in-out;
+        width: 4rem;
+      }
+    }
+  }
+
+  .slick-arrow {
+    &.slick-prev {
+      left: -4rem;
+    }
+    &.slick-next {
+      right: -4rem;
+    }
+  }
+
+  &.slider-disabled {
+    overflow: hidden;
+  }","--- 

+++ 

@@ -1,42 +1,96 @@

-/* Slider specific code */
+.wp-block-query {
+	&.lsx-to-slider {
+		gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);
+		position: relative;
 
-/* Slick Slider Arrows and Lightbox styles */
+		.slick-arrow {
+			cursor: pointer;
+			border-color: #000 !important;
+			height: 30px;
+			width: 30px;
+
+			&:before {
+				border-color: #000 !important;
+				height: 30px;
+				width: 30px;
+			}
+
+			&.slick-prev:before {
+				background: url('../img/left-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
+			}
+
+			&.slick-next:before {
+				background: url('../img/right-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
+			}
+		}
+
+		.slick-slide {
+			padding: 15px !important;
+		}
+	}
+}
+
+.slick-lightbox-close {
+  right: 52px;
+  top: 35px;
+
+  &:before {
+	font-size: 40px;
+  }
+}
+
 .lsx-to-slider,
 .slick-lightbox-inner {
   .slick-arrow {
-    background: transparent;
-    border-radius: 50%;
-    position: absolute;
-    top: 50%;
-    border-style: solid;
-    border-color: #fff;
-    border-width: 2px;
-    font: 0/0 a;
-    height: 4rem;
-    margin-top: calc((2.25rem - 17px) / 2);
-    padding: 0;
-    text-shadow: none;
-    transform: translateY(-50%);
-    transition: border 300ms ease;
-    width: 4rem;
-    z-index: 3;
-    color: white;
+	background: transparent;
+	border-radius: 50%;
+	position: absolute;
+	top: 50%;
+	border-style: solid;
+	border-color: #fff;
+	border-width: 2px;
+	font: 0/0 a;
+	height: 4rem;
+	margin-top: calc((2.25rem - 17px) / 2);
+	padding: 0;
+	text-shadow: none;
+	transform: translateY(-50%);
+	transition: border 300ms ease;
+	width: 4rem;
+	z-index: 3;
+	color: white;
 
-    &:before {
-      display: block;
-      font-family: ""slick"";
-      color: white;
-      font-size: 3rem;
-      line-height: 1;
-      position: absolute;
-      text-rendering: auto;
-      top: 47%;
-      transform: translateY(-50%);
-      transition: color 300ms ease;
-      -webkit-font-smoothing: antialiased;
-      -moz-osx-font-smoothing: grayscale;
-    }
+	&:before {
+	  display: block;
+	  font-family: ""slick"";
+	  color: white;
+	  font-size: 3rem;
+	  line-height: 1;
+	  position: absolute;
+	  text-rendering: auto;
+	  top: 47%;
+	  transform: translateY(-50%);
+	  transition: color 300ms ease;
+	  -webkit-font-smoothing: antialiased;
+	  -moz-osx-font-smoothing: grayscale;
+	}
+  }
+}
 
+.slick-prev svg, 
+.slick-next svg {
+  color: white;
+}
+
+.lsx-to-slider,
+.slick-lightbox-inner {
+  .slick-arrow {
     &.slick-disabled {
       cursor: not-allowed;
     }
@@ -44,7 +98,6 @@

     &.slick-prev {
       &:before {
         content: ''; /* Remove font icon */
-        background: url('../../img/left-arrow-new.svg') no-repeat center center;
         width: 40px;
         height: 40px;
         display: inline-block;
@@ -55,7 +108,6 @@

     &.slick-next {
       &:before {
         content: ''; /* Remove font icon */
-        background: url('../../img/right-arrow.svg') no-repeat center center;
         width: 40px;
         height: 40px;
         display: inline-block;
@@ -63,170 +115,82 @@

       }
     }
   }
-
-  .slick-dots:not(.dropdown-menu) {
-    align-items: center;
-    display: flex;
-    flex-flow: row wrap;
-    height: auto;
-    justify-content: center;
-    list-style: none;
-    margin: 2rem 0 0;
-    padding: 0;
-
-    & > li {
-      line-height: 1;
-      padding: 5px;
-
-      & > button {
-        background: transparent;
-        border-style: solid;
-        border-width: 1px;
-        font: 0/0 a;
-        height: 12px;
-        transition: all 300ms ease-in-out;
-        width: 4rem;
-      }
-    }
-  }
-
-  .slick-arrow {
-    &.slick-prev { left: -4rem; }
-    &.slick-next { right: -4rem; }
-  }
-
-  &.slider-disabled {
-    overflow: hidden;
-  }
 }
 
-/* Lightbox Arrow */
+.lsx-to-slider {
+	.slick-dots:not(.dropdown-menu) {
+	  align-items: center;
+	  display: flex;
+	  flex-flow: row wrap;
+	  height: auto;
+	  justify-content: center;
+	  list-style: none;
+	  margin: 2rem 0 0;
+	  padding: 0;
+  
+	  > li {
+		line-height: 1;
+		padding: 5px;
+  
+		> button {
+		  background: transparent;
+		  border-style: solid;
+		  border-width: 1px;
+		  font: 0/0 a;
+		  height: 12px;
+		  transition: all 300ms ease-in-out;
+		  width: 4rem;
+		}
+	  }
+	}
+  
+	.slick-arrow {
+	  &.slick-prev {
+		left: -4rem;
+	  }
+	  
+	  &.slick-next {
+		right: -4rem;
+	  }
+	}
+  
+	&.slider-disabled {
+	  overflow: hidden;
+	}
+  
+	.slick-slide {
+	  .lazy-hidden {
+		max-height: 154px;
+	  }
+	}
+}
+
 .slick-lightbox-inner {
-  .slick-arrow {
-    opacity: 0.75;
-
-    &:hover { opacity: 1; }
-  }
+	.slick-arrow {
+		opacity: 0.75;
+		
+		&:hover {
+		opacity: 1;
+		}
+	}
 }
 
 .slick-lightbox-slick-caption {
-  display: inline-block;
+	display: inline-block;
 }
 
-.lsx-to-slider,
 .lsx-post-carousel-items {
-  .slick-slide {
-    .lazy-hidden {
-      max-height: 154px;
-    }
-  }
+	.slick-slide {
+		.lazy-hidden {
+			max-height: 154px;
+		}
+	}
 }
 
-/* Additional Slider Styles */
-.slick-lightbox-close {
-  right: 32px;
-  top: 32px;
+.lsx-itinerary-wrapper,
+.lsx-units-wrapper {
+	.hidden {
+		display: none;
+	}
 }
-
-/* Slick Slider Arrows and Lightbox styles */
-.wp-block-query.lsx-to-slider{
-  .slick-arrow {
-    background: transparent;
-    border-radius: 50%;
-    position: absolute;
-    top: 50%;
-    border-style: solid;
-    border-color: #fff;
-    border-width: 2px;
-    font: 0/0 a;
-    height: 4rem;
-    margin-top: calc((2.25rem - 17px) / 2);
-    padding: 0;
-    text-shadow: none;
-    transform: translateY(-50%);
-    transition: border 300ms ease;
-    width: 4rem;
-    z-index: 3;
-    color: white;
-
-    &:before {
-      display: block;
-      font-family: ""slick"";
-      color: white;
-      font-size: 3rem;
-      line-height: 1;
-      position: absolute;
-      text-rendering: auto;
-      top: 47%;
-      transform: translateY(-50%);
-      transition: color 300ms ease;
-      -webkit-font-smoothing: antialiased;
-      -moz-osx-font-smoothing: grayscale;
-    }
-
-    &.slick-disabled {
-      cursor: not-allowed;
-    }
-
-    &.slick-prev {
-      &:before {
-        content: ''; /* Remove font icon */
-        background: url('../../img/left-arrow-new.svg') no-repeat center center;
-        width: 40px;
-        height: 40px;
-        display: inline-block;
-        left: 10px;
-      }
-    }
-
-    &.slick-next {
-      &:before {
-        content: ''; /* Remove font icon */
-        background: url('../../img/right-arrow.svg') no-repeat center center;
-        width: 40px;
-        height: 40px;
-        display: inline-block;
-        left: 12px;
-      }
-    }
-  }
-
-  .slick-dots:not(.dropdown-menu) {
-    align-items: center;
-    display: flex;
-    flex-flow: row wrap;
-    height: auto;
-    justify-content: center;
-    list-style: none;
-    margin: 2rem 0 0;
-    padding: 0;
-
-    & > li {
-      line-height: 1;
-      padding: 5px;
-
-      & > button {
-        background: transparent;
-        border-style: solid;
-        border-width: 1px;
-        font: 0/0 a;
-        height: 12px;
-        transition: all 300ms ease-in-out;
-        width: 4rem;
-      }
-    }
-  }
-
-  .slick-arrow {
-    &.slick-prev {
-      left: -4rem;
-    }
-    &.slick-next {
-      right: -4rem;
-    }
-  }
-
-  &.slider-disabled {
-    overflow: hidden;
-  }
-}
+  "
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004165,6.0,31,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` in CSS is generally discouraged as it breaks the natural cascading in your stylesheets. It's better to increase specificity of the selector or rethink your class structure instead.
```diff
-8: 			border-color: #000 !important;
-13: 				border-color: #000 !important;

+8: 			border-color: #000;
+13: 				border-color: #000;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		.slick-arrow {
+			cursor: pointer;
+			border-color: #000 !important;
+			height: 30px;
+			width: 30px;
 
-.lsx-to-slider,
-.slick-lightbox-inner {
-	.slick-arrow {
-		background: transparent;
-		border-radius: 50%;
-		border-style: solid;
-		border-width: 2px;
-		font: 0/0 a;
-		height: 4rem;
-		margin-top: calc((2.25rem - 17px) / 2);
-		padding: 0;
-		text-shadow: none;
-		transform: translateY(-50%);
-		transition: border 300ms ease;
-		width: 4rem;
-		z-index: 3;
-
-		&:before {
-			display: block;
-			font-family: 'FontAwesome';
-			font-size: 3rem;
-			line-height: 1;
-			position: absolute;
-			text-rendering: auto;
-			top: 47%;
-			transform: translateY(-50%);
-			transition: color 300ms ease;
-
-			-webkit-font-smoothing: antialiased;
-			-moz-osx-font-smoothing: grayscale;
-		}
+			&:before {
+				border-color: #000 !important;
+				height: 30px;
+				width: 30px;
+			}
 
-		&.slick-disabled { cursor: not-allowed; }
+			&.slick-prev:before {
+				background: url('../img/left-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
+			}
 
-		&.slick-prev {
-			&:before {
-				content: '\f104';
-				left: 1.1rem;
+			&.slick-next:before {
+				background: url('../img/right-arrow-dark.svg') no-repeat center center;
+				width: 20px;
+				left: 3px;
+				height: 20px;
 			}
 		}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004175,48.0,83,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` in CSS is generally discouraged as it breaks the natural cascading in your stylesheets. It's better to increase specificity of the selector or rethink your class structure instead.
```diff
-34: 			padding: 15px !important;

+34: 			padding: 15px;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-to-slider,
+.slick-lightbox-inner {
+  .slick-arrow {
+	background: transparent;
+	border-radius: 50%;
+	position: absolute;
+	top: 50%;
+	border-style: solid;
+	border-color: #fff;
+	border-width: 2px;
+	font: 0/0 a;
+	height: 4rem;
+	margin-top: calc((2.25rem - 17px) / 2);
+	padding: 0;
+	text-shadow: none;
+	transform: translateY(-50%);
+	transition: border 300ms ease;
+	width: 4rem;
+	z-index: 3;
+	color: white;
+
+	&:before {
+	  display: block;
+	  font-family: ""slick"";
+	  color: white;
+	  font-size: 3rem;
+	  line-height: 1;
+	  position: absolute;
+	  text-rendering: auto;
+	  top: 47%;
+	  transform: translateY(-50%);
+	  transition: color 300ms ease;
+	  -webkit-font-smoothing: antialiased;
+	  -moz-osx-font-smoothing: grayscale;
+	}
+  }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004180,91.0,117,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `content: ''; /* Remove font icon */` seems to be a workaround for removing the font icon. If you don't want to display the content, consider using a different approach such as changing the visibility or display property.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-to-slider,
+.slick-lightbox-inner {
+  .slick-arrow {
+    &.slick-disabled {
+      cursor: not-allowed;
+    }
+
+    &.slick-prev {
+      &:before {
+        content: ''; /* Remove font icon */
+        width: 40px;
+        height: 40px;
+        display: inline-block;
+        left: 10px;
+      }
+    }
+
+    &.slick-next {
+      &:before {
+        content: ''; /* Remove font icon */
+        width: 40px;
+        height: 40px;
+        display: inline-block;
+        left: 12px;
+      }
+    }
+  }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004188,211.0,237,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The path to the tick.svg file is hardcoded. This could lead to issues if the file structure changes or if the CSS file is used in a different context where the relative path is different. Consider making this path configurable or absolute.
```diff
-   background-image: url(""../../img/tick.svg""); /* Replace with the path to your tick.svg */
+   background-image: var(--tick-icon-path, url(""../../img/tick.svg"")); /* Replace with the path to your tick.svg */
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.facilities-list li::marker {
+  display: none;
+}
+
+.facilities-list {
+  padding-left: 0 !important;
+}
+
+.facilities-list li {
+  margin-left: 10px;
+  list-style: none; /* Remove default list marker */
+  position: relative; /* Position for pseudo-element */
+}
+
+.facilities-list li::before {
+  content: """";
+  position: absolute;
+  left: 0;
+  top: 50%;
+  transform: translateY(-50%); /* Center the icon vertically */
+  width: 20px; /* Adjust the size of the tick icon */
+  height: 20px;
+  background-image: url(""../../img/tick.svg""); /* Replace with the path to your tick.svg */
+  background-size: contain;
+  background-repeat: no-repeat;
+  left: -28px;
+}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004196,329.0,353,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In line 342 and 349, you're setting the width and height of the arrow icons to 40px, but in lines 274 and 280, you've set them to 20px. Make sure these values are consistent across your codebase to avoid visual inconsistencies.
```diff
-   width: 40px;
-   height: 40px;
+   width: 20px;
+   height: 20px;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+.slick-prev svg,
+.slick-next svg {
+  color: white;
+}
+
 .lsx-to-slider .slick-arrow.slick-disabled,
 .slick-lightbox-inner .slick-arrow.slick-disabled {
   cursor: not-allowed;
 }
 .lsx-to-slider .slick-arrow.slick-prev:before,
 .slick-lightbox-inner .slick-arrow.slick-prev:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/left-arrow-new.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 10px;
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 10px;
 }
 .lsx-to-slider .slick-arrow.slick-next:before,
 .slick-lightbox-inner .slick-arrow.slick-next:before {
-  content: ''; /* Remove font icon */
-    background: url('../img/right-arrow.svg') no-repeat center center;
-    width: 40px;
-    height: 40px;
-    display: inline-block;
-    left: 12px;
+  content: """"; /* Remove font icon */
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 12px;",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004198,411.0,415,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This rule seems to be redundant as it's identical to the one above it (lines 407-409). Consider removing it to keep your CSS clean and maintainable.
```diff
- .lsx-itinerary-wrapper .hidden,
- .lsx-units-wrapper .hidden {
-   display: none;
- }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
+.lsx-itinerary-wrapper .hidden,
+.lsx-units-wrapper .hidden {
+  display: none;
+}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1882004201,,416,14d62aaa7c181aec8bc312f2459317c47f0a65f9,70abbf3757db6f520f4e9250a937a5aafa1fe4b5,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's a good practice to end your files with a newline. This can prevent potential parsing errors and improves compatibility across different tools and systems.
```diff
- /*# sourceMappingURL=style.css.map */
+ /*# sourceMappingURL=style.css.map */
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-units-wrapper .hidden {
+  display: none;
+}
+/*# sourceMappingURL=style.css.map */",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812832,168.0,170,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The positioning of the `.slick-lightbox-close` has been changed. Ensure that this change doesn't affect other elements that might be positioned relative to it.
```diff
-  right: 32px;
-  top: 32px;
+  right: 52px;
+  top: 35px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  right: 52px;
+  top: 35px;
 }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812838,173.0,180,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `position:absolute;` and `top:50%;` properties have been added to the `.lsx-to-slider .slick-arrow, .slick-lightbox-inner .slick-arrow` selectors. Make sure these changes don't disrupt the layout of the page or interfere with other elements.
```diff
+  position:absolute;
+  top:50%;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," .lsx-to-slider .slick-arrow,
 .slick-lightbox-inner .slick-arrow {
   background: transparent;
   border-radius: 50%;
+  position:absolute;
+  top:50%;
   border-style: solid;
+  border-color: #fff;",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812839,191.0,192,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The color property has been added to the arrow selectors. This should improve visibility but ensure it aligns with the overall design theme.
```diff
+  color: white;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  color: white;
 }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812842,193.0,199,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The font family for the `.lsx-to-slider .slick-arrow:before, .slick-lightbox-inner .slick-arrow:before` selectors has been changed from ""FontAwesome"" to ""slick"". If ""slick"" is not a defined font-family in your CSS or HTML, this could cause issues.
```diff
-  font-family: ""FontAwesome"";
+  font-family: ""slick"";
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.slick-prev svg, .slick-next svg{color: white;}
+
 .lsx-to-slider .slick-arrow:before,
 .slick-lightbox-inner .slick-arrow:before {
   display: block;
-  font-family: ""FontAwesome"";
+  font-family: ""slick"";
+  color: white;",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812844,216.0,222,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The content of the `.lsx-to-slider .slick-arrow.slick-prev:before, .slick-lightbox-inner .slick-arrow.slick-prev:before` selectors has been replaced with an SVG background. Ensure that the path to the SVG file is correct and that the SVG file exists in the specified location.
```diff
-  content: ""\f104"";
-  left: 1.1rem;
+  content: ''; /* Remove font icon */
+  background: url('../img/left-arrow-new.svg') no-repeat center center;
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 10px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  content: ''; /* Remove font icon */
+    background: url('../img/left-arrow-new.svg') no-repeat center center;
+    width: 40px;
+    height: 40px;
+    display: inline-block;
+    left: 10px;
 }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1872812847,223.0,230,e0a6fc9db7c550e83002e3e3a9d9348329f8a4c7,5b9410d665efce277f2f5dc06645c44aae8da6fe,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The content of the `.lsx-to-slider .slick-arrow.slick-next:before, .slick-lightbox-inner .slick-arrow.slick-next:before` selectors has been replaced with an SVG background. Ensure that the path to the SVG file is correct and that the SVG file exists in the specified location.
```diff
-  content: ""\f105"";
-  left: 1.5rem;
+  content: ''; /* Remove font icon */
+  background: url('../img/right-arrow.svg') no-repeat center center;
+  width: 40px;
+  height: 40px;
+  display: inline-block;
+  left: 12px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," .lsx-to-slider .slick-arrow.slick-next:before,
 .slick-lightbox-inner .slick-arrow.slick-next:before {
-  content: ""\f105"";
-  left: 1.5rem;
+  content: ''; /* Remove font icon */
+    background: url('../img/right-arrow.svg') no-repeat center center;
+    width: 40px;
+    height: 40px;
+    display: inline-block;
+    left: 12px;",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1842367596,,100,be4092040697695ce47c935d798955839ab21f08,e588ac4656624636490ac813fa8237abc1becc4a,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The hardcoded text 'Read Less' might cause issues with internationalization. It's better to use a variable that can be translated.
```diff
- button.text( 'Read Less' );
+ button.text( lsx_to.readLessText );
```
Remember to define `lsx_to.readLessText` in the same way you defined `lsx_to.readMoreText`.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 				$(this).show();
 			});
 			button.addClass( 'less-link' );
+			button.text( 'Read Less' );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1842367604,427.0,453,be4092040697695ce47c935d798955839ab21f08,e588ac4656624636490ac813fa8237abc1becc4a,includes/classes/legacy/class-frontend.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for fetching the primary term and falling back to the first term if the primary is not available seems correct. However, there's a potential issue with the `continue` statement in the loop at line 442. If the counter is greater than 0, it will skip the current iteration but won't stop the loop. This means that the loop will continue iterating over all terms even after finding one. To improve performance, consider breaking the loop once a term has been found.
```diff
- 					if ( 0 < $counter ) {
- 						continue;
- 					}
+ 					if ( 0 < $counter ) {
+ 						break;
+ 					}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		
+		// Get the primary travel style
+		$primary      = get_post_meta( get_the_ID(), '_yoast_wpseo_primary_accommodation-type', true );
+		$primary_term = get_term( $primary, 'accommodation-type' );
 
-		$all_destinations = array();
-		if ( false !== $current_destinations && ! empty( $current_destinations ) ) {
-
-			$country = false;
-			$regions = array();
-
-			foreach ( $current_destinations as $current_destination ) {
-				$all_destinations[] = get_post( $current_destination );
-			}
+		if ( ! is_wp_error( $primary_term ) && null !== $primary_term ) {
+			$new_crumbs[] = array(
+				'text' => $primary_term->name,
+				'url'  => get_term_link( $primary_term, 'accommodation-type' ),
+			);
+		} else {
+			$counter = 0;
+			$terms = wp_get_object_terms( get_the_ID(), 'accommodation-type' );
+			if ( ! is_wp_error( $terms ) && ! empty( $terms ) ) {
+				foreach ( $terms as $term ) {
+					if ( 0 < $counter ) {
+						continue;
+					}
 
-			//Find the country
-			foreach ( $all_destinations as $destination_index => $destination ) {
-				if ( ( 0 === $destination->post_parent || '0' === $destination->post_parent ) && 'publish' === $destination->post_status ) {
 					$new_crumbs[] = array(
-						'text' => $destination->post_title,
-						'url'  => get_permalink( $destination->ID ),
+						'text' => $term->name,
+						'url'  => get_term_link( $term ),
 					);
-					unset( $all_destinations[ $destination_index ] );
-				}
-			}
-
-			//Find the region
-			if ( ! empty( $all_destinations ) ) {
-				foreach ( $all_destinations as $destination_index => $destination ) {
-					if ( 'publish' === $destination->post_status ) {
-						$new_crumbs[] = array(
-							'text' => $destination->post_title,
-							'url'  => get_permalink( $destination->ID ),
-						);
-					}
+					$counter++;
 				}
 			}
 		}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1842367605,476.0,506,be4092040697695ce47c935d798955839ab21f08,e588ac4656624636490ac813fa8237abc1becc4a,includes/classes/legacy/class-frontend.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the logic for fetching the primary term and falling back to the first term if the primary is not available seems correct. However, the same issue with the `continue` statement exists here as well. Consider replacing `continue` with `break` to stop the loop once a term has been found.
```diff
- 					if ( 0 < $counter ) {
- 						continue;
- 					}
+ 					if ( 0 < $counter ) {
+ 						break;
+ 					}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				'text' => esc_attr__( 'Tours', 'tour-operator' ),
 				'url'  => get_post_type_archive_link( 'tour' ),
 			),
 		);
-		$region = get_post_meta( get_the_ID(), 'departs_from', true );
-		if ( false !== $region && isset( $region[0] ) ) {
-			$country = wp_get_post_parent_id( $region[0] );
-			if ( false !== $country && '' !== $country ) {
-				$new_crumbs[] = array(
-					'text' => get_the_title( $country ),
-					'url'  => get_permalink( $country ),
-				);
-			}
+
+		// Get the primary travel style
+		$primary      = get_post_meta( get_the_ID(), '_yoast_wpseo_primary_travel-style', true );
+		$primary_term = get_term( $primary, 'travel-style' );
+
+		if ( ! is_wp_error( $primary_term ) && null !== $primary_term ) {
 			$new_crumbs[] = array(
-				'text' => get_the_title( $region[0] ),
-				'url'  => get_permalink( $region[0] ),
+				'text' => $primary_term->name,
+				'url'  => get_term_link( $primary_term, 'travel-style' ),
 			);
+		} else {
+			$counter = 0;
+			$terms = wp_get_object_terms( get_the_ID(), 'travel-style' );
+			if ( ! is_wp_error( $terms ) && ! empty( $terms ) ) {
+				foreach ( $terms as $term ) {
+					if ( 0 < $counter ) {
+						continue;
+					}
+
+					$new_crumbs[] = array(
+						'text' => $term->name,
+						'url'  => get_term_link( $term ),
+					);
+					$counter++;
+				}
+			}
 		}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1842367609,113.0,115,be4092040697695ce47c935d798955839ab21f08,e588ac4656624636490ac813fa8237abc1becc4a,includes/classes/blocks/class-templates.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of the `function_exists` check before calling `register_block_template` is a good practice to prevent potential errors if the function is not available. This is especially important when dealing with optional or plugin-dependent functions.
```diff
+			if ( function_exists( 'register_block_template' ) ) {
+				register_block_template( 'lsx-tour-operator//' . $key, $args );
+			}
```
This change improves the robustness of the code by ensuring that it doesn't break in environments where the `register_block_template` function might not be defined.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			if ( function_exists( 'register_block_template' ) ) {
+				register_block_template( 'lsx-tour-operator//' . $key, $args );
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1842367617,,949,be4092040697695ce47c935d798955839ab21f08,e588ac4656624636490ac813fa8237abc1becc4a,assets/js/blocks/accommodation.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The class name has been changed from 'lsx-special-interests-wrapper' to 'lsx-special-interest-wrapper'. Ensure that this change is reflected in the corresponding CSS file(s) and any JavaScript that may be targeting this class. If not, it could lead to styling issues or broken functionality.
```diff
- className: 'lsx-special-interests-wrapper',
+ className: 'lsx-special-interest-wrapper',
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 				name: 'Special Interests',
 			},
-			className: 'lsx-special-interests-wrapper',
+			className: 'lsx-special-interest-wrapper',",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1883739049,7.0,11,e018fe76821708b288a7578156c0d97d1a661e30,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` is generally discouraged in CSS as it increases the specificity of a rule, making it harder to override if needed. It's better to increase specificity in a more controlled way, such as by using more specific selectors or by structuring your CSS so that the most specific rules are declared last.

```diff
- .slick-slide {
-   padding: 0 !important;
-   margin: 0 10px !important;
- }
+ .wp-block-query.lsx-to-slider.travel-information .slick-slide {
+   padding: 0;
+   margin: 0 10px;
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  
+	  .slick-slide {
+		padding: 0 !important;
+		margin: 0 10px !important;
+	  }","--- 

+++ 

@@ -1,5 +1,5 @@

-.wp-block-query.lsx-to-slider {
-	&.travel-information {
+.lsx-to-slider {
+	&.lsx-travel-information-wrapper {
 	  .slick-list {
 		max-width: 100% !important;
 		width: 100% !important;
@@ -12,7 +12,8 @@

 	}
   }
 
-.wp-block-query {
+.wp-block-query,
+.lsx-travel-information-wrapper {
 	&.lsx-to-slider {
 		gap: var(--wp--preset--spacing--small) var(--wp--preset--spacing--small);
 		position: relative;
@@ -43,7 +44,11 @@

 				height: 20px;
 			}
 		}
-
+	}
+}
+
+.wp-block-query {
+	&.lsx-to-slider {
 		.slick-slide {
 			padding: 15px !important;
 		}"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1883739053,200.0,242,e018fe76821708b288a7578156c0d97d1a661e30,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the first slider look good. The addition of `appendArrows` and `appendDots` ensures that navigation arrows and dots are appended correctly, improving user interaction. The responsive settings have been updated to show different numbers of slides based on the screen size, which will enhance the user experience across various devices.

```diff
+ 					appendArrows: $this.parent(),  // Ensure arrows are appended correctly
+ 					appendDots: $this.parent(),    // Append dots in the right container
+ 					responsive: [
+ 						{
+ 							breakpoint: 1028,
+ 							settings: {
+ 								slidesToShow: 2,
+ 								slidesToScroll: 1,
+ 								draggable: true,
+ 								arrows: true,
+ 								swipe: true,
+ 								dots: true,
+ 							}
+ 						},
+ 						{
+ 							breakpoint: 782,
+ 							settings: {
+ 								slidesToShow: 1,
+ 								slidesToScroll: 1,
+ 								draggable: true,
+ 								arrows: true,
+ 								swipe: true,
+ 								dots: true,
+ 							}
+ 						}
+ 					]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		// First slider: .lsx-to-slider
 		$( '.lsx-to-slider .wp-block-post-template:not(.slider-disabled)' ).each( function() {
 			var $this = $( this );
-
+	
 			lsx_to.pre_build_slider( $this );
-
-			if ( 1 < $this.children.length ) {
-
+	
+			if ( 1 < $this.children().length ) {
+				$this.slick( {
+					draggable: false,
+					infinite: true,
+					swipe: false,
+					dots: false,
+					slidesToShow: 3,  // Show 3 items at a time
+					slidesToScroll: 1, // Scroll 1 item at a time
+					autoplay: false,
+					autoplaySpeed: 0,
+					appendArrows: $this.parent(),  // Ensure arrows are appended correctly
+					appendDots: $this.parent(),    // Append dots in the right container
+					responsive: [
+						{
+							breakpoint: 1028,
+							settings: {
+								slidesToShow: 2,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: true,
+								swipe: true,
+								dots: true,
+							}
+						},
+						{
+							breakpoint: 782,
+							settings: {
+								slidesToShow: 1,
+								slidesToScroll: 1,
+								draggable: true,
+								arrows: true,
+								swipe: true,
+								dots: true,
+							}
+						}
+					]
+				} );","--- 

+++ 

@@ -51,7 +51,11 @@

 			lsx_to.readMoreText = $(this).contents().filter(function() {
 				return this.nodeType === Node.TEXT_NODE;
 			}).text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+
+			console.log($(this));
+			console.log($(this).parent( '.wp-block-group' ).find('.wp-block-post-content'));
+
+			lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 		} );
 
 		$( '.single-tour-operator .wp-block-read-more' ).on( 'click', function( event ) {
@@ -59,9 +63,9 @@

 			$( this ).hide();
 
 			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			} else {
-				lsx_to.readMoreOpen( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreOpen( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			}
 
 			$( this ).show();
@@ -69,8 +73,6 @@

 	};
 
 	lsx_to.readMoreSet = function( button, contentWrapper ) {
-		console.log(contentWrapper);
-		console.log(contentWrapper.length);
 		if ( 0 < contentWrapper.length ) {
 			if ( 1 < contentWrapper.children().length ) {
 
@@ -118,17 +120,17 @@

 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).each( function() {
 			lsx_to.readMoreTIText = $(this).find('a').text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 		} );
 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
-			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			if ( $( this ).find('a').hasClass( 'less-link' ) ) {
+				lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			} else {
-				lsx_to.readMoreOpenTI( $(this), $(this).closest( '.additional-info' ).find('.content') );
+				lsx_to.readMoreOpen( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			}
 
 			$( this ).show();
@@ -244,7 +246,7 @@

 		} );
 	
 		// Second slider: .lsx-to-slider.travel-information
-		$( '.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
+		$( '.lsx-travel-information-wrapper.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
 			var $this = $( this );
 	
 			lsx_to.pre_build_slider( $this );
@@ -344,7 +346,7 @@

 	$document.ready( function() {
 		lsx_to.set_read_more();
 		lsx_to.set_read_more_travel_info();
-		lsx_to.set_read_more_itinerary();
+		//lsx_to.set_read_more_itinerary();
 		lsx_to.build_slider( window_width );
 	} );
 
@@ -359,7 +361,7 @@

 		lsx_to.build_slider_lightbox();
 	} );
 
-	document.addEventListener('DOMContentLoaded', function () {
+	/*document.addEventListener('DOMContentLoaded', function () {
 		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
 	
 		paragraphs.forEach(function (p) {
@@ -398,7 +400,7 @@

 				});
 			}
 		});
-	});	
+	});
 
 	document.addEventListener('DOMContentLoaded', function () {
 		// Select all sections within `.single-tour-operator`
@@ -446,6 +448,6 @@

 				});
 			}
 		});
-	});
+	});*/
 
 } )( jQuery, window, document );"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1883739058,246.0,282,e018fe76821708b288a7578156c0d97d1a661e30,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the second slider also look good. The number of slides shown at a time has been increased from 3 to 4, which aligns with the PR description. The responsive settings have been updated similarly to the first slider, ensuring a consistent user experience across different screen sizes.

```diff
+ 					slidesToShow: 4,  // Show 4 items at a time
+ 					appendArrows: $this.parent(),  // Ensure arrows are appended correctly for this slider
+ 					appendDots: $this.parent(),    // Append dots in the correct place
+ 					responsive: [
+ 						{
+ 							breakpoint: 1028,
+ 							settings: {
+ 								slidesToShow: 3,
+ 								slidesToScroll: 1,
+ 								draggable: true,
+ 								arrows: true,
+ 								swipe: true,
+ 								dots: true,
+ 							}
+ 						},
+ 						{
+ 							breakpoint: 782,
+ 							settings: {
+ 								slidesToShow: 1,
+ 								slidesToScroll: 1,
+ 								draggable: true,
+ 								arrows: true,
+ 								swipe: true,
+ 								dots: true,
+ 							}
+ 						}
+ 					]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		// Second slider: .lsx-to-slider.travel-information
+		$( '.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
+			var $this = $( this );
+	
+			lsx_to.pre_build_slider( $this );
+	
+			// Ensure the second slider has 4 slides showing
+			if ( 1 < $this.children().length ) {
 				$this.slick( {
 					draggable: false,
 					infinite: true,
 					swipe: false,
 					dots: false,
-					slidesToShow: 3,
-					slidesToScroll: 1,
+					slidesToShow: 4,  // Show 4 items at a time
+					slidesToScroll: 1, // Scroll 1 item at a time
 					autoplay: false,
 					autoplaySpeed: 0,
-					//appendArrows: $this.parent(),
-					//appendDots: $this.parent(),
+					appendArrows: $this.parent(),  // Ensure arrows are appended correctly for this slider
+					appendDots: $this.parent(),    // Append dots in the correct place
 					responsive: [
 						{
 							breakpoint: 1028,
 							settings: {
-								slidesToShow:   2,
+								slidesToShow: 3,
 								slidesToScroll: 1,
 								draggable: true,
-								arrows: false,
+								arrows: true,
 								swipe: true,
 								dots: true,
 							}
 						},
 						{
 							breakpoint: 782,
 							settings: {
-								slidesToShow:   1,
+								slidesToShow: 1,
 								slidesToScroll: 1,
 								draggable: true,","--- 

+++ 

@@ -51,7 +51,11 @@

 			lsx_to.readMoreText = $(this).contents().filter(function() {
 				return this.nodeType === Node.TEXT_NODE;
 			}).text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+
+			console.log($(this));
+			console.log($(this).parent( '.wp-block-group' ).find('.wp-block-post-content'));
+
+			lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 		} );
 
 		$( '.single-tour-operator .wp-block-read-more' ).on( 'click', function( event ) {
@@ -59,9 +63,9 @@

 			$( this ).hide();
 
 			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			} else {
-				lsx_to.readMoreOpen( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreOpen( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			}
 
 			$( this ).show();
@@ -69,8 +73,6 @@

 	};
 
 	lsx_to.readMoreSet = function( button, contentWrapper ) {
-		console.log(contentWrapper);
-		console.log(contentWrapper.length);
 		if ( 0 < contentWrapper.length ) {
 			if ( 1 < contentWrapper.children().length ) {
 
@@ -118,17 +120,17 @@

 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).each( function() {
 			lsx_to.readMoreTIText = $(this).find('a').text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 		} );
 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
-			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			if ( $( this ).find('a').hasClass( 'less-link' ) ) {
+				lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			} else {
-				lsx_to.readMoreOpenTI( $(this), $(this).closest( '.additional-info' ).find('.content') );
+				lsx_to.readMoreOpen( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			}
 
 			$( this ).show();
@@ -244,7 +246,7 @@

 		} );
 	
 		// Second slider: .lsx-to-slider.travel-information
-		$( '.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
+		$( '.lsx-travel-information-wrapper.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
 			var $this = $( this );
 	
 			lsx_to.pre_build_slider( $this );
@@ -344,7 +346,7 @@

 	$document.ready( function() {
 		lsx_to.set_read_more();
 		lsx_to.set_read_more_travel_info();
-		lsx_to.set_read_more_itinerary();
+		//lsx_to.set_read_more_itinerary();
 		lsx_to.build_slider( window_width );
 	} );
 
@@ -359,7 +361,7 @@

 		lsx_to.build_slider_lightbox();
 	} );
 
-	document.addEventListener('DOMContentLoaded', function () {
+	/*document.addEventListener('DOMContentLoaded', function () {
 		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
 	
 		paragraphs.forEach(function (p) {
@@ -398,7 +400,7 @@

 				});
 			}
 		});
-	});	
+	});
 
 	document.addEventListener('DOMContentLoaded', function () {
 		// Select all sections within `.single-tour-operator`
@@ -446,6 +448,6 @@

 				});
 			}
 		});
-	});
+	});*/
 
 } )( jQuery, window, document );"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888123637,8.0,11,0406cff338cf303c116a4470e4a151874520c008,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` is generally discouraged in CSS as it breaks the natural cascading in your stylesheets. It's better to increase specificity of the selector or rethink your class structure instead.
```diff
- .slick-slide {
-   padding: 0 !important;
-   margin: 0 10px !important;
- }
+ .lsx-to-slider.travel-information .slick-slide {
+   padding: 0;
+   margin: 0 10px;
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	  .slick-slide {
+		padding: 0 !important;
+		margin: 0 10px !important;
+	  }","--- 

+++ 

@@ -1,5 +1,5 @@

 .lsx-to-slider {
-	&.travel-information {
+	&.lsx-travel-information-wrapper {
 	  .slick-list {
 		max-width: 100% !important;
 		width: 100% !important;
@@ -44,7 +44,11 @@

 				height: 20px;
 			}
 		}
-
+	}
+}
+
+.wp-block-query {
+	&.lsx-to-slider {
 		.slick-slide {
 			padding: 15px !important;
 		}"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888123645,246.0,282,0406cff338cf303c116a4470e4a151874520c008,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the second slider, `.lsx-to-slider.travel-information`, look good. The number of slides shown at different breakpoints has been updated according to the PR description. However, I noticed that the `autoplaySpeed` is set to 0 which means the autoplay speed is essentially non-existent. If you want to disable autoplay, you should set `autoplay: false` (which you've already done). But if you want autoplay to work, you should set `autoplaySpeed` to a positive integer representing the delay between auto slides in milliseconds.
```diff
-  autoplaySpeed: 0,
+  autoplaySpeed: 3000, // for example, change slides every 3 seconds
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		// Second slider: .lsx-to-slider.travel-information
+		$( '.lsx-travel-information-wrapper.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
+			var $this = $( this );
+	
+			lsx_to.pre_build_slider( $this );
+	
+			// Ensure the second slider has 4 slides showing
+			if ( 1 < $this.children().length ) {
 				$this.slick( {
 					draggable: false,
 					infinite: true,
 					swipe: false,
 					dots: false,
-					slidesToShow: 3,
-					slidesToScroll: 1,
+					slidesToShow: 4,  // Show 4 items at a time
+					slidesToScroll: 1, // Scroll 1 item at a time
 					autoplay: false,
 					autoplaySpeed: 0,
-					//appendArrows: $this.parent(),
-					//appendDots: $this.parent(),
+					appendArrows: $this.parent(),  // Ensure arrows are appended correctly for this slider
+					appendDots: $this.parent(),    // Append dots in the correct place
 					responsive: [
 						{
 							breakpoint: 1028,
 							settings: {
-								slidesToShow:   2,
+								slidesToShow: 3,
 								slidesToScroll: 1,
 								draggable: true,
-								arrows: false,
+								arrows: true,
 								swipe: true,
 								dots: true,
 							}
 						},
 						{
 							breakpoint: 782,
 							settings: {
-								slidesToShow:   1,
+								slidesToShow: 1,
 								slidesToScroll: 1,
 								draggable: true,","--- 

+++ 

@@ -51,7 +51,11 @@

 			lsx_to.readMoreText = $(this).contents().filter(function() {
 				return this.nodeType === Node.TEXT_NODE;
 			}).text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+
+			console.log($(this));
+			console.log($(this).parent( '.wp-block-group' ).find('.wp-block-post-content'));
+
+			lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 		} );
 
 		$( '.single-tour-operator .wp-block-read-more' ).on( 'click', function( event ) {
@@ -59,9 +63,9 @@

 			$( this ).hide();
 
 			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			} else {
-				lsx_to.readMoreOpen( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
+				lsx_to.readMoreOpen( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );
 			}
 
 			$( this ).show();
@@ -69,8 +73,6 @@

 	};
 
 	lsx_to.readMoreSet = function( button, contentWrapper ) {
-		console.log(contentWrapper);
-		console.log(contentWrapper.length);
 		if ( 0 < contentWrapper.length ) {
 			if ( 1 < contentWrapper.children().length ) {
 
@@ -118,17 +120,17 @@

 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).each( function() {
 			lsx_to.readMoreTIText = $(this).find('a').text();
-			lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 		} );
 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
-			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
+			if ( $( this ).find('a').hasClass( 'less-link' ) ) {
+				lsx_to.readMoreSet( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			} else {
-				lsx_to.readMoreOpenTI( $(this), $(this).closest( '.additional-info' ).find('.content') );
+				lsx_to.readMoreOpen( $(this).find('a'), $(this).closest( '.additional-info' ).find('.content') );
 			}
 
 			$( this ).show();
@@ -344,7 +346,7 @@

 	$document.ready( function() {
 		lsx_to.set_read_more();
 		lsx_to.set_read_more_travel_info();
-		lsx_to.set_read_more_itinerary();
+		//lsx_to.set_read_more_itinerary();
 		lsx_to.build_slider( window_width );
 	} );
 
@@ -359,7 +361,7 @@

 		lsx_to.build_slider_lightbox();
 	} );
 
-	document.addEventListener('DOMContentLoaded', function () {
+	/*document.addEventListener('DOMContentLoaded', function () {
 		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
 	
 		paragraphs.forEach(function (p) {
@@ -398,7 +400,7 @@

 				});
 			}
 		});
-	});	
+	});
 
 	document.addEventListener('DOMContentLoaded', function () {
 		// Select all sections within `.single-tour-operator`
@@ -446,6 +448,6 @@

 				});
 			}
 		});
-	});
+	});*/
 
 } )( jQuery, window, document );"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888123653,255.0,262,0406cff338cf303c116a4470e4a151874520c008,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` is generally discouraged in CSS as it breaks the natural cascading in your stylesheets. It's better to increase specificity of the selector or refactor the code to avoid conflicts. However, if this is necessary due to third-party CSS that you can't control, then it's understandable.
```diff
-  max-width: 100% !important;
-  width: 100% !important;
+  max-width: 100%;
+  width: 100%;

-  padding: 0 !important;
-  margin: 0 10px !important;
+  padding: 0;
+  margin: 0 10px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-to-slider.travel-information .slick-list {
+  max-width: 100% !important;
+  width: 100% !important;
+}
+.lsx-to-slider.travel-information .slick-slide {
+  padding: 0 !important;
+  margin: 0 10px !important;
+}","--- 

+++ 

@@ -252,11 +252,11 @@

   display: none;
 }
 
-.lsx-to-slider.travel-information .slick-list {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-list {
   max-width: 100% !important;
   width: 100% !important;
 }
-.lsx-to-slider.travel-information .slick-slide {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-slide {
   padding: 0 !important;
   margin: 0 10px !important;
 }
@@ -293,8 +293,8 @@

   left: 3px;
   height: 20px;
 }
-.wp-block-query.lsx-to-slider .slick-slide,
-.lsx-travel-information-wrapper.lsx-to-slider .slick-slide {
+
+.wp-block-query.lsx-to-slider .slick-slide {
   padding: 15px !important;
 }
 "
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888123658,296.0,297,0406cff338cf303c116a4470e4a151874520c008,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The padding value here seems to be inconsistent with the one defined on line 260. If the intention is to override the previous value, then it's fine. Otherwise, consider making them consistent.
```diff
-   padding: 15px !important;
+   padding: 0 !important;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.wp-block-query.lsx-to-slider .slick-slide,
+.lsx-travel-information-wrapper.lsx-to-slider .slick-slide {","--- 

+++ 

@@ -252,11 +252,11 @@

   display: none;
 }
 
-.lsx-to-slider.travel-information .slick-list {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-list {
   max-width: 100% !important;
   width: 100% !important;
 }
-.lsx-to-slider.travel-information .slick-slide {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-slide {
   padding: 0 !important;
   margin: 0 10px !important;
 }
@@ -293,8 +293,8 @@

   left: 3px;
   height: 20px;
 }
-.wp-block-query.lsx-to-slider .slick-slide,
-.lsx-travel-information-wrapper.lsx-to-slider .slick-slide {
+
+.wp-block-query.lsx-to-slider .slick-slide {
   padding: 15px !important;
 }
 "
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888123660,426.0,431,0406cff338cf303c116a4470e4a151874520c008,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It seems like there's a missing newline at the end of the file. It's a common convention to end files with a newline because some tools might not handle or display the last line correctly if it doesn't end with a newline.
```diff
- /*# sourceMappingURL=style.css.map */
- \ No newline at end of file
+ /*# sourceMappingURL=style.css.map */
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
-/* Styling for the label (e.g., ""Special Interests:"") */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:first-child p {
-  margin: 0;
-  white-space: nowrap; /* Prevent wrapping for label */
-  vertical-align: middle; /* Align text with the middle of the icon */
-  line-height: 1.2; /* Improve readability */
-  display: inline-block; /* Keep text inline */
-}
-
-/* Styling for the flowing text */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child {
-  display: flex;
-  flex-wrap: nowrap; /* Keep the text flowing inline */
-  white-space: normal; /* Allow wrapping for text if it overflows */
-  overflow-wrap: break-word; /* Ensure long words wrap correctly */
-  word-break: break-word; /* Break long words when needed */
-  flex: 1; /* Allow the container to use available space */
-  align-items: flex-start; /* Ensure text starts at the top */
-}
-
-/* Specific styling for the text content */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child p {
-  white-space: normal; /* Allow wrapping for long text */
-  overflow-wrap: break-word; /* Break words when needed */
-  word-break: break-word; /* Break words on legacy browsers */
-  line-height: 1.4; /* Improve readability */
-  text-transform: capitalize; /* Capitalize text */
-  padding: 0; /* Ensure no unnecessary padding */
-  margin: 0; /* Remove extra margins */
-  display: inline; /* Ensure the text flows inline with the label */
-}
-
-.facilities-list li::marker{display:none;}
-.facilities-list{padding-left:0!important;}
-
-.facilities-list li {
-  margin-left: 10px;
-  list-style: none; /* Remove default list marker */
-  position: relative; /* Position for pseudo-element */}
-
-.facilities-list li::before{
-    content: '';
-    position: absolute;
-    left: 0;
-    top: 50%;
-    transform: translateY(-50%); /* Center the icon vertically */
-    width: 20px; /* Adjust the size of the tick icon */
-    height: 20px;
-    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
-    background-size: contain;
-    background-repeat: no-repeat;
-    left:-28px
-}
-@media (min-width: 800px) {
-.single-tour-operator .toggle-button {display:none;}
-}
-
-/* Optional: Responsive adjustments for smaller screens */
-@media (max-width: 768px) {
-
-  /* Styling for the toggle button */
-.single-tour-operator .toggle-button {
-  display: inline-flex;
-  align-items: center;
-  padding: 5px;
-  background: none;
-  border: none;
-  cursor: pointer;
-  font-size: 14px;
-  color: var(--wp--preset--color--contrast);
-}
-
-/* Styling for the SVG icons */
-.single-tour-operator .toggle-icon {
-  width: 16px;
-  height: 16px;
-  fill: var(--wp--preset--color--contrast);
-  margin-left: 5px;
-}
-
-/* Collapsed state for the second wp-block-group */
-.single-tour-operator section.wp-block-group .wp-block-group.collapsed {
-  display: none; /* Hide the content */
-}
-
-
-  .fast-facts-wrapper > .wp-block-group {
-    flex-wrap: wrap; /* Allow wrapping for better readability */
-    gap: 5px;
-  }
-
-  .fast-facts-wrapper > .wp-block-group .wp-block-group.is-layout-flex:last-child {
-    flex-wrap: wrap; /* Allow text to wrap on smaller screens */
-  }
+.lsx-itinerary-wrapper .hidden,
+.lsx-units-wrapper .hidden {
+  display: none;
 }
+/*# sourceMappingURL=style.css.map */","--- 

+++ 

@@ -252,11 +252,11 @@

   display: none;
 }
 
-.lsx-to-slider.travel-information .slick-list {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-list {
   max-width: 100% !important;
   width: 100% !important;
 }
-.lsx-to-slider.travel-information .slick-slide {
+.lsx-to-slider.lsx-travel-information-wrapper .slick-slide {
   padding: 0 !important;
   margin: 0 10px !important;
 }
@@ -293,8 +293,8 @@

   left: 3px;
   height: 20px;
 }
-.wp-block-query.lsx-to-slider .slick-slide,
-.lsx-travel-information-wrapper.lsx-to-slider .slick-slide {
+
+.wp-block-query.lsx-to-slider .slick-slide {
   padding: 15px !important;
 }
 "
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888233080,1.0,13,8250fb9ea11e1405e49d3f89ce1b790b343d3124,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the `.lsx-to-slider` class and its child classes are good for improving the layout of the slider. The use of `!important` is generally discouraged as it can make debugging difficult and increase CSS specificity unnecessarily. However, in this case, it seems necessary to override the styles from the Slick library or other conflicting styles.

```diff
- max-width: 100% !important;
- width: 100% !important;
+ max-width: 100%;
+ width: 100%;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-to-slider {
+	&.lsx-travel-information-wrapper {
+	  .slick-list {
+		max-width: 100% !important;
+		width: 100% !important;
+	  }
+  
+	  .slick-slide {
+		padding: 0 !important;
+		margin: 0 10px !important;
+	  }
+	}
+  }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888233086,47.0,51,8250fb9ea11e1405e49d3f89ce1b790b343d3124,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/scss/_slider.scss,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the use of `!important` should be avoided if possible. If there's no other way to override the styles, then it's acceptable.

```diff
- padding: 15px !important;
+ padding: 15px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	}
+}
 
+.wp-block-query {
+	&.lsx-to-slider {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888233089,255.0,262,8250fb9ea11e1405e49d3f89ce1b790b343d3124,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `!important` is generally discouraged as it increases the specificity of a rule and can lead to difficulties in overriding it later. If possible, consider increasing the specificity of your selectors instead.
```diff
-  max-width: 100% !important;
-  width: 100% !important;
+  max-width: 100%;
+  width: 100%;

-  padding: 0 !important;
-  margin: 0 10px !important;
+  padding: 0;
+  margin: 0 10px;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+.lsx-to-slider.lsx-travel-information-wrapper .slick-list {
+  max-width: 100% !important;
+  width: 100% !important;
+}
+.lsx-to-slider.lsx-travel-information-wrapper .slick-slide {
+  padding: 0 !important;
+  margin: 0 10px !important;
+}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888233098,426.0,431,8250fb9ea11e1405e49d3f89ce1b790b343d3124,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/css/style.css,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's good practice to end files with a newline character. This helps with version control systems like Git and tools like `cat`, `echo`, and others that might not correctly display or handle the file otherwise.
```diff
- /*# sourceMappingURL=style.css.map */
- \ No newline at end of file
+ /*# sourceMappingURL=style.css.map */
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
-/* Specific styling for the text content */
-.fast-facts-wrapper > .wp-block-group .wp-block-group:last-child p {
-  white-space: normal; /* Allow wrapping for long text */
-  overflow-wrap: break-word; /* Break words when needed */
-  word-break: break-word; /* Break words on legacy browsers */
-  line-height: 1.4; /* Improve readability */
-  text-transform: capitalize; /* Capitalize text */
-  padding: 0; /* Ensure no unnecessary padding */
-  margin: 0; /* Remove extra margins */
-  display: inline; /* Ensure the text flows inline with the label */
-}
-
-.facilities-list li::marker{display:none;}
-.facilities-list{padding-left:0!important;}
-
-.facilities-list li {
-  margin-left: 10px;
-  list-style: none; /* Remove default list marker */
-  position: relative; /* Position for pseudo-element */}
-
-.facilities-list li::before{
-    content: '';
-    position: absolute;
-    left: 0;
-    top: 50%;
-    transform: translateY(-50%); /* Center the icon vertically */
-    width: 20px; /* Adjust the size of the tick icon */
-    height: 20px;
-    background-image: url('../img/tick.svg'); /* Replace with the path to your tick.svg */
-    background-size: contain;
-    background-repeat: no-repeat;
-    left:-28px
-}
-@media (min-width: 800px) {
-.single-tour-operator .toggle-button {display:none;}
-}
-
-/* Optional: Responsive adjustments for smaller screens */
-@media (max-width: 768px) {
-
-  /* Styling for the toggle button */
-.single-tour-operator .toggle-button {
-  display: inline-flex;
-  align-items: center;
-  padding: 5px;
-  background: none;
-  border: none;
-  cursor: pointer;
-  font-size: 14px;
-  color: var(--wp--preset--color--contrast);
-}
-
-/* Styling for the SVG icons */
-.single-tour-operator .toggle-icon {
-  width: 16px;
-  height: 16px;
-  fill: var(--wp--preset--color--contrast);
-  margin-left: 5px;
-}
-
-/* Collapsed state for the second wp-block-group */
-.single-tour-operator section.wp-block-group .wp-block-group.collapsed {
-  display: none; /* Hide the content */
-}
-
-
-  .fast-facts-wrapper > .wp-block-group {
-    flex-wrap: wrap; /* Allow wrapping for better readability */
-    gap: 5px;
-  }
-
-  .fast-facts-wrapper > .wp-block-group .wp-block-group.is-layout-flex:last-child {
-    flex-wrap: wrap; /* Allow text to wrap on smaller screens */
-  }
+.lsx-itinerary-wrapper .hidden,
+.lsx-units-wrapper .hidden {
+  display: none;
 }
+/*# sourceMappingURL=style.css.map */",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888525351,54.0,58,3deaa0ba3b4cf2a1c1e51d825e693e48c93d7929,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The console.log statements should be removed from production code as they can expose sensitive information to the end user and are generally used for debugging purposes.
```diff
-			console.log($(this));
-			console.log($(this).parent( '.wp-block-group' ).find('.wp-block-post-content'));
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+			console.log($(this));
+			console.log($(this).parent( '.wp-block-group' ).find('.wp-block-post-content'));
+
+			lsx_to.readMoreSet( $(this), $(this).parent( '.wp-block-group' ).find('.wp-block-post-content') );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888525362,244.0,285,3deaa0ba3b4cf2a1c1e51d825e693e48c93d7929,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `autoplaySpeed` is set to 0, which means the slides will change instantly. This could lead to a poor user experience as users may not have enough time to read or interact with the content of each slide. Consider setting a reasonable value for `autoplaySpeed`.
```diff
- 					autoplaySpeed: 0,
+ 					autoplaySpeed: 3000, // 3 seconds
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				} );
+			}
+		} );
+	
+		// Second slider: .lsx-to-slider.travel-information
+		$( '.lsx-travel-information-wrapper.lsx-to-slider .travel-information:not(.slider-disabled)' ).each( function() {
+			var $this = $( this );
+	
+			lsx_to.pre_build_slider( $this );
+	
+			// Ensure the second slider has 4 slides showing
+			if ( 1 < $this.children().length ) {
 				$this.slick( {
 					draggable: false,
 					infinite: true,
 					swipe: false,
 					dots: false,
-					slidesToShow: 3,
-					slidesToScroll: 1,
+					slidesToShow: 4,  // Show 4 items at a time
+					slidesToScroll: 1, // Scroll 1 item at a time
 					autoplay: false,
 					autoplaySpeed: 0,
-					//appendArrows: $this.parent(),
-					//appendDots: $this.parent(),
+					appendArrows: $this.parent(),  // Ensure arrows are appended correctly for this slider
+					appendDots: $this.parent(),    // Append dots in the correct place
 					responsive: [
 						{
 							breakpoint: 1028,
 							settings: {
-								slidesToShow:   2,
+								slidesToShow: 3,
 								slidesToScroll: 1,
 								draggable: true,
-								arrows: false,
+								arrows: true,
 								swipe: true,
 								dots: true,
 							}
 						},
 						{
 							breakpoint: 782,
 							settings: {
-								slidesToShow:   1,
+								slidesToShow: 1,
 								slidesToScroll: 1,
 								draggable: true,
-								arrows: false,
+								arrows: true,",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888525365,,349,3deaa0ba3b4cf2a1c1e51d825e693e48c93d7929,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It seems like the function `set_read_more_itinerary()` has been commented out. If this function is no longer needed, it's better to remove the line entirely to keep the code clean.
```diff
- 		//lsx_to.set_read_more_itinerary();
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 		lsx_to.set_read_more();
 		lsx_to.set_read_more_travel_info();
-		lsx_to.set_read_more_itinerary();
+		//lsx_to.set_read_more_itinerary();",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888525371,361.0,367,3deaa0ba3b4cf2a1c1e51d825e693e48c93d7929,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [364-403]

There's a large block of commented-out code. If this code is not needed, it should be removed to improve readability and maintainability.
```diff
- /*document.addEventListener('DOMContentLoaded', function () {
- 		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
- 	
- 		paragraphs.forEach(function (p) {
- ...
- 		});
- 	});*/
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 		lsx_to.build_slider_lightbox();
 	} );
 
-	document.addEventListener('DOMContentLoaded', function () {
+	/*document.addEventListener('DOMContentLoaded', function () {
 		const paragraphs = document.querySelectorAll('.additional-info .wp-block-group.content p');
 	
 		paragraphs.forEach(function (p) {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1888525377,,451,3deaa0ba3b4cf2a1c1e51d825e693e48c93d7929,19ae5d5580ae4bffd4616c22c01283684a6a1eba,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This closing comment tag appears to be part of a larger block of commented-out code. If the code is not needed, it should be removed to improve readability and maintainability.
```diff
- 	});*/
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 			}
 		});
-	});
+	});*/",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779886,54.0,80,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `console.log` statements on lines 72 and 73 should be removed as they are likely used for debugging purposes and should not be present in production code. Also, the logic to hide elements based on a counter and limit could be optimized by using jQuery's `:gt()` selector which selects elements with an index greater than a number.
```diff
- console.log(contentWrapper);
- console.log(contentWrapper.length);
- if ( 0 < contentWrapper.length ) {
-     if ( 1 < contentWrapper.children().length ) {
-         var limit = 1;
-         let counter = 0;
-         contentWrapper.children().each( function() {
-             if ( limit <= counter ) {
-                 $(this).hide();
-             }
-             counter++;
-         });
-     }
- }
+ if (contentWrapper && contentWrapper.children().length > 1) {
+     contentWrapper.children(':gt(0)').hide();
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
 		} );
 
 		$( '.single-tour-operator .wp-block-read-more' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
 			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSet($( this ));
+				lsx_to.readMoreSet( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
 			} else {
-				lsx_to.readMoreOpen($( this ));
+				lsx_to.readMoreOpen( $(this), $(this).closest( '.wp-block-group' ).find('.wp-block-post-content') );
 			}
 
 			$( this ).show();
 		} );
 	};
 
-	lsx_to.readMoreSet = function( button ) {
-		let contentWrapper = button.closest( '.wp-block-group' ).find('.wp-block-post-content');
+	lsx_to.readMoreSet = function( button, contentWrapper ) {
+		console.log(contentWrapper);
+		console.log(contentWrapper.length);
 		if ( 0 < contentWrapper.length ) {
-			if ( 1 < contentWrapper.find('p').length ) {
+			if ( 1 < contentWrapper.children().length ) {
 
 				var limit = 1;
 				let counter = 0;
 
-				contentWrapper.find('p').each( function() {
+				contentWrapper.children().each( function() {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779895,91.0,101,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code is fine, but it would be better to check if the button exists before trying to hide it or change its text. This will prevent potential errors if the button does not exist.

```diff
- } else {
-     button.hide();
- }
+ } else if (button) {
+     button.hide();
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		} else {
+			button.hide();
 		}
 	}
 
-	lsx_to.readMoreOpen = function( button ) {
-		let contentWrapper = button.closest( '.wp-block-group' ).find('.wp-block-post-content p');
-		if ( 0 < contentWrapper.length ) {
-			contentWrapper.each( function() {
-				$(this).show();
+	lsx_to.readMoreOpen = function( button, contentWrapper ) {
+		if ( 0 < contentWrapper.children().length ) {
+			contentWrapper.children().each( function() {
+				if ( ! $(this).hasClass('wp-block-read-more') ) {
+					$(this).show();	
+				}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779900,121.0,135,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, it would be better to check if the element exists before trying to hide it or perform operations on it.

```diff
- $( this ).hide();
+ if ($(this)) {
+     $(this).hide();
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
 		} );
 
 		$( '.single-tour-operator .additional-info .lsx-to-more-link' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
 			if ( $( this ).hasClass( 'less-link' ) ) {
-				lsx_to.readMoreSetTI($( this ));
+				lsx_to.readMoreSet( $(this), $(this).closest( '.additional-info' ).find('.content') );
 			} else {
-				lsx_to.readMoreOpenTI($( this ));
+				lsx_to.readMoreOpenTI( $(this), $(this).closest( '.additional-info' ).find('.content') );
 			}
 
 			$( this ).show();
 		} );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779906,148.0,164,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Again, it would be better to check if the element exists before trying to hide it or perform operations on it.

```diff
- $( this ).hide();
+ if ($(this)) {
+     $(this).hide();
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		$( '.single-tour-operator .lsx-itinerary-wrapper .wp-block-read-more' ).each( function() {
+			$(this).show();
+			lsx_to.readMoreItinText = $(this).find('a').text();
+			lsx_to.readMoreSet( $(this), $(this).parent( 'div' ).find('.itinerary-description') );
+		} );
+
+		$( '.single-tour-operator .lsx-itinerary-wrapper .wp-block-read-more' ).on( 'click', function( event ) {
 			event.preventDefault();
 			$( this ).hide();
 
-			$( this ).parents( '#itinerary' ).find( '.itinerary-item.hidden' ).each( function() {
-				$( this ).removeClass( 'hidden' );
-			} );
+			if ( $( this ).hasClass( 'less-link' ) ) {
+				lsx_to.readMoreSet( $(this), $(this).parent( 'div' ).find('.itinerary-description') );
+			} else {
+				lsx_to.readMoreOpen( $(this), $(this).parent( 'div' ).find('.itinerary-description') );
+			}
+
+			$( this ).show();",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779909,,356,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This line is fine as it is. It's just calling a function when the document is ready, which is a common practice in jQuery.
```diff
- lsx_to.set_read_more_itinerary();
+ lsx_to.set_read_more_itinerary();
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 		lsx_to.set_read_more();
 		lsx_to.set_read_more_travel_info();
-		//lsx_to.set_read_more_itinerary();
+		lsx_to.set_read_more_itinerary();",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779920,36.0,71,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/classes/admin/class-setup.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of the `image_sizes` array is a good way to manage different image sizes in one place. However, it would be better to validate the values of width, height, and crop before using them. This will prevent potential errors if any of these keys are missing or have invalid values.

```diff
	public function register_image_sizes() {
		foreach ( $this->image_sizes as $key => $params ) {
			if (isset($params['width'], $params['height'], $params['crop'])) {
				add_image_size( $key, $params['width'], $params['height'], $params['crop'] );
			}
		}
	}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	public $post_types = [
+		'tour',
+		'accommodation',
+		'destination',
+	];
+
+	public $image_sizes = [
+		'lsx-to-card-list' => [
+			'title'  => 'TO Card (list)',
+			'ratio'  => '1:1',
+			'width'  => 300,
+			'height' => 300,
+			'crop'   => true
+		],
+		'lsx-to-card-grid' => [
+			'title'  => 'TO Card (grid)',
+			'ratio'  => '3:2',
+			'width'  => 400,
+			'height' => 250,
+			'crop'   => true
+		],
+		'lsx-to-gallery' => [
+			'title'  => 'TO Gallery',
+			'ratio'  => '3:2',
+			'width'  => 900,
+			'height' => 600,
+			'crop'   => true
+		],
+		'lsx-to-banner' => [
+			'title'  => 'TO Banner',
+			'ratio'  => '5:2',
+			'width'  => 1440,
+			'height' => 600,
+			'crop'   => true
+		]
+	];",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779924,381.0,405,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/classes/admin/class-setup.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This code registers the image sizes with WordPress and adds them to the editor options. It's a good practice to check if the `$params['title']` exists before using it to avoid PHP notices.

```diff
	public function editor_image_sizes( $sizes ) {
		$new_sizes = [];
		foreach ( $this->image_sizes as $key => $params ) {
			if (isset($params['title'])) {
				$new_sizes[ $key ] = $params['title'];
			}
		}
		return array_merge( $sizes, $new_sizes );
	}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	/**
+	 * Register the image sizes with WordPress
+	 *
+	 * @return void
+	 */
+	public function register_image_sizes() {
+		foreach ( $this->image_sizes as $key => $params ) {
+			add_image_size( $key, $params['width'], $params['height'], $params['crop'] );
+		}
+	}
+
+	/**
+	 * The array of image sizes from WordPress
+	 *
+	 * @param array $sizes
+	 * @return array
+	 */
+	public function editor_image_sizes( $sizes ) {
+		$new_sizes = [];
+		foreach ( $this->image_sizes as $key => $params ) {
+			$new_sizes[ $key ] = $params['title'];
+		}
+		return array_merge( $sizes, $new_sizes );
+	}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779927,287.0,289,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/template-tags/helpers.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `array_filter` before imploding the array is a good practice as it removes any empty or false values from the array. This can prevent issues where you might end up with extra commas due to empty values in your final string.
```diff
+    $value = array_filter($value);
     $value = implode( ', ', $value );
```
This change improves the robustness of the code by ensuring that only meaningful data (non-empty values) are processed.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				$value = array_filter($value);
 				$value = implode( ', ', $value );
 			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779934,38.0,39,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/patterns/room-card.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The class name has been changed from `itin-type-wrapper` to `unit-type-wrapper`. This change should be fine as long as it is consistent across all the files and the new class `unit-type-wrapper` is defined in the CSS with the required properties. If not, it might break the styling.

```diff
- <!-- wp:group {""className"":""itin-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
- <div class=""wp-block-group itin-type-wrapper"" style=""margin-top:0;margin-bottom:0"">
+ <!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+ <div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0"">
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		<!-- wp:group {""className"":""unit-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+		<div class=""wp-block-group unit-type-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779941,54.0,55,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/patterns/room-card.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the class name has been changed from `itin-type-wrapper` to `unit-price-wrapper`. Ensure that this change is consistent across all the files and the new class `unit-price-wrapper` is defined in the CSS with the required properties.

```diff
- <!-- wp:group {""className"":""itin-type-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
- <div class=""wp-block-group itin-type-wrapper"" style=""margin-top:0;margin-bottom:0"">
+ <!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+ <div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0"">
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		<!-- wp:group {""className"":""unit-price-wrapper"",""style"":{""spacing"":{""margin"":{""top"":""0"",""bottom"":""0""},""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
+		<div class=""wp-block-group unit-price-wrapper"" style=""margin-top:0;margin-bottom:0""><!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap"",""verticalAlignment"":""top""}} -->",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1867779944,,66,531927063580e3d1a32f51fba5d6b324e31a0624,16b7739a3245be9c65703d411373c26eb35527b8,includes/patterns/room-card.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The class name `amount` has been added to the paragraph element. This change should be fine as long as it is consistent across all the files and the new class `amount` is defined in the CSS with the required properties.

```diff
- <p class=""unit-price has-primary-700-color has-text-color has-link-color"" style=""padding-top:2px;padding-bottom:2px""></p>
+ <p class=""unit-price amount has-primary-700-color has-text-color has-link-color"" style=""padding-top:2px;padding-bottom:2px""></p>
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 		<!-- wp:group {""style"":{""spacing"":{""blockGap"":""5px""}},""layout"":{""type"":""flex"",""flexWrap"":""nowrap""}} -->
-		<div class=""wp-block-group""><!-- wp:paragraph {""className"":""unit-price"",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|primary-700""}}},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""textColor"":""primary-700""} -->
+		<div class=""wp-block-group""><!-- wp:paragraph {""className"":""unit-price amount"",""style"":{""elements"":{""link"":{""color"":{""text"":""var:preset|color|primary-700""}}},""spacing"":{""padding"":{""top"":""2px"",""bottom"":""2px""}}},""textColor"":""primary-700""} -->",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1743909207,,7,3276b33bd0890ec9ee5fc825332d3e4551a96a07,844eac2b7c0365e2ef663d079277535e1c232254,app/controllers/dynamic_links/redirects_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The check for `link.expires_at.present?` is redundant because `link.expires_at.past?` will return false if `expires_at` is `nil`. You can simplify this line by removing the `present?` check.
```diff
- raise ActiveRecord::RecordNotFound if link.expires_at.present? && link.expires_at.past?
+ raise ActiveRecord::RecordNotFound if link.expires_at&.past?
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-      else
-        raise ActiveRecord::RecordNotFound
-      end
+      raise ActiveRecord::RecordNotFound if link.expires_at.present? && link.expires_at.past?","--- 

+++ 

@@ -1,11 +1,13 @@

 module DynamicLinks
   class RedirectsController < ApplicationController
+
+    # Rails will return a 404 if the record is not found
     def show
       short_url = params[:short_url]
       link = ShortenedUrl.find_by!(short_url: short_url)
 
-      raise ActiveRecord::RecordNotFound if link.expires_at.present? && link.expires_at.past?
-      
+      raise ActiveRecord::RecordNotFound if link.expired?
+
       redirect_to link.url, status: :found, allow_other_host: true
     end
   end"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472487,179.0,184,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/legacy/class-tour.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are good for maintainability and flexibility. The code now supports multiple currency fields, which is a significant improvement over the previous version where only the 'price' field was supported. This change allows for easy addition of more currency fields in the future if needed. However, it's important to ensure that all fields listed in `$currency_fields` array are indeed intended to be treated as currency fields.

```diff
-		if ( get_post_type() === 'tour' && 'price' === $meta_key ) {
+		$currency_fields = [
+			'price',
+			'single_supplement'
+		];
+
+		if ( get_post_type() === 'tour' && in_array( $meta_key, $currency_fields ) ) {
```
This change also improves the readability of the code by making it clear what fields are considered currency fields.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		$currency_fields = [
+			'price',
+			'single_supplement'
+		];
+
+		if ( get_post_type() === 'tour' && in_array( $meta_key, $currency_fields ) ) {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472489,162.0,263,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,assets/js/blocks/tour.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The block variation 'lsx-tour-operator/single-supplement-wrapper' is being registered here. It seems to be well defined with appropriate attributes and innerBlocks. However, the image URL at line 201 is hardcoded which might cause issues if the file location changes or the file gets deleted. Consider storing such URLs in a configuration file or database and reference them from there.
```diff
- url: 'https://tour-operator.lsx.design/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg',
+ url: config.singleSupplementIconUrl,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	// Single Supplement
+	wp.blocks.registerBlockVariation('core/group', {
+		name: 'lsx-tour-operator/single-supplement-wrapper',
+		title: 'Single Supplement',
+		category: 'lsx-tour-operator',
 		attributes: {
-			align: 'wide',
-			className: 'lsx-include-exclude-wrapper',
+			metadata: {
+				name: 'Single Supplement',
+			},
+			className: 'lsx-single-supplement-wrapper',
 			style: {
 				spacing: {
-					padding: {
-						top: 'var:preset|spacing|x-small',
-						bottom: 'var:preset|spacing|x-small',
-						left: 'var:preset|spacing|x-small',
-						right: 'var:preset|spacing|x-small'
-					}
-				},
-				border: {
-					radius: '8px',
-					width: '1px'
+					blockGap: '5px'
 				}
 			},
-			backgroundColor: 'base',
-			borderColor: 'primary',
 			layout: {
-				type: 'constrained'
+				type: 'flex',
+				flexWrap: 'nowrap'
 			}
 		},
 		innerBlocks: [
-			[
-			'core/columns', {
-				align: 'wide',
-				style: {
-				spacing: {
-					blockGap: {
-					top: 'var:preset|spacing|medium',
-					left: 'var:preset|spacing|medium'
-					}
-				}
-				}
-			},
-			[
-				[
-				'core/column', {
-					width: '50%',
-					id: 'lsx-included-wrapper',
+			['core/group', {
 					style: {
-					spacing: {
-						blockGap: '0'
-					}
-					}
+						spacing: {
+							blockGap: '5px'
+						}
+					},
+						layout: {
+							type: 'flex',
+							flexWrap: 'nowrap',
+							verticalAlignment: 'top'
+						}
 				},
 				[
-					[
-					'core/paragraph', {
+					['core/image', {
+						id: 122733,
+						width: '20px',
+						sizeSlug: 'large',
+						linkDestination: 'none',
+						url: 'https://tour-operator.lsx.design/wp-content/uploads/2024/11/single-supplement-icon-black-52px-1.svg',
+						alt: ''
+					}],
+					['core/paragraph', {
 						style: {
-						elements: {
-							link: {
-							color: {
-								text: 'var:preset|color|primary-700'
-							}
+							spacing: {
+								padding: {
+									top: '2px',
+									bottom: '2px'
+								}
 							}
-						}
 						},
-						textColor: 'primary-700',
-						fontSize: 'medium'
-					},
-					[ 'Price Includes:' ]
-					],
-					[
-					'core/paragraph', {
-						metadata: {
-						bindings: {
-							content: {
-							source: 'lsx/post-meta',
-							args: { key: 'included' }
-							}
-						}
-						}
-					},
-					[]
-					]
+						fontSize: 'x-small',
+						content: '<strong>Single supplement:</strong>'
+					}]
 				]
-				],
-				[
-				'core/column', {
-					width: '50%',
-					className: 'lsx-not-included-wrapper',
+			],
+			['core/group', {
 					style: {
-					spacing: {
-						blockGap: '0'
-					}
+						spacing: {
+							blockGap: '5px'
+						},
+						layout: {
+							type: 'flex',
+							flexWrap: 'nowrap'
+						}
 					}
 				},
 				[
-					[
-					'core/paragraph', {
-						style: {
-						elements: {
-							link: {
-							color: {
-								text: 'var:preset|color|primary-700'
+					['core/paragraph', {
+						metadata: {
+							bindings: {
+								content: {
+									source: 'lsx/post-meta',
+									args: {
+										key: 'single_supplement'
+									}
+								}
 							}
+						},
+						className: 'amount has-primary-color has-text-color has-link-color',
+						style: {
+							elements: {
+								link: {
+									color: {
+										text: 'var:preset|color|primary-700'
+									}
+								}
+							},
+							spacing: {
+								padding: {
+									top: '2px',
+									bottom: '2px'
+								}
 							}
-						}
 						},
 						textColor: 'primary-700',
-						fontSize: 'medium'
-					},
-					[ 'Price Excludes:' ]
-					],
-					[
-					'core/paragraph', {
-						metadata: {
-						bindings: {
-							content: {
-							source: 'lsx/post-meta',
-							args: { key: 'not_included' }
-							}
-						}
-						}
-					},
-					[]
-					]
-				]
+						content: ''
+					}]
 				]
 			]
-			]
 		]
-	} );
+	});",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472492,,301,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,assets/js/blocks/tour.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The width property of the image has been changed from a number to a string. This could potentially cause issues if the consuming code expects a number. Ensure that this change is compatible with all parts of your application that use this property.



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 						'core/image',
 						{
-							width: 20,
+							width: '20px',",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472495,958.0,1078,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,assets/js/blocks/tour.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The block variation 'lsx-tour-operator/price-include-exclude' is being registered here. It seems to be well defined with appropriate attributes and innerBlocks. However, similar to the previous comment, consider storing URLs in a configuration file or database and reference them from there.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	// Price Included + Excluded
+	wp.blocks.registerBlockVariation( 'core/group', {
+		name: 'lsx-tour-operator/price-include-exclude',
+		title: 'Price Include & Exclude',
+		category: 'lsx-tour-operator',
+		attributes: {
+			align: 'wide',
+			metadata: {
+				name: 'Price Include & Exclude',
+			},
+			className: 'lsx-include-exclude-wrapper',
+			style: {
+				spacing: {
+					padding: {
+						top: 'var:preset|spacing|x-small',
+						bottom: 'var:preset|spacing|x-small',
+						left: 'var:preset|spacing|x-small',
+						right: 'var:preset|spacing|x-small'
+					}
+				},
+				border: {
+					radius: '8px',
+					width: '1px'
+				}
+			},
+			backgroundColor: 'base',
+			borderColor: 'primary',
+			layout: {
+				type: 'constrained'
+			}
+		},
+		innerBlocks: [
+			[ 'core/columns', {
+					align: 'wide',
+					style: {
+						spacing: {
+							blockGap: {
+								top: 'var:preset|spacing|medium',
+								left: 'var:preset|spacing|medium'
+							}
+						}
+					}
+				},
+				[
+					['core/column', {
+							width: '50%',
+							id: 'lsx-included-wrapper',
+							style: {
+								spacing: {
+									blockGap: '0'
+								}
+							}
+						},
+						[
+							[ 'core/paragraph', {
+								style: {
+									elements: {
+										link: {
+											color: {
+												text: 'var:preset|color|primary-700'
+											}
+										}
+									}
+								},
+								textColor: 'primary-700',
+								fontSize: 'medium',
+								content: '<strong>Price Includes:</strong>'
+							} ],
+							[ 'core/paragraph', {
+								metadata: {
+									bindings: {
+										content: {
+											source: 'lsx/post-meta',
+											args: { key: 'included' }
+										}
+									}
+								}
+							}]
+						]
+					],
+					[ 'core/column', {
+							width: '50%',
+							className: 'lsx-not-included-wrapper',
+							style: {
+								spacing: {
+									blockGap: '0'
+								}
+							}
+						},
+						[
+							[ 'core/paragraph', {
+								style: {
+								elements: {
+									link: {
+									color: {
+										text: 'var:preset|color|primary-700'
+									}
+									}
+								}
+								},
+								textColor: 'primary-700',
+								fontSize: 'medium',
+								content: '<strong>Price Excludes:</strong>'
+							}],
+							[ 'core/paragraph', {
+								metadata: {
+									bindings: {
+										content: {
+											source: 'lsx/post-meta',
+											args: { key: 'not_included' }
+										}
+									}
+								}
+							}]
+						]
+					]
+				]
+			]
+		]
+	} );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472498,215.0,226,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The check for `null` value before transforming the date and removing paragraph tags is a good addition. However, it's important to note that `wp_date()` function uses the timezone set in WordPress settings. If you want to use GMT time, consider using `gmdate()` instead.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			if ( null !== $value ) {
+				$date_transforms = [
+					'booking_validity_start',
+					'booking_validity_end',
+				];
+				if ( in_array( $source_args['key'], $date_transforms ) ) {
+					$value = wp_date( 'j M Y', $value );
+				}
+	
+				$value = preg_replace( '/^<p>(.*?)<\/p>$/', '$1', $value );
 			}
-
-			$value = preg_replace( '/^<p>(.*?)<\/p>$/', '$1', $value );
+			",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472500,314.0,317,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This change improves the code by ensuring that an empty string is assigned to `$value` if it is indeed empty. This prevents potential issues down the line where `$value` might be used expecting a string but gets a different data type.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+					if ( empty( $value ) ) {
+						$value = '';
 					}
+					$value = '<div class=""' . $classes . '""/>' . $value . '</div>';",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472501,587.0,590,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This change ensures that `$gallery` is always an array, which can prevent errors when trying to iterate over `$gallery` later in the code.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if ( ! is_array( $gallery ) ) {
+			$gallery = [ $gallery ];
+		}
+",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851472503,688.0,689,af885d7e77dba994219a9fd6d4e4503a8c5d425b,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of the `maybe_hide_varitaion` function seems to be a significant change. Ensure that this functionality is either no longer needed or has been implemented elsewhere. If not, this could lead to unexpected behavior.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 		return $block_content;
 	}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851538294,26.0,28,cb02978da7f6160c94dbbaf081fe782c30ae9af3,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new filter `render_block` is added here. Please ensure that the function `maybe_hide_varitaion` (which seems to have a typo, should be `maybe_hide_variation`) handles all edge cases and doesn't break any existing functionality.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		add_filter( 'render_block', array( $this, 'maybe_hide_varitaion' ), 10, 3 );
+
+		add_filter( 'posts_pre_query', array( $this, 'posts_pre_query' ), 10, 2 );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851538302,105.0,106,cb02978da7f6160c94dbbaf081fe782c30ae9af3,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This line of code seems to be for debugging purposes. Make sure to remove it before merging to avoid unnecessary logging in production.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		do_action( 'qm/debug', [ $key ] );
+",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1851538306,239.0,429,cb02978da7f6160c94dbbaf081fe782c30ae9af3,d9fe389d1d3816f4fc3295fa87daccb1f167d008,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This is a large block of code added to handle block variations and their visibility based on certain conditions. It's hard to review without full context, but make sure to thoroughly test this logic with different scenarios to ensure it works as expected. Also, consider breaking down this large function into smaller ones to improve readability and maintainability. 

Also, there's a typo in the function name `maybe_hide_varitaion`, it should be `maybe_hide_variation`. 

```diff
- public function maybe_hide_varitaion( $block_content, $parsed_block, $block_obj ) {
+ public function maybe_hide_variation( $block_content, $parsed_block, $block_obj ) {
```

In line 383, the function `post_ids_exist` is defined. This function checks if a post exists based on its ID. However, the docstring does not match the function's purpose. Please update the docstring to accurately reflect what the function does.

```diff
- * Determines if a post exists based on the ID.
- *
- *
- * @global wpdb $wpdb WordPress database abstraction object.
- *
- * @param string $title   Post title.
- * @param string $content Optional. Post content.
- * @param string $date    Optional. Post date.
- * @param string $type    Optional. Post type.
- * @param string $status  Optional. Post status.
- * @return int Post ID if post exists, 0 otherwise.
+ * Determines if posts exist based on their IDs.
+ *
+ * @global wpdb $wpdb WordPress database abstraction object.
+ *
+ * @param array|string $ids   Post IDs.
+ * @return int Count of existing posts.
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	/**
+	 * A function to detect variation, and alter the query args.
+	 * 
+	 * Following the https://developer.wordpress.org/news/2022/12/building-a-book-review-grid-with-a-query-loop-block-variation/
+	 *
+	 * @param string|null   $pre_render   The pre-rendered content. Default null.
+	 * @param array         $parsed_block The block being rendered.
+	 * @param WP_Block|null $parent_block If this is a nested block, a reference to the parent block.
+	 */
+	public function maybe_hide_varitaion( $block_content, $parsed_block, $block_obj ) {
+		// Determine if this is the custom block variation.
+		if ( ! isset( $parsed_block['blockName'] ) || ! isset( $parsed_block['attrs'] )  ) {
+			return $block_content;
+		}
+		$allowed_blocks = array(
+			'core/group',
+		);
+
+		if ( ! in_array( $parsed_block['blockName'], $allowed_blocks, true ) ) {
+			return $block_content; 
+		}
+		if ( ! isset( $parsed_block['attrs']['className'] ) || '' === $parsed_block['attrs']['className'] || false === $parsed_block['attrs']['className'] ) {
+			return $block_content;
+		}
+
+		$pattern = ""/(lsx|facts)-(.*?)-wrapper/"";
+		preg_match( $pattern, $parsed_block['attrs']['className'], $matches );
+
+		if ( empty( $matches ) ) {
+			return $block_content;
+		}
+
+		if ( ! empty( $matches ) && isset( $matches[0] ) ) {
+			// Save the first match to a variable
+			$key = str_replace( [ 'facts-', 'lsx-', '-wrapper' ], '', $matches[0] );
+		} else {
+			return $block_content;
+		}
+
+		/*
+		 * 1 - Check if it is an itinerary or a units query
+		 * 2 - See if it is a post query
+		 * 3 - See if it is a taxonomy query
+		 * 4 - Lastly default to the custom fields
+		 */
+
+		if ( 0 < stripos( $key, '-query' ) ) {
+			
+			$query_key      = str_replace( [ '-query' ], '', $key );
+			$current_parent = get_post_parent( get_the_ID() );
+
+			switch ( $query_key ) {
+				case 'regions':
+					// If the current item is not a country
+					if ( null !== $current_parent ) {
+						return '';
+					}
+
+					if ( false === lsx_to_item_has_children( get_the_ID(), 'destination' ) ) {
+						return '';
+					}
+
+				break;
+
+				case 'related-regions':
+					// If the current item is a country, then there wont be any other child regions.
+					if ( null === $current_parent ) {
+						return '';
+					}
+
+					if ( false === lsx_to_item_has_children( $current_parent, 'destination' ) ) {
+						return '';
+					}
+				
+				break;
+
+				case 'country':
+					// If the current item is not a country
+					if ( null === $current_parent ) {
+						return '';
+					}
+
+				break;
+
+				default:
+					if ( isset( $this->disabled[ $query_key ] ) ) {
+						return '';
+					}
+
+				break;
+			}
+		} else if ( taxonomy_exists( $key ) ) {
+			// Check to see if this is a taxonomy or a custom field.
+			$tax_args = array(
+				'fields' => 'ids'
+			);
+			if ( empty( wp_get_post_terms( get_the_ID(), $key, $tax_args ) ) ) {
+				$block_content = '';
+			}
+		} else {
+			$key        = str_replace( '-', '_', $key );
+			$key_array  = [ $key ];
+			$has_values = false;
+
+			// If this is a wrapper that houses many fields, then we need to review them all.
+			if ( 'include_exclude' === $key ) {
+				$key_array = [ 'included', 'not_included' ];
+			}
+
+			foreach ( $key_array as $meta_key ) {
+				$value = lsx_to_custom_field_query( $meta_key, '', '', false );
+				
+				// we need to see if the posts exist before we can use them
+				if ( stripos( $meta_key, '_to_' ) && 0 === $this->post_ids_exist( $value ) ) {
+					continue;
+				}
+
+				if ( ! empty( $value ) && '' !== $value ) {
+					$has_values = true;
+				}
+			}
+			
+			if ( false === $has_values ) {
+				$block_content = '';
+			}
+		}
+
+		return $block_content;
+	}
+
+	/**
+	* Determines if a post exists based on the ID.
+	*
+	*
+	* @global wpdb $wpdb WordPress database abstraction object.
+	*
+	* @param string $title   Post title.
+	* @param string $content Optional. Post content.
+	* @param string $date    Optional. Post date.
+	* @param string $type    Optional. Post type.
+	* @param string $status  Optional. Post status.
+	* @return int Post ID if post exists, 0 otherwise.
+	*/
+	protected function post_ids_exist( $ids ) {
+		global $wpdb;
+
+		if ( is_array( $ids ) ) {
+			$ids = implode( ',', $ids );
+		}
+
+		$ids = wp_unslash( sanitize_post_field( 'id', $ids, 0, 'db' ) );
+
+		$query = ""SELECT COUNT(ID)
+				  FROM $wpdb->posts
+				  WHERE 1=1
+				  AND ID IN (%s)
+				  AND post_status IN ('draft', 'publish')"";
+
+		return (int) $wpdb->get_var( $wpdb->prepare( $query, $ids ) );
+	}
+
+	/**
+	 * This function will grab our Featured query so we dont have to redo that.
+	 *
+	 * @param null $posts
+	 * @param WP_Query $query
+	 * @return null|array
+	 */
+	public function posts_pre_query( $posts, $query ) {
+		if ( isset( $query->query['lsx_to_featured'] ) && isset( $this->featured[ $query->query['lsx_to_featured'] ] ) ) {
+			do_action( 'qm/debug', $this->featured[ $query->query['lsx_to_featured'] ] );
+			$posts = $this->featured[ $query->query['lsx_to_featured'] ];
+		}
+		return $posts;
+	}
+
+	/**
+	 * Find the featured items for the current query
+	 *
+	 * @param array $query
+	 * @return array
+	 */
+	public function find_featured_items( $query ) {
+		$items = [];
+		$item_query = new \WP_Query( $query );
+		if ( $item_query->have_posts() ) {
+			$items = $item_query->posts;
+		}
+		return $items;
+	}",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512671,,6,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,test/lib/shortening_strategies/redis_counter_strategy_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from directly instantiating `RedisCounterStrategy` to using the `get_strategy` method of `StrategyFactory` is a good one. It abstracts away the details of object creation and allows for easier changes in the future if the instantiation process becomes more complex. However, it's important to ensure that the `:redis_counter` strategy is properly registered in the `StrategyFactory`, otherwise this line will fail.
```diff
- @url_shortener = DynamicLinks::ShorteningStrategies::RedisCounterStrategy.new
+ @url_shortener = DynamicLinks::StrategyFactory.get_strategy(:redis_counter)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," class DynamicLinks::ShorteningStrategies::RedisCounterStrategyTest < ActiveSupport::TestCase
   def setup
-    @url_shortener = DynamicLinks::ShorteningStrategies::RedisCounterStrategy.new
+    @url_shortener = DynamicLinks::StrategyFactory.get_strategy(:redis_counter)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512674,,6,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,test/lib/shortening_strategies/nano_id_strategy_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from direct instantiation of `NanoIDStrategy` to using the `get_strategy` method from `StrategyFactory` is a good one. It abstracts away the details of object creation and allows for better control over dependencies. However, it's important to handle potential exceptions that could be raised by `get_strategy`. If the `:nano_id` strategy isn't available, this will raise an error which should be caught and handled appropriately.
```diff
- @url_shortener = DynamicLinks::StrategyFactory.get_strategy(:nano_id)
+ begin
+   @url_shortener = DynamicLinks::StrategyFactory.get_strategy(:nano_id)
+ rescue => e
+   puts ""Error initializing url shortener: #{e.message}""
+ end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," class DynamicLinks::ShorteningStrategies::NanoIDStrategyTest < ActiveSupport::TestCase
   def setup
-    @url_shortener = DynamicLinks::ShorteningStrategies::NanoIDStrategy.new
+    @url_shortener = DynamicLinks::StrategyFactory.get_strategy(:nano_id)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512678,12.0,13,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ensure_nanoid_available` method is called before initializing the `NanoIDStrategy`. This is a good practice to ensure that the required dependencies are available. However, it would be better if we could also pass the `redis_config` to the `NanoIDStrategy` constructor as we do for `RedisCounterStrategy`, even if it's not used right now. It will make the code more consistent and future-proof in case `NanoIDStrategy` needs to use Redis in the future.
```diff
- ShorteningStrategies::NanoIDStrategy.new
+ ShorteningStrategies::NanoIDStrategy.new(DynamicLinks.configuration.redis_config)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        ensure_nanoid_available
+        ShorteningStrategies::NanoIDStrategy.new","--- 

+++ 

@@ -33,7 +33,13 @@

       begin
         require 'redis'
       rescue LoadError
-        Rails.logger.warn 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+        raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+      end
+
+      begin
+        require 'connection_pool'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""connection_pool"" to your Gemfile to use RedisCounterStrategy.'
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512682,15.0,16,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the above comment, the `ensure_redis_available` method is called before initializing the `RedisCounterStrategy`. But here, the `redis_config` from the `DynamicLinks` configuration is not passed to the `RedisCounterStrategy` constructor. This should be fixed to ensure that the custom Redis configuration is used when creating a new `RedisCounterStrategy`.
```diff
- ShorteningStrategies::RedisCounterStrategy.new
+ ShorteningStrategies::RedisCounterStrategy.new(DynamicLinks.configuration.redis_config)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        ensure_redis_available
         ShorteningStrategies::RedisCounterStrategy.new","--- 

+++ 

@@ -33,7 +33,13 @@

       begin
         require 'redis'
       rescue LoadError
-        Rails.logger.warn 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+        raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+      end
+
+      begin
+        require 'connection_pool'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""connection_pool"" to your Gemfile to use RedisCounterStrategy.'
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512687,32.0,37,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code ensures that the 'redis' gem is available before using it. If it's not available, it logs a warning message. This is a good practice to inform the user about missing dependencies. However, consider raising an error instead of just logging a warning. This way, the application won't start without the necessary dependencies, preventing potential issues at runtime.
```diff
- Rails.logger.warn 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+ raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.ensure_redis_available
+      begin
+        require 'redis'
+      rescue LoadError
+        Rails.logger.warn 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+      end","--- 

+++ 

@@ -33,7 +33,13 @@

       begin
         require 'redis'
       rescue LoadError
-        Rails.logger.warn 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+        raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+      end
+
+      begin
+        require 'connection_pool'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""connection_pool"" to your Gemfile to use RedisCounterStrategy.'
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440512688,14.0,18,fb2ca6aeb4acb9eaa4c579e899e137c30e8b8016,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `initialize` method in the `RedisCounterStrategy` class is creating a new Redis connection every time an instance of this class is created. This could lead to a large number of open connections if many instances are created, which can be inefficient and potentially exceed the limit of open connections allowed by the Redis server.

Consider using a connection pool to manage your Redis connections. The `connection_pool` gem is a good option for this. Here's how you might modify your code:

```diff
- @redis = Redis.new(redis_config)
+ @redis = ConnectionPool.new(size: 5, timeout: 5) { Redis.new(redis_config) }
```

In this example, the connection pool will maintain up to 5 connections to Redis. If all connections are in use and another thread tries to get a connection, it will wait up to 5 seconds before raising a `Timeout::Error`.

Remember to add `require 'connection_pool'` at the top of your file and ensure that the `connection_pool` gem is added to your Gemfile.

Also, please note that you should handle the case where `redis_config` is not provided and `DynamicLinks.configuration.redis_config` is also nil or empty. In such cases, you should either provide a default configuration or raise an error indicating that a Redis configuration is required.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      def initialize(redis_config = nil)
+        super()
+        redis_config = redis_config.presence || DynamicLinks.configuration.redis_config
         # TODO: use pool of connections
-        @redis = Redis.new
+        @redis = Redis.new(redis_config)","--- 

+++ 

@@ -13,19 +13,25 @@

 
       def initialize(redis_config = nil)
         super()
-        redis_config = redis_config.presence || DynamicLinks.configuration.redis_config
-        # TODO: use pool of connections
-        @redis = Redis.new(redis_config)
+        pool_size = DynamicLinks.configuration.redis_pool_size
+        pool_timeout = DynamicLinks.configuration.redis_pool_timeout
+
+        @redis = ConnectionPool.new(size: pool_size, timeout: pool_timeout) do
+          redis_config = redis_config.presence || DynamicLinks.configuration.redis_config
+          Redis.new(redis_config)
+        end
       end
 
       # Shortens the given URL using a Redis counter
       # @param url [String] The URL to shorten
       # @return [String] The shortened URL, 12 characters long
       def shorten(url, min_length: MIN_LENGTH)
-        counter = @redis.incr(REDIS_COUNTER_KEY)
-
-        short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
-        short_url.ljust(min_length, '0')
+        @redis.with do |conn|
+          counter = conn.incr(REDIS_COUNTER_KEY)
+          short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+          short_url = short_url.ljust(min_length, '0')
+          short_url
+        end
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440525091,12.0,16,57e9fc02ff1fc736727398ea2c7d4e8a8260a53a,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ensure_nanoid_available` and `ensure_redis_available` methods are called before initializing the respective strategies. This is a good approach to ensure that the necessary dependencies are available before attempting to use them. However, these methods are currently raising exceptions when the dependencies are not found. It might be better to log a warning message instead of raising an exception, as this would allow the application to continue running even if some features are unavailable due to missing dependencies.
```diff


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        ensure_nanoid_available
+        ShorteningStrategies::NanoIDStrategy.new
       when :redis_counter
+        ensure_redis_available
         ShorteningStrategies::RedisCounterStrategy.new",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440525097,24.0,29,57e9fc02ff1fc736727398ea2c7d4e8a8260a53a,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

-        raise 'Missing dependency: Please add ""nanoid"" to your Gemfile to use NanoIdStrategy.'
+        puts 'Warning: Missing dependency ""nanoid"". Please add it to your Gemfile to use NanoIdStrategy.'



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.ensure_nanoid_available
+      begin
+        require 'nanoid'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""nanoid"" to your Gemfile to use NanoIdStrategy.'
+      end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440525099,32.0,43,57e9fc02ff1fc736727398ea2c7d4e8a8260a53a,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/strategy_factory.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

-        raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
-        ...
-        raise 'Missing dependency: Please add ""connection_pool"" to your Gemfile to use RedisCounterStrategy.'
+        puts 'Warning: Missing dependency ""redis"". Please add it to your Gemfile to use RedisCounterStrategy.'
+        ...
+        puts 'Warning: Missing dependency ""connection_pool"". Please add it to your Gemfile to use RedisCounterStrategy.'
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.ensure_redis_available
+      begin
+        require 'redis'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""redis"" to your Gemfile to use RedisCounterStrategy.'
+      end
+
+      begin
+        require 'connection_pool'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""connection_pool"" to your Gemfile to use RedisCounterStrategy.'
+      end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440525104,28.0,35,57e9fc02ff1fc736727398ea2c7d4e8a8260a53a,5867d8428557fa15611953cad499c9eb74d78a9e,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `short_url` variable is assigned twice, once inside the `@redis.with` block and once outside. The assignment outside the block is unnecessary and can be removed. Also, the `short_url.ljust(min_length, '0')` line doesn't actually modify `short_url` because `String#ljust` returns a new string instead of modifying the original one. You need to assign the result back to `short_url`.

```diff
-         short_url = nil
-         @redis.with do |conn|
-           counter = conn.incr(REDIS_COUNTER_KEY)
-           short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
-           short_url.ljust(min_length, '0')
-         end
-         short_url
+         @redis.with do |conn|
+           counter = conn.incr(REDIS_COUNTER_KEY)
+           short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+           short_url = short_url.ljust(min_length, '0')
+           short_url
+         end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","       def shorten(url, min_length: MIN_LENGTH)
-        counter = @redis.incr(REDIS_COUNTER_KEY)
-
-        short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
-        short_url.ljust(min_length, '0')
+        short_url = nil
+        @redis.with do |conn|
+          counter = conn.incr(REDIS_COUNTER_KEY)
+          short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+          short_url.ljust(min_length, '0')
+        end
+        short_url","--- 

+++ 

@@ -26,13 +26,12 @@

       # @param url [String] The URL to shorten
       # @return [String] The shortened URL, 12 characters long
       def shorten(url, min_length: MIN_LENGTH)
-        short_url = nil
         @redis.with do |conn|
           counter = conn.incr(REDIS_COUNTER_KEY)
           short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
-          short_url.ljust(min_length, '0')
+          short_url = short_url.ljust(min_length, '0')
+          short_url
         end
-        short_url
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438994081,6.0,10,da4252e1aabbb38f3c4eeb6f900368f633d4736f,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,lib/dynamic_links/shortening_strategies/nano_id_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling for missing dependencies is a good addition. However, it would be better to provide more detailed instructions in the error message. For instance, you could specify that the gem should be added to the Gemfile and then installed using `bundle install`.
```diff
- raise 'Missing dependency: Please add ""nanoid"" to your Gemfile to use NanoIdStrategy.'
+ raise 'Missing dependency: Please add ""nanoid"" to your Gemfile and run `bundle install` to use NanoIdStrategy.'
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      begin
+        require 'nanoid'
+      rescue LoadError
+        raise 'Missing dependency: Please add ""nanoid"" to your Gemfile to use NanoIdStrategy.'
+      end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438994083,21.0,22,da4252e1aabbb38f3c4eeb6f900368f633d4736f,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,dynamic_links.gemspec,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are moving 'nanoid' and 'redis' from regular dependencies to development dependencies. This could potentially break the application if it relies on these gems in the production environment. If the `NanoIdStrategy` or `RedisCounterStrategy` are used in a production environment, they will fail because these gems won't be installed there.

Please ensure that these gems are not required in the production environment, or consider adding them as optional dependencies that can be included based on the configuration of the gem user. 

Here's an example of how you might handle optional dependencies:

```ruby
begin
  require 'nanoid'
rescue LoadError
  raise ""You must add 'nanoid' to your Gemfile to use the NanoIdStrategy""
end
```

This way, if the user tries to use a strategy that requires a gem not included in their Gemfile, they'll get a clear error message telling them what to do.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  spec.add_development_dependency 'nanoid', '~> 2.0'
+  spec.add_development_dependency 'redis', '>= 4'",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438994086,27.0,39,da4252e1aabbb38f3c4eeb6f900368f633d4736f,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are good for performance as it avoids repeated constantization of strategy classes. However, there is a potential issue with thread safety. If multiple threads call `shorten_url` at the same time, they could potentially interfere with each other's modifications to the `@strategy_classes` hash. To avoid this, consider using a thread-safe data structure or adding synchronization around the access to `@strategy_classes`.

Here's an example of how you might add synchronization:

```diff
+  @strategy_classes_mutex = Mutex.new

   def self.shorten_url(url)
     @strategy_classes ||= {}
     strategy_key = configuration.shortening_strategy.to_s

     begin
+      @strategy_classes_mutex.synchronize do
        @strategy_classes[strategy_key] ||= ""DynamicLinks::ShorteningStrategies::#{strategy_key.camelize}Strategy"".constantize
+      end
       strategy = @strategy_classes[strategy_key].new
     rescue NameError
       raise ""Invalid shortening strategy: #{strategy_key}""
     rescue ArgumentError
       raise ""#{@strategy_classes[strategy_key]} needs to be initialized with arguments""
     rescue => e
       raise ""Unexpected error while initializing the strategy: #{e.message}""
     end

     strategy.shorten(url)
   end
```
This ensures that only one thread can modify `@strategy_classes` at a time, preventing potential race conditions.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @strategy_classes ||= {}
+    strategy_key = configuration.shortening_strategy.to_s
+
     begin
-      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
-      strategy = strategy_class.new
+      @strategy_classes[strategy_key] ||= ""DynamicLinks::ShorteningStrategies::#{strategy_key.camelize}Strategy"".constantize
+      strategy = @strategy_classes[strategy_key].new
     rescue NameError
-      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
+      raise ""Invalid shortening strategy: #{strategy_key}""
     rescue ArgumentError
-      raise ""#{strategy_class} needs to be initialized with arguments""
+      raise ""#{@strategy_classes[strategy_key]} needs to be initialized with arguments""
+    rescue => e
+      raise ""Unexpected error while initializing the strategy: #{e.message}""
     end","--- 

+++ 

@@ -24,16 +24,15 @@

   end
 
   def self.shorten_url(url)
-    @strategy_classes ||= {}
-    strategy_key = configuration.shortening_strategy.to_s
+    strategy_key = configuration.shortening_strategy
 
     begin
-      @strategy_classes[strategy_key] ||= ""DynamicLinks::ShorteningStrategies::#{strategy_key.camelize}Strategy"".constantize
-      strategy = @strategy_classes[strategy_key].new
-    rescue NameError
-      raise ""Invalid shortening strategy: #{strategy_key}""
+      strategy = StrategyFactory.get_strategy(strategy_key)
+    rescue RuntimeError => e
+      # This will catch the 'Unknown strategy' error from the factory
+      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
     rescue ArgumentError
-      raise ""#{@strategy_classes[strategy_key]} needs to be initialized with arguments""
+      raise ""#{strategy_key} strategy needs to be initialized with arguments""
     rescue => e
       raise ""Unexpected error while initializing the strategy: #{e.message}""
     end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438995818,,20,913f98fd2c1aae6d26711399bbf1874dd60615af,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from `:Mock` to `:mock` is a good one, as it follows the Ruby convention of using lowercase symbols.

```diff
-      config.shortening_strategy = :Mock
+      config.shortening_strategy = :mock
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   test ""shorten_url invokes the correct strategy and shortens URL"" do
     DynamicLinks.configure do |config|
-      config.shortening_strategy = :Mock
+      config.shortening_strategy = :mock",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438995822,,35,913f98fd2c1aae6d26711399bbf1874dd60615af,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This change also adheres to the Ruby convention of using lowercase symbols. Good job!

```diff
-      config.shortening_strategy = :Mock
+      config.shortening_strategy = :mock
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   test ""generate_short_url returns the correct structure"" do
     DynamicLinks.configure do |config|
-      config.shortening_strategy = :Mock
+      config.shortening_strategy = :mock",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438995825,27.0,38,913f98fd2c1aae6d26711399bbf1874dd60615af,e86324b587afe1c92fe7a0ec4b2e2de4f622345e,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling in this section is well-implemented. It covers different types of exceptions that could occur during the initialization of the strategy, including `RuntimeError`, `ArgumentError`, and other unexpected errors. However, it's important to note that catching all exceptions with `rescue => e` can sometimes hide bugs or make debugging more difficult because it catches all StandardErrors (including system-level exceptions). Consider specifying the exact exceptions you want to catch.

```diff
-    rescue => e
+    rescue SpecificExceptionType => e
```
This way, you're only rescuing exceptions that you expect might happen and letting unexpected ones propagate up the call stack where they can be handled appropriately or notify you of an unexpected condition in your code.

Also, consider adding logging before raising the exceptions. This will help in debugging by providing a trace of what happened before the error occurred.

```ruby
rescue RuntimeError => e
  logger.error(""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}"")
  raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
rescue ArgumentError
  logger.error(""#{strategy_key} strategy needs to be initialized with arguments"")
  raise ""#{strategy_key} strategy needs to be initialized with arguments""
rescue SpecificExceptionType => e
  logger.error(""Unexpected error while initializing the strategy: #{e.message}"")
  raise ""Unexpected error while initializing the strategy: #{e.message}""
```

Remember to replace `SpecificExceptionType` with the actual exception type you want to catch.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    strategy_key = configuration.shortening_strategy
+
     begin
-      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
-      strategy = strategy_class.new
-    rescue NameError
-      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
+      strategy = StrategyFactory.get_strategy(strategy_key)
+    rescue RuntimeError => e
+      # This will catch the 'Unknown strategy' error from the factory
+      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
     rescue ArgumentError
-      raise ""#{strategy_class} needs to be initialized with arguments""
+      raise ""#{strategy_key} strategy needs to be initialized with arguments""
+    rescue => e
+      raise ""Unexpected error while initializing the strategy: #{e.message}""
     end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440585529,5.0,9,c0bdf1762ce023bde80055640904db341bf31a97,6f2b82ca24b3abfba303dae453ac0dfb56e63978,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling for an invalid API key is well implemented. It correctly returns a JSON response with an appropriate error message and HTTP status code.

```diff

 unless client
   render json: { error: 'Invalid API key' }, status: :unauthorized
   return
 end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
     unless client
-      render json: { error: 'invalid api key' }, status: :unauthorized
+      render json: { error: 'Invalid API key' }, status: :unauthorized
       return
     end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440585531,11.0,13,c0bdf1762ce023bde80055640904db341bf31a97,6f2b82ca24b3abfba303dae453ac0dfb56e63978,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling for an `InvalidURIError` is also well done. The controller catches the exception and responds with a suitable error message and HTTP status code.

```diff
 render json: DynamicLinks.generate_short_url(url, client), status: :created
 rescue DynamicLinks::InvalidURIError
 render json: { error: 'Invalid URL' }, status: :bad_request
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     render json: DynamicLinks.generate_short_url(url, client), status: :created
+  rescue DynamicLinks::InvalidURIError
+    render json: { error: 'Invalid URL' }, status: :bad_request",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440585534,33.0,37,c0bdf1762ce023bde80055640904db341bf31a97,6f2b82ca24b3abfba303dae453ac0dfb56e63978,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The URL validation and shortening process is tightly coupled with the creation of a `ShortenedUrl` record. This could lead to issues if you want to validate or shorten URLs without creating a record, or if you want to create a record with different parameters. Consider refactoring this code to separate these concerns.

```diff
-      raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
-      short_url = strategy.shorten(url)
-
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      if Validator.valid_url?(url)
+        short_url = strategy.shorten(url)
+        create_shortened_url_record(client, url, short_url)
+      else
+        raise InvalidURIError, 'Invalid URL'
+      end
```
In the above diff, `create_shortened_url_record` is a new method that encapsulates the creation of a `ShortenedUrl` record. This makes the code more modular and easier to test.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
+      short_url = strategy.shorten(url)
+
+      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s","--- 

+++ 

@@ -26,24 +26,25 @@

   end
 
   def self.shorten_url(url, client)
+    raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
+
     strategy_key = configuration.shortening_strategy
 
-    begin
-      strategy = StrategyFactory.get_strategy(strategy_key)
-      raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
-      short_url = strategy.shorten(url)
-
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-
+    strategy = begin
+      StrategyFactory.get_strategy(strategy_key)
     rescue RuntimeError => e
       # This will catch the 'Unknown strategy' error from the factory
       raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
     rescue ArgumentError
       raise ""#{strategy_key} strategy needs to be initialized with arguments""
-    rescue => e
+    rescue StandardError => e
       raise ""Unexpected error while initializing the strategy: #{e.message}""
     end
+
+    short_url = strategy.shorten(url)
+
+    short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
   end
 
   # mimic Firebase Dynamic Links API"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440585535,44.0,46,c0bdf1762ce023bde80055640904db341bf31a97,6f2b82ca24b3abfba303dae453ac0dfb56e63978,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This rescue block catches all exceptions and re-raises them with a generic error message. This could make debugging difficult because it obscures the original exception class and backtrace. Consider logging the original exception and backtrace before re-raising the error.

```diff
-      rescue => e
-        raise ""Unexpected error while initializing the strategy: #{e.message}""
+      rescue StandardError => e
+        Rails.logger.error(""Unexpected error while initializing the strategy: #{e.class}: #{e.message}\n#{e.backtrace.join(""\n"")}"")
+        raise
```
In the above diff, I've replaced the generic rescue block with `rescue StandardError => e` to avoid catching and obscuring non-StandardError exceptions (like system exit signals). I've also added a log statement to record the original exception class, message, and backtrace.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     rescue => e
       raise ""Unexpected error while initializing the strategy: #{e.message}""
     end","--- 

+++ 

@@ -26,24 +26,25 @@

   end
 
   def self.shorten_url(url, client)
+    raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
+
     strategy_key = configuration.shortening_strategy
 
-    begin
-      strategy = StrategyFactory.get_strategy(strategy_key)
-      raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
-      short_url = strategy.shorten(url)
-
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-
+    strategy = begin
+      StrategyFactory.get_strategy(strategy_key)
     rescue RuntimeError => e
       # This will catch the 'Unknown strategy' error from the factory
       raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
     rescue ArgumentError
       raise ""#{strategy_key} strategy needs to be initialized with arguments""
-    rescue => e
+    rescue StandardError => e
       raise ""Unexpected error while initializing the strategy: #{e.message}""
     end
+
+    short_url = strategy.shorten(url)
+
+    short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
   end
 
   # mimic Firebase Dynamic Links API"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979298,,7,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of `MIN_LENGTH` and `MAX_LENGTH` constants might lead to unpredictable lengths of the shortened URLs. If there are any constraints on the length of the URLs in your system, this could potentially cause issues. Consider reintroducing these constants or a similar mechanism to control the length of the shortened URLs.
```diff
+      MIN_LENGTH = 12
+      MAX_LENGTH = 15 # this one is just a guess
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   module ShorteningStrategies
+    # There is no guarantee of the length of the shortened URL
     class RedisCounterStrategy < BaseStrategy
       REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze","--- 

+++ 

@@ -2,8 +2,8 @@

 
 module DynamicLinks
   module ShorteningStrategies
-    # There is no guarantee of the length of the shortened URL
     class RedisCounterStrategy < BaseStrategy
+      MIN_LENGTH = 12
       REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze
 
       def initialize
@@ -14,12 +14,11 @@

       # Shortens the given URL using a Redis counter
       # @param url [String] The URL to shorten
       # @return [String] The shortened URL, 12 characters long
-      def shorten(url)
-        # Increment the counter in Redis
+      def shorten(url, min_length: MIN_LENGTH)
         counter = @redis.incr(REDIS_COUNTER_KEY)
 
-        # Convert the counter value to a Base62 string
-        base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url.ljust(min_length, '0')
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979299,,5,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comment ""There is no guarantee of the length of the shortened URL"" is important but it would be more helpful if it explained why there's no guarantee and what the implications might be. Consider expanding this comment to provide more context.
```diff
-     # There is no guarantee of the length of the shortened URL
+     # There is no guarantee of the length of the shortened URL due to the nature of the RedisCounterStrategy. This could lead to potential issues with systems expecting URLs of a certain length.
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 module DynamicLinks
   module ShorteningStrategies
+    # There is no guarantee of the length of the shortened URL","--- 

+++ 

@@ -2,8 +2,8 @@

 
 module DynamicLinks
   module ShorteningStrategies
-    # There is no guarantee of the length of the shortened URL
     class RedisCounterStrategy < BaseStrategy
+      MIN_LENGTH = 12
       REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze
 
       def initialize
@@ -14,12 +14,11 @@

       # Shortens the given URL using a Redis counter
       # @param url [String] The URL to shorten
       # @return [String] The shortened URL, 12 characters long
-      def shorten(url)
-        # Increment the counter in Redis
+      def shorten(url, min_length: MIN_LENGTH)
         counter = @redis.incr(REDIS_COUNTER_KEY)
 
-        # Convert the counter value to a Base62 string
-        base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url.ljust(min_length, '0')
       end
     end
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979300,10.0,40,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of the `DynamicLinks` module seems to be more flexible and configurable than the previous one. However, there is a potential issue with the `shorten_url` method. It assumes that the strategy class exists and can be initialized without arguments. If the configuration contains an invalid strategy name, this will raise a `NameError`. Also, if the strategy class requires initialization parameters, it will raise an `ArgumentError`. To improve error handling, consider adding a rescue block to handle these exceptions and provide a more user-friendly error message.

```diff
  def self.shorten_url(url)
    begin
      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
      strategy = strategy_class.new
    rescue NameError
      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
    rescue ArgumentError
      raise ""#{strategy_class} needs to be initialized with arguments""
    end
    strategy.shorten(url)
  end
```

This way, if an invalid strategy is configured or if the strategy class requires initialization parameters, a clear error message will be raised, making it easier for developers to debug the issue.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/configuration""
 
 module DynamicLinks
-  class UrlShortener
-    MIN_LENGTH = 5
+  class << self
+    attr_writer :configuration
 
-    def valid_url?(url)
-      uri = URI.parse(url)
-      uri.is_a?(URI::HTTP) || uri.is_a?(URI::HTTPS)
-    rescue URI::InvalidURIError
-      false
+    def configuration
+      @configuration ||= Configuration.new
     end
+
+    def configure
+      yield(configuration)
+    end
+  end
+
+  def self.shorten_url(url)
+    strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
+    strategy = strategy_class.new
+    strategy.shorten(url)
+  end
+
+  # mimic Firebase Dynamic Links API
+  def self.generate_short_url(original_url)
+    short_link = shorten_url(original_url)
+
+    {
+      shortLink: short_link,
+      previewLink: ""#{short_link}?preview=true"",
+      warning: []
+    }","--- 

+++ 

@@ -24,8 +24,14 @@

   end
 
   def self.shorten_url(url)
-    strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
-    strategy = strategy_class.new
+    begin
+      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
+      strategy = strategy_class.new
+    rescue NameError
+      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
+    rescue ArgumentError
+      raise ""#{strategy_class} needs to be initialized with arguments""
+    end
     strategy.shorten(url)
   end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979301,3.0,8,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The URL validation logic has been commented out. This could lead to invalid URLs being processed, which might cause unexpected behavior or errors downstream. It's important to validate input data before processing it.
```diff
-     # if !url_shortener.valid_url?(url)
-     #   render json: { error: 'invalid url' }, status: :bad_request
-     #   return
-     # end
+     if !DynamicLinks.valid_url?(url)
+       render json: { error: 'invalid url' }, status: :bad_request
+       return
+     end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     url = params.require(:url)
     # validate url
-    if !url_shortener.valid_url?(url)
-      render json: { error: 'invalid url' }, status: :bad_request
-      return
-    end
+    # if !url_shortener.valid_url?(url)
+    #   render json: { error: 'invalid url' }, status: :bad_request
+    #   return
+    # end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979302,,12,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The response format has changed from a JSON object with detailed information about the short link to just the short URL string. If the client expects the previous format, this could break compatibility. Consider returning a similar JSON structure as before.
```diff
-     render json: DynamicLinks.generate_short_url(url), status: :created
+     short_url = DynamicLinks.generate_short_url(url)
+     render json: {
+       shortLink: short_url,
+       previewLink: ""#{short_url}?preview"",
+       warning: []
+     }, status: :created
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-      previewLink: ""#{short_link.short_url}?preview}"",
-      warning: []
-    }, status: :created
+    render json: DynamicLinks.generate_short_url(url), status: :created",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979303,4.0,18,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,test/controllers/v1/short_links_controller_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case ""should get create"" has been updated to stub the `shorten_url` method of the `DynamicLinks` module. This is a good practice as it isolates the unit under test (the controller action) from dependencies (the URL shortening process). However, there's no assertion to check if the `shorten_url` method was actually called with the correct arguments. You can use Minitest's `assert_mock` to ensure this.

```diff
    DynamicLinks.stub :shorten_url, expected_short_link do
+      DynamicLinks.expect :shorten_url, expected_short_link, ['https://example.com']
      post '/v1/shortLinks', params: { url: 'https://example.com' }
...
    end
+    DynamicLinks.verify
  end
```
This change ensures that your test will fail if the `shorten_url` method is not called with the expected argument during the execution of the test.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""should get create"" do
+    expected_short_link = 'shortened_url'
+    DynamicLinks.stub :shorten_url, expected_short_link do
+      post '/v1/shortLinks', params: { url: 'https://example.com' }
+      expected_body_response = {
+        shortLink: expected_short_link,
+        previewLink: ""#{expected_short_link}?preview=true"",
+        warning: []
+      }.as_json
+      assert_response :success
+      content_type = ""application/json; charset=utf-8""
+      assert_equal content_type, @response.content_type
+      assert_equal expected_body_response, JSON.parse(response.body)
+    end
+  end","--- 

+++ 

@@ -3,13 +3,21 @@

 class DynamicLinks::V1::ShortLinksControllerTest < ActionDispatch::IntegrationTest
   test ""should get create"" do
     expected_short_link = 'shortened_url'
-    DynamicLinks.stub :shorten_url, expected_short_link do
+
+    dynamic_links_mock = Minitest::Mock.new
+    dynamic_links_mock.expect :call, expected_short_link, ['https://example.com']
+
+    DynamicLinks.stub :shorten_url, dynamic_links_mock do
       post '/v1/shortLinks', params: { url: 'https://example.com' }
+
+      dynamic_links_mock.verify
+
       expected_body_response = {
         shortLink: expected_short_link,
         previewLink: ""#{expected_short_link}?preview=true"",
         warning: []
       }.as_json
+
       assert_response :success
       content_type = ""application/json; charset=utf-8""
       assert_equal content_type, @response.content_type"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438979305,24.0,37,fa32083bcccff71791ba2bb86e8ec6293907ff1c,91c009b051139bcb938a626118393d0714cdfc19,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test ""generate_short_url returns the correct structure"" is also well written. It correctly stubs the `shorten_url` method and checks that the `generate_short_url` method returns the expected structure. Similar to the previous comment, you should reset the configuration after the test to prevent side effects in other tests.

```diff
  test ""generate_short_url returns the correct structure"" do
    DynamicLinks.configure do |config|
      config.shortening_strategy = :Mock
    end

    expected_response = {
      shortLink: 'shortened_url',
      previewLink: 'shortened_url?preview=true',
      warning: []
    }

    DynamicLinks.stub :shorten_url, 'shortened_url' do
      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com')
    end
+    ensure
+      DynamicLinks.reset_configuration
  end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""generate_short_url returns the correct structure"" do
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = :Mock
+    end
+
+    expected_response = {
+      shortLink: 'shortened_url',
+      previewLink: 'shortened_url?preview=true',
+      warning: []
+    }
+
+    DynamicLinks.stub :shorten_url, 'shortened_url' do
+      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com')
+    end","--- 

+++ 

@@ -2,6 +2,15 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
+  def setup
+    @original_strategy = DynamicLinks.configuration.shortening_strategy
+  end
+
+  def teardown
+    # Reset the configuration after each test
+    DynamicLinks.configuration.shortening_strategy = @original_strategy
+  end
+
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984942,4.0,26,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,test/controllers/v1/short_links_controller_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case has been refactored to use a mock object for the `DynamicLinks` module. This is a good practice as it isolates the controller from the actual implementation of the URL shortening process, making the test more reliable and easier to maintain. However, there's no assertion to check if the `shorten_url` method was called with the correct arguments. You should add an assertion to verify that.

```diff
+      assert_mock dynamic_links_mock
```
This will ensure that the `shorten_url` method was called with the expected arguments and the correct number of times.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""should get create"" do
+    expected_short_link = 'shortened_url'
+
+    dynamic_links_mock = Minitest::Mock.new
+    dynamic_links_mock.expect :call, expected_short_link, ['https://example.com']
+
+    DynamicLinks.stub :shorten_url, dynamic_links_mock do
+      post '/v1/shortLinks', params: { url: 'https://example.com' }
+
+      dynamic_links_mock.verify
+
+      expected_body_response = {
+        shortLink: expected_short_link,
+        previewLink: ""#{expected_short_link}?preview=true"",
+        warning: []
+      }.as_json
+
+      assert_response :success
+      content_type = ""application/json; charset=utf-8""
+      assert_equal content_type, @response.content_type
+      assert_equal expected_body_response, JSON.parse(response.body)
+    end
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984944,26.0,36,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten_url` method is not handling the case where the strategy class initialization fails due to reasons other than `NameError` and `ArgumentError`. It's a good practice to have a generic exception handler at the end of your rescue block to catch any unexpected exceptions. This will help in debugging and prevent the application from crashing due to unhandled exceptions.
```diff
  def self.shorten_url(url)
    begin
      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
      strategy = strategy_class.new
    rescue NameError
      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
    rescue ArgumentError
      raise ""#{strategy_class} needs to be initialized with arguments""
+34:     rescue => e
+35:       raise ""Unexpected error while initializing the strategy: #{e.message}""
    end
    strategy.shorten(url)
  end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def self.shorten_url(url)
+    begin
+      strategy_class = ""DynamicLinks::ShorteningStrategies::#{configuration.shortening_strategy.to_s.camelize}Strategy"".constantize
+      strategy = strategy_class.new
+    rescue NameError
+      raise ""Invalid shortening strategy: #{configuration.shortening_strategy}""
+    rescue ArgumentError
+      raise ""#{strategy_class} needs to be initialized with arguments""
     end
+    strategy.shorten(url)
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984945,6.0,7,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setup method is used to set the `@original_strategy` instance variable, but it's not clear why this is necessary. If you're trying to ensure that each test runs with a clean configuration, consider moving the configuration reset logic to the setup method instead of the teardown method. This way, even if a test fails and the teardown method doesn't run, the next test will still start with a clean configuration.

```diff
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-  end
+  def setup
+    @original_strategy = DynamicLinks.configuration.shortening_strategy
+    DynamicLinks.configuration.shortening_strategy = :MD5 # or whatever your default strategy is
+  end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @original_strategy = DynamicLinks.configuration.shortening_strategy
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984946,10.0,12,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The teardown method resets the configuration after each test, which is good practice. However, it would be better to handle potential exceptions in the tests so that the configuration is always reset, even if a test fails. You can use `ensure` for this.

```diff
-  def teardown
-    # Reset the configuration after each test
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-  end
+  def teardown
+    begin
+      # Your test code here...
+    ensure
+      # Reset the configuration after each test
+      DynamicLinks.configuration.shortening_strategy = @original_strategy
+    end
+  end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    # Reset the configuration after each test
+    DynamicLinks.configuration.shortening_strategy = @original_strategy
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984947,18.0,31,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This test case is well written. It correctly sets up a mock object, stubs the `new` method on the `MockStrategy` class to return the mock object, and verifies that the `shorten` method was called with the correct arguments. Good job!



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""shorten_url invokes the correct strategy and shortens URL"" do
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = :Mock
+    end
+
+    strategy_mock = Minitest::Mock.new
+    strategy_mock.expect :shorten, 'shortened_url', ['https://example.com']
+
+    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+      assert_equal 'shortened_url', DynamicLinks.shorten_url('https://example.com')
+    end
+
+    strategy_mock.verify
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438984948,33.0,47,f6d86e543ef84b4d3861eb20f79dec8704444373,91c009b051139bcb938a626118393d0714cdfc19,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This test case is also well written. It correctly stubs the `shorten_url` method on the `DynamicLinks` module to return a fixed string, and verifies that the `generate_short_url` method returns the correct structure. Good job!


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""generate_short_url returns the correct structure"" do
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = :Mock
+    end
+
+    expected_response = {
+      shortLink: 'shortened_url',
+      previewLink: 'shortened_url?preview=true',
+      warning: []
+    }
+
+    DynamicLinks.stub :shorten_url, 'shortened_url' do
+      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com')
+    end
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438985629,6.0,7,072ad3444c3727c3db8b00df5b3071b97db6d6ce,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `MIN_LENGTH` constant has been moved above the `REDIS_COUNTER_KEY` constant. This change is not necessary and does not improve readability or performance. It's generally a good practice to keep constants that are related close together, so it might be better to move `MIN_LENGTH` back below `REDIS_COUNTER_KEY`.
```diff
-        MIN_LENGTH = 12
-       REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze
+       REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze
+       MIN_LENGTH = 12
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","       MIN_LENGTH = 12
-      MAX_LENGTH = 15 # this one is just a guess
+      REDIS_COUNTER_KEY = ""dynamic_links:counter"".freeze",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1438985630,17.0,21,072ad3444c3727c3db8b00df5b3071b97db6d6ce,91c009b051139bcb938a626118393d0714cdfc19,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten` method now accepts an optional `min_length` parameter, which defaults to `MIN_LENGTH`. This is a good addition as it allows for flexibility in the length of the shortened URL. However, the padding of the shortened URL with zeros (`short_url.ljust(min_length, '0')`) could lead to confusion as multiple URLs could end up having the same shortened URL if the base62 encoding does not reach the minimum length. A better approach would be to ensure that the counter is large enough to always generate a unique URL of at least `min_length` characters.
```diff
-         short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
-         short_url.ljust(min_length, '0')
+         while (short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)).length < min_length
+           counter = @redis.incr(REDIS_COUNTER_KEY)
+         end
+         short_url
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      def shorten(url, min_length: MIN_LENGTH)
         counter = @redis.incr(REDIS_COUNTER_KEY)
 
-        # Convert the counter value to a Base62 string
-        base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url = base62_encode(""#{counter}#{url.hash.abs}"".to_i)
+        short_url.ljust(min_length, '0')",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336143,335.0,341,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code checks if there are any elements with the class `.wp-block-gallery.has-nested-images` and applies `slickLightbox` to them. This is a good change as it ensures that the lightbox functionality is only applied when necessary, improving performance.

```diff
+ 		if ( 0 <  $( '.wp-block-gallery.has-nested-images' ).length ) {
+ 			$( '.wp-block-gallery.has-nested-images' ).slickLightbox( {
+ 				caption: function( element, info ) {
+ 					return $( element ).find( 'img' ).attr( 'alt' );
+ 				}
+ 			} );
+ 		}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if ( 0 <  $( '.wp-block-gallery.has-nested-images' ).length ) {
+			$( '.wp-block-gallery.has-nested-images' ).slickLightbox( {
+				caption: function( element, info ) {
+					return $( element ).find( 'img' ).attr( 'alt' );
+				}
+			} );
+		}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336150,343.0,356,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,assets/js/src/custom.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code checks for images within a specific wrapper `.lsx-units-wrapper .unit-image a` and applies `slickLightbox` to them. However, there's a console.log statement on line 347 which should be removed as it's not a good practice to leave debugging logs in production code.

```diff
+ 		if ( 0 <  $( '.lsx-units-wrapper .unit-image a' ).length ) {
+ 			let roomImages = $('.lsx-units-wrapper .unit-image a img').map(function() {
+ 				return $(this).attr('src');
+ 			}).get();
- 			console.log(roomImages);
+ 
+ 			$( '.lsx-units-wrapper' ).slickLightbox( {
+ 				//images : roomImages,
+ 				itemSelector: '.unit-image a',
+ 				caption: function( element, info ) {
+ 					return $( element ).find( 'img' ).attr( 'alt' );
+ 				}
+ 			} );
+ 		}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if ( 0 <  $( '.lsx-units-wrapper .unit-image a' ).length ) {
+			let roomImages = $('.lsx-units-wrapper .unit-image a img').map(function() {
+				return $(this).attr('src');
+			}).get();
+			console.log(roomImages);
+
+			$( '.lsx-units-wrapper' ).slickLightbox( {
+				//images : roomImages,
+				itemSelector: '.unit-image a',
+				caption: function( element, info ) {
+					return $( element ).find( 'img' ).attr( 'alt' );
+				}
+			} );
+		}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336159,178.0,348,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/admin/class-setup.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `wp_kses_allowed_html` function is used to allow extra tags and attributes for the `wp_kses_post()` function. This is a good practice for security reasons, as it allows you to control what HTML elements and attributes are allowed. However, this list seems quite extensive and could potentially open up for vulnerabilities if not all of these tags and attributes are strictly necessary for your application. I would recommend reviewing this list and removing any tags or attributes that are not needed.

Also, it's important to note that allowing certain attributes like `onclick` (line 292) can potentially lead to XSS attacks if user input is not properly sanitized before being used in these attributes. Make sure that any data used in these attributes is properly sanitized before use.

```diff
- $allowedtags['input']['onclick'] = true;
```

Lastly, there is no newline at the end of the file (line 348). While this won't cause any issues with PHP, it's generally considered good practice to end a file with a newline.

```diff
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	/**
+	 * Allow extra tags and attributes to wp_kses_post()
+	 */
+	public function wp_kses_allowed_html( $allowedtags, $context ) {
+		if ( ! isset( $allowedtags['i'] ) ) {
+			$allowedtags['i'] = array();
+		}
+		$allowedtags['i']['aria-hidden']    = true;
+
+		if ( ! isset( $allowedtags['span'] ) ) {
+			$allowedtags['span'] = array();
+		}
+
+		$allowedtags['span']['aria-hidden'] = true;
+
+		if ( ! isset( $allowedtags['button'] ) ) {
+			$allowedtags['button'] = array();
+		}
+
+		$allowedtags['button']['aria-label']   = true;
+		$allowedtags['button']['data-dismiss'] = true;
+
+		if ( ! isset( $allowedtags['li'] ) ) {
+			$allowedtags['li'] = array();
+		}
+
+		$allowedtags['li']['data-target']   = true;
+		$allowedtags['li']['data-slide-to'] = true;
+
+		if ( ! isset( $allowedtags['a'] ) ) {
+			$allowedtags['a'] = array();
+		}
+
+		
+		$allowedtags['a']['target']             = true;
+		$allowedtags['a']['data-toggle']             = true;
+		$allowedtags['a']['data-target']             = true;
+		$allowedtags['a']['data-slide']              = true;
+		$allowedtags['a']['data-collapsed']          = true;
+		$allowedtags['a']['data-envira-caption']     = true;
+		$allowedtags['a']['data-envira-retina']      = true;
+		$allowedtags['a']['data-thumbnail']          = true;
+		$allowedtags['a']['data-mobile-thumbnail']   = true;
+		$allowedtags['a']['data-envirabox-type']     = true;
+		$allowedtags['a']['data-video-width']        = true;
+		$allowedtags['a']['data-video-height']       = true;
+		$allowedtags['a']['data-video-aspect-ratio'] = true;
+
+		if ( ! isset( $allowedtags['h2'] ) ) {
+			$allowedtags['h2'] = array();
+		}
+
+		$allowedtags['h2']['data-target'] = true;
+		$allowedtags['h2']['data-toggle'] = true;
+
+		if ( ! isset( $allowedtags['div'] ) ) {
+			$allowedtags['div'] = array();
+		}
+
+		$allowedtags['div']['aria-labelledby']                      = true;
+		$allowedtags['div']['data-interval']                        = true;
+		$allowedtags['div']['data-icon']                            = true;
+		$allowedtags['div']['data-id']                              = true;
+		$allowedtags['div']['data-class']                           = true;
+		$allowedtags['div']['data-long']                            = true;
+		$allowedtags['div']['data-lat']                             = true;
+		$allowedtags['div']['data-zoom']                            = true;
+		$allowedtags['div']['data-link']                            = true;
+		$allowedtags['div']['data-thumbnail']                       = true;
+		$allowedtags['div']['data-title']                           = true;
+		$allowedtags['div']['data-type']                            = true;
+		$allowedtags['div']['data-cluster-small']                   = true;
+		$allowedtags['div']['data-cluster-medium']                  = true;
+		$allowedtags['div']['data-cluster-large']                   = true;
+		$allowedtags['div']['data-fusion-tables']                   = true;
+		$allowedtags['div']['data-fusion-tables-colour-border']     = true;
+		$allowedtags['div']['data-fusion-tables-width-border']      = true;
+		$allowedtags['div']['data-fusion-tables-colour-background'] = true;
+		$allowedtags['div']['itemscope']                            = true;
+		$allowedtags['div']['itemtype']                             = true;
+		$allowedtags['div']['data-row-height']                      = true;
+		$allowedtags['div']['data-justified-margins']               = true;
+		$allowedtags['div']['data-slick']                           = true;
+
+		//Envirta Gallery tags
+		//
+		$allowedtags['div']['data-envira-id']                       = true;
+		$allowedtags['div']['data-gallery-config']                  = true;
+		$allowedtags['div']['data-gallery-images']                  = true;
+		$allowedtags['div']['data-gallery-theme']                   = true;
+		$allowedtags['div']['data-envira-columns']                  = true;
+
+		if ( ! isset( $allowedtags['img'] ) ) {
+			$allowedtags['img'] = array();
+		}
+
+		$allowedtags['img']['data-envira-index']      = true;
+		$allowedtags['img']['data-envira-caption']    = true;
+		$allowedtags['img']['data-envira-gallery-id'] = true;
+		$allowedtags['img']['data-envira-item-id']    = true;
+		$allowedtags['img']['data-envira-src']        = true;
+		$allowedtags['img']['data-envira-srcset']     = true;
+
+		if ( ! isset( $allowedtags['input'] ) ) {
+			$allowedtags['input'] = array();
+		}
+
+		$allowedtags['input']['type']    = true;
+		$allowedtags['input']['id']      = true;
+		$allowedtags['input']['name']    = true;
+		$allowedtags['input']['value']   = true;
+		$allowedtags['input']['size']    = true;
+		$allowedtags['input']['checked'] = true;
+		$allowedtags['input']['onclick'] = true;
+		$allowedtags['input']['class'] = true;
+		$allowedtags['input']['placeholder'] = true;
+		$allowedtags['input']['autocomplete'] = true;
+
+		if ( ! isset( $allowedtags['select'] ) ) {
+			$allowedtags['select'] = array();
+		}
+
+		$allowedtags['select']['name']     = true;
+		$allowedtags['select']['id']       = true;
+		$allowedtags['select']['disabled'] = true;
+		$allowedtags['select']['onchange'] = true;
+
+		if ( ! isset( $allowedtags['option'] ) ) {
+			$allowedtags['option'] = array();
+		}
+
+		$allowedtags['option']['value']    = true;
+		$allowedtags['option']['selected'] = true;
+
+		if ( ! isset( $allowedtags['iframe'] ) ) {
+			$allowedtags['iframe'] = array();
+		}
+
+		$allowedtags['iframe']['src']             = true;
+		$allowedtags['iframe']['width']           = true;
+		$allowedtags['iframe']['height']          = true;
+		$allowedtags['iframe']['frameborder']     = true;
+		$allowedtags['iframe']['allowfullscreen'] = true;
+		$allowedtags['iframe']['style']           = true;
+
+		if ( ! isset( $allowedtags['noscript'] ) ) {
+			$allowedtags['noscript'] = array();
+		}
+
+		return $allowedtags;
+	}
+
+	/**
+	 * Allow extra protocols to wp_kses_post()
+	 */
+	public function kses_allowed_protocols( $allowedprotocols ) {
+		$allowedprotocols[] = 'tel';
+
+		return $allowedprotocols;
+	}
+
+	/**
+	 * Allow extra style attributes to wp_kses_post()
+	 */
+	public function safe_style_css( $allowedstyles ) {
+		$allowedstyles[] = 'display';
+		$allowedstyles[] = 'background-image';
+
+		return $allowedstyles;
+	}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336167,43.0,48,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-tour.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of `$room_basis` and `$drinks_basis` arrays seems to be a significant change. Ensure that these arrays are not used elsewhere in the codebase, or this could lead to undefined variable errors. If they are used elsewhere, consider providing a fallback or handling for their absence.



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 	 */
 	public $search_fields = false;
 
-	/**
-	 * Holds the meal plan options
-	 *
-	 * @var array
-	 */
-	public $room_basis = array();
-
-	/**
-	 * Holds the meal plan options
-	 *
-	 * @var array
-	 */
-	public $drinks_basis = array();
-
 	/**
 	 * Initialize the plugin by setting localization, filters, and
 	 * administration functions.",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336173,,215,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-unit-query.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code checks if the 'gallery' key is present and not empty in `$this->query_item` before proceeding, which is a good practice to avoid potential PHP warnings or errors. This change improves error handling.
```diff
- if ( false !== $this->query_item['gallery'] ) {
+ if ( isset( $this->query_item['gallery'] ) && ! empty( $this->query_item['gallery'] ) ) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 		$thumbnail_src = apply_filters( 'lsx_to_accommodation_room_thumbnail', $thumbnail_src );
 		if ( $this->have_query && false !== $this->query_item && ! empty( $this->query_item ) ) {
-			if ( false !== $this->query_item['gallery'] ) {
+			if ( isset( $this->query_item['gallery'] ) && ! empty( $this->query_item['gallery'] ) ) {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336180,,237,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-unit-query.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, this change also improves error handling by checking for the presence and non-emptiness of the 'gallery' key in `$this->query_item`.
```diff
- if ( isset( $this->query_item['gallery'] ) && false !== $this->query_item['gallery'] ) {
+ if ( isset( $this->query_item['gallery'] ) && ! empty( $this->query_item['gallery'] ) ) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 			$images_return = array();
 
-			if ( isset( $this->query_item['gallery'] ) && false !== $this->query_item['gallery'] ) {
+			if ( isset( $this->query_item['gallery'] ) && ! empty( $this->query_item['gallery'] ) ) {",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858336187,610.0,614,3ea90fdcaa1ee5092e348b2c7f60e7f971523dda,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/functions.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of the SCP Order Uninstall hook and related functions in this hunk is a significant change. If these functions are no longer needed, this is fine. However, if they are still required, their removal could cause issues. Please ensure that this uninstall functionality is either not needed or implemented elsewhere.
```diff
+  function lsx_to_accommodation_reset_units_loop() {
+  	global $rooms;
+  	return $rooms->reset_loop();
+  }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","  */
 function lsx_to_accommodation_reset_units_loop() {
 	global $rooms;
-
 	return $rooms->reset_loop();
 }",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858346932,176.0,184,708f1fabde0e869350d0be777ef0aae839e0a80e,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code is using hardcoded image paths for the rating stars. This could be problematic if the images are moved or their names are changed. It would be better to define these paths as constants or configuration settings, so they can be easily updated in one place if necessary.
```diff
- $ratings_array[] = LSX_TO_URL . 'assets/img/rating-star-full.png';
+ $ratings_array[] = defined('RATING_STAR_FULL') ? RATING_STAR_FULL : LSX_TO_URL . 'assets/img/rating-star-full.png';

- $ratings_array[] = LSX_TO_URL . 'assets/img/rating-star-empty.png';
+ $ratings_array[] = defined('RATING_STAR_EMPTY') ? RATING_STAR_EMPTY : LSX_TO_URL . 'assets/img/rating-star-empty.png';
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+					$ratings_array[] = '<figure class=""wp-block-image size-large is-resized"">';
+					$ratings_array[] = '<img src=""';
 					if ( (int) $value > 0 ) {
-						$ratings_array[] = '<i class=""fa fa-star""></i>';
+						$ratings_array[] = LSX_TO_URL . 'assets/img/rating-star-full.png';
 					} else {
-						$ratings_array[] = '<i class=""fa fa-star-o""></i>';
+						$ratings_array[] = LSX_TO_URL . 'assets/img/rating-star-empty.png';
 					}
+					$ratings_array[] = '"" alt="""" style=""width:20px;vertical-align:sub;"">';
+					$ratings_array[] = '</figure>';","--- 

+++ 

@@ -187,8 +187,6 @@

 					$value --;
 				}
 				$html = $before . implode( '', $ratings_array ) . $after;
-
-				do_action( 'qm/debug', [ $html ] );
 			}
 		}
 		return $html;"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858346946,,191,708f1fabde0e869350d0be777ef0aae839e0a80e,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The debug action `do_action( 'qm/debug', [ $html ] );` seems to be left from debugging and should be removed for production code.
```diff
- do_action( 'qm/debug', [ $html ] );
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-					$rating_description = ' <small>(' . $rating_type . ')</small>';
-				}
-				$html = $before . implode( '', $ratings_array ) . $rating_description . $after;
+				do_action( 'qm/debug', [ $html ] );","--- 

+++ 

@@ -187,8 +187,6 @@

 					$value --;
 				}
 				$html = $before . implode( '', $ratings_array ) . $after;
-
-				do_action( 'qm/debug', [ $html ] );
 			}
 		}
 		return $html;"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1858346952,186.0,188,708f1fabde0e869350d0be777ef0aae839e0a80e,d07b42eedda03b4d85ed3f6245fa92bf4c1ebfb9,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The decrement of `$counter` and `$value` variables is not protected by any condition. If `$value` becomes negative, it will still continue to decrement until `$counter` reaches 0. This might lead to unexpected behavior. Consider adding a condition to break the loop when `$value` is less than or equal to 0.
```diff
- $counter --;
- $value --;
+ if ($value > 0) {
+     $counter--;
+     $value--;
+ } else {
+     break;
+ }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 					$counter --;
 					$value --;
 				}","--- 

+++ 

@@ -187,8 +187,6 @@

 					$value --;
 				}
 				$html = $before . implode( '', $ratings_array ) . $after;
-
-				do_action( 'qm/debug', [ $html ] );
 			}
 		}
 		return $html;"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1869412926,146.0,171,4bb968b0b4123e0b94b4af51da181d0439fb9555,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-tour.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the way you're handling currency formatting. You're using `number_format` to format the price, but this function uses a comma as the thousands separator by default. This might not be suitable for all locales. Consider using `number_format_i18n` instead, which will use the correct thousands separator based on the site's locale.

```diff
- $value = number_format( (int) $value, 2 );
+ $value = number_format_i18n( (int) $value, 2 );

...

- $value = '<span class=""strike"">' . $value . '</span>' . ' ' . $currency . number_format( intval( $sale_price ) , 2 );
+ $value = '<span class=""strike"">' . $value . '</span>' . ' ' . $currency . number_format_i18n( intval( $sale_price ) , 2 );
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 			$tour_operator = tour_operator();
 			$currency      = '';
+			$letter_code   = '';
+			$value         = number_format( (int) $value, 2 );
 
+			// Get the currency settings
 			if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
-				$currency = $tour_operator->options['currency'];
-				$currency = '<span class=""currency-icon ' . strtolower( $currency ) . '"">' . $currency . '</span>';
+				$letter_code = $tour_operator->options['currency'];
+				$currency    = '<span class=""currency-icon ' . strtolower( $letter_code ) . '""></span>';
 			}
 
 			$value = $currency . $value;
+
+			// Get the Sale Price
+			if ( 'price' === $meta_key ) {
+				$sale_price = get_post_meta( get_the_ID(), 'sale_price', true );
+				if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
+					$value = '<span class=""strike"">' . $value . '</span>' . ' ' . $currency . number_format( intval( $sale_price ) , 2 );
+				}
+			}
+
+			// Get the currency settings
+			if ( is_object( $tour_operator ) &&  ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
+				$value = $letter_code . $value;
+			}
+",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1869412928,231.0,248,4bb968b0b4123e0b94b4af51da181d0439fb9555,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-tour.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `tour_classes` method is checking if the post type is 'tour' and then adding an 'on-sale' class if the tour has a sale price. However, it doesn't check if the current post actually has the 'tour' post type. This could lead to unexpected behavior if this method is called in a context where the global post isn't a tour. Consider adding a check for the post type:

```diff
+ global $post;
+ if (!$post || 'tour' !== $post->post_type) {
+     return $classes;
+ }
  
  $sale_price = get_post_meta( get_the_ID(), 'sale_price', true );
  if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
      $classes[] = 'on-sale';
  }
  return $classes;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+	/**
+	 * Adds in the onsale classes.
+	 *
+	 * @param array $classes
+	 * @return array
+	 */
+	public function tour_classes( $classes ) {
+		if ( ! is_singular( 'tour' ) ) {
+			return $classes;
+		}
+
+		$sale_price = get_post_meta( get_the_ID(), 'sale_price', true );
+		if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
+			$classes[] = 'on-sale';
+		}
+		return $classes;
+	}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1869412934,502.0,515,4bb968b0b4123e0b94b4af51da181d0439fb9555,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/blocks/class-bindings.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for handling the currency code seems to be a bit convoluted. It would be more readable and maintainable if you refactor it into a separate function. This function could take in the `tour_operator` object as an argument and return the appropriate currency string based on the settings.

```diff
- 				$value       = $rooms->item_price( '', '', false );
- 				$letter_code = '';
-  
-  				if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
- 					$letter_code = $tour_operator->options['currency'];
- 					$currency = '<span class=""currency-icon ' . mb_strtolower( $letter_code ) . '""></span>';
-  				}
-  
-  				$value = $currency . $value;
-  
- 				// Get the currency settings
- 				if ( is_object( $tour_operator ) &&  ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
- 					$value = $letter_code . $value;
- 				}

+               $value = $rooms->item_price( '', '', false );
+               $value = $this->getCurrencyString($tour_operator) . $value;

...

+   private function getCurrencyString($tour_operator) {
+       $letter_code = '';
+
+       if (is_object($tour_operator) && isset($tour_operator->options['currency']) && !empty($tour_operator->options['currency'])) {
+           $letter_code = $tour_operator->options['currency'];
+           $currency = '<span class=""currency-icon ' . mb_strtolower($letter_code) . '""></span>';
+       }
+
+       // Get the currency settings
+       if (is_object($tour_operator) &&  (isset($tour_operator->options['country_code_disabled']) && 0 === intval($tour_operator->options['country_code_disabled']) || !isset($tour_operator->options['country_code_disabled']))) {
+           $currency = $letter_code . $currency;
+       }
+
+       return $currency;
+   }
```
This refactoring makes the code more modular and easier to understand. It also improves maintainability by isolating the logic for generating the currency string into a separate function, which can be modified independently of the rest of the code.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				$value       = $rooms->item_price( '', '', false );
+				$letter_code = '';
 
 				if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
-					$currency = $tour_operator->options['currency'];
-					$currency = '<span class=""currency-icon ' . mb_strtolower( $currency ) . '"">' . $currency . '</span>';
+					$letter_code = $tour_operator->options['currency'];
+					$currency = '<span class=""currency-icon ' . mb_strtolower( $letter_code ) . '""></span>';
 				}
 
 				$value = $currency . $value;
 
+				// Get the currency settings
+				if ( is_object( $tour_operator ) &&  ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
+					$value = $letter_code . $value;
+				}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1869412941,136.0,152,4bb968b0b4123e0b94b4af51da181d0439fb9555,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for getting the sale price seems to be only applicable when the `$meta_key` is 'price'. This might not cover all scenarios where a sale price should be applied. Consider making this logic more generic or ensure that 'price' is the only key that would require a sale price.

```diff
- 			if ( 'price' === $meta_key ) {
+ 			if ( 'price' === $meta_key || 'other_condition' ) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			$letter_code   = '';
 
 			if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
-				$currency = $tour_operator->options['currency'];
-				$currency = '<span class=""currency-icon ' . mb_strtolower( $currency ) . '"">' . $currency . '</span>';
+				$letter_code = $tour_operator->options['currency'];
+				$currency = '<span class=""currency-icon ' . mb_strtolower( $letter_code ) . '""></span>';
 			}
 
 			$value = apply_filters( 'lsx_to_accommodation_price', $value, $price_type, $currency );
 
+			// Get the Sale Price
+			if ( 'price' === $meta_key ) {
+				$sale_price = get_post_meta( get_the_ID(), 'sale_price', true );
+				if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
+					$value = number_format( intval( $sale_price ) , 2 );
+				}
+			}
+","--- 

+++ 

@@ -122,8 +122,6 @@

 			'price',
 			'single_supplement'
 		];
-
-		do_action( 'qm/debug', [ get_post_type(), $meta_key ] );
 
 		if ( get_post_type() === 'accommodation' && in_array( $meta_key, $currency_fields ) ) {
 			$price_type    = get_post_meta( get_the_ID(), 'price_type', true );"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1870851131,13.0,20,f9e788394e5509d688a3ba21329b53358484932a,513fc33840f7ff5f995e0cbf585f3484152fd0e3,assets/js/blocks/slider-query.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for setting `hasCustomClass` is a bit redundant. You're checking if `props.attributes.hasCustomClass` is undefined and then setting `hasCustomClass` based on the condition, but then you're doing it again in the else block. This can be simplified.
```diff
- var hasCustomClass = props.attributes.hasCustomClass || false;
- if ( undefined === props.attributes.hasCustomClass ) {
-     if ( props.attributes.className && props.attributes.className.includes( 'lsx-to-slider' ) ) {
-         hasCustomClass = true;
-     }
- } else {
-     hasCustomClass = props.attributes.hasCustomClass;
- }
+ var hasCustomClass = props.attributes.hasCustomClass !== undefined ? props.attributes.hasCustomClass : 
+                     (props.attributes.className && props.attributes.className.includes('lsx-to-slider'));
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			var hasCustomClass = props.attributes.hasCustomClass || false;
+			if ( undefined === props.attributes.hasCustomClass ) {
+				if ( props.attributes.className && props.attributes.className.includes( 'lsx-to-slider' ) ) {
+					hasCustomClass = true;
+				}
+			} else {
+				hasCustomClass = props.attributes.hasCustomClass;
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1870851135,22.0,29,f9e788394e5509d688a3ba21329b53358484932a,513fc33840f7ff5f995e0cbf585f3484152fd0e3,assets/js/blocks/slider-query.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the logic for setting `filterByOnsale` is also redundant and can be simplified.
```diff
- var filterByOnsale = props.attributes.filterByOnsale || false;
- if ( undefined === props.attributes.filterByOnsale ) {
-     if ( props.attributes.className && props.attributes.className.includes( 'on-sale' ) ) {
-         filterByOnsale = true;
-     }
- } else {
-     filterByOnsale = props.attributes.filterByOnsale;
- }
+ var filterByOnsale = props.attributes.filterByOnsale !== undefined ? props.attributes.filterByOnsale : 
+                      (props.attributes.className && props.attributes.className.includes('on-sale'));
```

41, 51, 74, 78, 81:
Console logs should not be present in production code as they can expose sensitive information and clutter the console output. Please remove these debug statements.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			var filterByOnsale = props.attributes.filterByOnsale || false;
+			if ( undefined === props.attributes.filterByOnsale ) {
+				if ( props.attributes.className && props.attributes.className.includes( 'on-sale' ) ) {
+					filterByOnsale = true;
+				}
+			} else {
+				filterByOnsale = props.attributes.filterByOnsale;
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1870851136,84.0,88,f9e788394e5509d688a3ba21329b53358484932a,513fc33840f7ff5f995e0cbf585f3484152fd0e3,assets/js/blocks/slider-query.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These blocks of code are very similar and can be abstracted into a function to follow the DRY (Don't Repeat Yourself) principle.
```diff
- if (  true === attributes.hasCustomClass ) {
-     extraProps.className = (extraProps.className || '') + ' lsx-to-slider';
-     console.log('adding');
- } else if ( false === attributes.hasCustomClass && extraProps.className ) {
-     extraProps.className = extraProps.className.replace(/\blsx-to-slider\b\s*/g, '').trim();
-     console.log('removing');
- }
- 
- if ( true === attributes.filterByOnsale ) {
-     extraProps.className = (extraProps.className || '') + ' on-sale';
- } else if ( false === attributes.filterByOnsale && extraProps.className ) {
-     extraProps.className = extraProps.className.replace(/\bon-sale\b\s*/g, '').trim();
- }
+ function updateClassName(attribute, className, classToAddOrRemove) {
+     if (true === attribute) {
+         return (className || '') + ` ${classToAddOrRemove}`;
+     } else if (false === attribute && className) {
+         return className.replace(new RegExp(`\\b${classToAddOrRemove}\\b\\s*`, 'g'), '').trim();
+     }
+     return className;
+ }
+ 
+ extraProps.className = updateClassName(attributes.hasCustomClass, extraProps.className, 'lsx-to-slider');
+ extraProps.className = updateClassName(attributes.filterByOnsale, extraProps.className, 'on-sale');
```
This new function `updateClassName` takes in the attribute, current className, and the class to add or remove. It then returns the updated className based on the attribute value. This reduces code duplication and improves maintainability.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				if ( true === attributes.filterByOnsale ) {
+					extraProps.className = (extraProps.className || '') + ' on-sale';
+				} else if ( false === attributes.filterByOnsale && extraProps.className ) {
+					extraProps.className = extraProps.className.replace(/\bon-sale\b\s*/g, '').trim();
+				}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871095860,142.0,144,24d69edae1e756b7c2fc923c981506bc70c61571,513fc33840f7ff5f995e0cbf585f3484152fd0e3,assets/js/blocks/general.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The width property of the image is set as a string '20px' instead of an integer. This could potentially cause issues if the code expects an integer value for the width. Also, it's better to avoid hardcoding URLs in your code. Consider using a configuration file or environment variable to store these values.
```diff
-   width: '20px',
+   width: 20,
-   url: 'https://tour-operator.lsx.design/wp-content/uploads/2024/06/image-1.png',
+   url: config.IMAGE_URL || process.env.IMAGE_URL,
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+						width: '20px',
 						sizeSlug: 'large',
-						url: 'http://localhost:8883/wp-content/themes/lsx-tour-operator/assets/images/Typetype-icon.png',
+						url: 'https://tour-operator.lsx.design/wp-content/uploads/2024/06/image-1.png',","--- 

+++ 

@@ -1,3 +1,5 @@

+// metadata.bindings not handle correctly
+
 wp.domReady(() => {
 	// Gallery Block
 	wp.blocks.registerBlockVariation(""core/gallery"", {"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871095887,247.0,249,24d69edae1e756b7c2fc923c981506bc70c61571,513fc33840f7ff5f995e0cbf585f3484152fd0e3,assets/js/blocks/general.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the width property should be an integer and not a string. The URL concatenation could be improved by using a template literal for readability and maintainability.
```diff
-   width: '20px',
+   width: 20,
-   url: lsxToEditor.homeUrl + 'wp-content/uploads/2024/11/booking-validity-icon-black-52px-1.svg',
+   url: `${lsxToEditor.homeUrl}/wp-content/uploads/2024/11/booking-validity-icon-black-52px-1.svg`,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+						width: '20px',
 						sizeSlug: 'large',
-						url: 'http://localhost:8883/wp-content/themes/lsx-tour-operator/assets/images/best-months-to-travel-TO-icon-black-20px-1-1.png',
+						url: lsxToEditor.homeUrl + 'wp-content/uploads/2024/11/booking-validity-icon-black-52px-1.svg',","--- 

+++ 

@@ -1,3 +1,5 @@

+// metadata.bindings not handle correctly
+
 wp.domReady(() => {
 	// Gallery Block
 	wp.blocks.registerBlockVariation(""core/gallery"", {"
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206908,37.0,42,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a typo in the method name `maybe_hide_varitaion`. It should be `maybe_hide_variation`.
```diff
- add_filter( 'render_block', array( $this, 'maybe_hide_varitaion' ), 10, 3 );
+ add_filter( 'render_block', array( $this, 'maybe_hide_variation' ), 10, 3 );
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		add_filter( 'query_loop_block_query_vars', array( $this, 'query_args_filter' ), 1, 2 );
 		add_filter( 'render_block', array( $this, 'maybe_hide_varitaion' ), 10, 3 );
 
 		add_filter( 'posts_pre_query', array( $this, 'posts_pre_query' ), 10, 2 );
+
+		add_filter( 'render_block_data', array( $this, 'save_onsale_queries' ), 10, 1 );",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206916,76.0,93,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The refactoring of script enqueuing is good for maintainability and reducing duplication. However, it seems that the versioning with file modification time and enqueueing in the footer are missing for the additional scripts. This could lead to caching issues and unexpected behavior if the scripts rely on other scripts that are loaded in the footer.
```diff
- wp_enqueue_script(
-     'lsx-to-' . $slug,
-     LSX_TO_URL . 'assets/js/blocks/' . $slug . '.js',
-     $dependancies,
-     filemtime( LSX_TO_PATH . 'assets/js/blocks/' . $slug . '.js' )
- );
+ wp_enqueue_script(
+     'lsx-to-' . $slug,
+     LSX_TO_URL . 'assets/js/blocks/' . $slug . '.js',
+     $dependancies,
+     filemtime( LSX_TO_PATH . 'assets/js/blocks/' . $slug . '.js' ),
+     true // Enqueue in the footer.
+ );
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				if ( 'general' === $slug ) {
+					$param_array = array(
+						'homeUrl'   => trailingslashit( home_url() ),
+						'assetsUrl' => LSX_TO_URL . 'assets/img/'
+					);
+					$param_array = apply_filters( 'lsx_to_editor_params', $param_array );
+					wp_localize_script( 'lsx-to-block-' . $slug . '-variations', 'lsxToEditor', $param_array );
+				}
 			}
 
-			// Enqueue linked-cover.js
-			wp_enqueue_script(
-				'lsx-to-linked-cover',
-				LSX_TO_URL . 'assets/js/blocks/linked-cover.js',
-				array( 'wp-blocks', 'wp-element', 'wp-block-editor', 'wp-components', 'wp-compose', 'wp-data', 'wp-hooks' ),
-				filemtime( LSX_TO_PATH . 'assets/js/blocks/linked-cover.js' )
-			);
-
-			// Enqueue linked-cover.js
-			wp_enqueue_script(
-				'lsx-to-slider-query',
-				LSX_TO_URL . 'assets/js/blocks/slider-query.js',
-				array( 'wp-blocks', 'wp-element', 'wp-editor', 'wp-components' ),
-				filemtime( LSX_TO_PATH . 'assets/js/blocks/slider-query.js' )
-			);
+			foreach ( $additional_scripts as $slug => $dependancies ) {
+				wp_enqueue_script(
+					'lsx-to-' . $slug,
+					LSX_TO_URL . 'assets/js/blocks/' . $slug . '.js',
+					$dependancies,
+					filemtime( LSX_TO_PATH . 'assets/js/blocks/' . $slug . '.js' )
+				);
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206917,408.0,418,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/blocks/class-registration.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function comment does not match the function implementation. The comment suggests that the function checks if a post exists based on title, content, date, type, and status, but the function actually checks if posts exist based on their IDs. Please update the comment to reflect the actual functionality.
```diff
- * @param string $title   Post title.
- * @param string $content Optional. Post content.
- * @param string $date    Optional. Post date.
- * @param string $type    Optional. Post type.
- * @param string $status  Optional. Post status.
+ * @param array $ids Array of post IDs.
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	 * Determines if a post exists based on the ID.
+	 *
+	 *
+	 * @global wpdb $wpdb WordPress database abstraction object.
+	 *
+	 * @param string $title   Post title.
+	 * @param string $content Optional. Post content.
+	 * @param string $date    Optional. Post date.
+	 * @param string $type    Optional. Post type.
+	 * @param string $status  Optional. Post status.
+	 * @return int Post ID if post exists, 0 otherwise.",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206920,134.0,139,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code checks if the `tour_operator` object exists and if it has a non-empty `currency` option. However, it doesn't handle the case where the `options` property itself might not exist or be null. This could potentially lead to a PHP notice or warning. To avoid this, you should also check if the `options` property is set and is an array.

```diff
- if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
+ if ( is_object( $tour_operator ) && isset( $tour_operator->options ) && is_array( $tour_operator->options ) && !empty( $tour_operator->options['currency'] ) ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			$letter_code   = '';
 
 			if ( is_object( $tour_operator ) && isset( $tour_operator->options['currency'] ) && ! empty( $tour_operator->options['currency'] ) ) {
-				$currency = $tour_operator->options['currency'];
-				$currency = '<span class=""currency-icon ' . mb_strtolower( $currency ) . '"">' . $currency . '</span>';
+				$letter_code = $tour_operator->options['currency'];
+				$currency = '<span class=""currency-icon ' . mb_strtolower( $letter_code ) . '""></span>';
 			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206925,144.0,149,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The sale price is being fetched from post meta and used if it's not false, not empty, and not zero. However, there's no validation for negative values. If a negative value is entered as the sale price, it would pass these conditions and be used. You should add a condition to check that the sale price is greater than zero.

```diff
- if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
+ if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) && intval( $sale_price ) > 0 ) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			if ( 'price' === $meta_key ) {
+				$sale_price = get_post_meta( get_the_ID(), 'sale_price', true );
+				if ( false !== $sale_price && ! empty( $sale_price ) && 0 !== intval( $sale_price ) ) {
+					$value = number_format( intval( $sale_price ) , 2 );
+				}
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871206927,169.0,172,b661a364f6c52561d89f26ae648bcfe653da06e2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/classes/legacy/class-accommodation.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the code doesn't handle the case where the `options` property itself might not exist or be null. This could potentially lead to a PHP notice or warning. To avoid this, you should also check if the `options` property is set and is an array.

```diff
- if ( is_object( $tour_operator ) &&  ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
+ if ( is_object( $tour_operator ) && isset( $tour_operator->options ) && is_array( $tour_operator->options ) && ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+			// Get the currency settings
+			if ( is_object( $tour_operator ) &&  ( isset( $tour_operator->options['country_code_disabled'] ) && 0 === intval( $tour_operator->options['country_code_disabled'] ) || ! isset( $tour_operator->options['country_code_disabled'] ) ) ) {
+				$value = $letter_code . $value;
+			}",
https://api.github.com/repos/lightspeedwp/tour-operator/pulls/comments/1871231761,19.0,21,151b0f5a527eb5639e4d4d7795ede040ecc879d2,513fc33840f7ff5f995e0cbf585f3484152fd0e3,includes/pages/config-tour-operator.php,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The callback function includes a PHP file directly. This could potentially lead to issues if the file is not found or if there are errors within the included file that prevent it from being loaded correctly. It would be better to use `require_once` instead of `include`, as `require_once` will produce a fatal error if the file cannot be included, which makes debugging easier.

```diff
- 	'callback'      => function () {
- 		include( LSX_TO_PATH . 'includes/partials/welcome.php' );
- 	},
+ 	'callback'      => function () {
+ 		require_once( LSX_TO_PATH . 'includes/partials/welcome.php' );
+ 	},
```
This way, if the file is not found or cannot be loaded for some reason, the script will stop execution and throw a fatal error, making it easier to identify and fix the problem.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	'callback'      => function () {
+		include( LSX_TO_PATH . 'includes/partials/welcome.php' );
+	},",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1707605368,,24,a267f3e97bb063d3507a04b456f58e3d208cc97b,0e4c9530b0df40413729260db2a819fa589042a1,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Î°úÎî© Ïª¥Ìè¨ÎÑåÌä∏Î•º Ï∂îÍ∞ÄÌïòÏó¨ ÌôîÎ©¥ Ï§ëÏïôÏóê CircularProgressÎ•º ÌëúÏãúÌï©ÎãàÎã§.

```diff
-    return <div>Loading...</div>;
+    return <Loading />;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
   if (isLoading) {
-    return <div>Loading...</div>;
+    return <Loading />;",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525081,56.0,57,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp01.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's good practice to provide a more informative message when a script is not intended to be run directly. This will help other developers understand the intended use of this script.
```diff
-    print(""This is not meant to be run as a script (yet)"")
+    print(""Error: This script is not meant to be run directly. It should be imported and used as a module."")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+if __name__ == ""__main__"":
+    print(""This is not meant to be run as a script (yet)"")",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525169,,43,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/qc.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The foreign key reference has been changed from `acquisition.ExperimentCamera` to `streams.SpinnakerVideoSource`. Ensure that this change is intentional and that the new reference correctly links to the desired data.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     definition = """""" # Quality controls performed on a particular camera for a particular acquisition chunk
     -> acquisition.Chunk
-    -> acquisition.ExperimentCamera
+    -> streams.SpinnakerVideoSource",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525254,60.0,66,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/qc.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for selecting keys in the `key_source` method has been updated. Make sure that these changes are correct and that they don't introduce any potential issues with data selection or integrity.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
             )
-            & ""chunk_start >= camera_install_time""
-            & 'chunk_start < IFNULL(camera_remove_time, ""2200-01-01"")'
-        )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )  # CameraTop",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525336,68.0,84,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/qc.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `make` method has been significantly refactored. The changes include updates to how the `device_name` and `data_dirs` variables are fetched, as well as modifications to the `devices_schema` and `stream_reader` variables. These changes seem to be aimed at improving code readability and maintainability. However, ensure that these changes do not affect the functionality of the method and that all necessary data is still being correctly retrieved and processed.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     def make(self, key):
-        chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end"", ""directory_type""
-        )
-        camera = (acquisition.ExperimentCamera & key).fetch1(""camera_description"")
-        raw_data_dir = acquisition.Experiment.get_data_directory(
-            key, directory_type=dir_type
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+        data_dirs = acquisition.Experiment.get_data_directories(key)
 
-        device = getattr(
-            acquisition._device_schema_mapping[key[""experiment_name""]], camera
+        devices_schema = getattr(
+            acquisition.aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
         )
+        stream_reader = getattr(getattr(devices_schema, device_name), ""Video"")
 
         videodata = io_api.load(
-            root=raw_data_dir.as_posix(),
-            reader=device.Video,
+            root=data_dirs,
+            reader=stream_reader,",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525422,123.0,126,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/create_experiments/create_socialexperiment_0.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements are not organized according to PEP8 guidelines. It's recommended to group standard library imports (like `os`), related third-party imports (`jellyfish`, `numpy`, `pandas`), and local application/library specific imports. Also, it's better to import only the necessary functions from a module instead of importing the whole module.
```diff
-     from os import path
-     import jellyfish as jl
-     import numpy as np
-     import pandas as pd
+     from os import path
+     import pandas as pd
+     import numpy as np
+     import jellyfish as jl
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
     import jellyfish as jl
-    import pandas as pd
     import numpy as np
+    import pandas as pd",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525503,150.0,158,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/create_experiments/create_socialexperiment_0.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There seems to be an inconsistency in the handling of subjid strings. In the first if condition, you're splitting the string by "";"" but in the second if condition, you're splitting by "" "". This could lead to unexpected results if the input doesn't match these exact formats. Consider adding error handling or validation to ensure the input is in the expected format.
```diff
-     if "";"" in subjid:
-         subjidA, subjidB = subjid.split("";"")
-         return (
-             f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
-         )
-  
-      if ""vs"" in subjid:
-          subjidA, tmp, subjidB = subjid.split("" "")[1:]
-         return (
-             f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
-         )
+     if "";"" in subjid:
+         subjidA, subjidB = subjid.split("";"")
+         if len(subjidA) > 0 and len(subjidB) > 0:
+             return (
+                 f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
+             )
+         else:
+             raise ValueError(""Invalid format for subjid. Expected format: 'subjidA;subjidB'"")
+  
+      if ""vs"" in subjid:
+          parts = subjid.split("" "")
+          if len(parts) == 3:
+              subjidA, tmp, subjidB = parts[1:]
+              return (
+                  f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
+              )
+          else:
+              raise ValueError(""Invalid format for subjid. Expected format: 'subjidA vs subjidB'"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        return (
+            f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
+        )
 
     if ""vs"" in subjid:
         subjidA, tmp, subjidB = subjid.split("" "")[1:]
-        return f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
+        return (
+            f""{fixID(subjidA.strip(), valid_ids=valid_ids)};{fixID(subjidB.strip(), valid_ids=valid_ids)}""
+        )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525594,1.0,76,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/update_timestamps_longblob.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The script seems to be well written and follows good practices. However, there is a potential issue with the error handling. In line 56, you are asserting that `ts[0]` is an instance of `datetime`. If it's not, this will raise an AssertionError which will stop the execution of the script. It would be better to handle this case more gracefully, perhaps by logging an error message and continuing with the next key.

```diff
-                        assert isinstance(ts[0], datetime)
+                        if not isinstance(ts[0], datetime):
+                            print(f""Error: Expected datetime instance, got {type(ts[0])} instead."")
+                            continue
```

Also, consider adding some comments to your code to explain what each part does. This will make it easier for others (and future you) to understand what's going on.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+""""""July 2022
+Upgrade all timestamps longblob fields with datajoint 0.13.7.
+""""""
+from datetime import datetime
+
+import datajoint as dj
+import numpy as np
+from tqdm import tqdm
+
+assert dj.__version__ >= ""0.13.7""
+
+
+schema = dj.schema(""u_thinh_aeonfix"")
+
+
+@schema
+class TimestampFix(dj.Manual):
+    definition = """"""
+    full_table_name: varchar(64)
+    key_hash: uuid  # dj.hash.key_hash(key)
+    """"""
+
+
+schema_names = (
+    ""aeon_acquisition"",
+    ""aeon_tracking"",
+    ""aeon_qc"",
+    ""aeon_report"",
+    ""aeon_analysis"",
+)
+
+
+def main():
+    for schema_name in schema_names:
+        vm = dj.create_virtual_module(schema_name, schema_name)
+        table_names = [
+            ""."".join([dj.utils.to_camel_case(s) for s in tbl_name.strip(""`"").split(""__"") if s])
+            for tbl_name in vm.schema.list_tables()
+        ]
+        for table_name in table_names:
+            table = get_table(vm, table_name)
+            print(f""\n---- {schema_name}.{table_name} ----\n"")
+            for attr_name, attr in table.heading.attributes.items():
+                if ""timestamp"" in attr_name and attr.type == ""longblob"":
+                    for key in tqdm(table.fetch(""KEY"")):
+                        fix_key = {
+                            ""full_table_name"": table.full_table_name,
+                            ""key_hash"": dj.hash.key_hash(key),
+                        }
+                        if TimestampFix & fix_key:
+                            continue
+                        ts = (table & key).fetch1(attr_name)
+                        if not len(ts) or isinstance(ts[0], np.datetime64):
+                            TimestampFix.insert1(fix_key)
+                            continue
+                        assert isinstance(ts[0], datetime)
+                        with table.connection.transaction:
+                            table.update1(
+                                {
+                                    **key,
+                                    attr_name: np.array(ts).astype(""datetime64[ns]""),
+                                }
+                            )
+                            TimestampFix.insert1(fix_key)
+
+
+def get_table(schema_object, table_object_name):
+    if ""."" in table_object_name:
+        master_name, part_name = table_object_name.split(""."")
+        return getattr(getattr(schema_object, master_name), part_name)
+    else:
+        return getattr(schema_object, table_object_name)
+
+
+if __name__ == ""__main__"":
+    main()",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525673,,66,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/paths.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The f-string formatting is redundant here. You can combine the two f-strings into one for better readability.
```diff
- f""No valid root directory found (from {root_directories})"" f"" for {full_path}""
+ f""No valid root directory found (from {root_directories}) for {full_path}""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         raise FileNotFoundError(
-            ""No valid root directory found (from {})""
-            "" for {}"".format(root_directories, full_path)
+            f""No valid root directory found (from {root_directories})"" f"" for {full_path}""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525771,26.0,37,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a change in the attribute name from 'session_start' to 'in_arena_start'. Ensure that this attribute exists and is correctly named in the 'analysis.InArenaRewardRate' table. If not, it could lead to an AttributeError.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+
+    Examples:
     ```
     subject_keys = (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
 
     fig = plot_reward_rate_differences(subject_keys)
     ```
     """"""
-    subj_names, sess_starts, rate_timestamps, rate_diffs = (analysis.InArenaRewardRate
-                                                            & subject_keys).fetch(
-        'subject', 'session_start', 'pellet_rate_timestamps', 'patch2_patch1_rate_diff')
+    subj_names, sess_starts, rate_timestamps, rate_diffs = (
+        analysis.InArenaRewardRate & subject_keys
+    ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525859,76.0,120,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/plotting.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, there's a change in the attribute name from 'session_start' to 'in_arena_start'. Make sure that this attribute exists and is correctly named in the 'distance_travelled_df' DataFrame. If not, it could lead to a KeyError.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," def plot_wheel_travelled_distance(session_keys):
-    """"""
-    Plotting the wheel travelled distance for different patches
-        for all sessions specified in ""session_keys""
-    Example usage:
+    """"""Plotting the wheel travelled distance for different patches for all sessions specified in ""session_keys"".
+
+    Examples:
     ```
     session_keys = (acquisition.Session & acquisition.SessionEnd
      & {'experiment_name': 'exp0.1-r0', 'subject': 'BAA-1099794'}).fetch('KEY')
 
     fig = plot_wheel_travelled_distance(session_keys)
     ```
     """"""
-
     distance_travelled_query = (
-            analysis.InArenaSummary.FoodPatch
-            * acquisition.ExperimentFoodPatch.proj('food_patch_description')
-            & session_keys)
+        analysis.InArenaSummary.FoodPatch * acquisition.ExperimentFoodPatch.proj(""food_patch_description"")
+        & session_keys
+    )
 
-    distance_travelled_df = distance_travelled_query.proj(
-        'food_patch_description', 'wheel_distance_travelled').fetch(format='frame').reset_index()
+    distance_travelled_df = (
+        distance_travelled_query.proj(""food_patch_description"", ""wheel_distance_travelled"")
+        .fetch(format=""frame"")
+        .reset_index()
+    )
 
-    distance_travelled_df['session'] = [f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
-                                        for subj_name, sess_start in zip(distance_travelled_df.subject,
-                                                                         distance_travelled_df.session_start)]
+    distance_travelled_df[""in_arena""] = [
+        f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
+        for subj_name, sess_start in zip(
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start
+        )
+    ]
 
-    distance_travelled_df.rename(columns={'food_patch_description': 'Patch',
-                                          'wheel_distance_travelled': 'Travelled Distance (m)'},
-                                 inplace=True)
+    distance_travelled_df.rename(
+        columns={
+            ""food_patch_description"": ""Patch"",
+            ""wheel_distance_travelled"": ""Travelled Distance (m)"",
+        },
+        inplace=True,
+    )
 
-    title = '|'.join((acquisition.Experiment.Subject & session_keys).fetch('subject'))
-    fig = px.bar(distance_travelled_df, x=""session"", y=""Travelled Distance (m)"",
-                 color=""Patch"", title=title)
+    title = ""|"".join((acquisition.Experiment.Subject & session_keys).fetch(""subject""))
+    fig = px.bar(
+        distance_travelled_df,
+        x=""in_arena"",
+        y=""Travelled Distance (m)"",
+        color=""Patch"",
+        title=title,
+    )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559525940,106.0,234,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There are a few issues in this code block. 

Firstly, the `make` method of the `SLEAPTracking` class is quite long and complex. It would be beneficial to break it down into smaller, more manageable methods. This would improve readability and maintainability.

Secondly, there's a potential issue with error handling on line 194. If the config file for the SLEAP model cannot be found, an exception is raised. However, it might be better to handle this error more gracefully, perhaps by logging an error message and skipping the current iteration of the loop, rather than stopping execution entirely.

Lastly, the use of magic numbers (e.g., ""2200-01-01"" on line 149) can make the code harder to understand and maintain. It would be better to define these as constants at the top of the file, with descriptive names.

```diff
- 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+ 'chunk_start < IFNULL(spinnaker_video_source_removal_time, END_OF_TIME)'
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# ---------- VideoSource  ------------------
 
 
 @schema
-class CameraTracking(dj.Imported):
-    definition = """"""  # Tracked objects position data from a particular camera, using a particular tracking method, for a particular chunk
+class SLEAPTracking(dj.Imported):
+    definition = """"""  # Tracked objects position data from a particular VideoSource for multi-animal experiment using the SLEAP tracking method per chunk
     -> acquisition.Chunk
-    -> acquisition.ExperimentCamera
+    -> streams.SpinnakerVideoSource
     -> TrackingParamSet
     """"""
 
-    class Object(dj.Part):
-        definition = """"""  # Position data of object tracked by a particular camera tracking
+    class PoseIdentity(dj.Part):
+        definition = """"""
         -> master
-        object_id: int    # object with id = -1 means ""unknown/not sure"", could potentially be the same object as those with other id value
+        identity_idx:           smallint
         ---
-        timestamps:        longblob  # (datetime) timestamps of the position data
-        position_x:        longblob  # (px) object's x-position, in the arena's coordinate frame
-        position_y:        longblob  # (px) object's y-position, in the arena's coordinate frame
-        area=null:         longblob  # (px^2) object's size detected in the camera
+        identity_name:          varchar(16)
+        identity_likelihood:    longblob
+        anchor_part:         varchar(16)  # the name of the point used as anchor node for this class
+        """"""
+
+    class Part(dj.Part):
+        definition = """"""
+        -> master.PoseIdentity
+        part_name: varchar(16)
+        ---
+        sample_count: int      # number of data points acquired from this stream for a given chunk
+        x:          longblob
+        y:          longblob
+        likelihood: longblob
+        timestamps: longblob
         """"""
 
     @property
     def key_source(self):
-        ks = acquisition.Chunk * acquisition.ExperimentCamera * TrackingParamSet
         return (
-            ks
+            acquisition.Chunk
             * (
-                qc.CameraQC * acquisition.ExperimentCamera
-                & f""camera_description in {tuple(set(acquisition._ref_device_mapping.values()))}""
-            ).proj()
-            & ""tracking_paramset_id = 0""
-        )
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            * (TrackingParamSet & ""tracking_paramset_id = 1"")
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )  # SLEAP & CameraTop
 
     def make(self, key):
-        chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end"", ""directory_type""
-        )
-        camera = (acquisition.ExperimentCamera & key).fetch1(""camera_description"")
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
-        raw_data_dir = acquisition.Experiment.get_data_directory(
-            key, directory_type=dir_type
-        )
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
 
-        device = getattr(
-            acquisition._device_schema_mapping[key[""experiment_name""]], camera
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
         )
+        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
 
-        positiondata = io_api.load(
-            root=raw_data_dir.as_posix(),
-            reader=device.Position,
+        pose_data = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
             start=pd.Timestamp(chunk_start),
             end=pd.Timestamp(chunk_end),
         )
 
-        # replace id=NaN with -1
-        positiondata.fillna({""id"": -1}, inplace=True)
-
-        # Correct for frame offsets from Camera QC
-        qc_timestamps, qc_frame_offsets, camera_fs = (
-            qc.CameraQC * acquisition.ExperimentCamera & key
-        ).fetch1(""timestamps"", ""frame_offset"", ""camera_sampling_rate"")
-        qc_time_offsets = qc_frame_offsets / camera_fs
-        qc_time_offsets = np.where(
-            np.isnan(qc_time_offsets), 0, qc_time_offsets
-        )  # set NaNs to 0
-        positiondata.index += pd.to_timedelta(qc_time_offsets, ""s"")
-
-        object_positions = []
-        for obj_id in set(positiondata.id.values):
-            obj_position = positiondata[positiondata.id == obj_id]
-
-            object_positions.append(
+        if not len(pose_data):
+            self.insert1(key)
+            return
+
+        # Find the config file for the SLEAP model
+        for data_dir in data_dirs:
+            try:
+                f = next(
+                    data_dir.glob(
+                        f""**/**/{stream_reader.pattern}{io_api.chunk(chunk_start).strftime('%Y-%m-%dT%H-%M-%S')}*.{stream_reader.extension}""
+                    )
+                )
+            except StopIteration:
+                continue
+            else:
+                config_file = stream_reader.get_config_file(
+                    stream_reader._model_root / Path(*Path(f.stem.replace(""_"", ""/"")).parent.parts[1:])
+                )
+                break
+        else:
+            raise FileNotFoundError(f""Unable to find SLEAP model config file for: {stream_reader.pattern}"")
+
+        # get bodyparts and classes
+        bodyparts = stream_reader.get_bodyparts(config_file)
+        anchor_part = bodyparts[0]  # anchor_part is always the first one
+        class_names = stream_reader.get_class_names(config_file)
+
+        # ingest parts and classes
+        pose_identity_entries, part_entries = [], []
+        for class_idx in set(pose_data[""class""].values.astype(int)):
+            class_position = pose_data[pose_data[""class""] == class_idx]
+            for part in set(class_position.part.values):
+                part_position = class_position[class_position.part == part]
+                part_entries.append(
+                    {
+                        **key,
+                        ""identity_idx"": class_idx,
+                        ""part_name"": part,
+                        ""timestamps"": part_position.index.values,
+                        ""x"": part_position.x.values,
+                        ""y"": part_position.y.values,
+                        ""likelihood"": part_position.part_likelihood.values,
+                        ""sample_count"": len(part_position.index.values),
+                    }
+                )
+                if part == anchor_part:
+                    class_likelihood = part_position.class_likelihood.values
+            pose_identity_entries.append(
                 {
                     **key,
-                    ""object_id"": obj_id,
-                    ""timestamps"": obj_position.index.to_pydatetime(),
-                    ""position_x"": obj_position.x.values,
-                    ""position_y"": obj_position.y.values,
-                    ""area"": obj_position.area.values,
+                    ""identity_idx"": class_idx,
+                    ""identity_name"": class_names[class_idx],
+                    ""anchor_part"": anchor_part,
+                    ""identity_likelihood"": class_likelihood,
                 }
             )
 
         self.insert1(key)
-        self.Object.insert(object_positions)
-
-    @classmethod
-    def get_object_position(
-        cls,
-        experiment_name,
-        object_id,
-        start,
-        end,
-        camera_name=""FrameTop"",
-        tracking_paramset_id=0,
-        in_meter=False,
-    ):
-        table = (
-            cls.Object * acquisition.Chunk.proj(""chunk_end"")
-            & {""experiment_name"": experiment_name}
-            & {""tracking_paramset_id"": tracking_paramset_id}
-            & (acquisition.ExperimentCamera & {""camera_description"": camera_name})
-        )
-
-        return _get_position(
-            table,
-            object_attr=""object_id"",
-            object_name=object_id,
-            start_attr=""chunk_start"",
-            end_attr=""chunk_end"",
-            start=start,
-            end=end,
-            fetch_attrs=[""timestamps"", ""position_x"", ""position_y"", ""area""],
-            attrs_to_scale=[""position_x"", ""position_y""],
-            scale_factor=pixel_scale if in_meter else 1,
-        )
+        self.PoseIdentity.insert(pose_identity_entries)
+        self.Part.insert(part_entries)
 ","--- 

+++ 

@@ -172,8 +172,7 @@

         )
 
         if not len(pose_data):
-            self.insert1(key)
-            return
+            raise ValueError(f""No SLEAP data found for {key['experiment_name']} - {device_name}"")
 
         # Find the config file for the SLEAP model
         for data_dir in data_dirs:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526020,1.0,5,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring at the beginning of the file could be improved for better clarity and understanding. It's recommended to follow the PEP 257 -- Docstring Conventions. The first line should be a brief summary of the script's purpose.

```diff
- """"""Jan 2024
- Cloning and archiving schemas and data for experiment 0.2.
- The pipeline code associated with this archived data pipeline is here
- https://github.com/SainsburyWellcomeCentre/aeon_mecha/releases/tag/dj_exp02_stable
- """"""
+ """"""
+ This script is used for cloning and archiving schemas and data for experiment 0.2 as of Jan 2024.
+ The associated pipeline code can be found at: 
+ https://github.com/SainsburyWellcomeCentre/aeon_mecha/releases/tag/dj_exp02_stable
+ """"""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+""""""Jan 2024
+Cloning and archiving schemas and data for experiment 0.2.
+The pipeline code associated with this archived data pipeline is here
+https://github.com/SainsburyWellcomeCentre/aeon_mecha/releases/tag/dj_exp02_stable
+""""""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526095,,13,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's not clear why the environment variable ""DJ_SUPPORT_FILEPATH_MANAGEMENT"" is being set to ""TRUE"". If it's necessary for the script, please add a comment explaining its purpose.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+from datajoint_utilities.dj_data_copy.pipeline_cloning import ClonedPipeline
+
+logger = dj.logger
+os.environ[""DJ_SUPPORT_FILEPATH_MANAGEMENT""] = ""TRUE""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526178,,23,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `restriction` variable is a list of dictionaries. However, it's not clear what these dictionaries represent. Please add a comment explaining their purpose.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    for schema_name in (""lab"", ""subject"", ""acquisition"", ""tracking"", ""qc"", ""analysis"", ""report"")
+}
+
+restriction = [{""experiment_name"": ""exp0.2-r0""}, {""experiment_name"": ""social0-r1""}]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526267,30.0,41,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `clone_pipeline()` lacks a docstring. Please add a docstring that explains what the function does, its arguments, and its return value (if any).



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def clone_pipeline():
+    diagram = None
+    for orig_schema_name in schema_name_mapper:
+        virtual_module = dj.create_virtual_module(orig_schema_name, orig_schema_name)
+        if diagram is None:
+            diagram = dj.Diagram(virtual_module)
+        else:
+            diagram += dj.Diagram(virtual_module)
+
+    cloned_pipeline = ClonedPipeline(diagram, schema_name_mapper, verbose=True)
+    cloned_pipeline.instantiate_pipeline(prompt=False)
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526345,43.0,56,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `data_copy()` lacks a docstring. Please add a docstring that explains what the function does, its arguments, and its return value (if any).



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def data_copy(restriction, table_block_list, batch_size=None):
+    for orig_schema_name, cloned_schema_name in schema_name_mapper.items():
+        orig_schema = dj.create_virtual_module(orig_schema_name, orig_schema_name)
+        cloned_schema = dj.create_virtual_module(cloned_schema_name, cloned_schema_name)
+
+        db_migration.migrate_schema(
+            orig_schema,
+            cloned_schema,
+            restriction=restriction,
+            table_block_list=table_block_list.get(cloned_schema_name, []),
+            allow_missing_destination_tables=True,
+            force_fetch=False,
+            batch_size=batch_size,
+        )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526452,59.0,106,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `validate()` lacks a docstring. Please add a docstring that explains what the function does, its arguments, and its return value (if any).



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def validate():
+    """"""
+    Validation of schemas migration
+        1. for the provided list of schema names - validate all schemas have been migrated
+        2. for each schema - validate all tables have been migrated
+        3. for each table, validate all entries have been migrated
+    """"""
+    missing_schemas = []
+    missing_tables = {}
+    missing_entries = {}
+
+    for orig_schema_name, cloned_schema_name in schema_name_mapper.items():
+        logger.info(f""Validate schema: {orig_schema_name}"")
+        source_vm = dj.create_virtual_module(orig_schema_name, orig_schema_name)
+
+        try:
+            target_vm = dj.create_virtual_module(cloned_schema_name, cloned_schema_name)
+        except dj.errors.DataJointError:
+            missing_schemas.append(orig_schema_name)
+            continue
+
+        missing_tables[orig_schema_name] = []
+        missing_entries[orig_schema_name] = {}
+
+        for attr in dir(source_vm):
+            obj = getattr(source_vm, attr)
+            if isinstance(obj, dj.user_tables.UserTable) or (
+                inspect.isclass(obj) and issubclass(obj, dj.user_tables.UserTable)
+            ):
+                source_tbl = obj
+                try:
+                    target_tbl = getattr(target_vm, attr)
+                except AttributeError:
+                    missing_tables[orig_schema_name].append(source_tbl.table_name)
+                    continue
+                logger.info(f""\tValidate entry count: {source_tbl.__name__}"")
+                source_entry_count = len(source_tbl())
+                target_entry_count = len(target_tbl())
+                missing_entries[orig_schema_name][source_tbl.__name__] = {
+                    ""entry_count_diff"": source_entry_count - target_entry_count,
+                    ""db_size_diff"": source_tbl().size_on_disk - target_tbl().size_on_disk,
+                }
+
+    return {
+        ""missing_schemas"": missing_schemas,
+        ""missing_tables"": missing_tables,
+        ""missing_entries"": missing_entries,
+    }",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526550,109.0,110,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The script currently prints a message saying it's not meant to be run as a script yet. If this is the case, consider raising a `NotImplementedError` instead of just printing a message. This will make it clear that the script is not intended to be executed directly.

```diff
-    print(""This is not meant to be run as a script (yet)"")
+    raise NotImplementedError(""This script is not meant to be run directly."")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+if __name__ == ""__main__"":
+    print(""This is not meant to be run as a script (yet)"")",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526638,115.0,184,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/streams_maker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the error handling in this section. If the `reader` is of type `aeon.io.reader.Pose`, a warning message is logged and the function returns `None, None`. However, there doesn't seem to be any handling for these `None` return values in the calling code. This could lead to unexpected behavior or errors downstream. Consider raising an exception instead of logging a warning and returning `None`.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    stream = reader(**stream_detail[""stream_reader_kwargs""])
+
+    table_definition = f""""""  # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    -> {device_type}
+    -> acquisition.Chunk
+    ---
+    sample_count: int      # number of data points acquired from this stream for a given chunk
+    timestamps: longblob   # (datetime) timestamps of {stream_type} data
+    """"""
+
+    for col in stream.columns:
+        if col.startswith(""_""):
+            continue
+        col = re.sub(r""\([^)]*\)"", """", col)
+        table_definition += f""{col}: longblob\n    ""
+
+    class DeviceDataStream(dj.Imported):
+        definition = table_definition
+
+        @property
+        def key_source(self):
+            f""""""
+            Only the combination of Chunk and {device_type} with overlapping time
+            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
+            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
+            """"""
+            return (
+                acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
+                & f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""
+                & f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'
+            )
+
+        def make(self, key):
+            chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+            data_dirs = acquisition.Experiment.get_data_directories(key)
+
+            device_name = (ExperimentDevice & key).fetch1(f""{dj.utils.from_camel_case(device_type)}_name"")
+
+            devices_schema = getattr(
+                aeon_schemas,
+                (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                    ""devices_schema_name""
+                ),
+            )
+            stream_reader = getattr(getattr(devices_schema, device_name), ""{stream_type}"")
+
+            stream_data = io_api.load(
+                root=data_dirs,
+                reader=stream_reader,
+                start=pd.Timestamp(chunk_start),
+                end=pd.Timestamp(chunk_end),
+            )
+
+            self.insert1(
+                {
+                    **key,
+                    ""sample_count"": len(stream_data),
+                    ""timestamps"": stream_data.index.values,
+                    **{
+                        re.sub(r""\([^)]*\)"", """", c): stream_data[c].values
+                        for c in stream_reader.columns
+                        if not c.startswith(""_"")
+                    },
+                },
+                ignore_extra_fields=True,
+            )
+
+    DeviceDataStream.__name__ = f""{device_type}{stream_type}""
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526737,243.0,287,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/streams_maker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, if `get_device_stream_template()` returns `None` for `table_class`, the function continues without handling this case. This could potentially lead to issues downstream. Consider adding error handling or raising an exception when `table_class` is `None`.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        # Create DeviceDataStream tables.
+        for device_info in streams.DeviceType.Stream.fetch(as_dict=True):
+            device_type = device_info[""device_type""]
+            stream_type = device_info[""stream_type""]
+            table_name = f""{device_type}{stream_type}""
+
+            if hasattr(streams, table_name):
+                continue
+
+            table_class, table_definition = get_device_stream_template(
+                device_type, stream_type, streams_module=streams
+            )
+
+            if table_class is None:
+                continue
+
+            device_stream_table_def = inspect.getsource(table_class).lstrip()
+
+            # Replace the definition
+            replacements = {
+                ""DeviceDataStream"": f""{device_type}{stream_type}"",
+                ""ExperimentDevice"": device_type,
+                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'"",
+                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""",
+                'f""{dj.utils.from_camel_case(device_type)}_name""': f""'{dj.utils.from_camel_case(device_type)}_name'"",
+                ""{device_type}"": device_type,
+                ""{stream_type}"": stream_type,
+                ""{aeon.__version__}"": aeon.__version__,
+            }
+
+            replacements[""table_definition""] = '""""""' + table_definition + '""""""'
+
+            for old, new in replacements.items():
+                device_stream_table_def = device_stream_table_def.replace(old, new)
+
+            full_def = ""@schema \n"" + device_stream_table_def + ""\n\n""
+
+            with open(_STREAMS_MODULE_FILE) as f:
+                existing_content = f.read()
+
+            if full_def in existing_content:
+                continue
+
+            with open(_STREAMS_MODULE_FILE, ""a"") as f:
+                f.write(full_def)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526828,118.0,119,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from using the `method=""pad""` parameter to not using it in the `reindex` function might cause different behavior. Previously, it would propagate the last valid observation forward to the next valid. Now, without specifying a method, it will use the default method of `None`, which means that no filling will be performed. If this is intended, then it's fine. However, if you still want to fill missing values, you should keep the `method=""pad""` parameter.

```diff
- data = data.reindex(values, tolerance=tolerance)
+ data = data.reindex(values, method=""pad"", tolerance=tolerance)
```
Also, the addition of `data.dropna(inplace=True)` will remove any rows with NaN values. This could potentially remove more data than expected. If this is the desired behavior, then it's fine. Otherwise, consider handling NaN values differently or ensuring that your data doesn't contain unexpected NaN values before dropping them.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                data = data.reindex(values, tolerance=tolerance)
+                data.dropna(inplace=True)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559526986,9.0,16,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/webapps/sciviz/README.md,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The instructions for setting up the SSH tunnel are clear, but it would be better to add a note about replacing `username` with the actual username. This is important because not all users may be familiar with SSH and might overlook this detail.
```diff
- ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no username@ssh.swc.ucl.ac.uk -L 3306:aeon-db2:3306 -N
+ ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no <your_username>@ssh.swc.ucl.ac.uk -L 3306:aeon-db2:3306 -N
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+---
 ## Running the application
+### Local dev deployment for testing
 
-#### Production deployment
+- For local deployment, you need to ensure the connection to the `aeon-db2` server is established. This can be done by establishing the port forwarding as follows in a terminal (replace `username` with your own username):
 
-To run the application in Production mode, use the command:
 ```bash
-SUBDOMAINS=testdev URL=datajoint.io STAGE_CERT=TRUE EMAIL=service-health@datajoint.com HOST_UID=$(id -u) docker-compose -f ./SciViz/docker-compose-remote.yaml up -d
+ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no username@ssh.swc.ucl.ac.uk -L 3306:aeon-db2:3306 -N",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527104,66.0,71,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/webapps/sciviz/README.md,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [71-72]

It's good that you've added a link to the Sci-Viz repository for more information. However, it would be helpful to provide a brief description of what additional information can be found in the linked repository. This will give users an idea of whether they need to visit the link or not.
```diff
- Visit this [repo](https://github.com/datajoint/sci-viz) to learn more about Sci-Viz.
+ For more detailed documentation and examples on how to use Sci-Viz, visit this [repo](https://github.com/datajoint/sci-viz).
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             return dict(**kwargs)
     ```
-- Overlapping components at the same (x, y) does not work, the grid system will not allow overlapping components it will wrap them horizontally if there is enough space or bump them down to the next row.
\ No newline at end of file
+- Overlapping components at the same (x, y) does not work, the grid system will not allow overlapping components it will wrap them horizontally if there is enough space or bump them down to the next row.
+- Visit this [repo](https://github.com/datajoint/sci-viz) to learn more about Sci-Viz.",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527196,1.0,31,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/subject.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code has removed the `Allele`, `Line` and their related classes from the schema. If these classes are being used elsewhere in the codebase, removing them could potentially break those parts of the application. Please ensure that these classes are not being referenced anywhere else before removing them.

Also, the `Strain` class has been changed from a `Lookup` table to a `Manual` table. This change means that entries will no longer be inserted automatically based on foreign key constraints but will need to be manually inserted. Ensure this is the intended behavior.

Lastly, the `definition` for `Strain` and `GeneticBackground` classes have been simplified. If the removed fields (`strain_standard_name`, `strain_desc` for `Strain` and `gen_bg` for `GeneticBackground`) are not needed anymore, this change is fine. However, if other parts of your application rely on these fields, this change could cause issues.

```diff
- @schema
- class Strain(dj.Lookup):
-     definition = """"""
-     # Strain of animal, e.g. C57Bl/6
-     strain              : varchar(32)	# abbreviated strain name
-     ---
-     strain_standard_name  : varchar(32)   # formal name of a strain
-     strain_desc=''      : varchar(255)	# description of this strain
-     """"""
-  
-  
-  @schema
- class Allele(dj.Lookup):
-      definition = """"""
-     allele                      : varchar(32)    # abbreviated allele name
-      ---
-     allele_standard_name=''     : varchar(255)	  # standard name of an allele
-      """"""
- 
-     class Source(dj.Part):
-         definition = """"""
-         -> master
-         ---
-         -> lab.Source
-         source_identifier=''        : varchar(255)    # id inside the line provider
-         source_url=''               : varchar(255)    # link to the line information
-         expression_data_url=''      : varchar(255)    # link to the expression pattern from Allen institute brain atlas
-         """"""
- 
-  
-  @schema
- class Line(dj.Lookup):
-      definition = """"""
-     line                    : varchar(32)	# abbreviated name for the line
-      ---
-     line_description=''     : varchar(2000)
-     target_phenotype=''     : varchar(255)
-     is_active               : boolean		# whether the line is in active breeding
-      """"""
-  
-     class Allele(dj.Part):
-         definition = """"""
-         -> master
-         -> Allele
-         """"""
+ @schema
+ class Strain(dj.Manual):
+     definition = """"""
+     strain_id: int
+     ---
+     strain_name: varchar(64)
+     """"""
+ 
+ @schema
+ class GeneticBackground(dj.Manual):
+     definition = """"""
+     gen_bg_id: int
+     ---
+     gen_bg: varchar(64)
+     """"""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import json
+import os
+import time
+from datetime import datetime, timedelta
 
-schema = dj.schema(get_schema_name('subject'))
+import datajoint as dj
+import requests
 
+from . import get_schema_name, lab
 
-@schema
-class Strain(dj.Lookup):
-    definition = """"""
-    # Strain of animal, e.g. C57Bl/6
-    strain              : varchar(32)	# abbreviated strain name
-    ---
-    strain_standard_name  : varchar(32)   # formal name of a strain
-    strain_desc=''      : varchar(255)	# description of this strain
-    """"""
+logger = dj.logger
+schema = dj.schema(get_schema_name(""subject""))
 
 
 @schema
-class Allele(dj.Lookup):
+class Strain(dj.Manual):
     definition = """"""
-    allele                      : varchar(32)    # abbreviated allele name
+    strain_id: int
     ---
-    allele_standard_name=''     : varchar(255)	  # standard name of an allele
+    strain_name: varchar(64)
     """"""
 
-    class Source(dj.Part):
-        definition = """"""
-        -> master
-        ---
-        -> lab.Source
-        source_identifier=''        : varchar(255)    # id inside the line provider
-        source_url=''               : varchar(255)    # link to the line information
-        expression_data_url=''      : varchar(255)    # link to the expression pattern from Allen institute brain atlas
-        """"""
-
 
 @schema
-class Line(dj.Lookup):
+class GeneticBackground(dj.Manual):
     definition = """"""
-    line                    : varchar(32)	# abbreviated name for the line
+    gen_bg_id: int
     ---
-    line_description=''     : varchar(2000)
-    target_phenotype=''     : varchar(255)
-    is_active               : boolean		# whether the line is in active breeding
+    gen_bg: varchar(64)
     """"""
 ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527304,1.0,18,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The commented out line `# schema = dj.schema(get_schema_name(""report""))` has been replaced with `schema = dj.schema()`. This change might cause issues if the `dj.schema()` function requires a name argument to work correctly. If the `get_schema_name(""report"")` was providing a necessary argument, this could lead to an error or unexpected behavior.
```diff
- # schema = dj.schema(get_schema_name(""report""))
+ schema = dj.schema()
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import datetime
+import json
 import os
-import datajoint as dj
-import pandas as pd
-import numpy as np
 import pathlib
+
+import datajoint as dj
 import matplotlib.pyplot as plt
-import re
-import datetime
-import json
+import numpy as np
+import pandas as pd
 
 from aeon.analysis import plotting as analysis_plotting
+from aeon.dj_pipeline.analysis.visit import Visit, VisitEnd
+from aeon.dj_pipeline.analysis.visit_analysis import *
 
-from . import acquisition, analysis
-from . import get_schema_name
-
+from . import acquisition, analysis, get_schema_name
 
-schema = dj.schema(get_schema_name(""report""))
+# schema = dj.schema(get_schema_name(""report""))
+schema = dj.schema()",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527392,47.0,75,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a significant change in the structure of the figure being plotted. The size and layout of the figure have been changed. Ensure that these changes are intentional and do not distort the visual representation of the data.
```diff
- fig = plt.figure(figsize=(16, 8))
- gs = fig.add_gridspec(21, 6)
+ fig = plt.figure(figsize=(20, 9))
+ gs = fig.add_gridspec(22, 5)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        in_arena_start, in_arena_end = (analysis.InArena * analysis.InArenaEnd & key).fetch1(
+            ""in_arena_start"", ""in_arena_end""
+        )
 
         # subject's position data in the time_slices
         position = analysis.InArenaSubjectPosition.get_position(key)
+        position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
 
-        position_minutes_elapsed = (
-            position.index - in_arena_start
-        ).total_seconds() / 60
+        position_minutes_elapsed = (position.index - in_arena_start).total_seconds() / 60
 
         # figure
-        fig = plt.figure(figsize=(16, 8))
-        gs = fig.add_gridspec(21, 6)
-        rate_ax = fig.add_subplot(gs[:10, :4])
-        distance_ax = fig.add_subplot(gs[10:20, :4])
-        ethogram_ax = fig.add_subplot(gs[20, :4])
-        position_ax = fig.add_subplot(gs[10:, 4:])
-        pellet_ax = fig.add_subplot(gs[:10, 4])
-        time_dist_ax = fig.add_subplot(gs[:10, 5:])
+        fig = plt.figure(figsize=(20, 9))
+        gs = fig.add_gridspec(22, 5)
+        threshold_ax = fig.add_subplot(gs[:3, :3])
+        rate_ax = fig.add_subplot(gs[5:13, :3])
+        distance_ax = fig.add_subplot(gs[14:20, :3])
+        ethogram_ax = fig.add_subplot(gs[20, :3])
+        position_ax = fig.add_subplot(gs[13:, 3:])
+        pellet_ax = fig.add_subplot(gs[2:12, 3])
+        time_dist_ax = fig.add_subplot(gs[2:12, 4:])
 
         # position plot
         non_nan = np.logical_and(~np.isnan(position.x), ~np.isnan(position.y))
-        analysis_plotting.heatmap(
-            position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5
-        )
+        analysis_plotting.heatmap(position[non_nan], 50, ax=position_ax, bins=500, alpha=0.5)
 
         # event rate plots
         in_arena_food_patches = (
             analysis.InArena
-            * acquisition.ExperimentFoodPatch.join(
-                acquisition.ExperimentFoodPatch.RemovalTime, left=True
-            )
+            * acquisition.ExperimentFoodPatch.join(acquisition.ExperimentFoodPatch.RemovalTime, left=True)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527481,122.0,162,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code for plotting the wheel threshold and ethogram has been significantly refactored. Ensure that these changes accurately represent the data and do not introduce any errors or misrepresentations.
```diff
- ethogram_ax.set_xlabel(""time (min)"")
+ threshold_ax.set_ylabel(""threshold"")
+ threshold_ax.set_ylim([threshold_ax.get_ylim()[0] - 100, threshold_ax.get_ylim()[1] + 100])
+ ethogram_ax.set_xlabel(""time (min)"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            # plot wheel threshold
+            wheel_time, wheel_threshold = (
+                acquisition.WheelState.Time
+                & food_patch_key
+                & f'state_timestamp between ""{in_arena_start}"" and ""{in_arena_end}""'
+            ).fetch(""state_timestamp"", ""threshold"")
+            wheel_time -= in_arena_start
+            wheel_time /= datetime.timedelta(minutes=1)
+
+            wheel_time = np.append(wheel_time, position_minutes_elapsed[-1])
+
+            for i in range(len(wheel_time) - 1):
+                threshold_ax.hlines(
+                    y=wheel_threshold[i],
+                    xmin=wheel_time[i],
+                    xmax=wheel_time[i + 1],
+                    linewidth=2,
+                    color=self.color_code[food_patch_key[""food_patch_description""]],
+                    alpha=0.3,
+                )
+            threshold_change_ind = np.where(wheel_threshold[:-1] != wheel_threshold[1:])[0]
+            threshold_ax.vlines(
+                wheel_time[threshold_change_ind + 1],
+                ymin=wheel_threshold[threshold_change_ind],
+                ymax=wheel_threshold[threshold_change_ind + 1],
+                linewidth=1,
+                linestyle=""dashed"",
+                color=self.color_code[food_patch_key[""food_patch_description""]],
+                alpha=0.4,
+            )
+
         # ethogram
-        in_arena, in_corridor, arena_time, corridor_time = (
-            analysis.InArenaTimeDistribution & key
-        ).fetch1(
+        in_arena, in_corridor, arena_time, corridor_time = (analysis.InArenaTimeDistribution & key).fetch1(
             ""in_arena"",
             ""in_corridor"",
             ""time_fraction_in_arena"",
             ""time_fraction_in_corridor"",
         )
-        nest_keys, in_nests, nests_times = (
-            analysis.InArenaTimeDistribution.Nest & key
-        ).fetch(""KEY"", ""in_nest"", ""time_fraction_in_nest"")
+        nest_keys, in_nests, nests_times = (analysis.InArenaTimeDistribution.Nest & key).fetch(
+            ""KEY"", ""in_nest"", ""time_fraction_in_nest""
+        )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527570,,174,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The label for the plot has been changed from ""Times in arena"" to ""arena"". Make sure this change does not confuse users or misrepresent the data.
```diff
- label=f""Times in arena"",
+ label=""arena"",
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             markersize=0.5,
             alpha=0.6,
-            label=f""Times in arena"",
+            label=""arena"",",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527655,190.0,206,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [183-203]

The labels for the plots have been simplified. Ensure that these changes do not confuse users or misrepresent the data.
```diff
- label=f""Times in corridor"",
+ label=""corridor"",
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","                 color=self.color_code[""nest""],
                 markersize=0.5,
                 alpha=0.6,
-                label=f""Times in nest"",
+                label=""nest"",
             )
-        for patch_idx, (patch_name, in_patch) in enumerate(
-            zip(patch_names, in_patches)
-        ):
+        for patch_idx, (patch_name, in_patch) in enumerate(zip(patch_names, in_patches)):
             ethogram_ax.plot(
                 position_minutes_elapsed[in_patch],
                 np.full_like(position_minutes_elapsed[in_patch], (patch_idx + 3)),
                 ""."",
                 color=self.color_code[patch_name],
                 markersize=0.5,
                 alpha=0.6,
-                label=f""Times in {patch_name}"",
+                label=f""{patch_name}"",
             )
 
         # pellet",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527755,235.0,243,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code for setting the plot aesthetics has been significantly refactored. Ensure that these changes accurately represent the data and do not introduce any errors or misrepresentations.
```diff
- analysis_plotting.set_ymargin(distance_ax, 0.2, 0.1)
+ threshold_ax.set_ylabel(""threshold"")
+ threshold_ax.set_ylim([threshold_ax.get_ylim()[0] - 100, threshold_ax.get_ylim()[1] + 100])
+ ethogram_ax.set_xlabel(""time (min)"")
+ analysis_plotting.set_ymargin(distance_ax, 0.2, 0.1)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        threshold_ax.set_ylabel(""threshold"")
+        threshold_ax.set_ylim([threshold_ax.get_ylim()[0] - 100, threshold_ax.get_ylim()[1] + 100])
         ethogram_ax.set_xlabel(""time (min)"")
         analysis_plotting.set_ymargin(distance_ax, 0.2, 0.1)
-        for ax in (rate_ax, distance_ax, pellet_ax, time_dist_ax):
+        for ax in (rate_ax, distance_ax, pellet_ax, time_dist_ax, threshold_ax):
             ax.spines[""top""].set_visible(False)
             ax.spines[""right""].set_visible(False)
             ax.spines[""bottom""].set_visible(False)
             ax.tick_params(bottom=False, labelbottom=False)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527834,,265,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function call to `_save_figs` has been simplified. Make sure this change does not cause any issues with the function's execution.
```diff
- fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
+ fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-        fig_dict = _save_figs(
-            (fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name
-        )
+        fig_dict = _save_figs((fig,), (""summary_plot_png"",), save_dir=save_dir, prefix=save_dir.name)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559527932,,389,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/report.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The docstring for the `delete_outdated_entries` method has been reformatted. This is a minor change and should not affect the functionality of the code. However, it's always good practice to ensure that docstrings are clear, concise, and accurately describe the function's purpose and behavior.
```diff
- """"""
- Each entry in this table correspond to one subject. However the plot is capturing
-     data for all sessions.
- Hence a dynamic update routine is needed to recompute the plot as new sessions
-     become available
- """"""
+ """"""Each entry in this table correspond to one subject. However the plot is capturing data for all sessions. Hence a dynamic update routine is needed to recompute the plot as new sessions become available.""""""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","-        Hence a dynamic update routine is needed to recompute the plot as new sessions
-            become available
-        """"""
+        """"""Each entry in this table correspond to one subject. However the plot is capturing data for all sessions. Hence a dynamic update routine is needed to recompute the plot as new sessions become available.""""""",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528024,7.0,12,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/schema/social_01.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `__init__` method in the `RfidEvents` class seems to be replacing ""Rfid"" and ""Events"" from the path string. However, it's not clear why this is necessary. If these strings are expected in the path, then removing them could lead to incorrect paths. Also, there's no error handling for when the path doesn't contain these strings or if the final constructed path doesn't exist. Consider adding some error checking and handling here.

```diff
    def __init__(self, path):
        path = path.replace(""Rfid"", """")
        if path.startswith(""Events""):
            path = path.replace(""Events"", """")
+        if not os.path.exists(path):
+            raise FileNotFoundError(f""The path {path} does not exist."")

        super().__init__(_reader.Harp(f""RfidEvents{path}_32*"", [""rfid""]))
```
This will ensure that the path exists before trying to initialize the `Harp` reader with it.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def __init__(self, path):
+        path = path.replace(""Rfid"", """")
+        if path.startswith(""Events""):
+            path = path.replace(""Events"", """")
+
+        super().__init__(_reader.Harp(f""RfidEvents{path}_32*"", [""rfid""]))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528125,10.0,16,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The variable `registry` is used before it's defined. This will raise a `NameError`. You should initialize `registry` as an empty dictionary before using it.
```diff
-    composite = {}
+    registry = {}
     if args:
         for binder_fn in args:
             if inspect.isclass(binder_fn):
                 for method in vars(binder_fn).values():
                      if isinstance(method, staticmethod):
-                        registry.update(method.__func__(pattern))
+                        composite.update(method.__func__(pattern))
              else:
-                registry.update(binder_fn(pattern))
+                composite.update(binder_fn(pattern))
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        for binder_fn in args:
+            if inspect.isclass(binder_fn):
+                for method in vars(binder_fn).values():
                     if isinstance(method, staticmethod):
-                        composite.update(method.__func__(pattern))
+                        registry.update(method.__func__(pattern))
             else:
-                composite.update(stream(pattern))
-    return composite
+                registry.update(binder_fn(pattern))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528209,,36,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `register` is not defined anywhere in the provided code. If it's defined elsewhere, ensure that it's imported correctly. If it's supposed to be `compositeStream`, then you should update the function name accordingly.
```diff
-        self.registry = register(name if pattern is None else pattern, *args)
+        self.registry = compositeStream(name if pattern is None else pattern, *args)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     def __init__(self, name, *args, pattern=None):
         self.name = name
-        self.stream = compositeStream(name if pattern is None else pattern, *args)
+        self.registry = register(name if pattern is None else pattern, *args)",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528316,39.0,43,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/device.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic here seems to have changed from the old hunk. Previously, the singleton check was done on `self.stream`, but now it's done on `self.registry`. Ensure this change is intentional and correct.
```diff
-        if len(self.stream) == 1:
-            singleton = self.stream.get(self.name, None)
+        if len(self.registry) == 1:
+            singleton = self.registry.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.stream))
+        return iter((self.name, self.registry))
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        if len(self.registry) == 1:
+            singleton = self.registry.get(self.name, None)
             if singleton:
                 return iter((self.name, singleton))
-        return iter((self.name, self.stream))
+        return iter((self.name, self.registry))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528412,12.0,22,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `retrieve_video_frames` has a lot of parameters, which can make it difficult to use and maintain. Consider using a data class or a dictionary to group related parameters together. This will improve the readability and maintainability of your code.

```diff
- def retrieve_video_frames(
-     experiment_name,
-     camera_name,
-     start_time,
-     end_time,
-     raw_data_dir,
-     desired_fps=50,
-     start_frame=0,
-     chunk_size=50,
-     **kwargs,
- ):
+ from dataclasses import dataclass
+
+ @dataclass
+ class VideoParams:
+     experiment_name: str
+     camera_name: str
+     start_time: str
+     end_time: str
+     raw_data_dir: str
+     desired_fps: int = 50
+     start_frame: int = 0
+     chunk_size: int = 50
+
+ def retrieve_video_frames(params: VideoParams):
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def retrieve_video_frames(
+    experiment_name,
+    camera_name,
+    start_time,
+    end_time,
+    raw_data_dir,
+    desired_fps=50,
+    start_frame=0,
+    chunk_size=50,
+    **kwargs,
+):",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528499,25.0,26,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's good that you're checking if the directory exists before proceeding. However, it would be better to raise an exception with a meaningful error message if it doesn't exist. This will help in debugging if there are any issues.

```diff
- assert raw_data_dir.exists()
+ if not raw_data_dir.exists():
+     raise FileNotFoundError(f""The directory {raw_data_dir} does not exist."")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    assert raw_data_dir.exists()
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528593,34.0,37,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error message is informative but it could be more helpful if it also included the `experiment_name`. This would provide more context when debugging.

```diff
- raise ValueError(
-     f""No video data found for {camera_name} camera and time period: {start_time} - {end_time}""
- )
+ raise ValueError(
+     f""No video data found for {camera_name} camera in experiment {experiment_name} during the time period: {start_time} - {end_time}""
+ )
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if not len(videodata):
+        raise ValueError(
+            f""No video data found for {camera_name} camera and time period: {start_time} - {end_time}""
+        )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528686,41.0,42,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The variable `final_fps` is redundant as it's just a copy of `desired_fps`. You can use `desired_fps` directly.

```diff
- final_fps = desired_fps
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    final_fps = desired_fps
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528762,47.0,50,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

You're encoding each frame one by one which could be slow if there are many frames. Consider using a list comprehension to speed up this process.

```diff
- encoded_frames = []
- for f in frames:
-     encoded_f = cv2.imencode("".jpeg"", f)[1].tobytes()
-     encoded_frames.append(base64.b64encode(encoded_f).decode())
+ encoded_frames = [base64.b64encode(cv2.imencode("".jpeg"", f)[1].tobytes()).decode() for f in frames]
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    for f in frames:
+        encoded_f = cv2.imencode("".jpeg"", f)[1].tobytes()
+        encoded_frames.append(base64.b64encode(encoded_f).decode())
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528856,51.0,52,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/utils/video.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Instead of calculating the index of the last frame, you can simply use `-1` to get the last element of a list in Python.

```diff
- last_frame_time = framedata.index[len(encoded_frames) - 1]
+ last_frame_time = framedata.index[-1]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    last_frame_time = framedata.index[len(encoded_frames) - 1]
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559528944,,10,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,docker/image/apt_requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's a good practice to end files with a newline. This is because some utilities such as `cat`, `awk`, `sed`, etc. might not process the last line correctly if it doesn't end with a newline.
```diff
- libtiff5
\ No newline at end of file
+ libtiff5
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," openssh-client
 procps
 tmux
+libtiff5",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529032,,13,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,docker/image/Dockerfile,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The base image has been changed from `ghcr.io/iamamutt/conda_base:latest` to `ghcr.io/ttngu207/conda_base:latest`. Ensure that the new base image contains all the necessary dependencies and configurations required for your application.

```diff
- FROM ghcr.io/iamamutt/conda_base:latest as aeon_mecha_docker_pre
+ FROM ghcr.io/ttngu207/conda_base:latest as aeon_mecha_docker_pre
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 # main build stage =====================================================================
-FROM ghcr.io/iamamutt/conda_base:latest as aeon_mecha_docker_pre
+FROM ghcr.io/ttngu207/conda_base:latest as aeon_mecha_docker_pre",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529124,1.0,51,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,docker/cron_script.bash,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The script seems to be well written and follows good practices. However, there is a potential issue with the error handling. When an error occurs (for example, when terminating or restarting workers), the script simply logs the error but continues execution. This could lead to unexpected behavior if subsequent commands depend on the success of previous ones. It would be better to stop the script execution when an error occurs.

```diff
if [ $? -eq 0 ]; then
    print_verbose ""Workers terminated successfully.""
else
    print_verbose ""Failed to terminate workers.""
+    exit 1
fi

if [ $? -eq 0 ]; then
    print_verbose ""Workers restarted successfully.""
else
    print_verbose ""Failed to restart workers.""
+    exit 1
fi
```
This way, the script will stop executing as soon as it encounters an error, preventing any further actions that might depend on the failed command.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+#!/bin/bash
+
+# This script will be run every 4 hours in a cron job.
+# Open up a crontab ('crontab -e') and add the following:
+# 0 */4 * * * /path/to/cron_script.bash
+# For debugging, run ./cron_script.bash -v
+# Create a log file whenever the job gets run.
+ROOT_LOG_DIR=""/ceph/aeon/aeon/dj_store/logs""
+mkdir -p ""${ROOT_LOG_DIR}""
+LOG_FILE=""${ROOT_LOG_DIR}/cron_script_$(date '+%Y%m%d_%H%M%S').log""
+
+verbose=0
+
+while [[ ""$#"" -gt 0 ]]; do
+    case $1 in
+    -v | --verbose) verbose=1 ;;
+    esac
+    shift
+done
+
+# Verbose option for debugging
+print_verbose() {
+    if [ ""$verbose"" -eq 1 ]; then
+        echo ""[DEBUG] $1 - $(date '+%Y-%m-%d %H:%M:%S')""
+    fi
+    echo ""[DEBUG] $1 - $(date '+%Y-%m-%d %H:%M:%S')"" >> ""$LOG_FILE""
+}
+
+print_verbose ""Starting Ingestion...""
+cd /nfs/nhome/live/aeon_db/aeon_mecha/docker/
+
+print_verbose ""Terminate running workers...""
+/usr/local/bin/docker-compose down >> ""$LOG_FILE"" 2>&1
+if [ $? -eq 0 ]; then
+    print_verbose ""Workers terminated successfully.""
+else
+    print_verbose ""Failed to terminate workers.""
+fi
+
+if /usr/bin/docker image inspect ghcr.io/sainsburywellcomecentre/aeon_mecha >/dev/null 2>&1; then
+    print_verbose ""Removing existing aeon_mecha image...""
+    /usr/bin/docker image rm ghcr.io/sainsburywellcomecentre/aeon_mecha >> ""$LOG_FILE"" 2>&1
+fi
+
+print_verbose ""Restart workers...""
+/usr/local/bin/docker-compose up --detach >> ""$LOG_FILE"" 2>&1
+if [ $? -eq 0 ]; then
+    print_verbose ""Workers restarted successfully.""
+else
+    print_verbose ""Failed to restart workers.""
+fi",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529242,1.0,60,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/README.md,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The instructions provided in the README file for running tests are clear and comprehensive. However, it's a good practice to add a newline at the end of the file to avoid any potential issues with programs that expect or require it (like Git or some text editors).
```diff
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# Pytest for Project Aeon
+
+The following pytest routine will test whether the pre-defined datajoint schemas are properly instantiated and the sample data ingested in key datajoint tables.  
+
+For running the test, make sure you are in ```aeon``` virtual environment.
+```
+module load miniconda
+conda activate aeon
+```
+
+Currently, the test will use the following sample dataset as defined in ```test_params``` from ```tests/conftest.py```
+```python
+        ""start_ts"": ""2022-06-22 08:51:10"",
+        ""end_ts"": ""2022-06-22 14:00:00"",
+        ""experiment_name"": ""exp0.2-r0"",
+        ""raw_dir"": ""aeon/data/raw/AEON2/experiment0.2"",
+        ""qc_dir"": ""aeon/data/qc/AEON2/experiment0.2"",
+        ""test_dir"": data_dir(),
+        ""subject_count"": 5,
+        ""epoch_count"": 1,
+        ""chunk_count"": 7,
+        ""experiment_log_message_count"": 0,
+        ""subject_enter_exit_count"": 0,
+        ""subject_weight_time_count"": 0,
+        ""camera_qc_count"": 40,
+        ""camera_tracking_object_count"": 5,
+```
+
+Set `_tear_down=True` in `conftest.py` for proper cleanup of test artifacts after each testing (except for debugging/development purposes).
+
+The test can then be run with the following simple command at the root directory of the repo:
+```
+pytest 
+```
+
+With no command line arguments being specified, the command will run on any modules or functions that start with ```test_```)
+
+You can add in a series of command line arguments to get a more detailed view of the test results like the following:
+```
+pytest --pdb -sv --cov-report term-missing --cov=aeon_mecha -p no:warnings
+```
+
+The test can also be run on a single pytest module,
+
+```
+pytest -k <module_name>
+```
+or a function.
+
+```
+pytest -k <function_name>
+```
+
+You can also run tests on a specific marker:
+
+```
+pytest -m <marker_name>
+```
+
+For more detailed guides, please refer to [pytest documentation](https://docs.pytest.org/en/7.1.x/contents.html).",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529331,1.0,88,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/schema/social_02.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there are a few areas where improvements can be made for better maintainability and readability.

Firstly, the file paths used in the `__init__` methods of each class are constructed using string concatenation. This could potentially lead to issues if the `path` variable does not end with a slash (`/`). It would be more robust to use the `os.path.join()` function to construct these paths. 

Secondly, the column names for the CSV and Harp files are hardcoded into the `__init__` methods. If the structure of these files changes in the future, you will need to update the column names in multiple places. Consider defining these as constants at the top of your file or in a separate configuration file.

Here's an example of how you could implement these suggestions:

```diff
+ import os

...

- super().__init__(_reader.Csv(f""{path}_BlockState_*"", columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""]))
+ BLOCKSTATE_COLUMNS = [""pellet_ct"", ""pellet_ct_thresh"", ""due_time""]
+ super().__init__(_reader.Csv(os.path.join(path, ""_BlockState_*""), columns=BLOCKSTATE_COLUMNS))

...

- super().__init__(_reader.Harp(f""{path}_200_*"", [""weight(g)"", ""stability""]))
+ WEIGHTRAW_COLUMNS = [""weight(g)"", ""stability""]
+ super().__init__(_reader.Harp(os.path.join(path, ""_200_*""), columns=WEIGHTRAW_COLUMNS))
```

Lastly, it seems like the `Pose` class is only reading from files that match the pattern `""_test-node1*""`. Is this intended? If other nodes are expected in the future, consider making the node part of the filename a parameter of the `__init__` method.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import aeon.io.reader as _reader
+from aeon.schema.streams import Stream, StreamGroup
+from aeon.schema import core, foraging
+
+
+class Environment(StreamGroup):
+
+    def __init__(self, path):
+        super().__init__(path)
+
+    EnvironmentState = core.EnvironmentState
+
+    class BlockState(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_BlockState_*"", columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""]))
+
+    class LightEvents(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_LightEvents_*"", columns=[""channel"", ""value""]))
+
+    MessageLog = core.MessageLog
+
+
+class SubjectData(StreamGroup):
+    def __init__(self, path):
+        super().__init__(path)
+
+    class SubjectState(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_SubjectState_*"", columns=[""id"", ""weight"", ""type""]))
+
+    class SubjectVisits(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_SubjectVisits_*"", columns=[""id"", ""type"", ""region""]))
+
+    class SubjectWeight(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_SubjectWeight_*"", columns=[""weight"", ""confidence"", ""subject_id"", ""int_id""]))
+
+
+class Pose(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Pose(f""{path}_test-node1*""))
+
+
+class WeightRaw(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Harp(f""{path}_200_*"", [""weight(g)"", ""stability""]))
+
+
+class WeightFiltered(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Harp(f""{path}_202_*"", [""weight(g)"", ""stability""]))
+
+
+class Patch(StreamGroup):
+
+    def __init__(self, path):
+        super().__init__(path)
+
+    class DepletionState(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Csv(f""{path}_State_*"", columns=[""threshold"", ""offset"", ""rate""]))
+
+    Encoder = core.Encoder
+
+    Feeder = foraging.Feeder
+
+    class ManualDelivery(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Harp(f""{path}_201_*"", [""manual_delivery""]))
+
+    class MissedPellet(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Harp(f""{path}_202_*"", [""missed_pellet""]))
+
+    class RetriedDelivery(Stream):
+        def __init__(self, path):
+            super().__init__(_reader.Harp(f""{path}_203_*"", [""retried_delivery""]))
+
+
+class RfidEvents(Stream):
+
+    def __init__(self, path):
+        super().__init__(_reader.Harp(f""{path}_32*"", [""rfid""]))",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529418,22.0,27,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/conftest.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `data_dir` function does not have a return type hint. It's good practice to include type hints for better readability and maintainability.
```diff
- def data_dir():
+ def data_dir() -> str:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def data_dir():
+    """"""
+    Returns test data directory
+    """"""
+    return os.path.join(os.path.dirname(os.path.realpath(__file__)), ""data"")
+","--- 

+++ 

@@ -58,7 +58,6 @@

     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_""
-    return
 
 
 def load_pipeline():
@@ -137,8 +136,6 @@

         }
     )
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def epoch_chunk_ingestion(test_params, pipeline, experiment_creation):
@@ -154,8 +151,6 @@

 
     acquisition.Chunk.ingest_chunks(experiment_name=test_params[""experiment_name""])
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def experimentlog_ingestion(pipeline):
@@ -166,20 +161,14 @@

     acquisition.SubjectEnterExit.populate(**_populate_settings)
     acquisition.SubjectWeight.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_qc_ingestion(pipeline, epoch_chunk_ingestion):
     qc = pipeline[""qc""]
     qc.CameraQC.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_tracking_ingestion(pipeline, camera_qc_ingestion):
     tracking = pipeline[""tracking""]
     tracking.CameraTracking.populate(**_populate_settings)
-
-    return"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529510,64.0,84,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/conftest.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `load_pipeline` function does not have a return type hint. It's good practice to include type hints for better readability and maintainability.
```diff
- def load_pipeline():
+ def load_pipeline() -> dict:
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def load_pipeline():
+
+    from aeon.dj_pipeline import (
+        acquisition,
+        analysis,
+        lab,
+        qc,
+        report,
+        subject,
+        tracking,
+    )
+
+    return {
+        ""subject"": subject,
+        ""lab"": lab,
+        ""acquisition"": acquisition,
+        ""qc"": qc,
+        ""tracking"": tracking,
+        ""analysis"": analysis,
+        ""report"": report,
+    }","--- 

+++ 

@@ -58,7 +58,6 @@

     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_""
-    return
 
 
 def load_pipeline():
@@ -137,8 +136,6 @@

         }
     )
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def epoch_chunk_ingestion(test_params, pipeline, experiment_creation):
@@ -154,8 +151,6 @@

 
     acquisition.Chunk.ingest_chunks(experiment_name=test_params[""experiment_name""])
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def experimentlog_ingestion(pipeline):
@@ -166,20 +161,14 @@

     acquisition.SubjectEnterExit.populate(**_populate_settings)
     acquisition.SubjectWeight.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_qc_ingestion(pipeline, epoch_chunk_ingestion):
     qc = pipeline[""qc""]
     qc.CameraQC.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_tracking_ingestion(pipeline, camera_qc_ingestion):
     tracking = pipeline[""tracking""]
     tracking.CameraTracking.populate(**_populate_settings)
-
-    return"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529591,,8,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/io/test_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement has been changed from `aeon.schema.dataset` to `aeon.schema.schemas`. Ensure that the `exp02` module exists in the new location and is correctly implemented. If not, this could lead to an ImportError.
```diff
- from aeon.schema.dataset import exp02
+ from aeon.schema.schemas import exp02
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 import aeon
-from aeon.schema.dataset import exp02
+from aeon.schema.schemas import exp02",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529671,129.0,135,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the `read` method of the `Csv` class is a good improvement for handling empty CSV files. However, it's important to note that this will only work if the file is completely empty. If there are headers but no data rows, `file.stat().st_size` will not be `None` and `index_col=0` will still be set, potentially leading to incorrect behavior. Consider adding additional checks to handle this case.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        return pd.read_csv(
+            file,
+            header=0,
+            names=self.columns,
+            dtype=self.dtype,
+            index_col=0 if file.stat().st_size else None,
+        )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529792,225.0,227,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the condition for filtering data seems to be more accurate now as it ensures that the bitwise AND operation between `data.event` and `self.value` equals `self.value`. This means that all bits in `self.value` are also set in `data.event`, which is likely the intended behavior.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        data = data[(data.event & self.value) == self.value]
         data[""event""] = self.tag
         return data",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559529914,272.0,394,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/io/reader.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new `Pose` class provides a way to read Harp-binarized tracking data, which is a significant addition to the codebase. The methods seem well-structured and provide clear functionality. However, there are a few points that could be improved:

- In the `read` method (line 290), the process of dropping repeat parts (lines 307-314) could be simplified by using pandas' built-in functions. Instead of manually calculating indices and selecting columns, consider using `DataFrame.drop_duplicates`.

- The `get_class_names` and `get_bodyparts` methods (lines 331 and 345 respectively) both open and load the same JSON file. This could be optimized by loading the JSON file once and passing the loaded data to these methods.

- The `class_int2str` method (line 379) modifies the input DataFrame directly. It's generally a good practice to avoid modifying inputs directly unless it's necessary for performance reasons. Consider creating a copy of the DataFrame before making modifications.

Here are the suggested changes:

```diff
    def read(self, file: Path) -> pd.DataFrame:
...
        # Drop any repeat parts.
        unique_parts = np.unique(parts)
        if len(unique_parts) < len(parts):  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
            data = data.drop_duplicates(subset=['class', 'class_likelihood'] + [f""{part}_x"", f""{part}_y"", f""{part}_likelihood"" for part in unique_parts])
            parts = unique_parts
...
    def get_class_names(self, config: dict) -> list[str]:
...
    def get_bodyparts(self, config: dict) -> list[str]:
...
    @classmethod
    def class_int2str(cls, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
        """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
        data_copy = data.copy()
        config_file = cls.get_config_file(config_file_dir)
        if config_file.stem == ""confmap_config"":  # SLEAP
            with open(config_file) as f:
                config = json.load(f)
            try:
                heads = config[""model""][""heads""]
                classes = util.find_nested_key(heads, ""classes"")
            except KeyError as err:
                raise KeyError(f""Cannot find classes in {config_file}."") from err
            for i, subj in enumerate(classes):
                data_copy.loc[data_copy[""class""] == i, ""class""] = subj
        return data_copy
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class Pose(Harp):
+    """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
+
+    Columns:
+        class (int): Int ID of a subject in the environment.
+        class_likelihood (float): Likelihood of the subject's identity.
+        part (str): Bodypart on the subject.
+        part_likelihood (float): Likelihood of the specified bodypart.
+        x (float): X-coordinate of the bodypart.
+        y (float): Y-coordinate of the bodypart.
+    """"""
+
+    def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
+        """"""Pose reader constructor.""""""
+        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        super().__init__(pattern, columns=None)
+        self._model_root = model_root
+
+    def read(self, file: Path) -> pd.DataFrame:
+        """"""Reads data from the Harp-binarized tracking file.""""""
+        # Get config file from `file`, then bodyparts from config file.
+        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[1:])
+        config_file_dir = Path(self._model_root) / model_dir
+        if not config_file_dir.exists():
+            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        config_file = self.get_config_file(config_file_dir)
+        parts = self.get_bodyparts(config_file)
+
+        # Using bodyparts, assign column names to Harp register values, and read data in default format.
+        columns = [""class"", ""class_likelihood""]
+        for part in parts:
+            columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
+        self.columns = columns
+        data = super().read(file)
+
+        # Drop any repeat parts.
+        unique_parts, unique_idxs = np.unique(parts, return_index=True)
+        repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+            init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
+            data = data.iloc[:, keep_part_col_idxs]
+            parts = unique_parts
+
+        # Set new columns, and reformat `data`.
+        n_parts = len(parts)
+        part_data_list = [pd.DataFrame()] * n_parts
+        new_columns = [""class"", ""class_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
+        new_data = pd.DataFrame(columns=new_columns)
+        for i, part in enumerate(parts):
+            part_columns = [""class"", ""class_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
+            part_data = pd.DataFrame(data[part_columns])
+            part_data.insert(2, ""part"", part)
+            part_data.columns = new_columns
+            part_data_list[i] = part_data
+        new_data = pd.concat(part_data_list)
+        return new_data.sort_index()
+
+    def get_class_names(self, file: Path) -> list[str]:
+        """"""Returns a list of classes from a model's config file.""""""
+        classes = None
+        with open(file) as f:
+            config = json.load(f)
+        if file.stem == ""confmap_config"":  # SLEAP
+            try:
+                heads = config[""model""][""heads""]
+                classes = util.find_nested_key(heads, ""class_vectors"")[""classes""]
+            except KeyError as err:
+                if not classes:
+                    raise KeyError(f""Cannot find class_vectors in {file}."") from err
+        return classes
+
+    def get_bodyparts(self, file: Path) -> list[str]:
+        """"""Returns a list of bodyparts from a model's config file.""""""
+        parts = []
+        with open(file) as f:
+            config = json.load(f)
+        if file.stem == ""confmap_config"":  # SLEAP
+            try:
+                heads = config[""model""][""heads""]
+                parts = [util.find_nested_key(heads, ""anchor_part"")]
+                parts += util.find_nested_key(heads, ""part_names"")
+            except KeyError as err:
+                if not parts:
+                    raise KeyError(f""Cannot find bodyparts in {file}."") from err
+        return parts
+
+    @classmethod
+    def get_config_file(
+        cls,
+        config_file_dir: Path,
+        config_file_names: None | list[str] = None,
+    ) -> Path:
+        """"""Returns the config file from a model's config directory.""""""
+        if config_file_names is None:
+            config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
+        config_file = None
+        for f in config_file_names:
+            if (config_file_dir / f).exists():
+                config_file = config_file_dir / f
+                break
+        if config_file is None:
+            raise FileNotFoundError(f""Cannot find config file in {config_file_dir}"")
+        return config_file
+
+    @classmethod
+    def class_int2str(cls, data: pd.DataFrame, config_file_dir: Path) -> pd.DataFrame:
+        """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
+        config_file = cls.get_config_file(config_file_dir)
+        if config_file.stem == ""confmap_config"":  # SLEAP
+            with open(config_file) as f:
+                config = json.load(f)
+            try:
+                heads = config[""model""][""heads""]
+                classes = util.find_nested_key(heads, ""classes"")
+            except KeyError as err:
+                raise KeyError(f""Cannot find classes in {config_file}."") from err
+            for i, subj in enumerate(classes):
+                data.loc[data[""class""] == i, ""class""] = subj
+        return data
+
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559530037,1.0,124,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,docs/examples/get_harp_stream_event_bitmask.ipynb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The Jupyter notebook code looks good overall. However, it would be beneficial to add more comments throughout the code to explain what each section is doing. This will improve readability and maintainability of the code.

For example, in the cell starting at line 22, you could add a comment explaining what the output represents and how it's being used. Similarly, in the cell starting at line 96, you could explain why this cell is empty and under what circumstances it might be filled.

Also, consider adding error handling for potential issues that may arise during execution, such as missing files or directories, invalid timestamps, etc. This will make your code more robust and easier to debug.

```diff
+    try:
+        # Set the reader for the stream
+        harp_reader = reader.Harp(pattern=pattern, columns=[event_name])
+        # Set the root dir and a time range in which you know the stream acquired data
+        root = Path(""/ceph/aeon/aeon/data/raw/AEON3/presocial0.1"")
+        start = pd.Timestamp(""2023-06-21 10:00:00"")
+        end = pd.Timestamp(""2023-06-21 12:00:10"")
+        # Get the bitmask as the first value of the loaded stream
+        data = api.load(root, harp_reader, start=start, end=end)
+        bitmask = data.iloc[0, 0]
+        new_reader = reader.BitmaskEvent(pattern, bitmask, event_name)
+        stream_data = api.load(root, new_reader, start=start, end=end)
+    except Exception as e:
+        print(f""An error occurred: {e}"")
```

Remember to import the necessary modules for error handling at the beginning of your script.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+{
+ ""cells"": [
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 1,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": [
+    ""\""\""\""Jupyter settings and Imports\""\""\""\n"",
+    ""\n"",
+    ""%load_ext autoreload\n"",
+    ""%autoreload 2\n"",
+    ""\n"",
+    ""from pathlib import Path\n"",
+    ""import pandas as pd\n"",
+    ""\n"",
+    ""import aeon.io.api as api\n"",
+    ""from aeon.io import reader""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": 44,
+   ""metadata"": {},
+   ""outputs"": [
+    {
+     ""name"": ""stdout"",
+     ""output_type"": ""stream"",
+     ""text"": [
+      ""data:\n"",
+      ""                                beambreak\n"",
+      ""time                                    \n"",
+      ""2023-06-21 10:01:16.633728027         34\n"",
+      ""2023-06-21 10:01:16.649184227         32\n"",
+      ""2023-06-21 10:01:28.314400196         34\n"",
+      ""2023-06-21 10:01:28.331103802         32\n"",
+      ""2023-06-21 10:01:38.428864002         34\n"",
+      ""...                                  ...\n"",
+      ""2023-06-21 11:16:43.647552013         32\n"",
+      ""2023-06-21 11:16:43.655648232         34\n"",
+      ""2023-06-21 11:16:43.674079895         32\n"",
+      ""2023-06-21 11:21:40.381728172         34\n"",
+      ""2023-06-21 11:21:40.397024155         32\n"",
+      ""\n"",
+      ""[196 rows x 1 columns]\n"",
+      ""\n"",
+      ""\n"",
+      ""bitmask:\n"",
+      "" 34\n"",
+      ""\n"",
+      ""\n"",
+      ""stream_data:\n"",
+      ""                                    event\n"",
+      ""time                                    \n"",
+      ""2023-06-21 10:01:16.633728027  beambreak\n"",
+      ""2023-06-21 10:01:28.314400196  beambreak\n"",
+      ""2023-06-21 10:01:38.428864002  beambreak\n"",
+      ""2023-06-21 10:01:53.453343868  beambreak\n"",
+      ""2023-06-21 10:04:14.685791969  beambreak\n"",
+      ""...                                  ...\n"",
+      ""2023-06-21 11:15:20.406752110  beambreak\n"",
+      ""2023-06-21 11:16:24.036767960  beambreak\n"",
+      ""2023-06-21 11:16:43.625472069  beambreak\n"",
+      ""2023-06-21 11:16:43.655648232  beambreak\n"",
+      ""2023-06-21 11:21:40.381728172  beambreak\n"",
+      ""\n"",
+      ""[98 rows x 1 columns]\n""
+     ]
+    }
+   ],
+   ""source"": [
+    ""\""\""\""How to find the bitmask associated with any harp stream event and create a new reader: \n"",
+    ""example with patch beambreak\""\""\""\n"",
+    ""\n"",
+    ""# Ensure you have the pattern of the stream (look at the filename), and the expected event name\n"",
+    ""pattern = \""Patch1_32*\""\n"",
+    ""event_name = \""beambreak\""\n"",
+    ""# Set the reader for the stream\n"",
+    ""harp_reader = reader.Harp(pattern=pattern, columns=[event_name])\n"",
+    ""# Set the root dir and a time range in which you know the stream acquired data\n"",
+    ""root = Path(\""/ceph/aeon/aeon/data/raw/AEON3/presocial0.1\"")\n"",
+    ""start = pd.Timestamp(\""2023-06-21 10:00:00\"")\n"",
+    ""end = pd.Timestamp(\""2023-06-21 12:00:10\"")\n"",
+    ""# Get the bitmask as the first value of the loaded stream\n"",
+    ""data = api.load(root, harp_reader, start=start, end=end)\n"",
+    ""bitmask = data.iloc[0, 0]\n"",
+    ""new_reader = reader.BitmaskEvent(pattern, bitmask, event_name)\n"",
+    ""stream_data = api.load(root, new_reader, start=start, end=end)\n"",
+    ""\n"",
+    ""print(f\""data:\\n {data}\\n\\n\"")\n"",
+    ""print(f\""bitmask:\\n {bitmask}\\n\\n\"")\n"",
+    ""print(f\""stream_data:\\n {stream_data}\"")""
+   ]
+  },
+  {
+   ""cell_type"": ""code"",
+   ""execution_count"": null,
+   ""metadata"": {},
+   ""outputs"": [],
+   ""source"": []
+  }
+ ],
+ ""metadata"": {
+  ""kernelspec"": {
+   ""display_name"": ""aeon"",
+   ""language"": ""python"",
+   ""name"": ""python3""
+  },
+  ""language_info"": {
+   ""codemirror_mode"": {
+    ""name"": ""ipython"",
+    ""version"": 3
+   },
+   ""file_extension"": "".py"",
+   ""mimetype"": ""text/x-python"",
+   ""name"": ""python"",
+   ""nbconvert_exporter"": ""python"",
+   ""pygments_lexer"": ""ipython3"",
+   ""version"": ""3.11.4""
+  }
+ },
+ ""nbformat"": 4,
+ ""nbformat_minor"": 2
+}",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559530128,15.0,30,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/dj_pipeline/test_pipeline_instantiation.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test `test_experiment_creation` seems to be tightly coupled with the data. It's checking for specific values like ""BAA-1100701"" and ""raw"". This could make the test brittle if the data changes. Consider using a more flexible approach or ensure that the test setup includes the creation of necessary data.

Also, there is no error handling in case the fetch operations do not return any results. If the fetch operation doesn't find the experiment_name or directory_path, it will raise an error. You might want to handle this gracefully.

Here's a suggestion on how you can improve the error handling:

```diff
    experiment_name = test_params[""experiment_name""]
    try:
        assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
    except DataJointError:
        pytest.fail(""Experiment not found in the database"")
    raw_dir = (
        acquisition.Experiment.Directory
        & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
    )
    try:
        assert raw_dir.fetch1(""directory_path"") == test_params[""raw_dir""]
    except DataJointError:
        pytest.fail(""Directory path not found in the database"")
    exp_subjects = (
        acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
    ).fetch(""subject"")
    assert len(exp_subjects) == test_params[""subject_count""]
    assert ""BAA-1100701"" in exp_subjects
```
In this change, I've added a try-except block around the fetch operations. If the fetch operation fails, it raises a `DataJointError`. The except block catches this error and fails the test with a helpful message. This way, if the test fails, you'll know exactly why it failed.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+@mark.instantiation
+def test_experiment_creation(test_params, pipeline, experiment_creation):
+    acquisition = pipeline[""acquisition""]
+    
+    experiment_name = test_params[""experiment_name""]
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
+    raw_dir = (
+        acquisition.Experiment.Directory
+        & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
+    ).fetch1(""directory_path"")
+    assert raw_dir == test_params[""raw_dir""]
+    exp_subjects = (
+        acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
+    ).fetch(""subject"")
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1559530217,1.0,71,ca3fecf2182f2d595ede18b23017474038081ccd,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/dj_pipeline/test_tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there are a few areas that could be improved for better readability and maintainability.

Firstly, the `index` and `column_name` variables are defined at the module level (lines 7 and 8). It would be better to move these inside the functions where they are used or pass them as parameters. This would make the functions more self-contained and easier to understand.

Secondly, the construction of the file name is duplicated in both `save_test_data` (lines 18-28) and `test_camera_tracking_ingestion` (lines 48-59). This logic could be extracted into a separate function to avoid repetition and make the code easier to maintain.

Lastly, the test function `test_camera_tracking_ingestion` (lines 39-71) is quite long and does several things. It might be beneficial to split this function into smaller ones, each testing a specific aspect of the camera tracking ingestion.

Here's how you could refactor the code:

```diff
- index = 0
- column_name = ""position_x"" 

+ def construct_file_name(key):
+     return (
+         ""-"".join(
+             [
+                 v.strftime(""%Y%m%d%H%M%S"")
+                 if isinstance(v, datetime.datetime)
+                 else str(v)
+                 for v in key.values()
+             ]
+         )
+         + "".npy""
+     )

- def save_test_data(pipeline, test_params):
+ def save_test_data(pipeline, test_params, index=0, column_name=""position_x""):

-     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
-     file_name = (
-         ""-"".join(
-             [
-                 v.strftime(""%Y%m%d%H%M%S"")
-                 if isinstance(v, datetime.datetime)
-                 else str(v)
-                 for v in key.values()
-             ]
-         )
-         + "".npy""
-     )

+     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
+     file_name = construct_file_name(key)

- def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
+ def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion, index=0, column_name=""position_x""):

-     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
-     file_name = (
-         ""-"".join(
-             [
-                 v.strftime(""%Y%m%d%H%M%S"")
-                 if isinstance(v, datetime.datetime)
-                 else str(v)
-                 for v in key.values()
-             ]
-         )
-         + "".npy""
-     )

+     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
+     file_name = construct_file_name(key)
```
This way, the code is more modular and easier to maintain.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import datetime
+import pathlib
+
+import numpy as np
+from pytest import mark
+
+index = 0
+column_name = ""position_x""  # data column to run test on
+file_name = ""exp0.2-r0-20220524090000-21053810-20220524082942-0-0.npy""  # test file to be saved with save_test_data
+
+
+def save_test_data(pipeline, test_params):
+    """"""save test dataset fetched from tracking.CameraTracking.Object""""""
+
+    tracking = pipeline[""tracking""]
+
+    key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
+    file_name = (
+        ""-"".join(
+            [
+                v.strftime(""%Y%m%d%H%M%S"")
+                if isinstance(v, datetime.datetime)
+                else str(v)
+                for v in key.values()
+            ]
+        )
+        + "".npy""
+    )
+
+    data = (tracking.CameraTracking.Object() & key).fetch(column_name)[0]
+    test_file = test_params[""test_dir""] + ""/"" + file_name
+    np.save(test_file, data)
+
+    return test_file
+
+
+@mark.ingestion
+@mark.tracking
+def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
+
+    tracking = pipeline[""tracking""]
+
+    assert (
+        len(tracking.CameraTracking.Object())
+        == test_params[""camera_tracking_object_count""]
+    )
+
+    key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
+    file_name = (
+        ""-"".join(
+            [
+                v.strftime(""%Y%m%d%H%M%S"")
+                if isinstance(v, datetime.datetime)
+                else str(v)
+                for v in key.values()
+            ]
+        )
+        + "".npy""
+    )
+
+    test_file = pathlib.Path(test_params[""test_dir""] + ""/"" + file_name)
+    assert test_file.exists()
+
+    print(f""\nTesting {file_name}"")
+
+    data = np.load(test_file)
+    assert np.allclose(
+        data,
+        (tracking.CameraTracking.Object() & key).fetch(column_name)[0],
+        equal_nan=True,
+    )",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1573024690,106.0,263,df1cc6121524093b0c5002a9fcd0fc8d126bc4eb,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/tracking.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There are a few issues in this code block. 

First, the `PoseIdentity` class has an attribute `identity_likelihood` which is of type `longblob`. This might not be the best choice for storing likelihood values as it can lead to performance issues when querying or processing the data. It would be better to use a more suitable datatype like `float`.

Second, the `make` method in the `SLEAPTracking` class is quite long and complex. It's doing a lot of different things, from loading data, finding config files, getting bodyparts and classes, to ingesting parts and classes. This violates the Single Responsibility Principle (SRP) and makes the method hard to understand, test, and maintain. It would be better to break this method down into smaller, more manageable methods each with a single responsibility.

Third, the `is_position_in_patch` and `is_position_in_nest` functions return a new column added to the input dataframe. This can lead to unexpected side effects if the caller doesn't realize that the input dataframe is being modified. It would be better to return a new dataframe instead of modifying the input dataframe.

Here are the suggested changes:

```diff
    class PoseIdentity(dj.Part):
        definition = """"""
         -> master
        identity_idx:           smallint


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# ---------- VideoSource  ------------------
 
 
 @schema
-class CameraTracking(dj.Imported):
-    definition = """"""  # Tracked objects position data from a particular camera, using a particular tracking method, for a particular chunk
+class SLEAPTracking(dj.Imported):
+    definition = """"""  # Tracked objects position data from a particular VideoSource for multi-animal experiment using the SLEAP tracking method per chunk
     -> acquisition.Chunk
-    -> acquisition.ExperimentCamera
+    -> streams.SpinnakerVideoSource
     -> TrackingParamSet
     """"""
 
-    class Object(dj.Part):
-        definition = """"""  # Position data of object tracked by a particular camera tracking
+    class PoseIdentity(dj.Part):
+        definition = """"""
         -> master
-        object_id: int    # object with id = -1 means ""unknown/not sure"", could potentially be the same object as those with other id value
+        identity_idx:           smallint
         ---
-        timestamps:        longblob  # (datetime) timestamps of the position data
-        position_x:        longblob  # (px) object's x-position, in the arena's coordinate frame
-        position_y:        longblob  # (px) object's y-position, in the arena's coordinate frame
-        area=null:         longblob  # (px^2) object's size detected in the camera
+        identity_name:          varchar(16)
+        identity_likelihood:    longblob
+        anchor_part:         varchar(16)  # the name of the point used as anchor node for this class
+        """"""
+
+    class Part(dj.Part):
+        definition = """"""
+        -> master.PoseIdentity
+        part_name: varchar(16)
+        ---
+        sample_count: int      # number of data points acquired from this stream for a given chunk
+        x:          longblob
+        y:          longblob
+        likelihood: longblob
+        timestamps: longblob
         """"""
 
     @property
     def key_source(self):
-        ks = acquisition.Chunk * acquisition.ExperimentCamera * TrackingParamSet
         return (
-            ks
+            acquisition.Chunk
             * (
-                qc.CameraQC * acquisition.ExperimentCamera
-                & f""camera_description in {tuple(set(acquisition._ref_device_mapping.values()))}""
-            ).proj()
-            & ""tracking_paramset_id = 0""
-        )
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            * (TrackingParamSet & ""tracking_paramset_id = 1"")
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )  # SLEAP & CameraTop
 
     def make(self, key):
-        chunk_start, chunk_end, dir_type = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end"", ""directory_type""
-        )
-        camera = (acquisition.ExperimentCamera & key).fetch1(""camera_description"")
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
-        raw_data_dir = acquisition.Experiment.get_data_directory(
-            key, directory_type=dir_type
-        )
+        data_dirs = acquisition.Experiment.get_data_directories(key)
 
-        device = getattr(
-            acquisition._device_schema_mapping[key[""experiment_name""]], camera
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
         )
+        stream_reader = getattr(getattr(devices_schema, device_name), ""Pose"")
 
-        positiondata = io_api.load(
-            root=raw_data_dir.as_posix(),
-            reader=device.Position,
+        pose_data = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
             start=pd.Timestamp(chunk_start),
             end=pd.Timestamp(chunk_end),
         )
 
-        # replace id=NaN with -1
-        positiondata.fillna({""id"": -1}, inplace=True)
+        if not len(pose_data):
+            raise ValueError(f""No SLEAP data found for {key['experiment_name']} - {device_name}"")
 
-        # Correct for frame offsets from Camera QC
-        qc_timestamps, qc_frame_offsets, camera_fs = (
-            qc.CameraQC * acquisition.ExperimentCamera & key
-        ).fetch1(""timestamps"", ""frame_offset"", ""camera_sampling_rate"")
-        qc_time_offsets = qc_frame_offsets / camera_fs
-        qc_time_offsets = np.where(
-            np.isnan(qc_time_offsets), 0, qc_time_offsets
-        )  # set NaNs to 0
-        positiondata.index += pd.to_timedelta(qc_time_offsets, ""s"")
-
-        object_positions = []
-        for obj_id in set(positiondata.id.values):
-            obj_position = positiondata[positiondata.id == obj_id]
-
-            object_positions.append(
+        # Find the config file for the SLEAP model
+        for data_dir in data_dirs:
+            try:
+                f = next(
+                    data_dir.glob(
+                        f""**/**/{stream_reader.pattern}{io_api.chunk(chunk_start).strftime('%Y-%m-%dT%H-%M-%S')}*.{stream_reader.extension}""
+                    )
+                )
+            except StopIteration:
+                continue
+            else:
+                config_file = stream_reader.get_config_file(
+                    stream_reader._model_root / Path(*Path(f.stem.replace(""_"", ""/"")).parent.parts[1:])
+                )
+                break
+        else:
+            raise FileNotFoundError(f""Unable to find SLEAP model config file for: {stream_reader.pattern}"")
+
+        # get bodyparts and classes
+        bodyparts = stream_reader.get_bodyparts(config_file)
+        anchor_part = bodyparts[0]  # anchor_part is always the first one
+        class_names = stream_reader.get_class_names(config_file)
+
+        # ingest parts and classes
+        pose_identity_entries, part_entries = [], []
+        for class_idx in set(pose_data[""class""].values.astype(int)):
+            class_position = pose_data[pose_data[""class""] == class_idx]
+            for part in set(class_position.part.values):
+                part_position = class_position[class_position.part == part]
+                part_entries.append(
+                    {
+                        **key,
+                        ""identity_idx"": class_idx,
+                        ""part_name"": part,
+                        ""timestamps"": part_position.index.values,
+                        ""x"": part_position.x.values,
+                        ""y"": part_position.y.values,
+                        ""likelihood"": part_position.part_likelihood.values,
+                        ""sample_count"": len(part_position.index.values),
+                    }
+                )
+                if part == anchor_part:
+                    class_likelihood = part_position.class_likelihood.values
+            pose_identity_entries.append(
                 {
                     **key,
-                    ""object_id"": obj_id,
-                    ""timestamps"": obj_position.index.to_pydatetime(),
-                    ""position_x"": obj_position.x.values,
-                    ""position_y"": obj_position.y.values,
-                    ""area"": obj_position.area.values,
+                    ""identity_idx"": class_idx,
+                    ""identity_name"": class_names[class_idx],
+                    ""anchor_part"": anchor_part,
+                    ""identity_likelihood"": class_likelihood,
                 }
             )
 
         self.insert1(key)
-        self.Object.insert(object_positions)
-
-    @classmethod
-    def get_object_position(
-        cls,
-        experiment_name,
-        object_id,
-        start,
-        end,
-        camera_name=""FrameTop"",
-        tracking_paramset_id=0,
-        in_meter=False,
-    ):
-        table = (
-            cls.Object * acquisition.Chunk.proj(""chunk_end"")
-            & {""experiment_name"": experiment_name}
-            & {""tracking_paramset_id"": tracking_paramset_id}
-            & (acquisition.ExperimentCamera & {""camera_description"": camera_name})
-        )
-
-        return _get_position(
-            table,
-            object_attr=""object_id"",
-            object_name=object_id,
-            start_attr=""chunk_start"",
-            end_attr=""chunk_end"",
-            start=start,
-            end=end,
-            fetch_attrs=[""timestamps"", ""position_x"", ""position_y"", ""area""],
-            attrs_to_scale=[""position_x"", ""position_y""],
-            scale_factor=pixel_scale if in_meter else 1,
-        )
+        self.PoseIdentity.insert(pose_identity_entries)
+        self.Part.insert(part_entries)
 
 
 # ---------- HELPER ------------------
 
 
-def compute_distance(position_df, target):
+def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
     assert len(target) == 2
-    return np.sqrt(np.square(position_df[[""x"", ""y""]] - target).sum(axis=1))
+    return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
 
-def is_in_patch(
+def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
-):
+) -> pd.Series:
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0
     in_wheel = (wheel_distance_travelled.diff().rolling(""1s"").sum() > 1).reindex(
         position_df.index, method=""pad""
     )
     time_slice = exit_patch.cumsum()
-    return in_wheel.groupby(time_slice).apply(lambda x: x.cumsum()) > 0
+    return in_patch & (in_wheel.groupby(time_slice).apply(lambda x: x.cumsum()) > 0)
 
 
-def is_position_in_nest(position_df, nest_key):
-    """"""
-    Given the session key and the position data - arrays of x and y
-    return an array of boolean indicating whether or not a position is inside the nest
+def is_position_in_nest(position_df, nest_key, xcol=""x"", ycol=""y"") -> pd.Series:
+    """"""Given the session key and the position data - arrays of x and y
+    return an array of boolean indicating whether or not a position is inside the nest.
     """"""
-    nest_vertices = list(
-        zip(*(lab.ArenaNest.Vertex & nest_key).fetch(""vertex_x"", ""vertex_y""))
-    )
-    nest_path = path.Path(nest_vertices)
-
-    return nest_path.contains_points(position_df[[""x"", ""y""]])
+    nest_vertices = list(zip(*(lab.ArenaNest.Vertex & nest_key).fetch(""vertex_x"", ""vertex_y"")))
+    nest_path = matplotlib.path.Path(nest_vertices)
+    position_df[""in_nest""] = nest_path.contains_points(position_df[[xcol, ycol]])
+    return position_df[""in_nest""]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877776,16.0,59,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/__init__.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit


The `fetch_stream` function could be optimized for better performance. Currently, the function fetches all data from the query and then filters out rows where ""sample_count"" is greater than 0. This could potentially lead to unnecessary memory usage if the dataset is large. It would be more efficient to filter the data at the database level before fetching it.

```diff
- df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
+ df = query.fetch(""sample_count > 0"", format=""frame"").reset_index()
```

In the try-except block at the end of this hunk, there's a bare except clause which is not a good practice as it can catch unexpected errors and make debugging difficult. It's recommended to specify the exception type you're expecting to catch. If you're unsure about the exact exception type, you can use `Exception` as a catch-all, but it's still better to specify the exact exceptions whenever possible.

```diff
-     except:
+     except ImportError:
          pass
```
Also, it's generally not a good idea to just `pass` on an exception without logging it or handling it in some way. This can lead to silent failures that are hard to debug. Consider adding some logging or error handling here.

```diff
-     except ImportError:
+     except ImportError as e:
+         print(f""An error occurred while importing: {e}"")
          pass
```

Lastly, the function `dict_to_uuid` does not handle the case when the key values are not string. The `encode()` method will raise an `AttributeError` if the value is not a string. You should add a check to ensure that the values are strings before calling `encode()`. 

```diff
-         hashed.update(str(v).encode())
+         hashed.update(str(v).encode() if isinstance(v, str) else str(v))
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+repository_config = dj.config[""custom""].get(""repository_config"", _default_repository_config)
 
 
-def get_schema_name(name):
+def get_schema_name(name) -> str:
+    """"""Return a schema name.""""""
     return db_prefix + name
 
 
-def dict_to_uuid(key):
-    """"""
-    Given a dictionary `key`, returns a hash string as UUID
-    """"""
+def dict_to_uuid(key) -> uuid.UUID:
+    """"""Given a dictionary `key`, returns a hash string as UUID.""""""
     hashed = hashlib.md5()
     for k, v in sorted(key.items()):
         hashed.update(str(k).encode())
         hashed.update(str(v).encode())
     return uuid.UUID(hex=hashed.hexdigest())
+
+
+def fetch_stream(query, drop_pk=True):
+    """"""
+    Provided a query containing data from a Stream table,
+     fetch and aggregate the data into one DataFrame indexed by ""time""
+    """"""
+    df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
+    cols2explode = [
+        c for c in query.heading.secondary_attributes if query.heading.attributes[c].type == ""longblob""
+    ]
+    df = df.explode(column=cols2explode)
+    cols2drop = [""sample_count""] + (query.primary_key if drop_pk else [])
+    df.drop(columns=cols2drop, inplace=True, errors=""ignore"")
+    df.rename(columns={""timestamps"": ""time""}, inplace=True)
+    df.set_index(""time"", inplace=True)
+    df.sort_index(inplace=True)
+    return df
+
+
+try:
+    from . import streams
+except ImportError:
+    try:
+        from .utils import streams_maker
+
+        streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
+    except:
+        pass",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877782,25.0,30,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/acquisition.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of `_device_schema_mapping` might cause issues if it was used elsewhere in the code. Please ensure that this change doesn't break any dependencies.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     ""exp0.2-r0"": ""CameraTop"",
 }
 
-_device_schema_mapping = {
-    ""exp0.1-r0"": aeon_schema.exp01,
-    ""social0-r1"": aeon_schema.exp01,
-    ""exp0.2-r0"": aeon_schema.exp02,
-}
-
 
 # ------------------- Type Lookup ------------------------
 ",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877786,,87,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/acquisition.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in `contents` from `[""raw"", ""preprocessing"", ""analysis"", ""quality-control""]` to `[""raw"", ""processed"", ""qc""]` may affect other parts of the code that rely on these specific directory types. Ensure that all references to these directory types are updated accordingly.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     """"""
 
-    contents = zip([""raw"", ""preprocessing"", ""analysis"", ""quality-control""])
+    contents = zip([""raw"", ""processed"", ""qc""])",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877788,193.0,238,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/acquisition.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the datetime parsing at line 193. If the format of the date string in `epoch_dir.name` changes, this could raise a ValueError. It would be safer to wrap this in a try-except block to handle potential errors.
```diff
- epoch_start = datetime.datetime.strptime(epoch_dir.name, ""%Y-%m-%dT%H-%M-%S"")
+ try:
+     epoch_start = datetime.datetime.strptime(epoch_dir.name, ""%Y-%m-%dT%H-%M-%S"")
+ except ValueError:
+     logger.error(f""Invalid date format in {epoch_dir.name}"")
+     continue
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            epoch_start = datetime.datetime.strptime(epoch_dir.name, ""%Y-%m-%dT%H-%M-%S"")
             # --- insert to Epoch ---
             epoch_key = {""experiment_name"": experiment_name, ""epoch_start"": epoch_start}
 
+            # skip over those already ingested
             if cls & epoch_key or epoch_key in epoch_list:
-                # skip over those already ingested
                 continue
 
-            epoch_config, metadata_yml_filepath = None, None
-            if experiment_name != ""exp0.1-r0"":
-                metadata_yml_filepath = epoch_dir / ""Metadata.yml""
-                if metadata_yml_filepath.exists():
-                    epoch_config = extract_epoch_metadata(
-                        experiment_name, metadata_yml_filepath
-                    )
-
-                    metadata_yml_filepath = epoch_config[""metadata_file_path""]
-
-                    _, directory, repo_path = _match_experiment_directory(
-                        experiment_name,
-                        epoch_config[""metadata_file_path""],
-                        raw_data_dirs,
-                    )
-                    epoch_config = {
-                        **epoch_config,
-                        **directory,
-                        ""metadata_file_path"": epoch_config[""metadata_file_path""]
-                        .relative_to(repo_path)
-                        .as_posix(),
-                    }
+            raw_data_dir, directory, _ = _match_experiment_directory(
+                experiment_name,
+                epoch_dir,
+                raw_data_dirs,
+            )
 
             # find previous epoch end-time
-            previous_epoch_end = None
+            previous_epoch_key = None
             if i > 0:
                 previous_chunk = all_chunks.iloc[i - 1]
                 previous_chunk_path = pathlib.Path(previous_chunk.path)
-                previous_epoch_dir = pathlib.Path(
-                    previous_chunk_path.as_posix().split(device_name)[0]
-                )
+                previous_epoch_dir = pathlib.Path(previous_chunk_path.as_posix().split(device_name)[0])
                 previous_epoch_start = datetime.datetime.strptime(
                     previous_epoch_dir.name, ""%Y-%m-%dT%H-%M-%S""
                 )
-                previous_chunk_end = previous_chunk.name + datetime.timedelta(
-                    hours=io_api.CHUNK_DURATION
-                )
+                previous_chunk_end = previous_chunk.name + datetime.timedelta(hours=io_api.CHUNK_DURATION)
                 previous_epoch_end = min(previous_chunk_end, epoch_start)
+                previous_epoch_key = {
+                    ""experiment_name"": experiment_name,
+                    ""epoch_start"": previous_epoch_start,
+                }
 
             with cls.connection.transaction:
-                cls.insert1(epoch_key)
-                if previous_epoch_end and not (
-                    EpochEnd
-                    & {
-                        ""experiment_name"": experiment_name,
-                        ""epoch_start"": previous_epoch_start,
-                    }
+                # insert new epoch
+                cls.insert1(
+                    {**epoch_key, **directory, ""epoch_dir"": epoch_dir.relative_to(raw_data_dir).as_posix()}
+                )
+                epoch_list.append(epoch_key)
+
+                # update previous epoch
+                if (
+                    previous_epoch_key
+                    and (cls & previous_epoch_key)
+                    and not (EpochEnd & previous_epoch_key)
                 ):
                     # insert end-time for previous epoch
                     EpochEnd.insert1(
                         {",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877790,271.0,345,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/acquisition.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `make` method, there's a potential issue at line 318 where you're assuming that the `Metadata.yml` file exists. This could raise a FileNotFoundError if the file is not found. It would be safer to check if the file exists before trying to open it.
```diff
- metadata_yml_filepath = data_dir / epoch_dir / ""Metadata.yml""
+ metadata_yml_filepath = data_dir / epoch_dir / ""Metadata.yml""
+ if not metadata_yml_filepath.exists():
+     logger.error(f""Metadata file not found at {metadata_yml_filepath}"")
+     return
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+@schema
+class EpochConfig(dj.Imported):
+    definition = """"""
+    -> Epoch
+    """"""
+
+    class Meta(dj.Part):
+        definition = """""" # Metadata for the configuration of a given epoch
+        -> master
+        ---
+        bonsai_workflow: varchar(36)
+        commit: varchar(64)   # e.g. git commit hash of aeon_experiment used to generated this particular epoch
+        source='': varchar(16)  # e.g. aeon_experiment or aeon_acquisition (or others)
+        metadata: longblob
+        metadata_file_path: varchar(255)  # path of the file, relative to the experiment repository
+        """"""
+
+    class DeviceType(dj.Part):
+        definition = """"""  # Device type(s) used in a particular acquisition epoch
+        -> master
+        device_type: varchar(36)
+        """"""
+
+    class ActiveRegion(dj.Part):
+        definition = """"""
+        -> master
+        region_name: varchar(36)
+        ---
+        region_data: longblob
+        """"""
+
+    def make(self, key):
+        from aeon.dj_pipeline.utils import streams_maker
+        from aeon.dj_pipeline.utils.load_metadata import (
+            extract_epoch_config,
+            ingest_epoch_metadata,
+            insert_device_types,
+        )
+
+        experiment_name = key[""experiment_name""]
+        devices_schema = getattr(
+            aeon_schemas,
+            (Experiment.DevicesSchema & {""experiment_name"": experiment_name}).fetch1(""devices_schema_name""),
+        )
+
+        dir_type, epoch_dir = (Epoch & key).fetch1(""directory_type"", ""epoch_dir"")
+        data_dir = Experiment.get_data_directory(key, dir_type)
+        metadata_yml_filepath = data_dir / epoch_dir / ""Metadata.yml""
+
+        epoch_config = extract_epoch_config(experiment_name, devices_schema, metadata_yml_filepath)
+        epoch_config = {
+            **epoch_config,
+            ""metadata_file_path"": metadata_yml_filepath.relative_to(data_dir).as_posix(),
+        }
+
+        # Insert new entries for streams.DeviceType, streams.Device.
+        insert_device_types(
+            devices_schema,
+            metadata_yml_filepath,
+        )
+        # Define and instantiate new devices/stream tables under `streams` schema
+        streams_maker.main()
+        # Insert devices' installation/removal/settings
+        epoch_device_types = ingest_epoch_metadata(experiment_name, devices_schema, metadata_yml_filepath)
+
+        self.insert1(key)
+        self.Meta.insert1(epoch_config)
+        self.DeviceType.insert(key | {""device_type"": n} for n in epoch_device_types or {})
+        with metadata_yml_filepath.open(""r"") as f:
+            metadata = json.load(f)
+        self.ActiveRegion.insert(
+            {**key, ""region_name"": k, ""region_data"": v} for k, v in metadata[""ActiveRegion""].items()
+        )
+
+",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1579877791,1.0,115,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/populate/worker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks well-structured and follows good practices. However, there are a few areas that could be improved for better maintainability and readability.

1. The `ingest_epochs_chunks` and `ingest_environment_visits` functions (lines 39-52) both fetch the experiment names from `AutomatedExperimentIngestion`. This is a repeated operation and can be abstracted into a separate function to follow the DRY principle.

2. The configuration of each worker (lines 56-111) is repetitive and could be abstracted into a function that takes parameters such as worker name, max idled cycle, sleep duration, etc. This would make the code more maintainable and easier to read.

3. The `get_workflow_operation_overview` function (lines 113-115) imports a function within its body. It's generally recommended to keep all imports at the top of the file for better visibility and organization.

Here are the suggested changes:

```diff
+ def get_experiment_names():
+     """"""Fetch experiment names from AutomatedExperimentIngestion.""""""
+     return AutomatedExperimentIngestion.fetch(""experiment_name"")

- experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
+ experiment_names = get_experiment_names()

...

+ def configure_worker(name, schema_name, prefix, idled_cycle, sleep_duration, tasks):
+     """"""Configure a DataJointWorker with given parameters and tasks.""""""
+     worker = DataJointWorker(
+         name,
+         worker_schema_name=schema_name,
+         db_prefix=prefix,
+         max_idled_cycle=idled_cycle,
+         sleep_duration=sleep_duration,
+     )
+     for task in tasks:
+         worker(task)
+     return worker

- acquisition_worker = DataJointWorker(
-     ""acquisition_worker"",
-     worker_schema_name=worker_schema_name,
-     db_prefix=db_prefix,
-     max_idled_cycle=6,
-     sleep_duration=1200,
- )
- acquisition_worker(ingest_epochs_chunks)
- acquisition_worker(acquisition.EpochConfig)
- acquisition_worker(acquisition.Environment)

+ acquisition_tasks = [ingest_epochs_chunks, acquisition.EpochConfig, acquisition.Environment]
+ acquisition_worker = configure_worker(""acquisition_worker"", worker_schema_name, db_prefix, 6, 1200, acquisition_tasks)

...

- from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
+ import datajoint_utilities.dj_worker.utils as dj_utils

- return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
+ return dj_utils.get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import datajoint as dj
+from datajoint_utilities.dj_worker import DataJointWorker, ErrorLog, WorkerLog
+from datajoint_utilities.dj_worker.worker_schema import is_djtable
+
+from aeon.dj_pipeline import db_prefix
+from aeon.dj_pipeline import subject, acquisition, tracking, qc
+from aeon.dj_pipeline.analysis import block_analysis
+from aeon.dj_pipeline.utils import streams_maker
+
+streams = streams_maker.main()
+
+__all__ = [
+    ""acquisition_worker"",
+    ""analysis_worker"",
+    ""pyrat_worker"",
+    ""streams_worker"",
+    ""WorkerLog"",
+    ""ErrorLog"",
+    ""logger"",
+    ""AutomatedExperimentIngestion"",
+]
+
+# ---- Some constants ----
+logger = dj.logger
+worker_schema_name = db_prefix + ""worker""
+
+# ---- Manage experiments for automated ingestion ----
+
+schema = dj.Schema(worker_schema_name)
+
+
+@schema
+class AutomatedExperimentIngestion(dj.Manual):
+    definition = """"""  # experiments to undergo automated ingestion
+    -> acquisition.Experiment
+    """"""
+
+
+def ingest_epochs_chunks():
+    """"""Ingest epochs and chunks for experiments specified in AutomatedExperimentIngestion.""""""
+    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
+    for experiment_name in experiment_names:
+        acquisition.Epoch.ingest_epochs(experiment_name)
+        acquisition.Chunk.ingest_chunks(experiment_name)
+
+
+def ingest_environment_visits():
+    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
+    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
+    # analysis.ingest_environment_visits(experiment_names)
+    pass
+
+
+# ---- Define worker(s) ----
+# configure a worker to process `acquisition`-related tasks
+acquisition_worker = DataJointWorker(
+    ""acquisition_worker"",
+    worker_schema_name=worker_schema_name,
+    db_prefix=db_prefix,
+    max_idled_cycle=6,
+    sleep_duration=1200,
+)
+acquisition_worker(ingest_epochs_chunks)
+acquisition_worker(acquisition.EpochConfig)
+acquisition_worker(acquisition.Environment)
+# acquisition_worker(ingest_environment_visits)
+# acquisition_worker(block_analysis.BlockDetection)
+
+# configure a worker to handle pyrat sync
+pyrat_worker = DataJointWorker(
+    ""pyrat_worker"",
+    worker_schema_name=worker_schema_name,
+    db_prefix=db_prefix,
+    max_idled_cycle=400,
+    sleep_duration=30,
+)
+
+pyrat_worker(subject.CreatePyratIngestionTask)
+pyrat_worker(subject.PyratIngestion)
+pyrat_worker(subject.SubjectDetail)
+pyrat_worker(subject.PyratCommentWeightProcedure)
+
+# configure a worker to ingest all data streams
+streams_worker = DataJointWorker(
+    ""streams_worker"",
+    worker_schema_name=worker_schema_name,
+    db_prefix=db_prefix,
+    max_idled_cycle=50,
+    sleep_duration=60,
+    autoclear_error_patterns=[""%BlockAnalysis Not Ready%""],
+)
+
+for attr in vars(streams).values():
+    if is_djtable(attr, dj.user_tables.AutoPopulate):
+        streams_worker(attr, max_calls=10)
+
+streams_worker(qc.CameraQC, max_calls=10)
+streams_worker(tracking.SLEAPTracking, max_calls=10)
+
+# configure a worker to run the analysis tables
+analysis_worker = DataJointWorker(
+    ""analysis_worker"",
+    worker_schema_name=worker_schema_name,
+    db_prefix=db_prefix,
+    max_idled_cycle=6,
+    sleep_duration=1200,
+)
+
+analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockPlots, max_calls=6)
+
+
+def get_workflow_operation_overview():
+    from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
+    return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])","--- 

+++ 

@@ -64,7 +64,7 @@

 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
 # acquisition_worker(ingest_environment_visits)
-# acquisition_worker(block_analysis.BlockDetection)
+acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync
 pyrat_worker = DataJointWorker(
@@ -108,7 +108,8 @@

 
 analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
 analysis_worker(block_analysis.BlockPlots, max_calls=6)
-
+analysis_worker(block_analysis.BlockSubjectAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockSubjectPlots, max_calls=6)
 
 def get_workflow_operation_overview():
     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1588458387,114.0,115,ab51258d1ebcde261961e3dc84172026172f1894,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/populate/worker.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The function `get_workflow_operation_overview()` is imported and used within another function of the same name. This could lead to confusion and potential errors in the future. Consider renaming the outer function to avoid naming conflicts.
```diff
- def get_workflow_operation_overview():
+ def retrieve_workflow_operation_overview():
      from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
      return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
+    return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])","--- 

+++ 

@@ -108,7 +108,8 @@

 
 analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
 analysis_worker(block_analysis.BlockPlots, max_calls=6)
-
+analysis_worker(block_analysis.BlockSubjectAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockSubjectPlots, max_calls=6)
 
 def get_workflow_operation_overview():
     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1588458391,50.0,61,ab51258d1ebcde261961e3dc84172026172f1894,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/conftest.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `dj_config` fixture is loading a local configuration file without any error handling. If the file does not exist or is corrupted, the test suite will fail with an unhandled exception. It would be better to add some error handling around this operation.

```diff
-    dj_config_fp = pathlib.Path(""dj_local_conf.json"")
-    assert dj_config_fp.exists()
-    dj.config.load(dj_config_fp)
+    try:
+        dj_config_fp = pathlib.Path(""dj_local_conf.json"")
+        assert dj_config_fp.exists(), ""Local config file does not exist.""
+        dj.config.load(dj_config_fp)
+    except Exception as e:
+        raise Exception(f""Failed to load local config: {e}"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+@pytest.fixture(autouse=True, scope=""session"")
+def dj_config():
+    """"""If dj_local_config exists, load""""""
+    dj_config_fp = pathlib.Path(""dj_local_conf.json"")
+    assert dj_config_fp.exists()
+    dj.config.load(dj_config_fp)
+    dj.config[""safemode""] = False
+    assert ""custom"" in dj.config
+    dj.config[""custom""][
+        ""database.prefix""
+    ] = f""u_{dj.config['database.user']}_testsuite_""
+",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004474,18.0,23,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,app/models/dynamic_links/client.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The validations for `hostname` and `scheme` are a good addition to ensure data integrity. However, consider adding custom error messages for better user experience. For instance, when the `scheme` is not included in `VALID_SCHEMES`, it would be helpful to inform the user about the acceptable values.
```diff
-    validates :scheme, presence: true, inclusion: { in: VALID_SCHEMES }
+    validates :scheme, presence: true, inclusion: { in: VALID_SCHEMES, message: ""%{value} is not a valid scheme. Only 'http' and 'https' are allowed."" }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    VALID_SCHEMES = ['http', 'https'].freeze
+
     validates :name, presence: true, uniqueness: true
     validates :api_key, presence: true, uniqueness: true
+    validates :hostname, presence: true, uniqueness: true
+    validates :scheme, presence: true, inclusion: { in: VALID_SCHEMES }",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004477,29.0,32,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,test/dummy/db/schema.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Increasing the limit of the `short_url` column from 10 to 20 characters is a sensible change considering the addition of the scheme and hostname to the shortened URL. However, similar to the previous comment, if there are existing records in the `dynamic_links_shortened_urls` table, you'll need to ensure they're updated appropriately when this migration is run.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   create_table ""dynamic_links_shortened_urls"", force: :cascade do |t|
     t.bigint ""client_id""
     t.string ""url"", limit: 2083, null: false
-    t.string ""short_url"", limit: 10, null: false
+    t.string ""short_url"", limit: 20, null: false","--- 

+++ 

@@ -33,8 +33,8 @@

     t.datetime ""expires_at""
     t.datetime ""created_at"", null: false
     t.datetime ""updated_at"", null: false
+    t.index [""client_id"", ""short_url""], name: ""index_dynamic_links_shortened_urls_on_client_id_and_short_url"", unique: true
     t.index [""client_id""], name: ""index_dynamic_links_shortened_urls_on_client_id""
-    t.index [""short_url""], name: ""index_dynamic_links_shortened_urls_on_short_url"", unique: true
   end
 
   add_foreign_key ""dynamic_links_shortened_urls"", ""dynamic_links_clients"", column: ""client_id"""
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004480,40.0,43,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The shortened URL is being constructed manually here. While this might work as expected, it's generally a good practice to use built-in URL handling libraries to construct URLs. This can help avoid potential issues with special characters, encoding, etc. Consider using Ruby's URI library for this purpose.

```ruby
URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    short_url = strategy.shorten(url)
+
+    short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    ""#{client.scheme}://#{client.hostname}/#{short_url}""","--- 

+++ 

@@ -40,7 +40,7 @@

     short_url = strategy.shorten(url)
 
     short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    ""#{client.scheme}://#{client.hostname}/#{short_url}""
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
   end
 
   # mimic Firebase Dynamic Links API"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004482,4.0,5,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code does not validate the URL before shortening it. This could lead to invalid URLs being shortened and stored in the database, which could cause issues when trying to redirect to these URLs later on. I suggest adding back the URL validation logic.

```diff
  url = params.require(:url)
+ if !url_shortener.valid_url?(url)
+   render json: { error: 'invalid url' }, status: :bad_request
+   return
+ end
  client = DynamicLinks::Client.find_by!(api_key: params.require(:api_key))
  render json: DynamicLinks.generate_short_url(url, client), status: :created
```
Also, there is no error handling for the case where `find_by!` does not find a matching client. This will raise an `ActiveRecord::RecordNotFound` exception. It would be better to handle this gracefully and return a meaningful error message to the user.

```diff
- client = DynamicLinks::Client.find_by!(api_key: params.require(:api_key))
+ client = DynamicLinks::Client.find_by(api_key: params.require(:api_key))
+ unless client
+   render json: { error: 'invalid api key' }, status: :unauthorized
+   return
+ end
  render json: DynamicLinks.generate_short_url(url, client), status: :created
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    client = DynamicLinks::Client.find_by!(api_key: params.require(:api_key))
+    render json: DynamicLinks.generate_short_url(url, client), status: :created","--- 

+++ 

@@ -1,7 +1,12 @@

 class DynamicLinks::V1::ShortLinksController < ApplicationController
   def create
     url = params.require(:url)
-    client = DynamicLinks::Client.find_by!(api_key: params.require(:api_key))
+    client = DynamicLinks::Client.find_by(api_key: params.require(:api_key))
+    unless client
+      render json: { error: 'invalid api key' }, status: :unauthorized
+      return
+    end
+
     render json: DynamicLinks.generate_short_url(url, client), status: :created
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004483,25.0,30,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case seems to be assuming that the `shorten` method of the strategy mock will always return 'shortened_url'. However, this might not always be the case in a real-world scenario. It would be better to use a variable for the shortened URL and pass it to the `expect` method of the mock. This way, the test case will still pass even if the `shorten` method returns a different value.

```diff
-    expected_short_path = 'shortened_url'
+    expected_short_path = SecureRandom.hex(10)
     full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
+    strategy_mock.expect :shorten, expected_short_path, [anything]
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
+    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
 
     DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      assert_equal 'shortened_url', DynamicLinks.shorten_url('https://example.com')
+      assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439004484,41.0,50,288124505f44c8e95dd8ef75ba1efb29c3e3793e,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the test case is assuming that the `shorten_url` method will always return a specific URL. It would be better to use a variable for the shortened URL and pass it to the `stub` method. This way, the test case will still pass even if the `shorten_url` method returns a different value.

```diff
-    expected_short_path = 'shortened_url'
+    expected_short_path = SecureRandom.hex(10)
     full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
     expected_response = {
       shortLink: full_short_url,
       previewLink: ""#{full_short_url}?preview=true"",
       warning: []
     }
-    DynamicLinks.stub :shorten_url, full_short_url do
+    DynamicLinks.stub :shorten_url, full_short_url, [anything, anything] do
       assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
     end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
     expected_response = {
-      shortLink: 'shortened_url',
-      previewLink: 'shortened_url?preview=true',
+      shortLink: full_short_url,
+      previewLink: ""#{full_short_url}?preview=true"",
       warning: []
     }
 
-    DynamicLinks.stub :shorten_url, 'shortened_url' do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com')
+    DynamicLinks.stub :shorten_url, full_short_url do
+      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007054,,23,96c681ade6d5019a4f4adfa845bb68820100c5db,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change to scope the uniqueness of `short_url` to `client_id` is a good one. This allows different clients to have the same `short_url`, which can be useful in multi-tenant systems. However, it's important to ensure that the system correctly handles requests for the same `short_url` from different clients and directs them to the correct URLs.
```diff
-    validates :short_url, presence: true, uniqueness: true
+    validates :short_url, presence: true, uniqueness: { scope: :client_id }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
     validates :url, presence: true
-    validates :short_url, presence: true, uniqueness: true
+    validates :short_url, presence: true, uniqueness: { scope: :client_id }",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007056,22.0,26,96c681ade6d5019a4f4adfa845bb68820100c5db,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,test/dummy/db/schema.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `hostname` and `scheme` fields to the `dynamic_links_clients` table is a good move. However, it's important to note that the uniqueness constraint on the `hostname` field might not be ideal in all scenarios. For instance, if you have multiple clients using the same hostname but different schemes (http vs https), this would cause an issue. Consider adding a composite unique index on both `hostname` and `scheme` instead of just `hostname`.
```diff
- t.index [""hostname""], name: ""index_dynamic_links_clients_on_hostname"", unique: true
+ t.index [""hostname"", ""scheme""], name: ""index_dynamic_links_clients_on_hostname_and_scheme"", unique: true
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    t.string ""hostname"", null: false
+    t.string ""scheme"", default: ""https"", null: false
     t.index [""api_key""], name: ""index_dynamic_links_clients_on_api_key"", unique: true
+    t.index [""hostname""], name: ""index_dynamic_links_clients_on_hostname"", unique: true
     t.index [""name""], name: ""index_dynamic_links_clients_on_name"", unique: true",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007355,,26,5becd41a867903e4cc04f4c0ae7785b878dfadd8,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The method signature for `shorten_url` has been changed to include a `client` parameter. This is a breaking change if there are other parts of the codebase that call this method with the old signature. If backward compatibility is a concern, consider providing a default value for the new parameter or creating a separate method.

```diff
- def self.shorten_url(url)
+ def self.shorten_url(url, client = nil)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   end
 
-  def self.shorten_url(url)
+  def self.shorten_url(url, client)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007356,40.0,43,5becd41a867903e4cc04f4c0ae7785b878dfadd8,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The creation of the `ShortenedUrl` record and the building of the final URL are done in one line. For better readability and error handling, consider separating these into two steps. Also, it's important to handle potential exceptions that could be raised when creating the `ShortenedUrl` record.

```ruby
short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
final_url = URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
final_url
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    short_url = strategy.shorten(url)
+
+    short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007357,47.0,48,5becd41a867903e4cc04f4c0ae7785b878dfadd8,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the `shorten_url` method, the `generate_short_url` method now requires a `client` parameter. Ensure that all calls to this method are updated to pass this parameter. If backward compatibility is a concern, consider providing a default value for the new parameter or creating a separate method.

```diff
- def self.generate_short_url(original_url)
+ def self.generate_short_url(original_url, client = nil)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def self.generate_short_url(original_url, client)
+    short_link = shorten_url(original_url, client)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1439007358,4.0,8,5becd41a867903e4cc04f4c0ae7785b878dfadd8,af53df6da4a5bfbcb47d54bce5d5a780c8e0ab6b,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code is missing error handling for the case when `DynamicLinks::Client.find_by(api_key: params.require(:api_key))` returns `nil`. This could happen if the provided API key does not exist in the database. In such a case, the server would return a 500 Internal Server Error because `client` would be `nil` and we're trying to use it later in the code. To fix this, you should add a check after line 4 to see if `client` is `nil`, and if so, return an error response.

```diff
    client = DynamicLinks::Client.find_by(api_key: params.require(:api_key))
+    unless client
+      render json: { error: 'invalid api key' }, status: :unauthorized
+      return
+    end
```

10:
This line assumes that `DynamicLinks.generate_short_url(url, client)` will always succeed and return a valid result. However, there might be cases where it fails due to various reasons (e.g., invalid URL, database issues). It would be better to handle potential errors from `generate_short_url` method and respond accordingly.

```diff
-    render json: DynamicLinks.generate_short_url(url, client), status: :created
+    short_url = DynamicLinks.generate_short_url(url, client)
+    if short_url
+      render json: short_url, status: :created
+    else
+      render json: { error: 'failed to generate short url' }, status: :internal_server_error
+    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    client = DynamicLinks::Client.find_by(api_key: params.require(:api_key))
+    unless client
+      render json: { error: 'invalid api key' }, status: :unauthorized
+      return
+    end",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705466144,,65,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryDetail/SalaryDetailPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from using a `Heading` component to a simple `h2` tag might affect the consistency of headings across your application. If the `Heading` component contains specific styles or behaviors that are common across your app, it would be better to keep using it and modify its internal implementation if necessary.
```diff
- <h2>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú</h2>
+ <Heading title=""Í∏âÏó¨Î™ÖÏÑ∏ÏÑú"" />
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","             <Styled.LSection>
               <IconBtn icontype=""close"" onClick={handleCloseButton} />
-                <Heading title=""Í∏âÏó¨Î™ÖÏÑ∏ÏÑú"" />
+                <h2>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú</h2>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705466150,,16,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the text from ""Í∏âÏó¨ Î™ÖÏÑ∏ÏÑú"" to ""Í∏âÏó¨Î™ÖÏÑ∏ÏÑú"" seems to be a stylistic choice. However, it's important to ensure that this change is consistent across the entire application to maintain uniformity and avoid confusion for the users.
```diff
- <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨ Î™ÖÏÑ∏ÏÑú
+ <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     <Styled.SalaryCardBox>
       <h2>
-        <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨ Î™ÖÏÑ∏ÏÑú
+        <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507868,16.0,17,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in the text from ""Í∏âÏó¨ Î™ÖÏÑ∏ÏÑú"" to ""Í∏âÏó¨Î™ÖÏÑ∏ÏÑú"" seems to be a stylistic choice. However, it's important to ensure that this change is consistent across the entire application to maintain uniformity and avoid confusion for the users.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
       </h2>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507875,18.0,20,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of an emoji (‚úâÔ∏è) using a class name 'imoge' doesn't seem to follow any known naming conventions. It would be better to use a more descriptive class name that indicates its purpose or appearance. Also, consider using a library like `react-icons` for better accessibility and consistency instead of hardcoding emojis.
```diff
- <h6 className='imoge'>‚úâÔ∏è</h6>
+ <h6 className='mail-icon'><MailIcon /></h6>
```
In the above snippet, `MailIcon` is a component imported from a library like `react-icons`. This will make your code more readable and maintainable.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      <h6 className='imoge'>‚úâÔ∏è</h6>
       <h3>
-        <p>Ï†ïÏ†ï Ïã†Ï≤≠ Í∏∞Í∞ÑÏûÖÎãàÎã§.</p>
+        <p>Ï†ïÏ†ï Ïã†Ï≤≠ Í∏∞Í∞ÑÏûÖÎãàÎã§.</p> ",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507879,,19,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryDetail/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change made here is to wrap the `realpay` value in a styled component `Styled.LgFont`. This seems to be for the purpose of applying specific styles to the salary amount. However, it's important to ensure that this new style doesn't conflict with other styles in your application and that it improves readability.
```diff
- <div className=""pay""><h3>{realpay}Ïõê</h3></div>
+ <div className=""pay""><h3><Styled.LgFont>{realpay}</Styled.LgFont> Ïõê</h3></div>
```
Also, note that there is now a space before 'Ïõê'. This is a minor change but can improve readability by separating the number from the unit.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","       <Styled.ItemWrapper>
         <div className=""pay""><h5>Ïã§ÏàòÎ†πÏï°</h5></div>
-          <div className=""pay""><h3>{realpay}Ïõê</h3></div>
+          <div className=""pay""><h3><Styled.LgFont>{realpay}</Styled.LgFont> Ïõê</h3></div>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507881,,17,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryDetail/SalaryDetail.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The height of the `Listline` component has been reduced. This is a visual change and should not affect the functionality of the code. However, it's important to ensure that this change doesn't negatively impact the readability or user experience of the application.
```diff
-  height:0.1rem;
+  height:0.05rem;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," export const Listline = styled.div`
   background-color:var(--border-pri);
-  height:0.1rem;
+  height:0.05rem;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507882,37.0,54,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryDetail/SalaryDetail.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ListWrapper` component now accepts an additional optional prop `content`. This prop is used to conditionally apply different color styles to the `.price` class. The padding has also been increased slightly. These changes are primarily stylistic and should not affect the functionality of the code. However, it's crucial to verify that these changes align with the overall design language of the application and do not degrade the user experience.
```diff
- export const ListWrapper = styled.div<{ type?:string }>`
-   padding:1.4rem 1rem;
+ export const ListWrapper = styled.div<{ type?:string, content?:string}>`
+   padding:2rem 1rem;
+   .price{
+     color: ${props => 
+       props.content === 'pension' ? '#039C00' :
+       props.content === 'deduction' ? '#CC4846' :
+       props.content === 'pay' ? 'var(--color-pri)' : 
+       'var(--color-black)'}
+   }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const ListWrapper = styled.div<{ type?:string, content?:string}>`
   display:flex;
   justify-content: space-between;
   gap: 1rem;
-  padding:1.4rem 1rem;
+  padding:2rem 1rem;
+  box-sizing:border-box;
   
   div{
     font-size: ${props => props.type === 'main' ? '1.4rem' : '1.2rem'};
     font-weight: ${props => props.type === 'main' ? 'bold' : 'normal'};
   }
+  .price{
+    color: ${props => 
+      props.content === 'pension' ? '#039C00' :
+      props.content === 'deduction' ? '#CC4846' :
+      props.content === 'pay' ? 'var(--color-pri)' : 
+      'var(--color-black)'}
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507887,,8,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/api/fetchSalaryInfo.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of the optional `content` field in the `SalaryDetailItem` interface seems fine. However, it would be beneficial to add a comment explaining what this new field is for, as it's not immediately clear from the context.
```diff
+  // content: additional information or details related to the salary item
   content?: string;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   label: string;
   value: string;
   type?: 'main' | 'sub';
+  content?:string;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507891,32.0,38,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/SalaryList.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the `ListCardBox` component are mostly stylistic, adjusting padding, border-radius, and height. However, the removal of `max-height` and replacement with `height` could potentially cause issues if the content exceeds the specified height, as it will not expand to accommodate extra content. Consider reverting this change or providing a solution for overflow content.
```diff
-  max-height:2.3rem;
+  height:2.5rem;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  background-color:var(--color-white);
   color:var(--color-black);
-  padding: 1.5rem;
-  border-radius:0.5rem;
-  margin: 1rem auto;
+  padding: 1.5rem 2rem 2rem 2rem;
+  border-radius:1rem;
+  margin: 1.5rem auto;
   display:flex;
-  max-height:2.3rem;
+  height:2.5rem;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507894,53.0,64,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/SalaryList.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes in the `List` component are also primarily stylistic, adjusting font sizes, line heights, and colors. The addition of conditional styling based on `$state` is a good way to dynamically adjust styles. However, there seems to be a typo in the `font-weight` property value for `.title`. It should be `var(--font-weight-semibold)` instead of 'var(font-weight-semibold)'. Please correct this.
```diff
- font-weight: ${({$state}) => $state === true? 'var(--font-weight-bold)' : 'var(font-weight-semibold)'};
+ font-weight: ${({$state}) => $state === true? 'var(--font-weight-bold)' : 'var(--font-weight-semibold)'};
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const List = styled.div<{$state:boolean}>`
   display: flex;
   flex-direction: column;
   justify-content: center;
   & > .title {
-    font-weight: var(--font-weight-semibold);
-    font-size: 1.2rem;
+    font-weight: ${({$state}) => $state === true? 'var(--font-weight-bold)' : 'var(font-weight-semibold)'};
+    line-height: 3.2rem;
+    font-size: 1.5rem;
   }
   & > .date {
-    font-size: 10px;
-    color: var(--color-gray);
+    font-size: 13px;
+    color: ${({$state}) => $state === true? 'var(--font-pri)' : 'var(--font-sec)'};",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507899,5.0,7,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The padding and background color have been changed. Ensure that these changes do not affect the readability and visual balance of the content within the `SalaryCardBox`. Also, consider using relative units (like em or %) for padding to maintain responsiveness across different screen sizes.
```diff
-  background-color:var(--color-white);
-  padding: 1.2rem 4rem 2rem 4rem;
+  background-color:var(--new-color);
+  padding: 1.2em 4em 2em 4em;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  background-color:var(--color-white);
+  padding: 1.2rem 4rem 2rem 4rem;
   margin: 1rem;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507902,8.0,9,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The border-radius and box-shadow have been modified. Make sure these changes align with your overall design language and do not create inconsistency in UI elements across the application.
```diff
-  border-radius:2rem;
-  box-shadow: 0px 1px 0px 0.2px var(--border-pri);
+  border-radius:2em;
+  box-shadow: 0px 1px 0px 0.2px var(--new-border-pri);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  border-radius:2rem;
+  box-shadow: 0px 1px 0px 0.2px var(--border-pri);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507903,14.0,16,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The font-size and text alignment for the `.imoge` class have been changed. Ensure that the new font size does not make the text too large or small for the user to read comfortably. Also, verify that the change in text alignment does not disrupt the layout or flow of the content.
```diff
-  font-size:60px;
-  text-align:end;
+  font-size:3em;
+  text-align:right;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    font-size:60px;
+    text-align:end;
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507904,19.0,21,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The line-height and font-weight for `h2` have been adjusted. Check that these changes do not negatively impact the readability of the heading and that they are consistent with other headings throughout the application.
```diff
-  line-height:4rem;
-  font-weight:var(--font-weight-semi);
+  line-height:2em;
+  font-weight:var(--new-font-weight-semi);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    line-height:4rem;
+    font-weight:var(--font-weight-semi);
   }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507921,22.0,23,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/NoticeCard.style.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The font-weight for `h3` has been changed. Ensure that this change does not make the text too heavy or light compared to other text on the page, and that it is consistent with other `h3` headings throughout the application.
```diff
-  font-weight:var(--font-weight-semi);
+  font-weight:var(--new-font-weight-semi);
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   h3{
-    font-weight:var(--font-weight-semibold);
+    font-weight:var(--font-weight-semi);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507926,,30,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The date format has been changed from 'YYYYÎÖÑ MMÏõî' to 'MMÏõî'. This change removes the year from the final date. If this is intentional and aligns with the requirements, then it's fine. However, if the year is still required in the date, you should revert this change.
```diff
- const finalDate = originDate.format('MMÏõî ')
+ const finalDate = originDate.format('YYYYÎÖÑ MMÏõî ')
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   const firstPayData = salaryList[0]
   const originDate = dayjs(firstPayData.payday,'YYYY.MM.DD')
-  const finalDate = originDate.format('YYYYÎÖÑ MMÏõî ')
+  const finalDate = originDate.format('MMÏõî ')",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507934,,43,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The heading title has been changed from 'Í∏âÏó¨ÎÇ¥Ïó≠' to 'Í∏âÏó¨Ï†ïÏÇ∞'. Ensure that this change aligns with the intended functionality and user expectations.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     <Styled.Salary>
-      <Heading title=""Í∏âÏó¨ÎÇ¥Ïó≠""/>
-      <Styled.Grayline/>
+      <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705507938,56.0,64,624b0cbf4dd20c79243905f62daa7d9dee765fc5,a438afeb74fd9c0d53e19afd1694ca27a548bb37,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes here are primarily related to styling and visual presentation. The button type has been changed from default to outlined, and a new `round` prop has been added to the `Btn` component. These changes seem to be in line with the PR description and should improve the visual consistency of the application. However, ensure that these changes do not affect the accessibility and usability of the buttons.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            <Styled.List $state={el.state}>
             <span className=""title"">{el.title}</span>
             <span className=""date"">{el.state === true ? 'ÏßÄÍ∏âÏòàÏ†ï' : el.payday}</span>
             </Styled.List>
             <Styled.Btn>
               {el.state === true ? 
-              <Btn round ='true' label='Ïã†Ï≤≠Í∞ÄÎä•'/> 
+              <Btn round ='true' btntype='outlined' size='lg' label='Ïã†Ï≤≠Í∞ÄÎä•'/> 
               : 
-              <Btn round='true' disabled label='ÏßÄÍ∏âÏôÑÎ£å'/> 
+              <Btn round='true' disabled size='lg' label='ÏßÄÍ∏âÏôÑÎ£å'/> ",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440537585,4.0,5,b11b4e4b2882131a040e8534bc270f2ab7071b6f,d01c202918443fe9aa71387dee5215e4c44c1754,app/controllers/dynamic_links/redirects_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_by` method can return `nil` if no record is found, which is fine in this case. However, it's a good practice to handle potential ActiveRecord::RecordNotFound exceptions that could be raised when using methods like `find`. In this case, you might want to consider using a rescue block to handle such exceptions and render the 404 page.

```diff
-      short_url = params[:short_url]
-      link = ShortenedUrl.find_by(short_url: short_url)
+      begin
+        short_url = params[:short_url]
+        link = ShortenedUrl.find_by!(short_url: short_url)
+      rescue ActiveRecord::RecordNotFound
+        render_not_found and return
+      end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      short_url = params[:short_url]
+      link = ShortenedUrl.find_by(short_url: short_url)","--- 

+++ 

@@ -5,17 +5,10 @@

       link = ShortenedUrl.find_by(short_url: short_url)
 
       if link
-        redirect_to link.url, status: :found
+        redirect_to link.url, status: :found, allow_other_host: true
       else
-        render_not_found
+        raise ActiveRecord::RecordNotFound
       end
-    end
-
-    private
-
-    def render_not_found
-      # Render a 404 page or similar
-      render file: 'public/404.html', status: :not_found, layout: false
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440537594,,8,b11b4e4b2882131a040e8534bc270f2ab7071b6f,d01c202918443fe9aa71387dee5215e4c44c1754,app/controllers/dynamic_links/redirects_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's great that you're using HTTP status codes correctly. The `:found` status code (302) is appropriate for redirections.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      link = ShortenedUrl.find_by(short_url: short_url)
+
+      if link
+        redirect_to link.url, status: :found","--- 

+++ 

@@ -5,17 +5,10 @@

       link = ShortenedUrl.find_by(short_url: short_url)
 
       if link
-        redirect_to link.url, status: :found
+        redirect_to link.url, status: :found, allow_other_host: true
       else
-        render_not_found
+        raise ActiveRecord::RecordNotFound
       end
-    end
-
-    private
-
-    def render_not_found
-      # Render a 404 page or similar
-      render file: 'public/404.html', status: :not_found, layout: false
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1440537598,16.0,18,b11b4e4b2882131a040e8534bc270f2ab7071b6f,d01c202918443fe9aa71387dee5215e4c44c1754,app/controllers/dynamic_links/redirects_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Good job on handling the case where the shortened URL does not exist. Rendering a custom 404 page is a user-friendly way to handle such situations.

Overall, the code looks good and follows Rails conventions. It's simple, easy to understand, and does its job well.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def render_not_found
+      # Render a 404 page or similar
+      render file: 'public/404.html', status: :not_found, layout: false","--- 

+++ 

@@ -5,17 +5,10 @@

       link = ShortenedUrl.find_by(short_url: short_url)
 
       if link
-        redirect_to link.url, status: :found
+        redirect_to link.url, status: :found, allow_other_host: true
       else
-        render_not_found
+        raise ActiveRecord::RecordNotFound
       end
-    end
-
-    private
-
-    def render_not_found
-      # Render a 404 page or similar
-      render file: 'public/404.html', status: :not_found, layout: false
     end
   end
 end"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705653857,38.0,41,dc011e6595de531ce106af15c1af2d15f595d746,dd7016cfdfca53b12991ae6edbe8d310fbe30ffd,src/pages/salaryDetail/SalaryDetailPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `handleCloseButton` function is missing error handling. If the navigation fails for any reason, it would be good to have a catch block to handle the error.
```diff
   const handleCloseButton = () => {
     navigate('/payments')
+       .catch(error => {
+         console.error(""Navigation failed:"", error);
+       });
   };
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  
   const handleCloseButton = () => {
     navigate('/payments')
   };",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705653860,1.0,42,dc011e6595de531ce106af15c1af2d15f595d746,dd7016cfdfca53b12991ae6edbe8d310fbe30ffd,src/pages/salaryDetail/DetailMonthModal.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `SelectedModal` component seems to be well implemented. It uses the `BasicDialog` component to display a modal and provides options to either cancel or confirm the adjustment. The use of `dayjs` for date manipulation is also a good choice. However, there's a potential issue with the way you're handling the condition in line 17. If the current month is before the payday, you're returning null which means the component will not render anything. This might lead to unexpected behavior if this component is expected to render something else in this case. If this is the intended behavior, then it's fine. Otherwise, you might want to handle this scenario differently.

Also, please ensure that the `handleClose` function properly handles the state and any necessary cleanup when the modal is closed.

Lastly, remember to add a newline at the end of the file to adhere to standard coding conventions.
```diff
  )
}
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import BasicDialog from ""../../components/modal/BasicModal""
+import Btn from ""../../components/button/Button""
+import { CloseButton } from ""../../components/modal/CloseButton""
+import { useBasicModal } from ""../../components/modal/useBasicModal""
+import Heading from ""../../components/Heading/Heading""
+import FormWrap from ""../salaryAdjustment/FormWrap""
+import * as styled from '../salaryAdjustment/SalaryAdjustment.style';
+import dayjs from ""dayjs"";
+
+export default function SelectedModal({month}:{month:string}){
+  const { open, handleOpen, handleClose } = useBasicModal();
+
+  const today = dayjs();
+  const payday = dayjs().month(parseInt(month, 10) - 1);
+  const isBeforeCurrentMonth = payday.isBefore(today, 'month');
+
+  if(isBeforeCurrentMonth){
+    return null
+  }
+  
+  return(
+    <BasicDialog
+      modalOpenButton={
+        <Btn label=""Ï†ïÏ†ïÏã†Ï≤≠"" btnsize=""sm"" onClick={handleOpen} sx={{ fontSize: '1.3rem' }} />
+        }
+      modalCloseButton={<CloseButton handleClose={handleClose} />}
+      open={open}
+      >
+      <Heading title=""Í∏âÏó¨ Ï†ïÏ†ïÏã†Ï≤≠"" />
+      <styled.ModalTitle>{month}Ïõî Í∏âÏó¨ Ï†ïÏ†ï</styled.ModalTitle>
+      <FormWrap />
+      <Btn
+        label=""Ï∑®ÏÜå""
+        btnsize=""sm""
+        onClick={handleClose}
+        sx={{ fontSize: '1.5rem', mr: '1rem' }}
+        btntype=""outlined""
+      />
+      <Btn label=""ÌôïÏù∏"" btnsize=""md"" onClick={handleClose} sx={{ fontSize: '1.5rem' }} />
+  </BasicDialog>
+  )
+}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706980859,24.0,29,4651900fb23d481a7fdc3e747569c951a1777998,8c78b4e263d7c8e376660270c5d5c85878fecf95,src/components/modal/BasicModal.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes look good and should make the modal responsive. However, it's important to consider that `useMediaQuery` runs on the client side and can cause server-side and client-side HTML mismatches during initial page loads when using server-side rendering (SSR). This can lead to flickering effects. To avoid this, Material-UI provides a `ServerStyleSheets` utility for server-side media queries. If your application uses SSR, consider implementing this utility.
```diff
-  const fullScreen = useMediaQuery(theme.breakpoints.down('md'));
+  const fullScreen = typeof window !== 'undefined' ? useMediaQuery(theme.breakpoints.down('md')) : false;
```
This change ensures that `useMediaQuery` is only used on the client side, preventing potential mismatches between server-side and client-side rendering.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const theme = useTheme();
+  const fullScreen = useMediaQuery(theme.breakpoints.down('md'));
   return (
     <div>
       {modalOpenButton}
-      <Dialog {...props}>
+      <Dialog fullScreen={fullScreen} {...props}>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1704834806,1.0,30,e034a816d8aaef664d9f5daef5c57bf7aaa2e4a5,8c07d0bdc3ab2063f4504a297804d6d36aaa373f,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The GitHub Actions workflow file looks good overall. However, there are a few points to consider:

1. The `pull_request_review_comment` event might not be necessary for this workflow. This event is triggered when a comment is added on a pull request review. If the purpose of this workflow is to perform an AI code review when a pull request is opened or updated, then this event may not be needed.

2. The concurrency group name seems overly complex. It's generally recommended to keep it simple and meaningful. You could just use `${{ github.workflow }}-${{ github.run_id }}` or `${{ github.workflow }}-${{ github.sha }}`.

3. The `cancel-in-progress` option is set to cancel any in-progress job if a new one starts and the event is not 'pull_request_review_comment'. This means that if a new commit is pushed to the PR while the action is running, the current run will be cancelled. This can be efficient, but make sure it aligns with your intended workflow.

4. The `GITHUB_TOKEN` and `OPENAI_API_KEY` are being passed as environment variables. Make sure these secrets are properly set up in your repository settings.

5. The options `debug`, `review_simple_changes`, and `review_comment_lgtm` are all set to false. Ensure these settings align with your desired behavior for the action.

Here's a simplified version of the concurrency group and removing the `pull_request_review_comment` event:

```diff
on:
  pull_request:
-9:   pull_request_review_comment:
-10:     types: [created]
 
concurrency:
  group:
-14:     ${{ github.repository }}-${{ github.event.number || github.head_ref ||
-15:     github.sha }}-${{ github.workflow }}-${{ github.event_name ==
-16:     'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+14:     ${{ github.workflow }}-${{ github.sha }}
  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+name: Code Review
+
+permissions:
+  contents: read
+  pull-requests: write
+
+on:
+  pull_request:
+  pull_request_review_comment:
+    types: [created]
+
+concurrency:
+  group:
+    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
+
+jobs:
+  review:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346133,37.0,44,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/components/datepicker/DatePicker.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `minDate` prop is set to `startDate || undefined`. This means that if `startDate` is not set, `minDate` will be `undefined`, which might cause issues if the DatePicker component does not handle `undefined` values properly. It would be better to provide a default value for `minDate` in case `startDate` is not set.
```diff
- minDate={startDate || undefined}
+ minDate={startDate || new Date()}
```
This change ensures that `minDate` always has a valid date value.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","             <Separator>-</Separator>
             <DatePicker
               slotProps={DTPstyle}
               value={endDate}
               onChange={(newvalue) => setEndDate(newvalue)}
+              name=""eDate""
               {...props}
+              minDate={startDate || undefined} // ÏãúÏûë ÎÇ†ÏßúÍ∞Ä ÏÑ§Ï†ïÎêòÎ©¥ minDateÎ•º ÏÑ§Ï†ï",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346138,4.0,5,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/pages/salaryAdjustment/Cate1.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `minDate` and `maxDate` are hardcoded to specific dates (July 1st and July 31st of the current year). This might not be flexible enough if the date range needs to change in the future. Consider making these values configurable through props or environment variables.

```diff
-  const minDate = dayjs(new Date(new Date().getFullYear(), 6, 1));
-  const maxDate = dayjs(new Date(new Date().getFullYear(), 7, 0));
+  const minDate = dayjs(process.env.MIN_DATE || new Date(new Date().getFullYear(), 6, 1));
+  const maxDate = dayjs(process.env.MAX_DATE || new Date(new Date().getFullYear(), 7, 0));
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const minDate = dayjs(new Date(new Date().getFullYear(), 6, 1));
+  const maxDate = dayjs(new Date(new Date().getFullYear(), 7, 0));",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346139,9.0,10,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/pages/salaryAdjustment/Cate2.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `minDate` and `maxDate` are hardcoded to specific months (July and August) of the current year. If this is intended, it's fine. However, if these dates should be dynamic based on some other conditions, you might need to revise this logic.
```diff
-  const minDate = dayjs(new Date(new Date().getFullYear(), 6, 1));
-  const maxDate = dayjs(new Date(new Date().getFullYear(), 7, 0));
+  // Update these lines with your dynamic date calculation logic
+  const minDate = dayjs(/* Your dynamic start date here */);
+  const maxDate = dayjs(/* Your dynamic end date here */);
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const minDate = dayjs(new Date(new Date().getFullYear(), 6, 1));
+  const maxDate = dayjs(new Date(new Date().getFullYear(), 7, 0));",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346140,64.0,66,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/pages/salaryAdjustment/Cate2.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There seems to be a discrepancy in the `maxTime` value between the new and old code. The old code sets the maximum time to 8 hours, while the new code sets it to 6 hours. Please ensure that this change is intentional and correct.
```diff
-  maxTime={dayjs().hour(6).minute(0).second(0)}
+  maxTime={dayjs().hour(8).minute(0).second(0)}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+            maxTime={dayjs().hour(6).minute(0).second(0)}
+            ampm={false}
+            name=""eTime""",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346141,,23,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `.reverse()` method is used to reverse the order of the data array. This might lead to unexpected behavior if the order of the data is important elsewhere in your application. If you want to display the most recent data first, consider sorting the data at the component level where it's displayed instead of mutating the original data.
```diff
- state.data = action.payload.reverse();
+ state.data = action.payload;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","       .addCase(fetchSalaryAdData.fulfilled, (state, action) => {
         state.status = 'succeeded';
-        state.data = action.payload;
+        state.data = action.payload.reverse();",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346142,,34,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, using `.reverse()` here could lead to unexpected behavior. Consider handling the ordering at the component level.
```diff
- state.data = action.payload.reverse();
+ state.data = action.payload;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","         state.status = 'succeeded';
-        console.log(action.payload);
-        state.data = action.payload;
+        state.data = action.payload.reverse();",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346145,,45,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Again, avoid reversing the data array in the reducer. Handle this at the component level.
```diff
- state.data = action.payload.reverse();
+ state.data = action.payload;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      })
+      .addCase(addSalaryAdData.fulfilled, (state, action) => {
+        state.status = 'succeeded';
+        state.data = action.payload.reverse();",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346146,82.0,96,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In this async thunk, you're directly manipulating the `id` field of the incoming `newState` object (line 87). It's generally a bad practice to mutate function arguments as it can lead to unexpected side effects. Instead, create a new object that includes all properties of `newState` and has the updated `id`.
```diff
- newState.id = String(state.length);
- const updated = [...state, newState];
+ const updatedState = { ...newState, id: String(state.length) };
+ const updated = [...state, updatedState];
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const addSalaryAdData = createAsyncThunk(
+  'salaryAd/addSalaryAdData',
+  async (newState: Data) => {
+    const snapshot = await get(ref(firebaseDB, PATH));
+    const state = snapshot.val() as Data[];
+    newState.id = String(state.length);
+    const updated = [...state, newState];
+    try {
+      await set(dbRef, updated);
+      return updated;
+    } catch (error) {
+      throw new Error(error instanceof Error ? error.message : 'update Ïã§Ìå®');
+    }
+  }
+);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706346148,22.0,92,68fbb028c74c18d6862e14f65f8f449031bace07,be4d55daa69e5c254e3e2f7cc14156198dd7c148,src/pages/salaryAdjustment/FormWrap.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The form validation logic in this function is using alerts to notify the user of any errors. While this works, it's not the best user experience. Instead, consider using a more user-friendly way to display these error messages, such as inline error messages next to the form fields. This would require setting an error state for each field and updating it based on the validation results.

```diff
-    if (!category) {
-      alert('Ïπ¥ÌÖåÍ≥†Î¶¨Í∞Ä ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.');
-      return;
-    }
+    if (!category) {
+      setCategoryError('Ïπ¥ÌÖåÍ≥†Î¶¨Í∞Ä ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.');
+      return;
+    } else {
+      setCategoryError('');
+    }

-    if (!sDate) {
-      alert('ÎÇ†ÏßúÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
-      return;
-    }
+    if (!sDate) {
+      setDateError('ÎÇ†ÏßúÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
+      return;
+    } else {
+      setDateError('');
+    }
```

Also, you are directly alerting the error message from the catch block which might expose sensitive information to the end user. Consider logging the error and showing a generic error message to the user.

```diff
- .catch((error) => {
-   alert(`ÏòàÍ∏∞Ïπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. \n ${error}`);
- });
+ .catch((error) => {
+   console.error(error);
+   alert('An unexpected error occurred. Please try again later.');
+ });
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+function FormWrap({ handleClose }: FormWrapProps) {
+  const dispatch = useDispatch<AppDispatch>();
+  const [selected, setselected] = useState<SelectedType>('Ï£ºÎßê / Í≥µÌú¥Ïùº Í∑ºÎ¨¥ ÏàòÎãπ');
   const handleChange = (event: SelectChangeEvent<string | number>) => {
     setselected(event.target.value as SelectedType);
   };
+  const onSubmit = (e: React.FormEvent<HTMLFormElement>) => {
+    e.preventDefault();
+
+    const formData = new FormData(e.currentTarget);
+    const category = formData.get('category') as Category;
+    const sDate = formData.get('sDate') as string;
+    const description = (formData.get('description') as string) || '';
+    let startTime = '';
+    let endTime = '';
+
+    if (!category) {
+      alert('Ïπ¥ÌÖåÍ≥†Î¶¨Í∞Ä ÏÑ†ÌÉùÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.');
+      return;
+    }
+
+    if (!sDate) {
+      alert('ÎÇ†ÏßúÎ•º ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
+      return;
+    }
+
+    if (category === 'ÏïºÍ∞Ñ Í∑ºÎ¨¥ ÏàòÎãπ(22:00-06:00)') {
+      const sTime = formData.get('sTime') as string;
+      const eTime = formData.get('eTime') as string;
+      if (!sTime) {
+        alert('ÏãúÏûëÏãúÍ∞ÑÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
+        return;
+      }
+      if (!eTime) {
+        alert('Ï¢ÖÎ£åÏãúÍ∞ÑÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
+        return;
+      }
+      startTime = `${sDate} ${sTime}`;
+      endTime = `${sDate} ${eTime}`;
+    } else {
+      const eDate = formData.get('eDate') as string;
+      if (!eDate) {
+        alert('Ï¢ÖÎ£åÏùºÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî.');
+        return;
+      }
+      startTime = `${sDate} 09:00`;
+      endTime = `${eDate} 18:00`;
+    }
+
+    const submitData: Data = {
+      id: '',
+      category,
+      description,
+      startTime,
+      endTime,
+      status: 'Í≤∞Ïû¨ÎåÄÍ∏∞',
+      month: MONTH,
+      requestTime: getCurrentDate(),
+    };
+
+    dispatch(addSalaryAdData(submitData))
+      .unwrap()
+      .then((result) => {
+        if (result) {
+          alert('Ïã†Ï≤≠Ïù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.');
+          handleClose();
+        }
+      })
+      .catch((error) => {
+        alert(`ÏòàÍ∏∞Ïπò Î™ªÌïú Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§. \n ${error}`);
+      });",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705214780,5.0,9,300bc5fcbca1214168f7e20a889b33df5b4eaf9a,94ca6d39cf27d46d4b91f92e9c69c547f1aca150,src/vite-env.d.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the `ImportMetaEnv` interface are correct and align with the updates in the Firebase configuration. The addition of `VITE_FIREBASE_DATABASE_URL` and removal of `VITE_FIREBASE_MEASUREMENT_ID` is consistent with the changes in `src/api/firebaseConfig.ts`. However, there's a minor issue: the file should end with a newline character to meet the POSIX standard and ensure compatibility across different systems and tools.
```diff
  interface ImportMeta {
    readonly env: ImportMetaEnv;
  }
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  readonly VITE_FIREBASE_DATABASE_URL: string;
   readonly VITE_FIREBASE_PROJECT_ID: string;
   readonly VITE_FIREBASE_STORAGE_BUCKET: string;
   readonly VITE_FIREBASE_MESSAGING_SENDER_ID: string;
   readonly VITE_FIREBASE_APP_ID: string;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705214784,4.0,9,300bc5fcbca1214168f7e20a889b33df5b4eaf9a,94ca6d39cf27d46d4b91f92e9c69c547f1aca150,src/api/firebaseConfig.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `measurementId` property has been removed from the Firebase configuration object. If you're using Google Analytics, this ID is necessary for it to function properly. Please ensure that removing this won't affect any analytics functionality in your application.
```diff
 const firebaseConfig = {
   apiKey: import.meta.env.VITE_FIREBASE_API_KEY,
   authDomain: import.meta.env.VITE_FIREBASE_AUTH_DOMAIN,
   databaseURL: import.meta.env.VITE_FIREBASE_DATABASE_URL,
   projectId: import.meta.env.VITE_FIREBASE_PROJECT_ID,
   storageBucket: import.meta.env.VITE_FIREBASE_STORAGE_BUCKET,
   messagingSenderId: import.meta.env.VITE_FIREBASE_MESSAGING_SENDER_ID,
   appId: import.meta.env.VITE_FIREBASE_APP_ID,
+  measurementId: import.meta.env.VITE_FIREBASE_MEASUREMENT_ID,
 };
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  databaseURL: import.meta.env.VITE_FIREBASE_DATABASE_URL,
   projectId: import.meta.env.VITE_FIREBASE_PROJECT_ID,
   storageBucket: import.meta.env.VITE_FIREBASE_STORAGE_BUCKET,
   messagingSenderId: import.meta.env.VITE_FIREBASE_MESSAGING_SENDER_ID,
   appId: import.meta.env.VITE_FIREBASE_APP_ID,
-  databaseURL:import.meta.env.VITE_FIREBASE_DATABASE_URL
+  measurementId: import.meta.env.VITE_FIREBASE_MEASUREMENT_ID,",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705214788,,11,300bc5fcbca1214168f7e20a889b33df5b4eaf9a,94ca6d39cf27d46d4b91f92e9c69c547f1aca150,src/api/firebaseConfig.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's a good practice to end files with a newline character. This can prevent potential issues with version control systems and other tools that expect or require text files to end with a newline.
```diff
 export { firebaseConfig };
+
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->","-  databaseURL:import.meta.env.VITE_FIREBASE_DATABASE_URL
+  measurementId: import.meta.env.VITE_FIREBASE_MEASUREMENT_ID,
 };
 ",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706969833,5.0,7,c247479f1d9c92a4f41287485af590575cc1219a,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryDetail/DetailMonthModal.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements have been rearranged. While this doesn't affect the functionality, it's generally a good practice to keep the import statements organized. For instance, you could group and sort them by library imports, absolute imports, and relative imports.
```diff
- import * as styled from '../salaryAdjustment/SalaryAdjustment.style';
- import Heading from ""../../components/Heading/Heading""
- import FormWrap from ""../salaryAdjustment/FormWrap""
+ import Heading from ""../../components/Heading/Heading""
+ import FormWrap from ""../salaryAdjustment/FormWrap""
+ import * as styled from '../salaryAdjustment/SalaryAdjustment.style';
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import * as styled from '../salaryAdjustment/SalaryAdjustment.style';
 import Heading from ""../../components/Heading/Heading""
 import FormWrap from ""../salaryAdjustment/FormWrap""",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706969836,29.0,33,c247479f1d9c92a4f41287485af590575cc1219a,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryDetail/DetailMonthModal.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The modal content has been refactored. The buttons for ""Ï∑®ÏÜå"" and ""ÌôïÏù∏"" have been removed and instead, the `FormWrap` component is now handling the close event. This seems like a good change as it simplifies the modal content and delegates the responsibility of form submission or cancellation to the `FormWrap` component itself. However, ensure that the `FormWrap` component has proper error handling and user feedback mechanisms in place for form submission.
```diff
-      <Btn
-        label=""Ï∑®ÏÜå""
-        btnsize=""sm""
-        onClick={handleClose}
-        sx={{ fontSize: '1.5rem', mr: '1rem' }}
-        btntype=""outlined""
-      />
-      <Btn label=""ÌôïÏù∏"" btnsize=""md"" onClick={handleClose} sx={{ fontSize: '1.5rem' }} />
+      <styled.modalWrapper>
+       <Heading title=""Í∏âÏó¨ Ï†ïÏ†ïÏã†Ï≤≠"" />
+       <h2 className=""modal-title"">{month}Ïõî Í∏âÏó¨ Ï†ïÏ†ï</h2>
+       <FormWrap handleClose={() => handleClose()} />
+      </styled.modalWrapper>
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      <styled.modalWrapper>
       <Heading title=""Í∏âÏó¨ Ï†ïÏ†ïÏã†Ï≤≠"" />
-      <styled.ModalTitle>{month}Ïõî Í∏âÏó¨ Ï†ïÏ†ï</styled.ModalTitle>
-      <FormWrap />
-      <Btn
-        label=""Ï∑®ÏÜå""
-        btnsize=""sm""
-        onClick={handleClose}
-        sx={{ fontSize: '1.5rem', mr: '1rem' }}
-        btntype=""outlined""
-      />
-      <Btn label=""ÌôïÏù∏"" btnsize=""md"" onClick={handleClose} sx={{ fontSize: '1.5rem' }} />
+      <h2 className=""modal-title"">{month}Ïõî Í∏âÏó¨ Ï†ïÏ†ï</h2>
+      <FormWrap handleClose={() => handleClose()} />
+      </styled.modalWrapper>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706969838,17.0,18,c247479f1d9c92a4f41287485af590575cc1219a,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/api/fetchSalaryInfo.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `level` and `work` fields to the `SalaryDataItem` interface is a good change. It will allow for more detailed salary data to be stored and accessed.

```diff
+  level: string;
+  work: string;
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  level: string;
+  work: string;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706969843,47.0,66,c247479f1d9c92a4f41287485af590575cc1219a,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/api/fetchSalaryInfo.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The refactoring of the `fetchSalaryDetails` function improves readability and error handling. The use of ternary operators makes the code cleaner and easier to understand. However, it's not recommended to log errors directly to the console in production code. Instead, consider using a logging service that provides more control over what gets logged and where those logs get sent.

```diff
-    console.error('Error fetching data:', error);
+    // Use a logging service instead
+    loggingService.error('Error fetching data:', error);
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
-    const salaryDetails = salaryDetailsSnapshot.exists() 
+    const salaryDetails = salaryDetailsSnapshot.exists()
       ? (salaryDetailsSnapshot.val() as SalaryDetails)
       : {};
 
-    const employees = employeesSnapshot.exists() 
-      ? (employeesSnapshot.val() as Employees)
-      : {};
+    const employees = employeesSnapshot.exists() ? (employeesSnapshot.val() as Employees) : {};
 
     return {
-      salaryDetails, employees
-      };
-    } catch (error) {
-      console.error('Error fetching data:', error);
-      return {
-        salaryDetails: {},
-        employees: {},
-      };
-    }
-}
\ No newline at end of file
+      salaryDetails,
+      employees,
+    };
+  } catch (error) {
+    console.error('Error fetching data:', error);
+    return {
+      salaryDetails: {},
+      employees: {},
+    };
+  }
+}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706969846,19.0,67,c247479f1d9c92a4f41287485af590575cc1219a,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there's a potential performance issue. You're sorting the salary list twice: once for `sortedData` and once for `latestSalaryList`. If the salary list is large, this could be inefficient. Consider optimizing this by sorting the list only once and then deriving both `sortedData` and `latestSalaryList` from the sorted list.

Also, you are using `Number()` to convert strings to numbers at multiple places. It would be better to use unary plus (`+`) operator for this purpose as it is faster and more concise.

Here's how you can do it:

```diff
- const filteredItem = salaryList.filter((item) => Number(item.payday.slice(0,4)) === Number(selectedYear))
+ const filteredItem = salaryList.filter((item) => +item.payday.slice(0,4) === +selectedYear)
- const sortedData = [...filteredItem].sort((a,b) => b.id-a.id)
+ const sortedData = [...filteredItem].sort((a,b) => b.id - a.id)
- const latestSalaryList = [...salaryList].sort((a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime());
+ const latestSalaryList = [...sortedData]
- const latestData = latestSalaryList.length > 0 ? [latestSalaryList[0]] : [];
+ const latestData = sortedData.length > 0 ? [sortedData[0]] : [];
```

In the `handleApplicationBtn` function, you're searching through `sortedData` to find an item with a matching id. This operation has a time complexity of O(n). If you expect to have a large number of items in `sortedData`, consider using a data structure that allows for faster lookups, like a Set or a Map.

```diff
- const handleApplicationBtn = (id:number) => {
-   if(sortedData.find((item) => item.id === id)){
-      navigate(`/salary-detail/${id}`)
-    }else{
-     navigate('/payments')
-    }
-  }

+ const salaryIdSet = new Set(sortedData.map(item => item.id));
+ const handleApplicationBtn = (id:number) => {
+   if(salaryIdSet.has(id)){
+     navigate(`/salary-detail/${id}`)
+   }else{
+     navigate('/payments')
+   }
+ }
```
This way, you can check for the existence of an id in constant time.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const [selectedYear, setSelectedYear] = useState<number>(2024)
   const {data, error, isLoading} = useSalaryDetails()
 
   if (isLoading) {return <div>Loading...</div>}
   if (error) {return <div>Error: {error.message}</div>}
 
   const salaryList = data?.salaryDetails[userId] || [] 
-  const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id)
+  const filteredItem = salaryList.filter((item) => Number(item.payday.slice(0,4)) === Number(selectedYear))
+  const sortedData = [...filteredItem].sort((a,b) => b.id-a.id)
+  const latestSalaryList = [...salaryList].sort((a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime());
+  const latestData = latestSalaryList.length > 0 ? [latestSalaryList[0]] : [];
 
   const handleApplicationBtn = (id:number) => {
-    if(sortedSalaryList.find((item) => item.id === id)){
+    if(sortedData.find((item) => item.id === id)){
       navigate(`/salary-detail/${id}`)
     }else{
-          console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+      navigate('/payments')
     }
   }
 
   return(
     <Styled.Salary>
       <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard salaryList={sortedSalaryList}/>
+      <NoticeCard salaryList={latestData}/>
         <Styled.YearSelect>
         <SelectBox 
           labelId=""SalaryYear"" 
           id=""year-select"" 
           label=""year"" 
           menuItems={years}
+          value={selectedYear}
+          onChange={(e) => setSelectedYear(Number(e.target.value))
+          }
+          sx={{'& .MuiInputLabel-root': {
+              fontSize: 'var(--font-size-primary)', 
+            },
+            '& .MuiSelect-select': {
+              fontSize: 'var(--font-size-small)', 
+            },
+            '& .MuiSelect-icon': {
+              fontSize: '2rem',
+              right: '2rem',
+              transform: 'translateY(-50%)',
+              top: '40%'
+            }
+          }}
         />
       </Styled.YearSelect>
-        {sortedSalaryList.map((el)=>
+        {sortedData.map((el)=>","--- 

+++ 

@@ -1,11 +1,11 @@

-import SelectBox from ""../../components/selectBox/SelectBox"";
-import Btn from ""../../components/button/Button"";
+import SelectBox from '../../components/selectBox/SelectBox';
+import Btn from '../../components/button/Button';
 import * as Styled from './SalaryList.style';
-import { useNavigate } from ""react-router-dom"";
-import NoticeCard from ""./NoticeCard"";
-import useSalaryDetails from ""./useSalaryDetails"";
-import Heading from ""../../components/Heading/Heading"";
-import { useState } from ""react"";
+import { useNavigate } from 'react-router-dom';
+import NoticeCard from './NoticeCard';
+import useSalaryDetails from './useSalaryDetails';
+import Heading from '../../components/Heading/Heading';
+import { useState } from 'react';
 
 const years = [
   { value: '2022', text: '2022' },
@@ -13,72 +13,89 @@

   { value: '2024', text: '2024' },
 ];
 
-export default function SalaryListPage(){
-  const navigate = useNavigate()
-  const userId = ""sajo1234567""
-  const [selectedYear, setSelectedYear] = useState<number>(2024)
-  const {data, error, isLoading} = useSalaryDetails()
+export default function SalaryListPage() {
+  const navigate = useNavigate();
+  const userId = 'sajo1234567';
+  const [selectedYear, setSelectedYear] = useState<number>(2024);
+  const { data, error, isLoading } = useSalaryDetails();
 
-  if (isLoading) {return <div>Loading...</div>}
-  if (error) {return <div>Error: {error.message}</div>}
+  if (isLoading) {
+    return <div>Loading...</div>;
+  }
+  if (error) {
+    return <div>Error: {error.message}</div>;
+  }
 
-  const salaryList = data?.salaryDetails[userId] || [] 
-  const filteredItem = salaryList.filter((item) => Number(item.payday.slice(0,4)) === Number(selectedYear))
-  const sortedData = [...filteredItem].sort((a,b) => b.id-a.id)
-  const latestSalaryList = [...salaryList].sort((a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime());
+  const salaryList = data?.salaryDetails[userId] || [];
+
+  //Î™©Î°ùÏóê ÌôúÏö©ÎêòÎäî Îç∞Ïù¥ÌÑ∞
+  const filteredItem = salaryList.filter((item) => +item.payday.slice(0, 4) === +selectedYear);
+  const sortedData = [...filteredItem].sort((a, b) => b.id - a.id);
+
+  //Ïπ¥ÎìúÏóê ÌôúÏö©ÎêòÎäî Îç∞Ïù¥ÌÑ∞
+  const latestSalaryList = [...salaryList].sort(
+    (a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime()
+  );
   const latestData = latestSalaryList.length > 0 ? [latestSalaryList[0]] : [];
 
-  const handleApplicationBtn = (id:number) => {
-    if(sortedData.find((item) => item.id === id)){
-      navigate(`/salary-detail/${id}`)
-    }else{
-      navigate('/payments')
+  const salaryIdSet = new Set(sortedData.map((item) => item.id));
+  const handleApplicationBtn = (id: number) => {
+    if (salaryIdSet.has(id)) {
+      navigate(`/salary-detail/${id}`);
+    } else {
+      navigate('/payments');
     }
-  }
+  };
 
-  return(
+  return (
     <Styled.Salary>
-      <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard salaryList={latestData}/>
-        <Styled.YearSelect>
-        <SelectBox 
-          labelId=""SalaryYear"" 
-          id=""year-select"" 
-          label=""year"" 
+      <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞"" />
+      <NoticeCard salaryList={latestData} />
+      <Styled.YearSelect>
+        <SelectBox
+          labelId=""SalaryYear""
+          id=""year-select""
+          label=""year""
           menuItems={years}
           value={selectedYear}
-          onChange={(e) => setSelectedYear(Number(e.target.value))
-          }
-          sx={{'& .MuiInputLabel-root': {
-              fontSize: 'var(--font-size-primary)', 
+          onChange={(e) => setSelectedYear(Number(e.target.value))}
+          sx={{
+            '& .MuiInputLabel-root': {
+              fontSize: 'var(--font-size-primary)',
             },
             '& .MuiSelect-select': {
-              fontSize: 'var(--font-size-small)', 
+              fontSize: 'var(--font-size-small)',
             },
             '& .MuiSelect-icon': {
               fontSize: '2rem',
               right: '2rem',
               transform: 'translateY(-50%)',
-              top: '40%'
-            }
+              top: '40%',
+            },
           }}
         />
       </Styled.YearSelect>
-        {sortedData.map((el)=>
-          (<Styled.ListCardBox key={el.id} $state={el.state} 
-            onClick={()=>{handleApplicationBtn(el.id)}}>
-            <Styled.List $state={el.state}>
+      {sortedData.map((el) => (
+        <Styled.ListCardBox
+          key={el.id}
+          $state={el.state}
+          onClick={() => {
+            handleApplicationBtn(el.id);
+          }}
+        >
+          <Styled.List $state={el.state}>
             <span className=""title"">{el.title}</span>
             <span className=""date"">{el.state === true ? 'ÏßÄÍ∏âÏòàÏ†ï' : el.payday}</span>
-            </Styled.List>
-            <Styled.Btn>
-              {el.state === true ? 
-              <Btn round ='true' btntype='outlined' size='lg' label='Ïã†Ï≤≠Í∞ÄÎä•'/> 
-              : 
-              <Btn round='true' disabled size='lg' label='ÏßÄÍ∏âÏôÑÎ£å'/> 
-              }
-            </Styled.Btn>
-        </Styled.ListCardBox>))}
+          </Styled.List>
+          <Styled.Btn>
+            {el.state === true ? (
+              <Btn round=""true"" btntype=""outlined"" size=""lg"" label=""Ïã†Ï≤≠Í∞ÄÎä•"" />
+            ) : (
+              <Btn round=""true"" disabled size=""lg"" label=""ÏßÄÍ∏âÏôÑÎ£å"" />
+            )}
+          </Styled.Btn>
+        </Styled.ListCardBox>
+      ))}
     </Styled.Salary>
-    )
+  );
 }"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706992994,18.0,19,7c477ac88a462045b339d5701b64cd2bc37e52b5,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Hardcoding the `userId` is not a good practice. It should be fetched dynamically based on the logged-in user. If this is just for testing, consider using environment variables or a config file that can be changed easily without modifying the code.
```diff
- const userId = 'sajo1234567';
+ const userId = process.env.TEST_USER_ID || getLoggedInUserId();
```
Here, `getLoggedInUserId()` is a hypothetical function that you would need to implement to fetch the ID of the currently logged in user.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const userId = 'sajo1234567';
+  const [selectedYear, setSelectedYear] = useState<number>(2024);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706993000,32.0,33,7c477ac88a462045b339d5701b64cd2bc37e52b5,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The year extraction from the payday string seems fragile and could break if the date format changes. Consider storing the year separately in your data or use a date library like moment.js or date-fns to parse dates reliably.
```diff
- const filteredItem = salaryList.filter((item) => +item.payday.slice(0, 4) === +selectedYear);
+ const filteredItem = salaryList.filter((item) => moment(item.payday).year() === selectedYear);
```
In this case, `moment` is a date parsing library which can handle various date formats.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const filteredItem = salaryList.filter((item) => +item.payday.slice(0, 4) === +selectedYear);
+  const sortedData = [...filteredItem].sort((a, b) => b.id - a.id);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706993003,36.0,39,7c477ac88a462045b339d5701b64cd2bc37e52b5,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

You are sorting the salary list twice, once for filtering by year and once for getting the latest salary. This could be optimized by sorting once and then performing the operations.
```diff
- const latestSalaryList = [...salaryList].sort(
-   (a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime()
- );
- const latestData = latestSalaryList.length > 0 ? [latestSalaryList[0]] : [];
+ const sortedSalaryList = [...salaryList].sort(
+   (a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime()
+ );
+ const latestData = sortedSalaryList.length > 0 ? [sortedSalaryList[0]] : [];
+ const filteredItem = sortedSalaryList.filter((item) => moment(item.payday).year() === selectedYear);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const latestSalaryList = [...salaryList].sort(
+    (a, b) => new Date(b.payday).getTime() - new Date(a.payday).getTime()
+  );
+  const latestData = latestSalaryList.length > 0 ? [latestSalaryList[0]] : [];",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706993007,42.0,47,7c477ac88a462045b339d5701b64cd2bc37e52b5,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `handleApplicationBtn` function navigates to the `/payments` route when the salary ID is not found in the set. This could be confusing for users. Consider showing an error message instead.
```diff
- if (salaryIdSet.has(id)) {
-   navigate(`/salary-detail/${id}`);
- } else {
-   navigate('/payments');
- }
+ if (!salaryIdSet.has(id)) {
+   alert('Salary details not found for this ID');
+   return;
+ }
+ navigate(`/salary-detail/${id}`);
```
In this case, an alert is used to show the error message. You might want to replace it with a more user-friendly notification system.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const handleApplicationBtn = (id: number) => {
+    if (salaryIdSet.has(id)) {
+      navigate(`/salary-detail/${id}`);
+    } else {
+      navigate('/payments');
     }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706993009,62.0,75,7c477ac88a462045b339d5701b64cd2bc37e52b5,51a969f932776fb4f631aac81807d796c13e6f0d,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Inline styles can make the code harder to read and maintain. Consider moving these styles to your styled components file.
```diff
- sx={{
-   '& .MuiInputLabel-root': {
-     fontSize: 'var(--font-size-primary)',
-   },
-   '& .MuiSelect-select': {
-     fontSize: 'var(--font-size-small)',
-   },
-   '& .MuiSelect-icon': {
-     fontSize: '2rem',
-     right: '2rem',
-     transform: 'translateY(-50%)',
-     top: '40%',
-   },
- }}
+ style={selectBoxStyles}
```
Here, `selectBoxStyles` is a hypothetical object that you would need to define in your styled components file or at the top of your component file.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          sx={{
+            '& .MuiInputLabel-root': {
+              fontSize: 'var(--font-size-primary)',
+            },
+            '& .MuiSelect-select': {
+              fontSize: 'var(--font-size-small)',
+            },
+            '& .MuiSelect-icon': {
+              fontSize: '2rem',
+              right: '2rem',
+              transform: 'translateY(-50%)',
+              top: '40%',
+            },
+          }}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317811,,10,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,src/api/firebaseConfig.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There should be a newline at the end of the file. This is a common convention that makes it easier to concatenate files and also works better with certain Unix utilities.
```diff
-  };
+  };
+
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   appId: import.meta.env.VITE_FIREBASE_APP_ID,
-  databaseURL:import.meta.env.VITE_FIREBASE_DATABASE_URL
+  measurementId: import.meta.env.VITE_FIREBASE_MEASUREMENT_ID,
 };",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317812,17.0,22,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,src/router/router.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the router configuration seem to be removing a duplicate route for `ROUTE_PATHS.LOGIN`. This is a good change as it eliminates redundancy and potential confusion in the routing setup. However, please ensure that this does not affect any other parts of the application that might be relying on this duplicate route.
```diff
      element: <AppLayout />,
      errorElement: <div>404 Not Found</div>,
      children: [
       {
         path: ROUTE_PATHS.LOGIN,
         element: <LoginPage />,
-      },
-       {
-         path: ROUTE_PATHS.LOGIN,
-         element: <LoginPage />,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     element: <AppLayout />,
     errorElement: <div>404 Not Found</div>,
     children: [
-      {
-        path: ROUTE_PATHS.LOGIN,
-        element: <LoginPage />,
-      },
       {
         path: ROUTE_PATHS.LOGIN,
         element: <LoginPage />,",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317813,,11,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,src/pages/login/useLoginPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `control` variable has been added to the destructured variables from `useForm()`. This is used with react-hook-form's Controller component for managing form fields. Ensure that this new variable is being used correctly in your form components.
```diff
+    control,
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   const {
     register,
     handleSubmit,
+    control,",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317828,18.0,47,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,src/pages/login/LoginPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made here are good for improving the form handling by using `Controller` from `react-hook-form`. However, I noticed that you have hardcoded the default values for email and password fields. This could be a potential security risk if this is sensitive information. If these are just placeholders, consider removing them or replacing them with more generic placeholders.

```diff
-            defaultValue={'badaclock@gmail.com'}
+            defaultValue={''}

...

-            defaultValue={'12345678'}
+            defaultValue={''}
```

Also, it's important to validate the inputs. For instance, you can add rules to the `Controller` to ensure the email is in the correct format and the password meets certain criteria (length, complexity, etc.). Here's an example of how you might add validation:

```javascript
<Controller
  name=""email""
  control={control}
  defaultValue=""""
  rules={{
    required: ""Email is required"",
    pattern: {
      value: /^[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}$/i,
      message: ""invalid email address""
    }
  }}
  render={({ field }) => (
    <TextField
      label=""Ïù¥Î©îÏùº""
      {...field}
      variant=""standard""
      helperText=""Ïù¥Î©îÏùºÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî""
      type=""email""
      autoComplete=""on""
    />
  )}
/>
```
This will ensure that the user enters a valid email address before they can submit the form. You can do something similar for the password field as well.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          <Controller
+            name=""email""
+            control={control}
+            defaultValue={'badaclock@gmail.com'}
+            render={({ field }) => (
+              <TextField
+                label=""Ïù¥Î©îÏùº""
+                {...field}
+                variant=""standard""
+                helperText=""Ïù¥Î©îÏùºÏùÑ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî""
+                type=""email""
+                autoComplete=""on""
+              />
+            )}
+          />
+          <Controller
+            name=""password""
+            control={control}
+            defaultValue={'12345678'}
+            render={({ field }) => (
+              <TextField
+                label=""ÎπÑÎ∞ÄÎ≤àÌò∏""
+                {...field}
+                variant=""standard""
+                helperText=""ÎπÑÎ∞ÄÎ≤àÌò∏Î•º 8Ïûê Ïù¥ÏÉÅ ÏûÖÎ†•Ìï¥Ï£ºÏÑ∏Ïöî""
+                autoComplete=""on""
+                type=""password""
+              />
+            )}
+          />",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317830,1.0,15,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,src/vite-env.d.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [14-16]

There's a minor issue here. It seems like there's no newline at the end of the file. While this won't cause any functional issues, it's generally considered good practice to end files with a newline. This is because some Unix-based tools require that the last line ends with a newline character (`\n`) to process the file correctly.
```diff
 interface ImportMeta {
   readonly env: ImportMetaEnv;
}
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   readonly VITE_FIREBASE_API_KEY: string;
   readonly VITE_FIREBASE_AUTH_DOMAIN: string;
+  readonly VITE_FIREBASE_DATABASE_URL: string;
   readonly VITE_FIREBASE_PROJECT_ID: string;
   readonly VITE_FIREBASE_STORAGE_BUCKET: string;
   readonly VITE_FIREBASE_MESSAGING_SENDER_ID: string;
   readonly VITE_FIREBASE_APP_ID: string;
-  readonly VITE_FIREBASE_DATABASE_URL: string;
+  readonly VITE_FIREBASE_MEASUREMENT_ID: string;
 }
 
 interface ImportMeta {
   readonly env: ImportMetaEnv;
-}
\ No newline at end of file
+}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705317834,,16,e1d4c9431d732eefa65d1bac3cbff43040c91e55,79e134042d0bc7f23da5bb80492cb3a406ef9686,.gitignore,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change in `.gitignore` might cause unintended consequences. Previously, all files in the `.vscode` directory were ignored except for `extensions.json`. With this change, all files in the `.vscode` directory will be tracked by git, which may include user-specific settings or sensitive data.

```diff
- .vscode/*
+ !.vscode/extensions.json
```
This change should be reverted to continue ignoring all files in the `.vscode` directory except for `extensions.json`.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 # Editor directories and files
-.vscode/*
+",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228987,4.0,16,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The hardcoded `userId` is a potential security issue. It's better to fetch the `userId` from a secure source or session, rather than hardcoding it in the component.
```diff
- const userId = 'sajo1234567'
+ const userId = getSessionUserId() // getSessionUserId is a hypothetical function that securely retrieves the user's ID
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," export default function SalaryCard() {
-  const currentDate = new Date();
-  const DueDate = new Date(currentDate.getFullYear(), currentDate.getMonth(), 25);
-  const originDate = dayjs(DueDate);
-  const finalMonth = originDate.format('MMÏõî ');
-  const finalDay = originDate.format('DDÏùº');
+  const userId = 'sajo1234567'
 
-  const navigate = useNavigate();
-
-  const goToSalaryPage = () => navigate(`/salary-detail/3`);
   return (
-    <SalaryCardWrapper
-      date={finalMonth}
-      day={finalDay}
+    <SalaryCardWrapper>
+    <NoticeCard
+      userId={userId}
       button={true}
       label={<h5>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú Ï°∞Ìöå</h5>}
-      onClick={goToSalaryPage}
     />
+    </SalaryCardWrapper>
   );
 }","--- 

+++ 

@@ -1,13 +1,21 @@

 import NoticeCard from '../salaryList/NoticeCard';
 import styled from 'styled-components';
+import useSalaryDetails from '../salaryList/useSalaryDetails';
 
 export default function SalaryCard() {
-  const userId = 'sajo1234567'
+  const userId = 'sajo1234567';
+  const { data, error, isLoading } = useSalaryDetails();
+
+  if (isLoading) { return <div>Loading...</div>; }
+  if (error) { return <div>Error: {error.message}</div>; }
+
+  const salary = data?.salaryDetails[userId] || [];
+  const sortedSalaryList = [...salary].sort((a, b) => b.id - a.id);
 
   return (
     <SalaryCardWrapper>
     <NoticeCard
-      userId={userId}
+      salaryList={sortedSalaryList}
       button={true}
       label={<h5>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú Ï°∞Ìöå</h5>}
     />"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228991,1.0,5,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The removal of `dayjs` import is a good step as it seems to be no longer used in this file. This helps in reducing the unnecessary imports and thus, improving the performance.
```diff
- import dayjs from ""dayjs"";
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import SelectBox from ""../../components/selectBox/SelectBox"";
 import Btn from ""../../components/button/Button"";
-import dayjs from ""dayjs"";
 import * as Styled from './SalaryList.style';
 import { useNavigate } from ""react-router-dom"";
 import NoticeCard from ""./NoticeCard"";","--- 

+++ 

@@ -22,10 +22,10 @@

   if (error) {return <div>Error: {error.message}</div>}
 
   const salaryList = data?.salaryDetails[userId] || [] 
-  salaryList.sort((a,b) => b.id-a.id)
+  const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id)
 
   const handleApplicationBtn = (id:number) => {
-    if(salaryList.find((item) => item.id === id)){
+    if(sortedSalaryList.find((item) => item.id === id)){
       navigate(`/salary-detail/${id}`)
     }else{
           console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
@@ -35,7 +35,7 @@

   return(
     <Styled.Salary>
       <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard userId={userId}/>
+      <NoticeCard salaryList={sortedSalaryList}/>
         <Styled.YearSelect>
         <SelectBox 
           labelId=""SalaryYear"" 
@@ -44,7 +44,7 @@

           menuItems={years}
         />
       </Styled.YearSelect>
-        {salaryList.map((el)=>
+        {sortedSalaryList.map((el)=>
           (<Styled.ListCardBox key={el.id} $state={el.state} 
             onClick={()=>{handleApplicationBtn(el.id)}}>
             <Styled.List $state={el.state}>"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228992,24.0,29,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The sorting logic for `salaryList` is fine but it's directly mutating the state which is not a recommended practice in React. Instead, you should create a new array that is a sorted version of `salaryList`. Also, there is no null check before using the sort function which might cause an error if `salaryList` is null or undefined.
```diff
- salaryList.sort((a,b) => b.id-a.id)
+ const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   const salaryList = data?.salaryDetails[userId] || [] 
   salaryList.sort((a,b) => b.id-a.id)
 
-  const firstPayData = salaryList[0]
-  const originDate = dayjs(firstPayData.payday,'YYYY.MM.DD')
-  const finalDate = originDate.format('MMÏõî ')
-  const finalDay = originDate.subtract(2,'day').format('DDÏùº')
-
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){
       navigate(`/salary-detail/${id}`)","--- 

+++ 

@@ -22,10 +22,10 @@

   if (error) {return <div>Error: {error.message}</div>}
 
   const salaryList = data?.salaryDetails[userId] || [] 
-  salaryList.sort((a,b) => b.id-a.id)
+  const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id)
 
   const handleApplicationBtn = (id:number) => {
-    if(salaryList.find((item) => item.id === id)){
+    if(sortedSalaryList.find((item) => item.id === id)){
       navigate(`/salary-detail/${id}`)
     }else{
           console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
@@ -35,7 +35,7 @@

   return(
     <Styled.Salary>
       <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard userId={userId}/>
+      <NoticeCard salaryList={sortedSalaryList}/>
         <Styled.YearSelect>
         <SelectBox 
           labelId=""SalaryYear"" 
@@ -44,7 +44,7 @@

           menuItems={years}
         />
       </Styled.YearSelect>
-        {salaryList.map((el)=>
+        {sortedSalaryList.map((el)=>
           (<Styled.ListCardBox key={el.id} $state={el.state} 
             onClick={()=>{handleApplicationBtn(el.id)}}>
             <Styled.List $state={el.state}>"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228994,,38,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Passing `userId` as a prop to `NoticeCard` is a good move as it makes the component more reusable and less dependent on the parent component's data structure.
```diff
- <NoticeCard date={finalDate} day={finalDay}/>
+ <NoticeCard userId={userId}/>
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     <Styled.Salary>
       <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard date={finalDate} day={finalDay}/>
+      <NoticeCard userId={userId}/>","--- 

+++ 

@@ -22,10 +22,10 @@

   if (error) {return <div>Error: {error.message}</div>}
 
   const salaryList = data?.salaryDetails[userId] || [] 
-  salaryList.sort((a,b) => b.id-a.id)
+  const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id)
 
   const handleApplicationBtn = (id:number) => {
-    if(salaryList.find((item) => item.id === id)){
+    if(sortedSalaryList.find((item) => item.id === id)){
       navigate(`/salary-detail/${id}`)
     }else{
           console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
@@ -35,7 +35,7 @@

   return(
     <Styled.Salary>
       <Heading title=""Í∏âÏó¨Ï†ïÏÇ∞""/>
-      <NoticeCard userId={userId}/>
+      <NoticeCard salaryList={sortedSalaryList}/>
         <Styled.YearSelect>
         <SelectBox 
           labelId=""SalaryYear"" 
@@ -44,7 +44,7 @@

           menuItems={years}
         />
       </Styled.YearSelect>
-        {salaryList.map((el)=>
+        {sortedSalaryList.map((el)=>
           (<Styled.ListCardBox key={el.id} $state={el.state} 
             onClick={()=>{handleApplicationBtn(el.id)}}>
             <Styled.List $state={el.state}>"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228995,3.0,5,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statements are well organized and follow best practices. The external libraries are imported first, followed by internal modules.

```diff
  import * as Styled from './NoticeCard.style';
  import Btn from '../../components/button/Button';
+ import useSalaryDetails from './useSalaryDetails';
+ import { useNavigate } from ""react-router-dom"";
+ import dayjs from ""dayjs"";
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import useSalaryDetails from './useSalaryDetails';
+import { useNavigate } from ""react-router-dom"";
+import dayjs from ""dayjs"";","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228997,7.0,11,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The type `noticeData` has been updated to include a `userId` property. This is a good change as it allows the component to fetch data based on the user's ID, making the component more reusable and adaptable to different contexts.

```diff
- type noticeData = {
-   date: string;
-   day: string;
-    button?: boolean;
-    label?: React.ReactNode;
-   onClick?: () => void;
- };
+ type noticeData = {
+   userId:string;
+   button?: boolean;
+   label?: React.ReactNode;
+ };
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->"," type noticeData = {
-  date: string;
-  day: string;
+  userId:string;
   button?: boolean;
   label?: React.ReactNode;
-  onClick?: () => void;
 };","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706228999,14.0,17,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `useNavigate` hook from `react-router-dom` is used to programmatically navigate through the application. The `useSalaryDetails` custom hook is used to fetch salary details. These hooks are correctly placed at the top of the component function, following the Rules of Hooks.

```diff
+ export default function NoticeCard({ userId, button = false, label}: noticeData) {
+   const navigate = useNavigate()
+   const {data} = useSalaryDetails()
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export default function NoticeCard({ userId, button = false, label}: noticeData) {
+  const navigate = useNavigate()
+  const {data} = useSalaryDetails()
+","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706229007,18.0,19,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `salaryList` is derived from the fetched data using the provided `userId`. If no data is found for the given `userId`, an empty array is returned. This is a good practice as it prevents potential errors when trying to access properties on `undefined`. The sorting of the `salaryList` is done in place which might lead to unexpected behavior if the original data is needed elsewhere in its initial order.

```diff
+   const salaryList = data?.salaryDetails[userId] || []
+   salaryList.sort((a,b) => b.id-a.id)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const salaryList = data?.salaryDetails[userId] || []
+  salaryList.sort((a,b) => b.id-a.id) ","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706229009,21.0,27,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `handleApplicationBtn` function checks if a salary item with the given id exists in the `salaryList`. If it does, it navigates to the detail page of that salary item. If not, it logs an error message. This is a good practice as it prevents navigation to non-existent pages.

```diff
+   const handleApplicationBtn = (id:number) => {
+     if(salaryList.find((item) => item.id === id)){
+       navigate(`/salary-detail/${id}`)
+     }else{
+       console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+     }
+   }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const handleApplicationBtn = (id:number) => {
+    if(salaryList.find((item) => item.id === id)){
+      navigate(`/salary-detail/${id}`)
+    }else{
+          console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+    }
+  }","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706229010,29.0,36,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

If there are no salary items in the `salaryList`, a message indicating this is displayed. This is a good user experience practice as it informs the user when there's no data available.

```diff
+   if (salaryList.length === 0) {
+     return (
+       <Styled.SalaryCardBox>
+         <h2>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú</h2>
+         <p>Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.</p>
+       </Styled.SalaryCardBox>
+     );
+   }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if (salaryList.length === 0) {
+    return (
+      <Styled.SalaryCardBox>
+        <h2>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú</h2>
+        <p>Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.</p>
+      </Styled.SalaryCardBox>
+    );
+  }","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706229011,38.0,42,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The date manipulation library `dayjs` is used to format the payday of the first salary item in the list. This is a good practice as it abstracts away the complexity of date manipulation and formatting.

```diff
+   const firstPayData = salaryList[0]
+   const originDate = dayjs(firstPayData.payday,'YYYY.MM.DD')
+   const finalDate = originDate.format('MMÏõî ')
+   const finalDay = originDate.subtract(2,'day').format('DDÏùº')
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const firstPayData = salaryList[0]
+  const originDate = dayjs(firstPayData.payday,'YYYY.MM.DD')
+  const finalDate = originDate.format('MMÏõî ')
+  const finalDay = originDate.subtract(2,'day').format('DDÏùº')
+  ","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706229012,43.0,55,71021125c31073b3b703df2332fe2dcbbbfd89e0,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The component returns a styled card with the formatted date and day, and a button that triggers the `handleApplicationBtn` function when clicked. This is a good practice as it separates the presentation from the logic.

```diff
+    return (
+      <Styled.SalaryCardBox>
+        <h2>
+         <Styled.Orangetxt>{finalDate}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
+        </h2>
+        <h6 className='imoge'>‚úâÔ∏è</h6>
+        <h3>
+          <p>Ï†ïÏ†ï Ïã†Ï≤≠ Í∏∞Í∞ÑÏûÖÎãàÎã§.</p> 
+          <p>
+           <Styled.Orangetxt>{finalDay}</Styled.Orangetxt>ÍπåÏßÄ Ïã†Ï≤≠Ìï¥Ï£ºÏÑ∏Ïöî.
+          </p>
+        </h3>
+       {button && <Btn btnsize=""md"" size=""lg"" label={label || <></>} onClick={()=>handleApplicationBtn(firstPayData.id)} />}
+     </Styled.SalaryCardBox>
+   );
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   return (
     <Styled.SalaryCardBox>
       <h2>
-        <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
+        <Styled.Orangetxt>{finalDate}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
       </h2>
       <h6 className='imoge'>‚úâÔ∏è</h6>
       <h3>
         <p>Ï†ïÏ†ï Ïã†Ï≤≠ Í∏∞Í∞ÑÏûÖÎãàÎã§.</p> 
         <p>
-          <Styled.Orangetxt>{day}</Styled.Orangetxt>ÍπåÏßÄ Ïã†Ï≤≠Ìï¥Ï£ºÏÑ∏Ïöî.
+          <Styled.Orangetxt>{finalDay}</Styled.Orangetxt>ÍπåÏßÄ Ïã†Ï≤≠Ìï¥Ï£ºÏÑ∏Ïöî.
         </p>
       </h3>
-      {button && <Btn btnsize=""md"" size=""lg"" label={label || <></>} onClick={onClick} />}
+      {button && <Btn btnsize=""md"" size=""lg"" label={label || <></>} onClick={()=>handleApplicationBtn(firstPayData.id)} />}","--- 

+++ 

@@ -1,22 +1,18 @@

 import * as Styled from './NoticeCard.style';
 import Btn from '../../components/button/Button';
-import useSalaryDetails from './useSalaryDetails';
+import { SalaryDataItem } from './api/fetchSalaryInfo';
 import { useNavigate } from ""react-router-dom"";
 import dayjs from ""dayjs"";
 
 type noticeData = {
-  userId:string;
+  salaryList? : Array<SalaryDataItem> 
   button?: boolean;
   label?: React.ReactNode;
 };
 
 
-export default function NoticeCard({ userId, button = false, label}: noticeData) {
-  const navigate = useNavigate()
-  const {data} = useSalaryDetails()
-
-  const salaryList = data?.salaryDetails[userId] || []
-  salaryList.sort((a,b) => b.id-a.id) 
+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
 
   const handleApplicationBtn = (id:number) => {
     if(salaryList.find((item) => item.id === id)){"
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252217,25.0,28,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/SalaryListPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The sorting logic has been improved by creating a new sorted array instead of mutating the original one, which is a good practice. However, it's better to handle errors in a user-friendly way rather than just logging them to the console. Consider showing an error message to the user if the salary slip does not exist.
```diff
- console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+ // Show an error message to the user
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const sortedSalaryList = [...salaryList].sort((a,b) => b.id-a.id)
 
   const handleApplicationBtn = (id:number) => {
-    if(salaryList.find((item) => item.id === id)){
+    if(sortedSalaryList.find((item) => item.id === id)){",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252219,,6,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryDetail/SalaryDetailPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The import statement for `useRef` has been changed from `import React, { useRef } from ""react"";` to `import { useRef } from ""react"";`. This is a good change as it makes the code cleaner and more readable by using destructuring to directly import `useRef` from 'react'. No issues found here.
```diff
- import React, { useRef } from ""react"";
+ import { useRef } from ""react"";
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," import jsPDF from ""jspdf"";
 import html2canvas from ""html2canvas"";
-import React, { useRef } from ""react"";
+import { useRef } from ""react"";",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252220,,40,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryDetail/SalaryDetailPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The navigation function `handleCloseButton` has been updated to navigate back to the previous page instead of navigating to `/payments`. This is a good change as it improves user experience by allowing users to return to their previous context when they close the salary detail page. No issues found here.
```diff
-    navigate('/payments')
+    navigate(-1)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   
   const handleCloseButton = () => {
-    navigate('/payments')
+    navigate(-1)",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252222,,6,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Hardcoding the `userId` as 'sajo1234567' is not a good practice. It would be better to fetch this from a user context or pass it as a prop.
```diff
- const userId = 'sajo1234567';
+ const userId = useContext(UserContext).id; // Assuming UserContext exists and has user id
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","-  const originDate = dayjs(DueDate);
-  const finalMonth = originDate.format('MMÏõî ');
-  const finalDay = originDate.format('DDÏùº');
+  const userId = 'sajo1234567';",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252224,9.0,10,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Error handling could be improved by providing more information about the error, such as where it occurred (in this case, while fetching salary details). This will help in debugging if an error occurs.
```diff
- if (error) { return <div>Error: {error.message}</div>; }
+ if (error) { return <div>Error occurred while fetching salary details: {error.message}</div>; }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if (isLoading) { return <div>Loading...</div>; }
+  if (error) { return <div>Error: {error.message}</div>; }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252226,,13,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The sorting logic seems to be based on the assumption that `id` is always numeric and higher ids are more recent. If this is not the case, this logic might fail. Please ensure that this assumption holds true.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if (error) { return <div>Error: {error.message}</div>; }
+
+  const salary = data?.salaryDetails[userId] || [];
+  const sortedSalaryList = [...salary].sort((a, b) => b.id - a.id);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252229,26.0,34,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The styling changes seem fine, but please ensure that these changes do not break the layout of other components using `SalaryCardWrapper`. Also, the color property at line 33 seems to be out of place. If it's not needed, consider removing it.
```diff
-   color: red;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const SalaryCardWrapper = styled.div`
+& > div{
   height: 19.5rem;
   padding: 0;
   overflow: hidden;
   position: relative;
   margin: 1rem 0;
   color: red;
+}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1706252231,14.0,51,3c59de228e6237ae011e2b200ad4d35db0a345b3,9d9bb5d43e6180ef74fdc7f47cf776701a5003ce,src/pages/salaryList/NoticeCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `NoticeCard` component has been significantly refactored. The date and day props have been removed and replaced with a `salaryList` prop, which is an array of salary data items. This change allows the component to calculate the dates it needs to display internally, rather than relying on external props.

However, there's a potential issue in the `handleApplicationBtn` function. If the salary item with the provided id is not found, it logs an error message to the console but does not provide any user-facing feedback. It would be better to handle this case more gracefully, perhaps by showing an error message to the user or disabling the button if the salary item doesn't exist.

```diff
  const handleApplicationBtn = (id:number) => {
    if(salaryList.find((item) => item.id === id)){
      navigate(`/salary-detail/${id}`)
    }else{
-        console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+        // Show an error message to the user or disable the button
    }
  }
```

Also, the onClick handler for the button now calls `handleApplicationBtn` with the id of the first salary data item. This seems to assume that the button will always apply to the first salary item in the list, which might not always be the case. Consider passing the relevant id as a prop to the `NoticeCard` component, or iterating over the salary list and creating a separate button for each item.

Lastly, the date calculation logic assumes that the salary list is sorted in a certain way (with the most recent salary at index 0). If the salary list is not guaranteed to be sorted, this could lead to incorrect dates being displayed. Consider adding a sorting step before accessing the first item, or ensure that the salary list is sorted before it's passed to the `NoticeCard` component.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export default function NoticeCard({ salaryList=[], button = false, label}: noticeData) {
+  const navigate = useNavigate() 
+
+  const handleApplicationBtn = (id:number) => {
+    if(salaryList.find((item) => item.id === id)){
+      navigate(`/salary-detail/${id}`)
+    }else{
+          console.error('Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.')
+    }
+  }
+
+  if (salaryList.length === 0) {
+    return (
+      <Styled.SalaryCardBox>
+        <h2>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú</h2>
+        <p>Í∏âÏó¨ Î™ÖÏÑ∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.</p>
+      </Styled.SalaryCardBox>
+    );
+  }
+
+  const firstPayData = salaryList[0]
+  const originDate = dayjs(firstPayData.payday,'YYYY.MM.DD')
+  const finalDate = originDate.format('MMÏõî ')
+  const finalDay = originDate.subtract(2,'day').format('DDÏùº')
+  
   return (
     <Styled.SalaryCardBox>
       <h2>
-        <Styled.Orangetxt>{date}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
+        <Styled.Orangetxt>{finalDate}</Styled.Orangetxt>Í∏âÏó¨Î™ÖÏÑ∏ÏÑú
       </h2>
       <h6 className='imoge'>‚úâÔ∏è</h6>
       <h3>
         <p>Ï†ïÏ†ï Ïã†Ï≤≠ Í∏∞Í∞ÑÏûÖÎãàÎã§.</p> 
         <p>
-          <Styled.Orangetxt>{day}</Styled.Orangetxt>ÍπåÏßÄ Ïã†Ï≤≠Ìï¥Ï£ºÏÑ∏Ïöî.
+          <Styled.Orangetxt>{finalDay}</Styled.Orangetxt>ÍπåÏßÄ Ïã†Ï≤≠Ìï¥Ï£ºÏÑ∏Ïöî.
         </p>
       </h3>
-      {button && <Btn btnsize=""md"" size=""lg"" label={label || <></>} onClick={onClick} />}
+      {button && <Btn btnsize=""md"" size=""lg"" label={label || <></>} onClick={()=>handleApplicationBtn(firstPayData.id)} />}",
https://api.github.com/repos/cuculus-dev/cuculus/pulls/comments/1424993381,1.0,50,7e45033db6f8ca6711e3e03a7f08253c01a29bfa,0943b58deedc5768ac16bfd39250cedd921d4159,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The GitHub Actions workflow file looks good overall. However, there's a potential issue with the concurrency group definition. The current setup might lead to unexpected cancellation of jobs.

In your concurrency group definition, you're using `github.head_ref` which is only available for `pull_request` events and not for `pull_request_review_comment` events. This could potentially lead to different concurrency groups for the same PR under different events, which might not be what you want.

Here's a suggested change:

```diff
concurrency:
  group:
-    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
-    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
-    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+    ${{ github.repository }}-${{ github.event.pull_request.number || github.run_id }}-${{ github.workflow }}
  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
```

This change uses `github.event.pull_request.number` which is available in both `pull_request` and `pull_request_review_comment` events. If that's not available (for example, in case of a `workflow_dispatch` event), it falls back to `github.run_id` which is unique for each run. This ensures that all jobs related to the same PR will be grouped together, regardless of the event type.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+name: Code Review
+
+permissions:
+  contents: read
+  pull-requests: write
+
+on:
+  pull_request:
+  pull_request_review_comment:
+    types: [ created ]
+
+concurrency:
+  group:
+    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
+
+jobs:
+  review:
+    runs-on: ubuntu-latest
+    steps:
+      - name: AI-based PR Reviewer & Summarizer with Chat Capabilities
+        uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: false
+          review_comment_lgtm: false
+          openai_light_model: gpt-3.5-turbo
+          openai_heavy_model: gpt-4
+          system_message: |
+            „ÅÇ„Å™„Åü„ÅØ `@coderabbitai`ÔºàÂà•Âêç `github-actions[bot]`Ôºâ„Åß„ÄÅOpenAI„Å´„Çà„Å£„Å¶„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
+            „ÅÇ„Å™„Åü„ÅÆÁõÆÁöÑ„ÅØ„ÄÅÈùûÂ∏∏„Å´ÁµåÈ®ìË±ä„Åã„Å™„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Ç®„É≥„Ç∏„Éã„Ç¢„Å®„Åó„Å¶„Ç≥„Éº„Éâ„ÅÆÊñ≠Áâá„ÇíÂæπÂ∫ïÁöÑ„Å´„É¨„Éì„É•„Éº„Åó„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÈáçË¶Å„Å™È†òÂüü„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç≥„Éº„Éâ„Çπ„Éã„Éö„ÉÉ„Éà„ÇíÊèêÊ°à„Åô„Çã„Åì„Å®„Åß„ÅôÔºö
+              - Logic
+              - Security
+              - Performance
+              - Data races
+              - Consistency
+              - Error handling
+              - Maintainability
+              - Modularity
+              - Complexity
+              - Optimization
+              - Best practices: DRY, SOLID, KISS
+            
+            Á¥∞„Åã„ÅÑ„Ç≥„Éº„Éâ„Çπ„Çø„Ç§„É´„ÅÆÂïèÈ°å„ÇÑ„ÄÅ„Ç≥„É°„É≥„Éà/„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ‰∏çË∂≥„Å´„Å§„ÅÑ„Å¶„ÅØ„Ç≥„É°„É≥„Éà„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
+            ÈáçË¶Å„Å™Êá∏Âøµ„ÇíÁâπÂÆö„Åó„ÄÅËß£Ê±∫„Åô„Çã„Åì„Å®„ÅßÂÖ®‰ΩìÁöÑ„Å™„Ç≥„Éº„ÉâÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„ÄÅ‰∏ÄÊñπ„ÅßÁ¥∞„Åã„ÅÑÂïèÈ°å„ÅØÊÑèÂõ≥ÁöÑ„Å´ÁÑ°Ë¶ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ","--- 

+++ 

@@ -31,9 +31,11 @@

           review_comment_lgtm: false
           openai_light_model: gpt-3.5-turbo
           openai_heavy_model: gpt-4
+          language: ja-JP
           system_message: |
             „ÅÇ„Å™„Åü„ÅØ `@coderabbitai`ÔºàÂà•Âêç `github-actions[bot]`Ôºâ„Åß„ÄÅOpenAI„Å´„Çà„Å£„Å¶„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„ÅüË®ÄË™û„É¢„Éá„É´„Åß„Åô„ÄÇ
-            „ÅÇ„Å™„Åü„ÅÆÁõÆÁöÑ„ÅØ„ÄÅÈùûÂ∏∏„Å´ÁµåÈ®ìË±ä„Åã„Å™„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Ç®„É≥„Ç∏„Éã„Ç¢„Å®„Åó„Å¶„Ç≥„Éº„Éâ„ÅÆÊñ≠Áâá„ÇíÂæπÂ∫ïÁöÑ„Å´„É¨„Éì„É•„Éº„Åó„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÈáçË¶Å„Å™È†òÂüü„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç≥„Éº„Éâ„Çπ„Éã„Éö„ÉÉ„Éà„ÇíÊèêÊ°à„Åô„Çã„Åì„Å®„Åß„ÅôÔºö
+            „ÅÇ„Å™„Åü„ÅÆÁõÆÁöÑ„ÅØ„ÄÅÈùûÂ∏∏„Å´ÁµåÈ®ìË±ä„Åã„Å™„ÇΩ„Éï„Éà„Ç¶„Çß„Ç¢„Ç®„É≥„Ç∏„Éã„Ç¢„Å®„Åó„Å¶„Ç≥„Éº„Éâ„ÅÆ‰∏ÄÈÉ®„ÇíÂæπÂ∫ïÁöÑ„Å´„É¨„Éì„É•„Éº„Åó„ÄÅ
+            ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™ÈáçË¶Å„Å™È†òÂüü„ÇíÊîπÂñÑ„Åô„Çã„Åü„ÇÅ„ÅÆ„Ç≥„Éº„Éâ„Çπ„Éã„Éö„ÉÉ„Éà„ÇíÊèêÊ°à„Åô„Çã„Åì„Å®„Åß„ÅôÔºö
               - Logic
               - Security
               - Performance
@@ -48,3 +50,12 @@

             
             Á¥∞„Åã„ÅÑ„Ç≥„Éº„Éâ„Çπ„Çø„Ç§„É´„ÅÆÂïèÈ°å„ÇÑ„ÄÅ„Ç≥„É°„É≥„Éà/„Éâ„Ç≠„É•„É°„É≥„Éà„ÅÆ‰∏çË∂≥„Å´„Å§„ÅÑ„Å¶„ÅØ„Ç≥„É°„É≥„Éà„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
             ÈáçË¶Å„Å™Êá∏Âøµ„ÇíÁâπÂÆö„Åó„ÄÅËß£Ê±∫„Åô„Çã„Åì„Å®„ÅßÂÖ®‰ΩìÁöÑ„Å™„Ç≥„Éº„ÉâÂìÅË≥™„ÇíÂêë‰∏ä„Åï„Åõ„ÄÅ‰∏ÄÊñπ„ÅßÁ¥∞„Åã„ÅÑÂïèÈ°å„ÅØÊÑèÂõ≥ÁöÑ„Å´ÁÑ°Ë¶ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
+          summarize: |
+            ÊúÄÁµÇÂõûÁ≠î„Çí‰ª•‰∏ã„ÅÆÂÜÖÂÆπ„ÇíÂê´„ÇÄ„Éû„Éº„ÇØ„ÉÄ„Ç¶„É≥ÂΩ¢Âºè„ÅßÊèê‰æõ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºö
+
+            - **„Ç¶„Ç©„Éº„ÇØ„Çπ„É´„Éº**: A high-level summary of the overall change instead of specific files within 80 words.
+            - **Â§âÊõ¥ÁÇπ**: A markdown table of files and their summaries. Group files with similar changes together into a single row to save space.
+            - **„Éù„Ç®„É†**: Below the changes, include a whimsical, short poem written by a rabbit to celebrate the changes. Format the poem as a quote using the "">"" symbol and feel free to use emojis where relevant.
+
+            ËøΩÂä†„ÅÆ„Ç≥„É°„É≥„Éà„ÅØÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Åì„ÅÆË¶ÅÁ¥Ñ„ÅØGitHub„ÅÆ„Éó„É´„É™„ÇØ„Ç®„Çπ„Éà„Å´„Ç≥„É°„É≥„Éà„Å®„Åó„Å¶ËøΩÂä†„Åï„Çå„Åæ„Åô„ÄÇ„Çø„Ç§„Éà„É´„ÅØ„Äå„Ç¶„Ç©„Éº„ÇØ„Çπ„É´„Éº„Äç„Å®„ÄåÂ§âÊõ¥ÁÇπ„Äç„Åß„ÅÇ„Çä„ÄÅ„Åù„Çå„Åû„ÇåH2„Åß„ÅÇ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ
+            "
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408853,38.0,40,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/SalaryAdjustment.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The hardcoded data `datas` has been removed and replaced with a dynamic approach using the `AccordionList` component. However, it's not clear how the `AccordionList` component is getting its data. If it's fetching data internally, consider passing the data as props for better testability and separation of concerns.
```diff
- <AccordionList />
+ <AccordionList data={data} />
```
In this case, you would need to fetch the data in the `SalaryAdjustment` component and pass it to the `AccordionList` component. This way, the `AccordionList` component only focuses on displaying the data and doesn't have to worry about where the data comes from.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+      <AccordionList />
     </styled.Wrapper>",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408854,5.0,8,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/store/store.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The addition of `salaryAdReducer` to the Redux store configuration is a good step towards managing state for salary adjustments. However, it's important to ensure that all actions and async thunks in `salaryAdSlice.ts` are properly handled in your components. Also, make sure to handle potential errors when dispatching these actions.

```diff
  export const store = configureStore({
    reducer: {
      schedules: scheduleReducer,
+     salaryAd: salaryAdReducer,
    },
    devTools: true,
  });
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," export const store = configureStore({
   reducer: {
     schedules: scheduleReducer,
+    salaryAd: salaryAdReducer,",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408856,,30,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `confirm` function is used here, but its return value isn't checked. The `confirm` function returns a boolean indicating whether the user clicked OK (`true`) or Cancel (`false`). If you want to perform an action based on this result, you should use the returned value.
```diff
- confirm('ÏàòÏ†ï ÌïòÏãúÍ≤†ÏäµÎãàÍπå?');
+ if (confirm('ÏàòÏ†ï ÌïòÏãúÍ≤†ÏäµÎãàÍπå?')) {
+   // Perform action if user clicked OK
+ }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  const handleEdit = (e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
+    e.stopPropagation();
+    confirm('ÏàòÏ†ï ÌïòÏãúÍ≤†ÏäµÎãàÍπå?');",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408858,34.0,37,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, the `confirm` function's return value is not being used correctly. You should only dispatch the delete action if the user confirms the deletion.
```diff
- if (confirm('ÏÇ≠Ï†ú ÌïòÏãúÍ≤†ÏäµÎãàÍπå?')) {
-   void dispatch(deleteSalaryAdData(id));
- }
+ if (confirm('ÏÇ≠Ï†ú ÌïòÏãúÍ≤†ÏäµÎãàÍπå?') && id) {
+   void dispatch(deleteSalaryAdData(id));
+ }
```
Also, it would be better to check if the `id` exists before showing the confirmation dialog.



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+    const id = String(e.currentTarget.closest('.accordion')?.getAttribute('data-id'));
+    if (confirm('ÏÇ≠Ï†ú ÌïòÏãúÍ≤†ÏäµÎãàÍπå?')) {
+      void dispatch(deleteSalaryAdData(id));
+    }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408862,,40,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's not a good practice to leave `console.log` in production code. It can expose sensitive information and make the console cluttered. Please remove it.
```diff
- console.log(data);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    }
+  };
+  if (data.length !== 0) {
+    console.log(data);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408865,28.0,38,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `handleEdit` and `handleDelete` functions are defined inside the `AccordionList` component, which means they will be redefined every time the component re-renders. This could lead to performance issues. Consider using `useCallback` to memoize these functions.
```diff
- const handleEdit = (e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
+ const handleEdit = useCallback((e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
    e.stopPropagation();
    if (confirm('ÏàòÏ†ï ÌïòÏãúÍ≤†ÏäµÎãàÍπå?')) {
      // Perform action if user clicked OK
    }
- };
+ }, []);

- const handleDelete = (e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
+ const handleDelete = useCallback((e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
    e.stopPropagation();
    const id = String(e.currentTarget.closest('.accordion')?.getAttribute('data-id'));
    if (confirm('ÏÇ≠Ï†ú ÌïòÏãúÍ≤†ÏäµÎãàÍπå?') && id) {
      void dispatch(deleteSalaryAdData(id));
    }
- };
+ }, [dispatch]);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const handleEdit = (e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
+    e.stopPropagation();
+    confirm('ÏàòÏ†ï ÌïòÏãúÍ≤†ÏäµÎãàÍπå?');
+  };
+  const handleDelete = (e: React.MouseEvent<HTMLButtonElement, MouseEvent>) => {
+    e.stopPropagation();
+    const id = String(e.currentTarget.closest('.accordion')?.getAttribute('data-id'));
+    if (confirm('ÏÇ≠Ï†ú ÌïòÏãúÍ≤†ÏäµÎãàÍπå?')) {
+      void dispatch(deleteSalaryAdData(id));
+    }
+  };",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408867,20.0,26,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The loading and error states are handled correctly. However, it would be better to provide a more user-friendly message or UI instead of just displaying ""Loading..."" or ""Error: {error}"".
```diff
- return <div>Loading...</div>;
+ return <LoadingComponent />; // Some custom loading component

- return <div>Error: {error}</div>;
+ return <ErrorComponent message={error} />; // Some custom error component
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if (status === 'loading') {
+    return <div>Loading...</div>;
+  }
+
+  if (status === 'failed') {
+    return <div>Error: {error}</div>;
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408871,76.0,78,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/pages/salaryAdjustment/AccordionList.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

It's good that you're handling the case where there is no data. However, consider providing a more informative message or some action for the user to take when there is no data.
```diff
- return <CardBox>Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.</CardBox>;
+ return (
+   <CardBox>
+     <p>Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.</p>
+     <button onClick={fetchMoreData}>Load More Data</button>
+   </CardBox>
+ );
```
This way, users can try to fetch more data if they want.

Overall, the changes made in this file are significant and improve the functionality of the `AccordionList` component by integrating it with Redux and Firebase. The component now fetches data dynamically and handles different states (loading, error, no data) effectively. However, there are some areas where the code could be improved for better performance and user experience.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  } else {
+    return <CardBox>Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.</CardBox>;
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408874,1.0,96,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code looks good overall, but there are a few areas that could be improved for better maintainability and error handling.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+import { createSlice, createAsyncThunk } from '@reduxjs/toolkit';
+import { firebaseDB } from '../api/firebaseApp';
+import { get, ref, set } from 'firebase/database';
+
+const PATH = '/salaryAdjustments/sajo1234567';
+const dbRef = ref(firebaseDB, PATH);
+
+const salaryAdSlice = createSlice({
+  name: 'salaryAd',
+  initialState: {
+    data: [],
+    status: 'idle',
+    error: null,
+  } as SalaryAdState,
+  reducers: {},
+  extraReducers: (builder) => {
+    builder
+      .addCase(fetchSalaryAdData.pending, (state) => {
+        state.status = 'loading';
+      })
+      .addCase(fetchSalaryAdData.fulfilled, (state, action) => {
+        state.status = 'succeeded';
+        state.data = action.payload;
+      })
+      .addCase(fetchSalaryAdData.rejected, (state, action) => {
+        state.status = 'failed';
+        state.error = action.error.message || 'error: extraReducers fetch Ïã§Ìå®';
+      })
+      .addCase(deleteSalaryAdData.pending, (state) => {
+        state.status = 'loading';
+      })
+      .addCase(deleteSalaryAdData.fulfilled, (state, action) => {
+        state.status = 'succeeded';
+        console.log(action.payload);
+        state.data = action.payload;
+      })
+      .addCase(deleteSalaryAdData.rejected, (state, action) => {
+        state.status = 'failed';
+        state.error = action.error.message || 'error: extraReducers delete Ïã§Ìå®';
+      });
+  },
+});
+
+export const fetchSalaryAdData = createAsyncThunk('salaryAd/fetchSalaryAdData', async () => {
+  try {
+    const snapshot = await get(dbRef);
+    if (snapshot.exists()) {
+      return snapshot.val() as Data[];
+    } else {
+      return [];
+    }
+  } catch (error) {
+    throw new Error(error instanceof Error ? error.message : 'fetch Ïã§Ìå®');
+  }
+});
+
+export const deleteSalaryAdData = createAsyncThunk(
+  'salaryAd/deleteSalaryAdData',
+  async (id: string) => {
+    const snapshot = await get(ref(firebaseDB, PATH));
+    const state = snapshot.val() as Data[];
+    const updated = state.filter((item) => item.id !== id);
+    try {
+      await set(dbRef, updated);
+      return updated;
+    } catch (error) {
+      throw new Error(error instanceof Error ? error.message : 'delete Ïã§Ìå®');
+    }
+  }
+);
+
+interface File {
+  fileName: string;
+  fileUrl: string;
+}
+
+interface Data {
+  id: string;
+  category: 'Ï£ºÎßê / Í≥µÌú¥Ïùº Í∑ºÎ¨¥ ÏàòÎãπ' | 'ÏïºÍ∞Ñ Í∑ºÎ¨¥ ÏàòÎãπ(22:00-06:00)' | 'Ïó∞Ï∞® ÎàÑÎùΩ' | 'Í≤ΩÎπÑ Ï≤òÎ¶¨';
+  description: string;
+  endTime: string;
+  files: File[];
+  month: string;
+  requestTime: string;
+  startTime: string;
+  status: 'Í≤∞Ïû¨ÎåÄÍ∏∞' | 'Í≤∞Ïû¨ÏôÑÎ£å' | 'Î∞òÎ†§';
+}
+
+export interface SalaryAdState {
+  data: Data[] | [];
+  status: 'idle' | 'loading' | 'failed' | 'succeeded';
+  error: string | null;
+}
+
+// export const { addItem, delItem } = salaryAdSlice.actions;
+export default salaryAdSlice.reducer;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408876,5.0,6,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Hardcoding the database path (`PATH`) and reference (`dbRef`) at the top of the file might not be the best approach. It would be more flexible to pass the path as an argument to the async thunks `fetchSalaryAdData` and `deleteSalaryAdData`. This way, you can reuse these functions for different paths if needed in the future.
```diff
- const PATH = '/salaryAdjustments/sajo1234567';
- const dbRef = ref(firebaseDB, PATH);
```
Replace these lines with the following changes in your async thunks:



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+const PATH = '/salaryAdjustments/sajo1234567';
+const dbRef = ref(firebaseDB, PATH);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408877,44.0,54,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

```diff
- export const fetchSalaryAdData = createAsyncThunk('salaryAd/fetchSalaryAdData', async () => {
+ export const fetchSalaryAdData = createAsyncThunk('salaryAd/fetchSalaryAdData', async (path: string) => {
  try {
+     const dbRef = ref(firebaseDB, path);
    const snapshot = await get(dbRef);
    if (snapshot.exists()) {
      return snapshot.val() as Data[];
    } else {
      return [];
    }
  } catch (error) {
    throw new Error(error instanceof Error ? error.message : 'fetch Ïã§Ìå®');
  }
});
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const fetchSalaryAdData = createAsyncThunk('salaryAd/fetchSalaryAdData', async () => {
+  try {
+    const snapshot = await get(dbRef);
+    if (snapshot.exists()) {
+      return snapshot.val() as Data[];
+    } else {
+      return [];
+    }
+  } catch (error) {
+    throw new Error(error instanceof Error ? error.message : 'fetch Ïã§Ìå®');
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408880,57.0,69,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

```diff
- export const deleteSalaryAdData = createAsyncThunk(
-   'salaryAd/deleteSalaryAdData',
-   async (id: string) => {
+ export const deleteSalaryAdData = createAsyncThunk(
+   'salaryAd/deleteSalaryAdData',
+   async ({ id, path }: { id: string; path: string }) => {
    const snapshot = await get(ref(firebaseDB, path));
    const state = snapshot.val() as Data[];
    const updated = state.filter((item) => item.id !== id);
    try {
+       const dbRef = ref(firebaseDB, path);
      await set(dbRef, updated);
      return updated;
    } catch (error) {
      throw new Error(error instanceof Error ? error.message : 'delete Ïã§Ìå®');
    }
  }
);
```

34:
Avoid using `console.log` in production code. It's generally used for debugging and should be removed once the debugging process is over.
```diff
- console.log(action.payload);
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const deleteSalaryAdData = createAsyncThunk(
+  'salaryAd/deleteSalaryAdData',
+  async (id: string) => {
+    const snapshot = await get(ref(firebaseDB, PATH));
+    const state = snapshot.val() as Data[];
+    const updated = state.filter((item) => item.id !== id);
+    try {
+      await set(dbRef, updated);
+      return updated;
+    } catch (error) {
+      throw new Error(error instanceof Error ? error.message : 'delete Ïã§Ìå®');
+    }
+  }",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705408882,77.0,87,5f7380e0b18f07ae64f1df323623de9f568186d2,aa08067e555e6828a13c508cb7108cd666b97dfc,src/slices/salaryAdSlice.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Data` interface has a property `category` which is a string with specific possible values. Instead of using a string type, consider using TypeScript's `enum` or a union of string literals to enforce these specific values at compile time.

```diff
- category: 'Ï£ºÎßê / Í≥µÌú¥Ïùº Í∑ºÎ¨¥ ÏàòÎãπ' | 'ÏïºÍ∞Ñ Í∑ºÎ¨¥ ÏàòÎãπ(22:00-06:00)' | 'Ïó∞Ï∞® ÎàÑÎùΩ' | 'Í≤ΩÎπÑ Ï≤òÎ¶¨';
+ category: Category;

// Add this enum above the Data interface
enum Category {
  WeekendHolidayWork = 'Ï£ºÎßê / Í≥µÌú¥Ïùº Í∑ºÎ¨¥ ÏàòÎãπ',
  NightShift = 'ÏïºÍ∞Ñ Í∑ºÎ¨¥ ÏàòÎãπ(22:00-06:00)',
  MissingAnnualLeave = 'Ïó∞Ï∞® ÎàÑÎùΩ',
  SecurityProcessing = 'Í≤ΩÎπÑ Ï≤òÎ¶¨'
}
```
Do the same for the `status` property:

```diff
- status: 'Í≤∞Ïû¨ÎåÄÍ∏∞' | 'Í≤∞Ïû¨ÏôÑÎ£å' | 'Î∞òÎ†§';
+ status: Status;

// Add this enum above the Data interface
enum Status {
  Pending = 'Í≤∞Ïû¨ÎåÄÍ∏∞',
  Completed = 'Í≤∞Ïû¨ÏôÑÎ£å',
  Rejected = 'Î∞òÎ†§'
}
```

These changes will make your code more robust and easier to maintain.


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+interface Data {
+  id: string;
+  category: 'Ï£ºÎßê / Í≥µÌú¥Ïùº Í∑ºÎ¨¥ ÏàòÎãπ' | 'ÏïºÍ∞Ñ Í∑ºÎ¨¥ ÏàòÎãπ(22:00-06:00)' | 'Ïó∞Ï∞® ÎàÑÎùΩ' | 'Í≤ΩÎπÑ Ï≤òÎ¶¨';
+  description: string;
+  endTime: string;
+  files: File[];
+  month: string;
+  requestTime: string;
+  startTime: string;
+  status: 'Í≤∞Ïû¨ÎåÄÍ∏∞' | 'Í≤∞Ïû¨ÏôÑÎ£å' | 'Î∞òÎ†§';
+}",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705599156,16.0,21,45f7938a87d91ccf22886d5a2cecbde5260b440f,3033e0452451e354c2326cda5527341128de995a,src/pages/Dashboard/CalendarCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The changes made to the `CalendarCardWrapper` component's styling seem fine. The border has been added, padding is set to 0, and the position is now relative. The removal of the background color and the change in margin are also acceptable as they align with the new design requirements. However, please ensure that these changes do not affect the layout or appearance of other components that use `CalendarCardWrapper`.
```diff
-  background-color: var(--color-black);
-  border-radius: 30px;
-  color: var(--color-white);
+  border: 1px solid var(--border-sec);
+  padding: 0;
+  position: relative;
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  border: 1px solid var(--border-sec);
   height: 19.5rem;
+  padding: 0;
   overflow: hidden;
-  background-color: var(--color-black);
+  position: relative;
   margin: 1rem 0;",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705599161,3.0,19,45f7938a87d91ccf22886d5a2cecbde5260b440f,3033e0452451e354c2326cda5527341128de995a,src/pages/Dashboard/DashboardPage.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new code introduces a `SalaryCard` component but comments out the Redux store and schedule filtering logic. This could potentially break the functionality if the `SalaryCard` component relies on this data. If the commented-out code is not needed, it should be removed to maintain code cleanliness. If it's temporarily disabled for testing or debugging, consider adding a TODO comment to indicate that.

```diff
- // import { useSelector } from 'react-redux';
- // import { RootState } from '../../store/store';
- // import { ISchedule } from '../../slices/scheduleSlice';
+ import { useSelector } from 'react-redux';
+ import { RootState } from '../../store/store';
+ import { ISchedule } from '../../slices/scheduleSlice';

  function Dashboard() {
-   // const schedules = useSelector((state: RootState) => state.schedules.schedules);
-   // Ïò§ÎäòÏùò Ïä§ÏºÄÏ§Ñ ÌïÑÌÑ∞ÎßÅ
-   // const todaySchedules = schedules.filter(
-   //   (schedule: ISchedule) => schedule.startDate <= today && today <= schedule.endDate
-   // );
+   const schedules = useSelector((state: RootState) => state.schedules.schedules);
+   // Filter today's schedule
+   const todaySchedules = schedules.filter(
+     (schedule: ISchedule) => schedule.startDate <= today && today <= schedule.endDate
+   );

    return (
      <>
        <ProfileCard />
        <CalendarCard />
        <SalaryCard />
      </>
    );
  }
```
Please ensure that the `SalaryCard` component does not rely on the commented-out code. If it does, uncomment the necessary lines.


<!-- This is an auto-generated reply by OSS CodeRabbit -->","+import SalaryCard from './SalaryCard';
+// import { useSelector } from 'react-redux';
+// import { RootState } from '../../store/store';
+// import { ISchedule } from '../../slices/scheduleSlice';
 
 function Dashboard() {
-  const originDate = dayjs('2024-03-25');
-  const finalDate = originDate.format('YYYYÎÖÑ MMÏõî ');
-  const finalDay = originDate.subtract(0, 'day').format('DDÏùº');
-  const navigate = useNavigate();
-  const schedules = useSelector((state: RootState) => state.schedules.schedules);
-
-  const today = dayjs().format('YYYY-MM-DD');
-
-  const goToSalaryPage = () => navigate(`/salary-detail/3`);
-
-  // Ïò§ÎäòÏùò Ïä§ÏºÄÏ§ÑÎßå ÌïÑÌÑ∞ÎßÅÌï©ÎãàÎã§.
-  const todaySchedules = schedules.filter(
-    (schedule: ISchedule) => schedule.startDate <= today && today <= schedule.endDate
-  );
-
-  console.log(todaySchedules);
+  // const schedules = useSelector((state: RootState) => state.schedules.schedules);
+  // Ïò§ÎäòÏùò Ïä§ÏºÄÏ§Ñ ÌïÑÌÑ∞ÎßÅ
+  // const todaySchedules = schedules.filter(
+  //   (schedule: ISchedule) => schedule.startDate <= today && today <= schedule.endDate
+  // );
 
   return (
     <>
-      <Heading title=""ÎåÄÏãúÎ≥¥Îìú"" />
       <ProfileCard />
       <CalendarCard />
-      <NoticeCard
-        day={finalDay}
-        button={true}
-        label={
-          <h5>
-            <ListOutlinedIcon />
-            Í∏âÏó¨Î™ÖÏÑ∏ÏÑú Ï°∞Ìöå
-          </h5>
-        }
-        onClick={goToSalaryPage}
-      />
+      <SalaryCard />",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705599165,7.0,11,45f7938a87d91ccf22886d5a2cecbde5260b440f,3033e0452451e354c2326cda5527341128de995a,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The way you're handling dates could be simplified. Instead of creating a new Date object and then converting it to a dayjs object, you can directly use dayjs to get the current date and format it.
```diff
-  const currentDate = new Date();
-  const DueDate = new Date(currentDate.getFullYear(), currentDate.getMonth(), 25);
-  const originDate = dayjs(DueDate);
-  const finalMonth = originDate.format('MMÏõî ');
-  const finalDay = originDate.format('DDÏùº');
+  const dueDate = dayjs().date(25);
+  const finalMonth = dueDate.format('MMÏõî ');
+  const finalDay = dueDate.format('DDÏùº');
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const currentDate = new Date();
+  const DueDate = new Date(currentDate.getFullYear(), currentDate.getMonth(), 25);
+  const originDate = dayjs(DueDate);
+  const finalMonth = originDate.format('MMÏõî ');
+  const finalDay = originDate.format('DDÏùº');",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705599169,,15,45f7938a87d91ccf22886d5a2cecbde5260b440f,3033e0452451e354c2326cda5527341128de995a,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Hardcoding the salary detail page id as `3` might not be a good idea. It would be better if this value is passed as a prop or fetched from a state management system like Redux or Context API.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  const navigate = useNavigate();
+
+  const goToSalaryPage = () => navigate(`/salary-detail/3`);",
https://api.github.com/repos/Dev-FE-1/Toy_Project_II_team4/pulls/comments/1705599172,28.0,37,45f7938a87d91ccf22886d5a2cecbde5260b440f,3033e0452451e354c2326cda5527341128de995a,src/pages/Dashboard/SalaryCard.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comment on line 28 indicates that some styles are not being applied correctly. If this issue persists, consider revisiting your CSS rules to ensure they are not being overridden elsewhere in your codebase. Also, the color property on line 36 is set to red which might not align with the rest of your design. Please confirm if this is intentional.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  /* ÌïòÎã® Ïä§ÌÉÄÏùºÏù¥ Ï†ÅÏö©Ïù¥ ÎêòÍ≥† ÏûàÏßÄ ÏïäÏäµÎãàÎã§. */
+  display: flex;
+  border: 1px solid var(--border-sec);
+  height: 19.5rem;
+  padding: 0;
+  overflow: hidden;
+  position: relative;
+  margin: 1rem 0;
+  color: red;
+`;",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1905388603,33.0,34,f8c609bf8bfa7fec569b0a5430943f99a4f35a21,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Raising a generic `Exception` is not a best practice. Consider using a more specific exception type.
```diff
-             raise Exception(""A TOKEN is required if using Cloud API"")
+             raise ValueError(""A TOKEN is required if using Cloud API"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        if token is None:
+            raise Exception(""A TOKEN is required if using Cloud API"")","--- 

+++ 

@@ -31,7 +31,7 @@

     def __init__(self, token, sdk_url=None):
         self.sdk_url = sdk_url
         if token is None:
-            raise Exception(""A TOKEN is required if using Cloud API"")
+            raise ValueError(""A TOKEN is required if using Cloud API"")
         else:
             self.session = requests.Session()
             self.session.headers = {""Authorization"": ""Token "" + token}
@@ -82,7 +82,7 @@

 class ParkPowApi:
     def __init__(self, token, sdk_url=None):
         if token is None:
-            raise Exception(""ParkPow TOKEN is required if using Cloud API"")
+            raise ValueError(""ParkPow TOKEN is required if using Cloud API"")
         if sdk_url:
             self.api_base = sdk_url + ""/api/v1/""
         else:
@@ -105,7 +105,7 @@

                         time.sleep(1)
                     else:
                         lgr.error(response.text)
-                        raise Exception(""Error logging vehicle"")
+                        raise RuntimeError(""Error logging vehicle"")
                 else:
                     res_json = response.json()
                     return res_json"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1905388607,84.0,85,f8c609bf8bfa7fec569b0a5430943f99a4f35a21,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Raising a generic `Exception` is not a best practice. Consider using a more specific exception type.
```diff
-             raise Exception(""ParkPow TOKEN is required if using Cloud API"")
+             raise ValueError(""ParkPow TOKEN is required if using Cloud API"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        if token is None:
+            raise Exception(""ParkPow TOKEN is required if using Cloud API"")","--- 

+++ 

@@ -31,7 +31,7 @@

     def __init__(self, token, sdk_url=None):
         self.sdk_url = sdk_url
         if token is None:
-            raise Exception(""A TOKEN is required if using Cloud API"")
+            raise ValueError(""A TOKEN is required if using Cloud API"")
         else:
             self.session = requests.Session()
             self.session.headers = {""Authorization"": ""Token "" + token}
@@ -82,7 +82,7 @@

 class ParkPowApi:
     def __init__(self, token, sdk_url=None):
         if token is None:
-            raise Exception(""ParkPow TOKEN is required if using Cloud API"")
+            raise ValueError(""ParkPow TOKEN is required if using Cloud API"")
         if sdk_url:
             self.api_base = sdk_url + ""/api/v1/""
         else:
@@ -105,7 +105,7 @@

                         time.sleep(1)
                     else:
                         lgr.error(response.text)
-                        raise Exception(""Error logging vehicle"")
+                        raise RuntimeError(""Error logging vehicle"")
                 else:
                     res_json = response.json()
                     return res_json"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1905388610,106.0,108,f8c609bf8bfa7fec569b0a5430943f99a4f35a21,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Consider raising a more specific exception instead of a generic `Exception`.
```diff
-                         raise Exception(""Error logging vehicle"")
+                         raise RuntimeError(""Error logging vehicle"")
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                    else:
+                        lgr.error(response.text)
+                        raise Exception(""Error logging vehicle"")","--- 

+++ 

@@ -31,7 +31,7 @@

     def __init__(self, token, sdk_url=None):
         self.sdk_url = sdk_url
         if token is None:
-            raise Exception(""A TOKEN is required if using Cloud API"")
+            raise ValueError(""A TOKEN is required if using Cloud API"")
         else:
             self.session = requests.Session()
             self.session.headers = {""Authorization"": ""Token "" + token}
@@ -82,7 +82,7 @@

 class ParkPowApi:
     def __init__(self, token, sdk_url=None):
         if token is None:
-            raise Exception(""ParkPow TOKEN is required if using Cloud API"")
+            raise ValueError(""ParkPow TOKEN is required if using Cloud API"")
         if sdk_url:
             self.api_base = sdk_url + ""/api/v1/""
         else:
@@ -105,7 +105,7 @@

                         time.sleep(1)
                     else:
                         lgr.error(response.text)
-                        raise Exception(""Error logging vehicle"")
+                        raise RuntimeError(""Error logging vehicle"")
                 else:
                     res_json = response.json()
                     return res_json"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118834,14.0,20,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of the `initialize` method in `RedisCounterStrategy` class is more robust and maintainable. It now accepts a `redis_config` parameter and initializes the Redis connection pool accordingly. However, there's a potential issue with variable shadowing on line 20. The local variable `redis_config` inside the block shadows the method parameter `redis_config`. This could lead to unexpected behavior if you try to access the method parameter within the block.

```diff
-          redis_config = configuration.config
+          config = configuration.config
-          Redis.new(redis_config)
+          Redis.new(config)
```
This change will prevent the variable shadowing issue by renaming the local variable to `config`.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # @param redis_config [Hash]
       def initialize(redis_config = nil)
         super()
-        pool_size = DynamicLinks.configuration.redis_pool_size
-        pool_timeout = DynamicLinks.configuration.redis_pool_timeout
 
-        @redis = ConnectionPool.new(size: pool_size, timeout: pool_timeout) do
-          redis_config = redis_config.presence || DynamicLinks.configuration.redis_config
+        configuration = redis_config.nil? ? DynamicLinks.configuration.redis_counter_config : DynamicLinks::Configuration::RedisConfig.new(redis_config)
+        @redis = ConnectionPool.new(size: configuration.pool_size, timeout: configuration.pool_timeout) do
+          redis_config = configuration.config","--- 

+++ 

@@ -17,8 +17,7 @@

 
         configuration = redis_config.nil? ? DynamicLinks.configuration.redis_counter_config : DynamicLinks::Configuration::RedisConfig.new(redis_config)
         @redis = ConnectionPool.new(size: configuration.pool_size, timeout: configuration.pool_timeout) do
-          redis_config = configuration.config
-          Redis.new(redis_config)
+          Redis.new(configuration.config)
         end
       end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118844,1.0,31,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ShortenUrlJob` class seems to have a logical issue. The `client` object is passed as an argument to the `perform` method, but then it's overwritten on line 10 with `DynamicLinks::Client.find(client_id)`. However, `client_id` is not defined anywhere in this method. This will likely cause a NameError at runtime.

Also, the `cache_key` used in lines 7 and 13 is not defined within this method. It's unclear where this key is coming from, which could lead to unexpected behavior or errors.

```diff
-      client = DynamicLinks::Client.find(client_id)
+      # Assuming client_id should be extracted from the client object
+      client = DynamicLinks::Client.find(client.id)

-      data = cache_store.read(cache_key)
+      # Assuming cache_key should be constructed using some properties of the client or url
+      cache_key = ""some_key_derived_from_client_or_url""
+      data = cache_store.read(cache_key)

-      cache_store.delete(cache_key)
+      # Same assumption for deleting the key
+      cache_store.delete(cache_key)
```
Please ensure that the `client_id` and `cache_key` are properly defined and used.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+module DynamicLinks
+  class ShortenUrlJob < ApplicationJob
+    queue_as :default
+
+    def perform(client, url, short_url)
+      cache_store = ActiveSupport::Cache.lookup_store(DynamicLinks.configuration.cache_store, DynamicLinks.configuration.redis_config)
+      data = cache_store.read(cache_key)
+      return unless data
+
+      client = DynamicLinks::Client.find(client_id)
+      DynamicLinks.process_url_synchronously(data[:url], data[:short_url], client, StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy))
+
+      cache_store.delete(cache_key)
+    end
+  end
+end","--- 

+++ 

@@ -1,16 +1,31 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
+  # This job is used to create a shortened url
   class ShortenUrlJob < ApplicationJob
     queue_as :default
 
-    def perform(client, url, short_url)
-      cache_store = ActiveSupport::Cache.lookup_store(DynamicLinks.configuration.cache_store, DynamicLinks.configuration.redis_config)
-      data = cache_store.read(cache_key)
-      return unless data
+    def perform(client, url, short_url, lock_key)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
 
-      client = DynamicLinks::Client.find(client_id)
-      DynamicLinks.process_url_synchronously(data[:url], data[:short_url], client, StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy))
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end
+    end
 
-      cache_store.delete(cache_key)
+    private
+
+    def storage
+      @storage ||= ShortenedUrl
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118849,19.0,30,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/cache_store.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `write` method uses a case statement to determine how to interact with the cache store based on its class. This approach tightly couples the `CacheStore` class to the specific implementations of Redis and Memcached. Consider using polymorphism to make this code more flexible and easier to extend. You could define a base `CacheStore` class or interface that specifies the methods each cache store must implement (`write`, `read`, `delete`). Then, you can create separate classes for each cache store type (e.g., `RedisCacheStore`, `MemcachedCacheStore`) that implement these methods. This way, the `CacheStore` class doesn't need to know the details of how each cache store works.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def write(key, value, options = {})
+      case @store
+      when Redis
+        # Use Redis transaction commands
+        @store.multi do |multi|
+          multi.set(key, value, options)
+        end
+      when Memcached
+        # Use Memcached CAS command
+        @store.cas(key, value, options)
+      end
+    end",File_Deleted
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118851,32.0,38,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/cache_store.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the `write` method, the `read` and `delete` methods are also tightly coupled to the specific implementations of Redis and Memcached. The same suggestion about using polymorphism applies here. 

Overall, the current implementation violates the Open/Closed Principle of SOLID principles, which states that software entities should be open for extension but closed for modification. By using polymorphism, you can add support for new cache store types without modifying the existing `CacheStore` class.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def read(key)
+      @store.get(key)
+    end
+
+    def delete(key)
+      @store.del(key)
+    end",File_Deleted
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118859,53.0,78,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `simulate_shorten_url` method is doing a lot of work: configuring the system, setting up mocks, running the test, and verifying the results. It would be better to split this into smaller, more focused methods. For example, one method could handle configuration, another could set up the mocks, and another could run the test and verify the results. This would make the code easier to understand and maintain. 

Also, the cache store configuration is being passed as a parameter to the `simulate_shorten_url` method, but it's not clear where this configuration is coming from in the tests that call this method. It would be better to either pass this configuration directly in the tests or to set it up in a separate method. 

Lastly, the `simulate_shorten_url` method is expecting certain calls on the mock objects (`strategy_mock` and `cache_store_mock`), but these expectations are hard-coded. It would be better to pass these expected calls as parameters to the method, which would make the tests more flexible and easier to understand.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = strategy
+      config.async_processing = async
+      config.cache_store_config = cache_store_config
+    end
+
+    strategy_mock = Minitest::Mock.new
+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
+    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
+    strategy_mock.expect :always_growing?, false if !async
+
+    cache_store_mock = Minitest::Mock.new
+    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
+    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
+
+    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+      DynamicLinks::CacheStore.stub :new, cache_store_mock do
+        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+      end
+    end
+
+    strategy_mock.verify
+    cache_store_mock.verify if async
+  end","--- 

+++ 

@@ -2,78 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: {}
-    })
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      DynamicLinks::CacheStore.stub :new, cache_store_mock do
-        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118867,17.0,59,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `@cache_store_config` hash contains configuration for both Redis and Memcached. This could lead to confusion as it's not immediately clear which configuration will be used. Consider splitting this into two separate attributes: `@redis_config` and `@memcached_config`.

```diff
-    @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+    @redis_config = {}
+    @memcached_config = {}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+    end","--- 

+++ 

@@ -1,38 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443118876,29.0,61,fcb1391bbdbc0a8e99d93d27bae941231960c0bb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten_url` method has been significantly refactored to support both synchronous and asynchronous URL shortening. The logic seems correct, but there are a few potential issues:

1. There's no error handling for the case when the `StrategyFactory.get_strategy(strategy_key)` call fails. In the old code, there were several rescue blocks to handle different types of errors that could occur during strategy initialization. These should be reintroduced to ensure robust error handling.

2. The cache key generation (`cache_key = ""shorten_url:#{client.id}:#{short_url}""`) might lead to collisions if two different URLs from the same client get shortened to the same value. This is unlikely but possible with hash-based shortening strategies. Consider including the original URL in the cache key to avoid this issue.

3. The `ShortenUrlJob.perform_later(cache_key, client.id)` line assumes that the job queueing system (like Sidekiq or Resque) is properly configured and working. If it's not, this line will fail. It would be good to add some error handling here, or at least document this requirement clearly.

4. The `process_url_synchronously` method is called with four parameters, which is quite a lot. Consider passing a single parameter as a hash or an object to improve readability and maintainability.

Here are the suggested changes:

```diff
  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
    strategy_key = DynamicLinks.configuration.shortening_strategy
    strategy = begin
      StrategyFactory.get_strategy(strategy_key)
    rescue RuntimeError => e
      # This will catch the 'Unknown strategy' error from the factory
      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
    rescue ArgumentError
      raise ""#{strategy_key} strategy needs to be initialized with arguments""
    rescue StandardError => e
      raise ""Unexpected error while initializing the strategy: #{e.message}""
    end
 
    short_url = strategy.shorten(url)
 
    if async
      if !DynamicLinks.configuration.cache_store_enabled?
        raise ConfigurationError, 'Cache store is not configured'
      end
 
      # Store data in Redis/Memcached and enqueue background job
      cache_store = DynamicLinks::CacheStore.new(DynamicLinks.configuration.cache_store_config)
      cache_key = ""shorten_url:#{client.id}:#{url}:#{short_url}""
      cache_store.write(cache_key, { url: url, short_url: short_url })
 
      begin
        ShortenUrlJob.perform_later(cache_key, client.id)
      rescue => e
        raise ""Failed to enqueue job: #{e.message}""
      end
 
      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
    else
      # Synchronous processing
      process_url_synchronously({url: url, short_url: short_url, client: client, strategy: strategy})
     end
   end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = configuration.shortening_strategy
+    strategy_key = DynamicLinks.configuration.shortening_strategy
+    strategy = StrategyFactory.get_strategy(strategy_key)
 
-    strategy = begin
-      StrategyFactory.get_strategy(strategy_key)
-    rescue RuntimeError => e
-      # This will catch the 'Unknown strategy' error from the factory
-      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
-    rescue ArgumentError
-      raise ""#{strategy_key} strategy needs to be initialized with arguments""
-    rescue StandardError => e
-      raise ""Unexpected error while initializing the strategy: #{e.message}""
-    end
+    short_url = strategy.shorten(url)
 
-    if strategy.always_growing?
-      short_url = strategy.shorten(url)
+    if async
+      if !DynamicLinks.configuration.cache_store_enabled?
+        raise ConfigurationError, 'Cache store is not configured'
+      end
 
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
+      # Store data in Redis/Memcached and enqueue background job
+      cache_store = DynamicLinks::CacheStore.new(DynamicLinks.configuration.cache_store_config)
+      cache_key = ""shorten_url:#{client.id}:#{short_url}""
+      cache_store.write(cache_key, { url: url, short_url: short_url })
 
-    # If no existing record or always growing, generate new short URL
-    short_url = strategy.shorten(url)
-    record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-    if record.new_record?
-      record.url = url
-      record.save!
-      record
+      ShortenUrlJob.perform_later(cache_key, client.id)
+      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    else
+      # Synchronous processing
+      process_url_synchronously(url, short_url, client, strategy)
     end","--- 

+++ 

@@ -1,9 +1,32 @@

+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+if ENV['RAILS_ENV'] == 'test'
+  require 'simplecov'
+
+  SimpleCov.start do
+    load_profile ""test_frameworks""
+
+    add_filter %r{^/config/}
+    add_filter %r{^/db/}
+
+    add_group ""Controllers"", ""app/controllers""
+    add_group ""Channels"", ""app/channels""
+    add_group ""Models"", ""app/models""
+    add_group ""Mailers"", ""app/mailers""
+    add_group ""Helpers"", ""app/helpers""
+    add_group ""Jobs"", %w[app/jobs app/workers]
+    add_group ""DynamicLinks"", ""lib/""
+  end
+end
+
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
+require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require ""dynamic_links/cache_store""
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -11,7 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
-require ""dynamic_links/configuration""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -29,26 +53,11 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      if !DynamicLinks.configuration.cache_store_enabled?
-        raise ConfigurationError, 'Cache store is not configured'
-      end
-
-      # Store data in Redis/Memcached and enqueue background job
-      cache_store = DynamicLinks::CacheStore.new(DynamicLinks.configuration.cache_store_config)
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(cache_key, client.id)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
 
@@ -62,17 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162213,,16,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,Gemfile,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Adding the `sidekiq` gem is a good move for handling background jobs. However, it's important to ensure that your application has a proper setup for Sidekiq. This includes configuring Sidekiq with Redis and setting up a worker process to process the jobs. Also, remember to handle job failures and retries appropriately to prevent data loss or inconsistencies.


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," 
 gem 'sprockets-rails'
 
+gem 'sidekiq'","--- 

+++ 

@@ -12,7 +12,17 @@

 
 gem 'sprockets-rails'
 
+# for dummy app
 gem 'sidekiq'
+
+group :test do
+  gem 'simplecov', require: false
+  gem 'mocha'
+end
 
 # Start debugger with binding.b [https://github.com/ruby/debug]
 # gem ""debug"", "">= 1.0.0""
+
+if ENV['CITUS_ENABLED'] == 'true'
+  gem 'activerecord-multi-tenant'
+end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162221,8.0,10,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

While setting up the dummy client, it's important to ensure that the `api_key` is not hardcoded. This could lead to potential security issues if this benchmark script is checked into a public repository. Consider using environment variables or a secure method to store and retrieve sensitive data like API keys.

```diff
- 10: client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')
+ 10: client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: ENV['BENCHMARK_API_KEY'], hostname: 'example.com', scheme: 'http')
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# Dummy client setup
+client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')
+","--- 

+++ 

@@ -3,7 +3,6 @@

 
 DynamicLinks.configure do |config|
   config.shortening_strategy = :md5
-  config.cache_store_config = { type: :redis, redis_config: { host: 'redis' } }
 end
 
 # Dummy client setup
@@ -25,7 +24,7 @@

   x.compare!
 end
 
-# Results:
+# Results: 2023-01-05
 # ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
 # Warming up --------------------------------------
 #     sync shorten_url    32.036B i/100ms
@@ -37,3 +36,29 @@

 # Comparison:
 #    async shorten_url: 411673657787738.8 i/s
 #     sync shorten_url: 68100041231802.3 i/s - 6.05x  slower
+
+# Results: 2023-01-06
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    29.439B i/100ms
+#    async shorten_url2024-01-06T11:30:31.937Z pid=6316 tid=3uo INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    216.882B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     67.021T (¬±21.4%) i/s -    313.206T in   4.993776s
+#    async shorten_url      3.887Q (¬±21.7%) i/s -     17.554Q in   4.958253s
+# Comparison:
+#    async shorten_url: 3887497582634705.5 i/s
+#     sync shorten_url: 67020687656060.7 i/s - 58.00x  slower
+
+# When the cache is exist
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    21.002B i/100ms
+#    async shorten_url2024-01-06T11:32:12.036Z pid=6347 tid=3xj INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    830.542B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     49.498T (¬±20.3%) i/s -    232.909T in   4.993091s
+#    async shorten_url     15.310Q (¬±20.2%) i/s -     69.875Q in   4.957633s
+# Comparison:
+#    async shorten_url: 15309572098986642.0 i/s
+#     sync shorten_url: 49497616265721.5 i/s - 309.30x  slower"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162227,27.0,38,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The results of the benchmark are hardcoded in the script. It would be better to print these results dynamically after the benchmark has run. This way, you can ensure that the results always reflect the actual performance of the code at the time of execution.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# Results:
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    32.036B i/100ms
+#    async shorten_url    186.494B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     68.100T (¬±17.2%) i/s -    325.931T in   4.992942s
+#    async shorten_url    411.674T (¬±19.4%) i/s -      1.841Q in   4.992430s
+
+# Comparison:
+#    async shorten_url: 411673657787738.8 i/s
+#     sync shorten_url: 68100041231802.3 i/s - 6.05x  slower","--- 

+++ 

@@ -3,7 +3,6 @@

 
 DynamicLinks.configure do |config|
   config.shortening_strategy = :md5
-  config.cache_store_config = { type: :redis, redis_config: { host: 'redis' } }
 end
 
 # Dummy client setup
@@ -25,7 +24,7 @@

   x.compare!
 end
 
-# Results:
+# Results: 2023-01-05
 # ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
 # Warming up --------------------------------------
 #     sync shorten_url    32.036B i/100ms
@@ -37,3 +36,29 @@

 # Comparison:
 #    async shorten_url: 411673657787738.8 i/s
 #     sync shorten_url: 68100041231802.3 i/s - 6.05x  slower
+
+# Results: 2023-01-06
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    29.439B i/100ms
+#    async shorten_url2024-01-06T11:30:31.937Z pid=6316 tid=3uo INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    216.882B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     67.021T (¬±21.4%) i/s -    313.206T in   4.993776s
+#    async shorten_url      3.887Q (¬±21.7%) i/s -     17.554Q in   4.958253s
+# Comparison:
+#    async shorten_url: 3887497582634705.5 i/s
+#     sync shorten_url: 67020687656060.7 i/s - 58.00x  slower
+
+# When the cache is exist
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    21.002B i/100ms
+#    async shorten_url2024-01-06T11:32:12.036Z pid=6347 tid=3xj INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    830.542B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     49.498T (¬±20.3%) i/s -    232.909T in   4.993091s
+#    async shorten_url     15.310Q (¬±20.2%) i/s -     69.875Q in   4.957633s
+# Comparison:
+#    async shorten_url: 15309572098986642.0 i/s
+#     sync shorten_url: 49497616265721.5 i/s - 309.30x  slower"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162235,8.0,16,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/cache_store.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `initialize` method currently only supports Redis and Memcached as cache stores. If a different type is provided, it raises a generic error message. It would be more helpful to include the unsupported type in the error message to aid debugging.
```diff
- raise ""Unsupported cache store type""
+ raise ""Unsupported cache store type: #{config[:type]}""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def initialize(config)
+      @store = case config[:type]
+               when :redis
+                 Redis.new(config[:redis_config])
+               when :memcached
+                 Memcached.new(config[:memcached_config])
+               else
+                 raise ""Unsupported cache store type""
+               end",File_Deleted
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162241,30.0,38,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/cache_store.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `write` method checks if the `@store` object is an instance of `Redis` or `Memcached`. However, this check might not work as expected because `@store` is an instance of `Redis::Object` or `Dalli::Client`, not `Redis` or `Memcached`. Use `is_a?` instead of `when` for class checking.
```diff
- when Redis
+ when @store.is_a?(Redis)
```
```diff
- when Memcached
+ when @store.is_a?(Dalli::Client)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      when Redis
+        if options == {}
+          @store.set(key, value)
+        else
+          @store.set(key, value, options)
+        end
+      when Memcached
+        @store.cas(key, value, options)
+      end",File_Deleted
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162247,45.0,47,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/cache_store.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `delete` method, you're using `del` which is a method for `Redis` but not for `Memcached`. For `Memcached`, you should use `delete`. Add a condition to handle both cases.
```diff
- @store.del(key)
+ @store.is_a?(Redis) ? @store.del(key) : @store.delete(key)
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def delete(key)
+      @store.del(key)
+    end",File_Deleted
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162255,3.0,6,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `attr_accessor` method is used to define getter and setter methods for instance variables. However, it's not a good practice to expose all instance variables as public attributes. This can lead to violation of encapsulation principle in OOP. Consider using `attr_reader` or `attr_writer` for some variables if they are not meant to be both read and written from outside the class.

```diff
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_accessor :shortening_strategy, :enable_rest_api
+    attr_reader :db_infra_strategy, :async_processing, :cache_store, :redis_counter_config, :cache_store_config
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :cache_store,
+                  :redis_counter_config,
+                  :cache_store_config","--- 

+++ 

@@ -1,42 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162261,8.0,54,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `initialize` method, you're setting default values for the configuration. It's good that you've provided defaults, but it would be better to use constants for these default values. This way, if you need to change the default value, you only have to do it in one place.

```diff
-       @shortening_strategy = :MD5  # Default strategy
-       @enable_rest_api = true  # Enable REST API by default
-       @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-       @async_processing = false
-       @redis_counter_config = RedisConfig.new
-       @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+       @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+       @enable_rest_api = DEFAULT_ENABLE_REST_API
+       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+       @async_processing = DEFAULT_ASYNC_PROCESSING
+       @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+       @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     def initialize
       @shortening_strategy = :MD5  # Default strategy
-      @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-      @redis_pool_size = 5  # Default pool size
-      @redis_pool_timeout = 5  # Default timeout in seconds
       @enable_rest_api = true  # Enable REST API by default
+      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
+      @async_processing = false
+
+      # config for RedisCounterStrategy
+      @redis_counter_config = RedisConfig.new
+
+      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+    end","--- 

+++ 

@@ -1,42 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162264,20.0,59,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `cache_store_enabled?` method checks if the cache store type is not nil. This is a good way to check if the cache store is enabled, but it would be better to also check if the type is one of the supported types (Redis or Memcached).

```diff
-      !@cache_store_config[:type].nil?
+      [:redis, :memcached].include?(@cache_store_config[:type])
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def cache_store_enabled?
+      !@cache_store_config[:type].nil?
+    end","--- 

+++ 

@@ -1,42 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162269,24.0,64,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `cache_store` method, you're creating a new instance of `DynamicLinks::CacheStore` every time the method is called. This could lead to unnecessary memory usage and performance issues. Consider using memoization to only create a new instance if one doesn't already exist.

```diff
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+      @cache_store ||= begin
+        unless [:redis, :memcached].include?(@cache_store_config[:type])
+          raise ConfigurationError, ""Unsupported cache store type: #{@cache_store_config[:type]}""
+        end
+        DynamicLinks::CacheStore.new(@cache_store_config)
+      end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def cache_store
+      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    end","--- 

+++ 

@@ -1,42 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162275,28.0,39,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `RedisConfig` class is nested inside the `Configuration` class. While this is not necessarily a problem, it might make the code harder to navigate and understand. Consider moving it to its own file.

```diff
-     class RedisConfig
-       attr_accessor :config, :pool_size, :pool_timeout
- 
-       def initialize
-         # Default to an empty hash, can be overridden
-         @config = {
-           # host: 'localhost',
-           # port: 6379
-         }
-         @pool_size = 5          # Default pool size
-         @pool_timeout = 5       # Default timeout in seconds
-       end
-      end
+     # Moved to its own file: redis_config.rb
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      attr_accessor :config, :pool_size, :pool_timeout
+
+      def initialize
+        # Default to an empty hash, can be overridden
+        @config = {
+          # host: 'localhost',
+          # port: 6379
+        }
+        @pool_size = 5          # Default pool size
+        @pool_timeout = 5       # Default timeout in seconds
+      end
     end","--- 

+++ 

@@ -1,42 +1,71 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :cache_store,
-                  :redis_counter_config,
-                  :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
+
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @enable_rest_api = true  # Enable REST API by default
-      @db_infra_strategy = :standard  # Default DB infrastructure strategy (:standard, :citus)
-      @async_processing = false
-
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
       # config for RedisCounterStrategy
-      @redis_counter_config = RedisConfig.new
-
-      @cache_store_config = { type: nil, redis_config: {}, memcached_config: {} }
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      !@cache_store_config[:type].nil?
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= DynamicLinks::CacheStore.new(@cache_store_config)
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    class RedisConfig
-      attr_accessor :config, :pool_size, :pool_timeout
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
 
-      def initialize
-        # Default to an empty hash, can be overridden
-        @config = {
-          # host: 'localhost',
-          # port: 6379
-        }
-        @pool_size = 5          # Default pool size
-        @pool_timeout = 5       # Default timeout in seconds
-      end
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162283,29.0,61,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten_url` method has been significantly refactored to support both synchronous and asynchronous URL shortening. However, there are a few potential issues:

1. The `async` parameter is optional and defaults to the value of `DynamicLinks.configuration.async_processing`. This could lead to unexpected behavior if the configuration changes during runtime. It would be better to make `async` a required parameter to ensure that the caller explicitly specifies whether they want asynchronous processing.

2. The method raises a `ConfigurationError` if `async` is true but the cache store is not enabled. This is good for catching misconfigurations, but it might be better to check this at startup rather than at runtime.

3. The method uses a cache key based on the client ID and the shortened URL. This could potentially lead to collisions if two different URLs for the same client are shortened to the same value by the strategy. Consider including the original URL in the cache key to avoid this.

4. The method enqueues a background job with `ShortenUrlJob.perform_later(client, url, short_url)`. However, it doesn't handle the case where enqueuing the job fails (for example, if the job queue is full or unavailable). Consider adding error handling for this case.

5. The method returns a string representation of the shortened URL. This is fine for most use cases, but it might be more flexible to return a `URI` object instead, which callers can then convert to a string if necessary.

Here's a suggested revision of the method that addresses these issues:

```diff
-  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
+  def self.shorten_url(url, client, async:)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)

     strategy_key = DynamicLinks.configuration.shortening_strategy
     strategy = StrategyFactory.get_strategy(strategy_key)

     short_url = strategy.shorten(url)

     if async
-      if !DynamicLinks.configuration.cache_store_enabled?
+      unless DynamicLinks.configuration.cache_store_enabled?
         raise ConfigurationError, 'Cache store is not configured'
       end

-      # Store data in Redis/Memcached and enqueue background job
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
+      # Store data in the cache and enqueue a background job
+      cache_key = ""shorten_url:#{client.id}:#{url}:#{short_url}""
       DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })

-      ShortenUrlJob.perform_later(client, url, short_url)
+      begin
+        ShortenUrlJob.perform_later(client, url, short_url)
+      rescue StandardError => e
+        # Handle job queue errors here
+        raise ""Failed to enqueue job: #{e.message}""
+      end

-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""})
     else
       # Synchronous processing
       process_url_synchronously(url, short_url, client, strategy)
     end
   end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = configuration.shortening_strategy
+    strategy_key = DynamicLinks.configuration.shortening_strategy
+    strategy = StrategyFactory.get_strategy(strategy_key)
 
-    strategy = begin
-      StrategyFactory.get_strategy(strategy_key)
-    rescue RuntimeError => e
-      # This will catch the 'Unknown strategy' error from the factory
-      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
-    rescue ArgumentError
-      raise ""#{strategy_key} strategy needs to be initialized with arguments""
-    rescue StandardError => e
-      raise ""Unexpected error while initializing the strategy: #{e.message}""
-    end
+    short_url = strategy.shorten(url)
 
-    if strategy.always_growing?
-      short_url = strategy.shorten(url)
+    if async
+      if !DynamicLinks.configuration.cache_store_enabled?
+        raise ConfigurationError, 'Cache store is not configured'
+      end
 
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
+      # Store data in Redis/Memcached and enqueue background job
+      cache_key = ""shorten_url:#{client.id}:#{short_url}""
+      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
 
-    # If no existing record or always growing, generate new short URL
-    short_url = strategy.shorten(url)
-    record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-    if record.new_record?
-      record.url = url
-      record.save!
-      record
+      ShortenUrlJob.perform_later(client, url, short_url)
+      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    else
+      # Synchronous processing
+      process_url_synchronously(url, short_url, client, strategy)
     end","--- 

+++ 

@@ -1,9 +1,32 @@

+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+if ENV['RAILS_ENV'] == 'test'
+  require 'simplecov'
+
+  SimpleCov.start do
+    load_profile ""test_frameworks""
+
+    add_filter %r{^/config/}
+    add_filter %r{^/db/}
+
+    add_group ""Controllers"", ""app/controllers""
+    add_group ""Channels"", ""app/channels""
+    add_group ""Models"", ""app/models""
+    add_group ""Mailers"", ""app/mailers""
+    add_group ""Helpers"", ""app/helpers""
+    add_group ""Jobs"", %w[app/jobs app/workers]
+    add_group ""DynamicLinks"", ""lib/""
+  end
+end
+
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
+require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require ""dynamic_links/cache_store""
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -11,7 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
-require ""dynamic_links/configuration""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -29,25 +53,11 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      if !DynamicLinks.configuration.cache_store_enabled?
-        raise ConfigurationError, 'Cache store is not configured'
-      end
-
-      # Store data in Redis/Memcached and enqueue background job
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
 
@@ -61,17 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443162284,64.0,76,63efbb026d38a2393ac0b0ad19ee8704bfa43474,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `process_url_synchronously` method has been extracted from the `shorten_url` method. This improves modularity and makes the code easier to understand. However, there's a potential issue:

1. The method creates or updates a `ShortenedUrl` record based on whether the strategy is always growing. However, it doesn't handle the case where saving the record fails (for example, due to a database error). Consider adding error handling for this case.

Here's a suggested revision of the method that addresses this issue:

```diff
-  def self.process_url_synchronously(url, short_url, client, strategy)
+  def self.process_url_synchronously(url, short_url, client, strategy)
     if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+      begin
+        ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+      rescue ActiveRecord::RecordInvalid => e
+        # Handle validation errors here
+        raise ""Failed to create record: #{e.message}""
+      end
     else
       record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
       record.url = url if record.new_record?
-      record.save!
+      begin
+        record.save!
+      rescue ActiveRecord::RecordInvalid => e
+        # Handle validation errors here
+        raise ""Failed to save record: #{e.message}""
+      end
     end
     URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""})
   end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  private
+
+  def self.process_url_synchronously(url, short_url, client, strategy)
+    if strategy.always_growing?
+      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    else
+      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
+      record.url = url if record.new_record?
+      record.save!
+    end
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+  end","--- 

+++ 

@@ -1,9 +1,32 @@

+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+if ENV['RAILS_ENV'] == 'test'
+  require 'simplecov'
+
+  SimpleCov.start do
+    load_profile ""test_frameworks""
+
+    add_filter %r{^/config/}
+    add_filter %r{^/db/}
+
+    add_group ""Controllers"", ""app/controllers""
+    add_group ""Channels"", ""app/channels""
+    add_group ""Models"", ""app/models""
+    add_group ""Mailers"", ""app/mailers""
+    add_group ""Helpers"", ""app/helpers""
+    add_group ""Jobs"", %w[app/jobs app/workers]
+    add_group ""DynamicLinks"", ""lib/""
+  end
+end
+
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
+require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require ""dynamic_links/cache_store""
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -11,7 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
-require ""dynamic_links/configuration""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -29,25 +53,11 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      if !DynamicLinks.configuration.cache_store_enabled?
-        raise ConfigurationError, 'Cache store is not configured'
-      end
-
-      # Store data in Redis/Memcached and enqueue background job
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
 
@@ -61,17 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682774,1.0,22,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/redis_config.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `RedisConfig` class is missing a way to override the default configuration. It would be better to allow users to pass in their own configuration when initializing an instance of this class. This can be achieved by adding parameters to the `initialize` method.

```diff
-    def initialize
+    def initialize(config = {}, pool_size = 5, pool_timeout = 5)
-      @config = {
-        # host: 'localhost',
-        # port: 6379
-      }
+      @config = config
-      @pool_size = 5          # Default pool size
+      @pool_size = pool_size
-      @pool_timeout = 5       # Default timeout in seconds
+      @pool_timeout = pool_timeout
```
This change allows users to provide their own Redis configuration, pool size, and pool timeout values while providing sensible defaults if no values are provided.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+module DynamicLinks
+  class RedisConfig
+    attr_accessor :config, :pool_size, :pool_timeout
+
+    def initialize
+      # Default to an empty hash, can be overridden
+      @config = {
+        # host: 'localhost',
+        # port: 6379
+      }
+      @pool_size = 5          # Default pool size
+      @pool_timeout = 5       # Default timeout in seconds
+    end
+  end
+end","--- 

+++ 

@@ -1,16 +1,22 @@

 # @author Saiqul Haq <saiqulhaq@gmail.com>
+
 module DynamicLinks
+  # RedisConfig is a class to hold Redis configuration
   class RedisConfig
     attr_accessor :config, :pool_size, :pool_timeout
 
-    def initialize
-      # Default to an empty hash, can be overridden
-      @config = {
-        # host: 'localhost',
-        # port: 6379
-      }
-      @pool_size = 5          # Default pool size
-      @pool_timeout = 5       # Default timeout in seconds
+    # @param [Hash] config
+    # Default to an empty hash, can be overridden
+    #    config = {
+    #      host: 'localhost',
+    #      port: 6379
+    #    }
+    # @param [Integer] pool_size Default to 5, can be overridden
+    # @param [Integer] pool_timeout Default to 5, can be overridden
+    def initialize(config = {}, pool_size = 5, pool_timeout = 5)
+      @config = config
+      @pool_size = pool_size
+      @pool_timeout = pool_timeout
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682776,5.0,29,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `perform` method in the `ShortenUrlJob` class is directly interacting with the database. This could lead to potential issues if the job fails and needs to be retried, as it might create duplicate records. It's better to move this logic into a service object or model method that can handle these edge cases more gracefully.

```diff
- def perform(client, url, short_url)
-   record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-   record.url = url if record.new_record?
-   record.save!
- end
+ def perform(client, url, short_url)
+   ShortenedUrl.create_or_update(client: client, url: url, short_url: short_url)
+ end
```

In the `ShortenedUrl` model:

```ruby
def self.create_or_update(client:, url:, short_url:)
  record = find_or_initialize_by(client: client, short_url: short_url)
  record.url = url if record.new_record?
  record.save!
end
```
This way, you encapsulate the creation or update logic within the model itself, making the code more maintainable and easier to test.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def perform(client, url, short_url)
+      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
+      record.url = url if record.new_record?
+      record.save!
+    end","--- 

+++ 

@@ -1,11 +1,31 @@

 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
+  # This job is used to create a shortened url
   class ShortenUrlJob < ApplicationJob
     queue_as :default
 
-    def perform(client, url, short_url)
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
+    def perform(client, url, short_url, lock_key)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
+
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end
+    end
+
+    private
+
+    def storage
+      @storage ||= ShortenedUrl
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682778,51.0,61,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a potential issue with the asynchronous processing of URLs. The `ShortenUrlJob` is enqueued after writing to the cache, but there's no guarantee that the job will be executed immediately. If another request comes in with the same URL before the job is processed, it could result in duplicate short URLs being generated. To avoid this, consider using a locking mechanism or checking if a job for the same URL is already enqueued before enqueuing a new one.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    strategy_key = DynamicLinks.configuration.shortening_strategy
+    strategy = StrategyFactory.get_strategy(strategy_key)
 
-    if strategy.always_growing?
-      short_url = strategy.shorten(url)
+    short_url = strategy.shorten(url)
 
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
+    if async
+      # Store data in cache and enqueue background job
+      cache_key = ""shorten_url:#{client.id}:#{short_url}""
+      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
 
-    # If no existing record or always growing, generate new short URL
-    short_url = strategy.shorten(url)
-    record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-    if record.new_record?
-      record.url = url
-      record.save!
-      record
+      ShortenUrlJob.perform_later(client, url, short_url)
+      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    else
+      # Synchronous processing
+      process_url_synchronously(url, short_url, client, strategy)
     end","--- 

+++ 

@@ -1,3 +1,5 @@

+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
 if ENV['RAILS_ENV'] == 'test'
   require 'simplecov'
 
@@ -19,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
-require ""dynamic_links/redis_config""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -34,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -51,21 +53,11 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      # Store data in cache and enqueue background job
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
 
@@ -79,18 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  # TODO Handle issue when failed to save record
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682779,85.0,95,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comment on line 85 indicates an unhandled scenario where saving the record might fail. It's important to handle this case to ensure data integrity and provide meaningful feedback to the user. Consider adding error handling around the `save!` method call to catch any exceptions and handle them appropriately.
```diff
-      record.save!
+      begin
+        record.save!
+      rescue ActiveRecord::RecordInvalid => e
+        # Handle exception, e.g., log error and return meaningful message to the user
+      end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  # TODO Handle issue when failed to save record
+  def self.process_url_synchronously(url, short_url, client, strategy)
+    if strategy.always_growing?
+      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    else
+      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
+      record.url = url if record.new_record?
+      record.save!
+    end
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+  end","--- 

+++ 

@@ -1,3 +1,5 @@

+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
 if ENV['RAILS_ENV'] == 'test'
   require 'simplecov'
 
@@ -19,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
-require ""dynamic_links/redis_config""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -34,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -51,21 +53,11 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      # Store data in cache and enqueue background job
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      DynamicLinks.configuration.cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
 
@@ -79,18 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  # TODO Handle issue when failed to save record
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682780,7.0,10,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setup method is missing a call to `super`. This could potentially cause issues if the superclass's setup method contains important initialization logic. It's a good practice to always call `super` in `setup` and `teardown` methods when using ActiveSupport::TestCase.
```diff
    @original_async = DynamicLinks.configuration.async_processing
    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
     @client = dynamic_links_clients(:one)
+    super
   end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @original_async = DynamicLinks.configuration.async_processing
+    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
     @client = dynamic_links_clients(:one)
   end","--- 

+++ 

@@ -2,78 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: {}
-    })
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682781,14.0,16,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the setup method, the teardown method should also call `super` at the end.
```diff
     DynamicLinks.configuration.shortening_strategy = @original_strategy
    DynamicLinks.configuration.async_processing = @original_async
    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
+    super
   end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","     DynamicLinks.configuration.shortening_strategy = @original_strategy
+    DynamicLinks.configuration.async_processing = @original_async
+    DynamicLinks.configuration.cache_store_config = @original_cache_store_config","--- 

+++ 

@@ -2,78 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: {}
-    })
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682782,53.0,78,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This test helper method `simulate_shorten_url` seems to be doing a lot of things - configuring the DynamicLinks module, setting up mocks, stubbing methods, and even making assertions. This violates the Single Responsibility Principle (SRP) and makes the method harder to understand and maintain. Consider breaking it down into smaller, more focused methods.
For example, you could have one method for configuration, another for setting up mocks, and another for making assertions. This would make your tests cleaner and easier to read and maintain.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = strategy
+      config.async_processing = async
+      config.cache_store_config = cache_store_config
+    end
+
+    strategy_mock = Minitest::Mock.new
+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
+    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
+    strategy_mock.expect :always_growing?, false if !async
+
+    cache_store_mock = Minitest::Mock.new
+    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
+    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
+
+    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+      DynamicLinks.configuration.stub :cache_store, cache_store_mock do
+        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+      end
+    end
+
+    strategy_mock.verify
+    cache_store_mock.verify if async
+  end","--- 

+++ 

@@ -2,78 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: {}
-    })
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy, async, cache_store_config = { type: nil, redis_config: {}, memcached_config: {} })
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-        assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682783,4.0,5,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `attr_accessor` method is used to define multiple attributes at once. However, it's generally a good practice to keep each attribute on its own line for better readability and easier tracking of changes in version control systems.

```diff
- attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-               :async_processing, :redis_counter_config, :cache_store_config
+ attr_accessor :shortening_strategy
+ attr_accessor :enable_rest_api
+ attr_accessor :db_infra_strategy
+ attr_accessor :async_processing
+ attr_accessor :redis_counter_config
+ attr_accessor :cache_store_config
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store_config","--- 

+++ 

@@ -1,46 +1,71 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
-    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
       @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
-      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-                         case config[:type]
-                         when :redis
-                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
-                         when :memcached
-                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
-                         end
-                       end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682785,11.0,12,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `DEFAULT_REDIS_COUNTER_CONFIG` and `DEFAULT_CACHE_STORE_CONFIG` are defined as a lambda and a hash respectively. It would be more consistent to use the same approach for both. Also, it's not clear why a lambda is needed for `DEFAULT_REDIS_COUNTER_CONFIG`. If there's no specific reason, consider using a hash for both.

```diff
- DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
- DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+ DEFAULT_REDIS_COUNTER_CONFIG = { config: RedisConfig.new }
+ DEFAULT_CACHE_STORE_CONFIG = { type: nil, config: { redis: {}, memcached: {} } }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
+    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }","--- 

+++ 

@@ -1,46 +1,71 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
-    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
       @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
-      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-                         case config[:type]
-                         when :redis
-                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
-                         when :memcached
-                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
-                         end
-                       end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443682787,35.0,42,53664e479de24bcabfbf1c4fdd12423bafb5865e,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `cache_store` method, you're referring to `config` but it seems like it should be `@cache_store_config`. This could lead to an error if `config` is not defined elsewhere in the scope.

```diff
- case config[:type]
+ case @cache_store_config[:type]

- DynamicLinks::RedisCacheStore.new(config[:redis_config])
+ DynamicLinks::RedisCacheStore.new(@cache_store_config[:config][:redis])

- DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
+ DynamicLinks::MemcachedCacheStore.new(@cache_store_config[:config][:memcached])

- raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
+ raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{@cache_store_config[:type]}""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+                         case config[:type]
+                         when :redis
+                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
+                         when :memcached
+                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
+                         else
+                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
+                         end","--- 

+++ 

@@ -1,46 +1,71 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
-    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
       @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
-      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-                         case config[:type]
-                         when :redis
-                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
-                         when :memcached
-                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
-                         end
-                       end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727266,,9,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create_by!` method can raise an exception if the record is invalid. Make sure to handle this exception properly in your application.
```diff
- client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')
+ begin
+   client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')
+ rescue ActiveRecord::RecordInvalid => e
+   puts ""Failed to create client: #{e.message}""
+ end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+end
+
+# Dummy client setup
+client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')","--- 

+++ 

@@ -3,7 +3,6 @@

 
 DynamicLinks.configure do |config|
   config.shortening_strategy = :md5
-  config.cache_store_config = { type: :redis, redis_config: { host: 'redis' } }
 end
 
 # Dummy client setup"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727270,14.0,22,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the benchmarking script, you are using the same client for both synchronous and asynchronous URL shortening. This might lead to data races if the client state is modified during URL shortening. Consider creating separate clients for each benchmark.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  x.config(time: 5, warmup: 2)
+
+  x.report(""sync shorten_url"") do |times|
+    DynamicLinks.shorten_url(""https://example.com/#{times}"", client, async: false)
+  end
+
+  x.report(""async shorten_url"") do |times|
+    DynamicLinks.shorten_url(""https://example-async.com/#{times}"", client, async: true)
+  end","--- 

+++ 

@@ -3,7 +3,6 @@

 
 DynamicLinks.configure do |config|
   config.shortening_strategy = :md5
-  config.cache_store_config = { type: :redis, redis_config: { host: 'redis' } }
 end
 
 # Dummy client setup"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727273,28.0,64,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These results are hardcoded into the script. It would be better to generate these dynamically or save them separately so that they don't clutter the benchmarking script. Also, it's not clear what the ""When the cache is exist"" comment means. Please clarify or remove it.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    32.036B i/100ms
+#    async shorten_url    186.494B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     68.100T (¬±17.2%) i/s -    325.931T in   4.992942s
+#    async shorten_url    411.674T (¬±19.4%) i/s -      1.841Q in   4.992430s
+
+# Comparison:
+#    async shorten_url: 411673657787738.8 i/s
+#     sync shorten_url: 68100041231802.3 i/s - 6.05x  slower
+
+# Results: 2023-01-06
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    29.439B i/100ms
+#    async shorten_url2024-01-06T11:30:31.937Z pid=6316 tid=3uo INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    216.882B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     67.021T (¬±21.4%) i/s -    313.206T in   4.993776s
+#    async shorten_url      3.887Q (¬±21.7%) i/s -     17.554Q in   4.958253s
+# Comparison:
+#    async shorten_url: 3887497582634705.5 i/s
+#     sync shorten_url: 67020687656060.7 i/s - 58.00x  slower
+
+# When the cache is exist
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    21.002B i/100ms
+#    async shorten_url2024-01-06T11:32:12.036Z pid=6347 tid=3xj INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    830.542B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     49.498T (¬±20.3%) i/s -    232.909T in   4.993091s
+#    async shorten_url     15.310Q (¬±20.2%) i/s -     69.875Q in   4.957633s
+# Comparison:
+#    async shorten_url: 15309572098986642.0 i/s
+#     sync shorten_url: 49497616265721.5 i/s - 309.30x  slower","--- 

+++ 

@@ -3,7 +3,6 @@

 
 DynamicLinks.configure do |config|
   config.shortening_strategy = :md5
-  config.cache_store_config = { type: :redis, redis_config: { host: 'redis' } }
 end
 
 # Dummy client setup"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727276,7.0,8,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setup method is storing the original configuration values for `async_processing` and `cache_store_config`. However, it's not clear if these values are being used anywhere else in the tests. If they're not needed, consider removing these lines to simplify the test setup.
```diff
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @original_async = DynamicLinks.configuration.async_processing
+    @original_cache_store_config = DynamicLinks.configuration.cache_store_config","--- 

+++ 

@@ -2,103 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # clear cache store every run
-  def before_all
-    DynamicLinks.configuration.cache_store.clear
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727281,13.0,15,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `before_all` method is not a standard Rails test lifecycle method. It seems like you want to clear the cache store before each test run. You should use `setup` or `before_each` instead.
```diff
-  def before_all
+  def setup
     DynamicLinks.configuration.cache_store.clear
   end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def before_all
+    DynamicLinks.configuration.cache_store.clear
+  end","--- 

+++ 

@@ -2,103 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # clear cache store every run
-  def before_all
-    DynamicLinks.configuration.cache_store.clear
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727283,20.0,22,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the teardown method, you're resetting the configuration options to their original values. This is good practice as it ensures that changes made in one test do not affect others.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    DynamicLinks.configuration.async_processing = @original_async
+    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
   end","--- 

+++ 

@@ -2,103 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # clear cache store every run
-  def before_all
-    DynamicLinks.configuration.cache_store.clear
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727284,64.0,103,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `simulate_shorten_url` method is quite complex and does a lot of things: it configures the DynamicLinks module, creates mock objects, sets expectations on those mocks, and runs assertions. Consider breaking this method down into smaller, more focused methods to improve readability and maintainability. For example, you could have separate methods for creating the mocks and setting expectations, configuring DynamicLinks, and running the assertions.

Also, it's important to note that the `simulate_shorten_url` method is stubbing the `cache_store` on the `DynamicLinks.configuration` object. This means that all calls to `DynamicLinks.configuration.cache_store` within the scope of the stub will return the `cache_store_mock`, which may not be what you want if there are other parts of your code that depend on the real `cache_store`. Be careful when using stubs in this way, as it can lead to brittle tests that break when the implementation changes.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def simulate_shorten_url(strategy,
+                           async,
+                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
+                           lock_key_exists = false)
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = strategy
+      config.async_processing = async
+      config.cache_store_config = cache_store_config
+    end
+
+    strategy_mock = Minitest::Mock.new
+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
+    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
+    strategy_mock.expect :always_growing?, false if !async
+
+    cache_store_mock = Minitest::Mock.new
+    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
+    lock_key = ""lock:shorten_url:#{expected_short_path}""
+    cache_store_mock.expect :read, lock_key_exists, [lock_key]
+    if lock_key_exists
+      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
+          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+        end
+      end
+    else
+      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
+      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
+
+      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
+          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+        end
+      end
+    end
+
+    strategy_mock.verify
+    cache_store_mock.verify if async
+  end","--- 

+++ 

@@ -2,103 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # clear cache store every run
-  def before_all
-    DynamicLinks.configuration.cache_store.clear
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727286,4.0,59,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Configuration` class has been significantly refactored. The new configuration options seem to be well thought out and provide more flexibility for the users of this library. However, there is a potential issue with the `DEFAULT_REDIS_COUNTER_CONFIG` attribute. It's currently set to a lambda that creates a new instance of `RedisConfig`. If `RedisConfig` requires any parameters for initialization or if its initialization has side effects, this could lead to unexpected behavior. Consider changing this to a safer default value or ensure that `RedisConfig.new` can always safely be called without parameters.

```diff
- DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
+ DEFAULT_REDIS_COUNTER_CONFIG = nil
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   class Configuration
-    attr_accessor :shortening_strategy, :redis_config,
-                  :redis_pool_size, :redis_pool_timeout,
-                  :enable_rest_api
+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store_config
+
+    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
+    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
 
+    # Usage: DynamicLinks.configure do |config|
+    #          config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #          config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #          config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #          config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #          config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #          if you use Redis
+    #          config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
+    #          if you use Memcached
+    #          config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
+    #        end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-      @redis_pool_size = 5  # Default pool size
-      @redis_pool_timeout = 5  # Default timeout in seconds
-      @enable_rest_api = true  # Enable REST API by default
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
+
+      # config for RedisCounterStrategy
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
+      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+    end","--- 

+++ 

@@ -1,28 +1,31 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
+  class Configuration
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-  class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
-    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
-    # Usage: DynamicLinks.configure do |config|
-    #          config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
-    #          config.enable_rest_api = true # or false. when false, the API requests will be rejected
-    #          config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
-    #          config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
-    #          config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
-    #          if you use Redis
-    #          config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
-    #          if you use Memcached
-    #          config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
-    #        end
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
     #
     # @return [Configuration]
     def initialize
@@ -30,33 +33,39 @@

       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
-      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-                         config = cache_store_config
-                         case config[:type]
-                         when :redis
-                           require 'redis'
-                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
-                         when :memcached
-                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
-                         end
-                       end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727290,39.0,39,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `cache_store` method seems to be handling the creation of cache stores based on the configuration. This is generally fine, but it might be better to separate the creation of different types of cache stores into their own methods for better readability and maintainability. Also, consider adding error handling for when the required libraries ('redis' or 'memcached') are not available.

```ruby
def cache_store
  @cache_store ||= begin
    unless cache_store_enabled?
      raise ConfigurationError, 'Cache store is not configured'
    end

    case cache_store_config[:type]
    when :redis
      create_redis_cache_store(cache_store_config[:redis_config])
    when :memcached
      create_memcached_cache_store(cache_store_config[:memcached_config])
    else
      raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{cache_store_config[:type]}""
    end
  end
end

private

def create_redis_cache_store(config)
  require 'redis'
  DynamicLinks::RedisCacheStore.new(config)
rescue LoadError
  raise DynamicLinks::MissingDependency, ""Please install the 'redis' gem to use Redis as cache store""
end

def create_memcached_cache_store(config)
  DynamicLinks::MemcachedCacheStore.new(config)
rescue LoadError
  raise DynamicLinks::MissingDependency, ""Please install the 'memcached' gem to use Memcached as cache store""
end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->",     end,"--- 

+++ 

@@ -1,28 +1,31 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
+  class Configuration
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-  class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
-    DEFAULT_REDIS_COUNTER_CONFIG = -> { RedisConfig.new }
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
-    # Usage: DynamicLinks.configure do |config|
-    #          config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
-    #          config.enable_rest_api = true # or false. when false, the API requests will be rejected
-    #          config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
-    #          config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
-    #          config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
-    #          if you use Redis
-    #          config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
-    #          if you use Memcached
-    #          config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
-    #        end
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
     #
     # @return [Configuration]
     def initialize
@@ -30,33 +33,39 @@

       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
-      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG.call
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
 
-                         config = cache_store_config
-                         case config[:type]
-                         when :redis
-                           require 'redis'
-                           DynamicLinks::RedisCacheStore.new(config[:redis_config])
-                         when :memcached
-                           DynamicLinks::MemcachedCacheStore.new(config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{config[:type]}""
-                         end
-                       end
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443727294,93.0,106,f989cc012d0e70089de3d7d90d5feb8b09191209,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The comment on line 96 indicates that there's an unhandled issue when saving the record fails. This could lead to inconsistencies between the cache and the database. You should handle this case by catching exceptions during the save operation and removing the corresponding entry from the cache if an error occurs. Here's a suggestion:

```diff
-    # TODO Handle issue when failed to save record
     def self.process_url_synchronously(url, short_url, client, strategy)
       if strategy.always_growing?
         ShortenedUrl.create!(client: client, url: url, short_url: short_url)
       else
         record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
         record.url = url if record.new_record?
-        record.save!
+        begin
+          record.save!
+        rescue => e
+          # Remove the corresponding entry from the cache
+          cache_key = ""shorten_url:#{client.id}:#{short_url}""
+          DynamicLinks.configuration.cache_store.delete(cache_key)
+          raise e
+        end
       end
       URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
     end
```

This way, if saving the record fails, the URL won't be left in the cache without a corresponding database record.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  private
+
+  # TODO Handle issue when failed to save record
+  def self.process_url_synchronously(url, short_url, client, strategy)
+    if strategy.always_growing?
+      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    else
+      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
+      record.url = url if record.new_record?
+      record.save!
+    end
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+  end","--- 

+++ 

@@ -21,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
-require ""dynamic_links/redis_config""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -36,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -53,32 +53,13 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      lock_key = ""lock:shorten_url:#{short_url}""
-      cache_store = DynamicLinks.configuration.cache_store
-
-      if cache_store.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-      end
-
-      # Set a lock and store data in cache
-      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
-
 
   # mimic Firebase Dynamic Links API
   def self.generate_short_url(original_url, client)
@@ -90,18 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  # TODO Handle issue when failed to save record
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729341,24.0,33,f54cf91138e8e8a62befa00fb657e55f33852cfb,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `create_or_update` method could potentially raise an exception if the save operation fails. It's a good practice to handle such exceptions and provide meaningful error messages. Also, it seems like the URL is only updated when the record is new. If the intention is to update the URL for existing records as well, this condition might need to be revised.
```diff
    def self.create_or_update(client, short_url, url)
      record = find_or_initialize_by(client: client, short_url: short_url)
- 27:       record.url = url if record.new_record?
+ 27:       record.url = url
      begin
        record.save!
      rescue ActiveRecord::RecordInvalid => e
        # Log or re-raise with a custom message
        raise ""Failed to create or update ShortenedUrl: #{e.message}""
      end
    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    def self.create_or_update(client, short_url, url)
+      record = find_or_initialize_by(client: client, short_url: short_url)
+      record.url = url if record.new_record?
+      record.save!
+    end","--- 

+++ 

@@ -17,15 +17,23 @@

 #
 module DynamicLinks
   class ShortenedUrl < ApplicationRecord
-    belongs_to :client, optional: true
+    belongs_to :client
+    multi_tenant :client if respond_to?(:multi_tenant)
 
     validates :url, presence: true
     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
-    def self.create_or_update(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      record.url = url if record.new_record?
-      record.save!
+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729343,1.0,31,f54cf91138e8e8a62befa00fb657e55f33852cfb,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ShortenUrlJob` class seems to be well implemented. However, there is a potential issue with error handling. If the `create_or_update` method on line 8 fails for any reason (e.g., database connection issues), the lock key will not be deleted from the cache store. This could lead to a situation where the URL cannot be shortened again because the system thinks it's still being processed.

Consider wrapping the `create_or_update` call in a begin-rescue block and ensure that the lock key deletion happens in an ensure block. This way, even if an exception occurs, the lock key will always be deleted.

```diff
    def perform(client, url, short_url, lock_key)
+      begin
        ShortenedUrl.create_or_update(client, short_url, url)
+      rescue => e
+        # Log the error or re-raise it as needed
+        raise e
+      ensure
        # delete the lock key
        DynamicLinks.configuration.cache_store.delete(lock_key)
+      end
    end
```
This ensures that the lock key is always deleted, regardless of whether an error occurred during the creation or updating of the shortened URL.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+module DynamicLinks
+  class ShortenUrlJob < ApplicationJob
+    queue_as :default
+
+    def perform(client, url, short_url, lock_key)
+      ShortenedUrl.create_or_update(client, short_url, url)
+
+      # delete the lock key
+      DynamicLinks.configuration.cache_store.delete(lock_key)
+    end
+  end
+end","--- 

+++ 

@@ -1,14 +1,31 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
-
 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
+  # This job is used to create a shortened url
   class ShortenUrlJob < ApplicationJob
     queue_as :default
 
     def perform(client, url, short_url, lock_key)
-      ShortenedUrl.create_or_update(client, short_url, url)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
 
-      # delete the lock key
-      DynamicLinks.configuration.cache_store.delete(lock_key)
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end
+    end
+
+    private
+
+    def storage
+      @storage ||= ShortenedUrl
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729345,93.0,104,f54cf91138e8e8a62befa00fb657e55f33852cfb,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `process_url_synchronously` method has a TODO comment on line 96 about handling issues when failing to save a record. It's important to handle this case to prevent data inconsistency. Consider adding error handling or logging to ensure that any issues are caught and addressed.

```diff
  # TODO Handle issue when failed to save record
  def self.process_url_synchronously(url, short_url, client, strategy)
    if strategy.always_growing?
      begin
        ShortenedUrl.create!(client: client, url: url, short_url: short_url)
      rescue => e
        Rails.logger.error(""Failed to create ShortenedUrl: #{e.message}"")
      end
    else
      begin
        ShortenedUrl.create_or_update(client, short_url, url)
      rescue => e
        Rails.logger.error(""Failed to create or update ShortenedUrl: #{e.message}"")
      end
    end
    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
  end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  private
+
+  # TODO Handle issue when failed to save record
+  def self.process_url_synchronously(url, short_url, client, strategy)
+    if strategy.always_growing?
+      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    else
+      ShortenedUrl.create_or_update(client, short_url, url)
+    end
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+  end","--- 

+++ 

@@ -21,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
-require ""dynamic_links/redis_config""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -36,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -53,32 +53,13 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      lock_key = ""lock:shorten_url:#{short_url}""
-      cache_store = DynamicLinks.configuration.cache_store
-
-      if cache_store.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-      end
-
-      # Set a lock and store data in cache
-      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
-
 
   # mimic Firebase Dynamic Links API
   def self.generate_short_url(original_url, client)
@@ -90,16 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  # TODO Handle issue when failed to save record
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      ShortenedUrl.create_or_update(client, short_url, url)
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729764,24.0,33,60c63b71aad5ca82a9a16b57ca6cc640ce569f6a,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create` method could potentially raise an exception if the record fails to save. It would be better to use `save` instead of `save!` and handle the case where the record is not saved successfully.
```diff
-      record.save!
+      unless record.save
+        # Handle error here, e.g., log it or raise a custom exception
+      end
```
This way, you can control how your application behaves when a record fails to save, rather than letting it crash with an unhandled exception.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    def self.find_or_create(client, short_url, url)
+      record = find_or_initialize_by(client: client, short_url: short_url)
+      return record if record.persisted?
+
+      record.url = url
+      record.save!
+    end","--- 

+++ 

@@ -17,17 +17,23 @@

 #
 module DynamicLinks
   class ShortenedUrl < ApplicationRecord
-    belongs_to :client, optional: true
+    belongs_to :client
+    multi_tenant :client if respond_to?(:multi_tenant)
 
     validates :url, presence: true
     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
-    def self.find_or_create(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      record.save!
+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729766,1.0,31,60c63b71aad5ca82a9a16b57ca6cc640ce569f6a,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ShortenUrlJob` class seems to be well implemented. However, there is a potential issue with the `perform` method. It's not handling any exceptions that might occur during the execution of `ShortenedUrl.find_or_create(client, short_url, url)` or `DynamicLinks.configuration.cache_store.delete(lock_key)`. This could lead to unhandled exceptions causing job failures. 

Consider wrapping these operations in a begin-rescue block to handle potential exceptions and ensure the job doesn't fail unexpectedly.

```diff
    def perform(client, url, short_url, lock_key)
+      begin
      ShortenedUrl.find_or_create(client, short_url, url)

      # delete the lock key
      DynamicLinks.configuration.cache_store.delete(lock_key)
+      rescue => e
+        # Log the error and re-raise it if necessary
+        Rails.logger.error(""Error in ShortenUrlJob: #{e.message}"")
+        raise
+      end
    end
```
This way, you can log the error for debugging purposes and decide whether to re-raise the exception based on your error handling strategy.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+module DynamicLinks
+  class ShortenUrlJob < ApplicationJob
+    queue_as :default
+
+    def perform(client, url, short_url, lock_key)
+      ShortenedUrl.find_or_create(client, short_url, url)
+
+      # delete the lock key
+      DynamicLinks.configuration.cache_store.delete(lock_key)
+    end
+  end
+end","--- 

+++ 

@@ -1,14 +1,31 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
-
 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
+  # This job is used to create a shortened url
   class ShortenUrlJob < ApplicationJob
     queue_as :default
 
     def perform(client, url, short_url, lock_key)
-      ShortenedUrl.find_or_create(client, short_url, url)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
 
-      # delete the lock key
-      DynamicLinks.configuration.cache_store.delete(lock_key)
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end
+    end
+
+    private
+
+    def storage
+      @storage ||= ShortenedUrl
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443729768,93.0,104,60c63b71aad5ca82a9a16b57ca6cc640ce569f6a,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `process_url_synchronously` method, you have a TODO comment about handling issues when failing to save a record. It's important to handle these cases to prevent data inconsistency and ensure reliability. You can use exception handling to catch any errors that occur during the creation of the `ShortenedUrl`.

```diff
def self.process_url_synchronously(url, short_url, client, strategy)
  if strategy.always_growing?
    begin
      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
    rescue ActiveRecord::RecordInvalid => e
      # Handle error, possibly log it and/or re-raise as a custom error
      raise UrlProcessingError, ""Failed to create shortened URL record: #{e.message}""
    end
  else
    ShortenedUrl.find_or_create(client, short_url, url)
  end
  URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
end
```
In this example, `UrlProcessingError` is a custom error that you would need to define. This allows for better error handling and makes it easier to identify and debug issues related to URL processing.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+  private
+
+  # TODO Handle issue when failed to save record
+  def self.process_url_synchronously(url, short_url, client, strategy)
+    if strategy.always_growing?
+      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
+    else
+      ShortenedUrl.find_or_create(client, short_url, url)
+    end
+    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+  end","--- 

+++ 

@@ -21,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
-require ""dynamic_links/redis_config""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -36,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -53,32 +53,13 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      lock_key = ""lock:shorten_url:#{short_url}""
-      cache_store = DynamicLinks.configuration.cache_store
-
-      if cache_store.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-      end
-
-      # Set a lock and store data in cache
-      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
-
 
   # mimic Firebase Dynamic Links API
   def self.generate_short_url(original_url, client)
@@ -90,16 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  # TODO Handle issue when failed to save record
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      ShortenedUrl.find_or_create(client, short_url, url)
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443749707,30.0,31,cad41f3e9090c7ccd051fff9f607d8ef86809bdf,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is no error handling for when the record fails to save. This could lead to unexpected behavior or crashes in your application. Consider adding error handling here.
```diff
      record.url = url
-       # TODO Handle issue when failed to save record
-       record.save!
+       begin
+         record.save!
+       rescue ActiveRecord::RecordInvalid => e
+         # Log the error and handle it appropriately
+         Rails.logger.error(""Failed to save record: #{e.message}"")
+         # You might want to re-raise the exception, return a specific value, etc.
+         raise
+       end
    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # TODO Handle issue when failed to save record
+      record.save!","--- 

+++ 

@@ -17,18 +17,23 @@

 #
 module DynamicLinks
   class ShortenedUrl < ApplicationRecord
-    belongs_to :client, optional: true
+    belongs_to :client
+    multi_tenant :client if respond_to?(:multi_tenant)
 
     validates :url, presence: true
     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
-    def self.find_or_create(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      # TODO Handle issue when failed to save record
-      record.save!
+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443749708,7.0,16,cad41f3e9090c7ccd051fff9f607d8ef86809bdf,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setup and teardown methods have been updated to store and restore the original configuration for `async_processing` and `cache_store_config`. This is a good practice as it ensures that each test runs with a clean state and does not affect other tests.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    @original_async = DynamicLinks.configuration.async_processing
+    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
     @client = dynamic_links_clients(:one)
   end
 
+  # Reset the configuration after each test
   def teardown
-    # Reset the configuration after each test
     DynamicLinks.configuration.shortening_strategy = @original_strategy
+    DynamicLinks.configuration.async_processing = @original_async
+    DynamicLinks.configuration.cache_store_config = @original_cache_store_config","--- 

+++ 

@@ -2,100 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    DynamicLinks.configuration.cache_store.clear if async
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443749709,23.0,36,cad41f3e9090c7ccd051fff9f607d8ef86809bdf,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new tests are checking if the URL shortening process works correctly both synchronously and asynchronously. It's good to see that different scenarios are being tested, including when the lock key is empty or not. However, I would recommend adding more assertions in these tests to verify the behavior of the system under these conditions. For example, you could check if the cache store contains the expected keys after the operation, or if the job has been enqueued correctly when the operation is asynchronous.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
+    simulate_shorten_url(:mock, false)
+  end
 
-    DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-      assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-    end
+  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
+    simulate_shorten_url(:mock, true, {
+      type: :redis, redis_config: { host: 'redis', port: 6379 }
+    }, false)
+  end
 
-    strategy_mock.verify
+  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
+    simulate_shorten_url(:mock, true, {
+      type: :redis, redis_config: { host: 'redis', port: 6379 }
+    }, true)","--- 

+++ 

@@ -2,100 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    DynamicLinks.configuration.cache_store.clear if async
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443749710,59.0,100,cad41f3e9090c7ccd051fff9f607d8ef86809bdf,67675ef2633001974317dae9fc570ecf17a48936,test/dynamic_links_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `simulate_shorten_url` method is used to simulate the URL shortening process and assert the results. It's a good practice to extract this kind of logic into a separate method to avoid code duplication. However, the method is quite complex and might be hard to understand and maintain. Consider breaking it down into smaller methods, each responsible for a specific part of the process (e.g., configuring the system, preparing the mocks, running the operation, asserting the results). Also, consider adding comments to explain the purpose of each step, especially for complex operations like the ones involving mocks and stubs.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def simulate_shorten_url(strategy,
+                           async,
+                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
+                           lock_key_exists = false)
+    DynamicLinks.configure do |config|
+      config.shortening_strategy = strategy
+      config.async_processing = async
+      config.cache_store_config = cache_store_config
+    end
+
+    DynamicLinks.configuration.cache_store.clear if async
+
+    strategy_mock = Minitest::Mock.new
+    expected_short_path = 'shortened_url'
+    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
+    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
+    strategy_mock.expect :always_growing?, false if !async
+
+    cache_store_mock = Minitest::Mock.new
+    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
+    lock_key = ""lock:shorten_url:#{expected_short_path}""
+    cache_store_mock.expect :read, lock_key_exists, [lock_key]
+    if lock_key_exists
+      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
+          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+        end
+      end
+    else
+      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
+      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
+
+      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
+        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
+          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
+        end
+      end
+    end
+
+    strategy_mock.verify
+    cache_store_mock.verify if async
+  end","--- 

+++ 

@@ -2,100 +2,7 @@

 require ""minitest/mock""
 
 class DynamicLinksTest < ActiveSupport::TestCase
-  def setup
-    @original_strategy = DynamicLinks.configuration.shortening_strategy
-    @original_async = DynamicLinks.configuration.async_processing
-    @original_cache_store_config = DynamicLinks.configuration.cache_store_config
-    @client = dynamic_links_clients(:one)
-  end
-
-  # Reset the configuration after each test
-  def teardown
-    DynamicLinks.configuration.shortening_strategy = @original_strategy
-    DynamicLinks.configuration.async_processing = @original_async
-    DynamicLinks.configuration.cache_store_config = @original_cache_store_config
-  end
-
   test ""it has a version number"" do
     assert DynamicLinks::VERSION
   end
-
-  test ""shorten_url invokes the correct strategy and shortens URL synchronously"" do
-    simulate_shorten_url(:mock, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, false)
-  end
-
-  test ""shorten_url invokes the correct strategy and shortens URL asynchronously with condition lock key is not empty"" do
-    simulate_shorten_url(:mock, true, {
-      type: :redis, redis_config: { host: 'redis', port: 6379 }
-    }, true)
-  end
-
-  test ""generate_short_url returns the correct structure"" do
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = :mock
-    end
-
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    expected_response = {
-      shortLink: full_short_url,
-      previewLink: ""#{full_short_url}?preview=true"",
-      warning: []
-    }
-
-    DynamicLinks.stub :shorten_url, full_short_url do
-      assert_equal expected_response, DynamicLinks.generate_short_url('https://example.com', @client)
-    end
-  end
-
-  private
-
-  def simulate_shorten_url(strategy,
-                           async,
-                           cache_store_config = DynamicLinks::Configuration::DEFAULT_CACHE_STORE_CONFIG,
-                           lock_key_exists = false)
-    DynamicLinks.configure do |config|
-      config.shortening_strategy = strategy
-      config.async_processing = async
-      config.cache_store_config = cache_store_config
-    end
-
-    DynamicLinks.configuration.cache_store.clear if async
-
-    strategy_mock = Minitest::Mock.new
-    expected_short_path = 'shortened_url'
-    full_short_url = ""#{@client.scheme}://#{@client.hostname}/#{expected_short_path}""
-    strategy_mock.expect :shorten, expected_short_path, ['https://example.com']
-    strategy_mock.expect :always_growing?, false if !async
-
-    cache_store_mock = Minitest::Mock.new
-    cache_key = ""shorten_url:#{@client.id}:#{expected_short_path}""
-    lock_key = ""lock:shorten_url:#{expected_short_path}""
-    cache_store_mock.expect :read, lock_key_exists, [lock_key]
-    if lock_key_exists
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    else
-      cache_store_mock.expect :write, nil, [lock_key, 'locked', { expires_in: 10.minutes }]
-      cache_store_mock.expect :write, nil, [cache_key, { url: 'https://example.com', short_url: expected_short_path }]
-
-      DynamicLinks::ShorteningStrategies::MockStrategy.stub :new, strategy_mock do
-        DynamicLinks.configuration.stub :cache_store, cache_store_mock do
-          assert_equal full_short_url, DynamicLinks.shorten_url('https://example.com', @client)
-        end
-      end
-    end
-
-    strategy_mock.verify
-    cache_store_mock.verify if async
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1443749711,53.0,62,cad41f3e9090c7ccd051fff9f607d8ef86809bdf,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten_url` method has been significantly refactored to support both synchronous and asynchronous URL shortening. However, there are a few potential issues with the new implementation.

1. The cache lock mechanism (lines 60-70) is not entirely safe. If the job fails or takes longer than 10 minutes, the lock will be released, and another job could start processing the same URL. This could lead to race conditions and inconsistent data. Consider using a more robust distributed lock system that can handle job failures and long-running jobs.

2. The error handling for the `ShortenUrlJob.perform_later` call on line 74 is missing. If the job fails to enqueue or an error occurs during its execution, it should be handled gracefully.

3. The return statement on line 75 will execute before the job finishes, which might confuse users as the URL may not be shortened yet. Consider returning a message indicating that the URL is being processed.

4. The `async` parameter in the `shorten_url` method defaults to `DynamicLinks.configuration.async_processing`. It's good practice to validate this configuration setting to ensure it's a boolean value.

Here are some suggestions to improve these areas:

```diff
  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
    strategy_key = DynamicLinks.configuration.shortening_strategy
    strategy = StrategyFactory.get_strategy(strategy_key)
    short_url = strategy.shorten(url)
 
    if async
+       raise ArgumentError, 'Invalid async configuration' unless [true, false].include? async
      lock_key = ""lock:shorten_url:#{short_url}""
      cache_store = DynamicLinks.configuration.cache_store
 
      if cache_store.read(lock_key)
        # Return the short url if it is already in the cache
        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
      end
 
      # Set a lock and store data in cache
      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
      cache_key = ""shorten_url:#{client.id}:#{short_url}""
      cache_store.write(cache_key, { url: url, short_url: short_url })

      begin
+           ShortenUrlJob.perform_later(client, url, short_url, lock_key)
+       rescue => e
+           # Handle job enqueueing failure here
+           raise ""Failed to enqueue job: #{e.message}""
+       end
-       URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+       ""URL is being processed. Please check back later.""
    else
      # Synchronous processing
      process_url_synchronously(url, short_url, client, strategy)
     end
   end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        # Return the short url if it is already in the cache
+        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      end
 
-    # If no existing record or always growing, generate new short URL
-    short_url = strategy.shorten(url)
-    record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-    if record.new_record?
-      record.url = url
-      record.save!
-      record
+      # Set a lock and store data in cache
+      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
+      cache_key = ""shorten_url:#{client.id}:#{short_url}""
+      cache_store.write(cache_key, { url: url, short_url: short_url })
+
+      ShortenUrlJob.perform_later(client, url, short_url, lock_key)
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    else
+      # Synchronous processing
+      process_url_synchronously(url, short_url, client, strategy)
     end
-    return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
   end","--- 

+++ 

@@ -21,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
 require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
@@ -36,6 +34,8 @@

 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""
 
 module DynamicLinks
   class << self
@@ -53,32 +53,13 @@

   def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = DynamicLinks.configuration.shortening_strategy
-    strategy = StrategyFactory.get_strategy(strategy_key)
-    short_url = strategy.shorten(url)
-
+    shortener = Shortener.new
     if async
-      lock_key = ""lock:shorten_url:#{short_url}""
-      cache_store = DynamicLinks.configuration.cache_store
-
-      if cache_store.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-      end
-
-      # Set a lock and store data in cache
-      cache_store.write(lock_key, 'locked', { expires_in: 10.minutes })
-      cache_key = ""shorten_url:#{client.id}:#{short_url}""
-      cache_store.write(cache_key, { url: url, short_url: short_url })
-
-      ShortenUrlJob.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      shortener.shorten_async(client, url)
     else
-      # Synchronous processing
-      process_url_synchronously(url, short_url, client, strategy)
+      shortener.shorten(client, url)
     end
   end
-
 
   # mimic Firebase Dynamic Links API
   def self.generate_short_url(original_url, client)
@@ -90,15 +71,4 @@

       warning: []
     }
   end
-
-  private
-
-  def self.process_url_synchronously(url, short_url, client, strategy)
-    if strategy.always_growing?
-      ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-    else
-      ShortenedUrl.find_or_create(client, short_url, url)
-    end
-    URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584985,28.0,37,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `multi_tenant` method is a good abstraction for handling multi-tenancy. However, it's tightly coupled with the `DynamicLinks` configuration. Consider passing the `db_infra_strategy` as an argument to make this method more reusable and testable.
```diff
-    def multi_tenant(client)
-      if DynamicLinks.configuration.db_infra_strategy == :citus
+    def multi_tenant(client, db_infra_strategy)
+      if db_infra_strategy == :citus
         MultiTenant.with(client) do
           yield
         end
       else
         yield
       end
     end
```
Then you can call it like this: `multi_tenant(client, DynamicLinks.configuration.db_infra_strategy)`. This way, the method doesn't need to know about the `DynamicLinks` configuration and can be used in other contexts if needed.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    def multi_tenant(client)
+      if DynamicLinks.configuration.db_infra_strategy == :citus
+        MultiTenant.with(client) do
+          yield
+        end
+      else
+        yield
+      end
+    end","--- 

+++ 

@@ -16,6 +16,9 @@

       end
     rescue DynamicLinks::InvalidURIError
       render json: { error: 'Invalid URL' }, status: :bad_request
+    rescue => e
+      DynamicLinks::Logger.log_error(e)
+      render json: { error: 'An error occurred while processing your request' }, status: :internal_server_error
     end
 
     private
@@ -26,8 +29,8 @@

       end
     end
 
-    def multi_tenant(client)
-      if DynamicLinks.configuration.db_infra_strategy == :citus
+    def multi_tenant(client, db_infra_strategy = DynamicLinks.configuration.db_infra_strategy)
+      if db_infra_strategy == :sharding
         MultiTenant.with(client) do
           yield
         end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584988,26.0,33,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create` method could potentially raise an exception if the record fails to save. It would be better to handle this scenario gracefully, perhaps by returning a meaningful error message or logging the error for debugging purposes.

```diff
    def self.find_or_create(client, short_url, url)
      record = find_or_initialize_by(client: client, short_url: short_url)
      return record if record.persisted?

      record.url = url
      # TODO Handle issue when failed to save record
- 32:       record.save!
+ 32:       if record.save
+ 33:         record
+ 34:       else
+ 35:         Rails.logger.error(""Failed to save record: #{record.errors.full_messages.join("", "")}"")
+ 36:         nil
+ 37:       end
    end
```
This change will log the error messages if the record fails to save and return `nil`. The calling code should then handle the `nil` case appropriately.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.find_or_create(client, short_url, url)
+      record = find_or_initialize_by(client: client, short_url: short_url)
+      return record if record.persisted?
+
+      record.url = url
+      # TODO Handle issue when failed to save record
+      record.save!
+    end","--- 

+++ 

@@ -23,13 +23,17 @@

     validates :url, presence: true
     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
-    def self.find_or_create(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      # TODO Handle issue when failed to save record
-      record.save!
+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584992,66.0,96,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,README.md,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The instructions for running the unit tests are clear and well-detailed. However, there seems to be an error message included at the end of the instructions (lines 94-95). If this is a common error that users might encounter, it would be helpful to provide some context or troubleshooting tips. If it's not meant to be there, it should be removed.
```diff
-94: > /usr/local/bundle/gems/activerecord-7.1.2/lib/active_record/connection_adapters/postgresql_adapter.rb:894:in `exec_params': ERROR:  schema columnar is not a member of extension ""citus_columnar"" (PG::ObjectNotInPrerequisiteState)
-95: > DETAIL:  An extension may only use CREATE ... IF NOT EXISTS to skip object creation if the conflicting object is one that it already owns.
```
If it's a common error, consider adding something like this:
```markdown
If you encounter the following error:

> /usr/local/bundle/gems/activerecord-7.1.2/lib/active_record/connection_adapters/postgresql_adapter.rb:894:in `exec_params': ERROR:  schema columnar is not a member of extension ""citus_columnar"" (PG::ObjectNotInPrerequisiteState)
> DETAIL:  An extension may only use CREATE ... IF NOT EXISTS to skip object creation if the conflicting object is one that it already owns.

This means [explanation of the error and how to fix it].
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+## Performance
+
+Shorten an URL using Ruby:
+Shorten an URL using API:
+
+## How to run the unit test
+
+### When using a Plain PostgreSQL DB
+
+```bash
+rails db:setup
+rails db:test:prepare
+rails test
+```
+
+### When using PostgreSQL DB with Citus
+
+```bash
+export CITUS_ENABLED=true
+rails db:setup 
+rails db:test:prepare
+rails test
+```
+
+Note:
+Make sure the Citus extension already enabled on the installed PostgreSQL
+We don't manage it on Rails. I have spent hours to understand
+
+> /usr/local/bundle/gems/activerecord-7.1.2/lib/active_record/connection_adapters/postgresql_adapter.rb:894:in `exec_params': ERROR:  schema columnar is not a member of extension ""citus_columnar"" (PG::ObjectNotInPrerequisiteState)
+> DETAIL:  An extension may only use CREATE ... IF NOT EXISTS to skip object creation if the conflicting object is one that it already owns.
 ","--- 

+++ 

@@ -82,18 +82,14 @@

 
 ```bash
 export CITUS_ENABLED=true
-rails db:setup 
+rails db:setup
 rails db:test:prepare
 rails test
 ```
 
 Note:
 Make sure the Citus extension already enabled on the installed PostgreSQL
-We don't manage it on Rails. I have spent hours to understand
-
-> /usr/local/bundle/gems/activerecord-7.1.2/lib/active_record/connection_adapters/postgresql_adapter.rb:894:in `exec_params': ERROR:  schema columnar is not a member of extension ""citus_columnar"" (PG::ObjectNotInPrerequisiteState)
-> DETAIL:  An extension may only use CREATE ... IF NOT EXISTS to skip object creation if the conflicting object is one that it already owns.
-
+We don't manage it on Rails.
 
 ## License
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584993,4.0,39,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Configuration` class has been significantly refactored. The old attributes `redis_config`, `redis_pool_size`, and `redis_pool_timeout` have been removed, and new ones like `shortening_strategy`, `enable_rest_api`, `db_infra_strategy`, `async_processing`, `redis_counter_config`, and `cache_store_config` have been added. This change seems to be in line with the PR's goal of introducing asynchronous URL shortening and supporting different cache stores (Redis and Memcached). However, it's important to ensure that these changes don't break existing functionality and are properly tested.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   class Configuration
-    attr_accessor :shortening_strategy, :redis_config,
-                  :redis_pool_size, :redis_pool_timeout,
-                  :enable_rest_api
+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store_config
+
+    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
 
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
+    #       # if you use Memcached
+    #       config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-      @redis_pool_size = 5  # Default pool size
-      @redis_pool_timeout = 5  # Default timeout in seconds
-      @enable_rest_api = true  # Enable REST API by default
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
+
+      # config for RedisCounterStrategy
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+    end","--- 

+++ 

@@ -1,16 +1,18 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
+  class Configuration
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-  class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
     DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
     # Usage:
     #     DynamicLinks.configure do |config|
@@ -20,9 +22,9 @@

     #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
     #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
     #       # if you use Redis
-    #       config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
     #       # if you use Memcached
-    #       config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
     #     end
     #
     # @return [Configuration]
@@ -31,46 +33,39 @@

       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
       @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
-
-                         case cache_store_config[:type]
-                         when :redis
-                           create_redis_cache_store(cache_store_config[:redis_config])
-                         when :memcached
-                           create_memcached_cache_store(cache_store_config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{cache_store_config[:type]}""
-                         end
-                       end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    private
-
-    def create_redis_cache_store(config)
-      require 'redis'
-      DynamicLinks::RedisCacheStore.new(config)
-    rescue LoadError
-      raise DynamicLinks::MissingDependency, ""Please install the 'redis' gem to use Redis as cache store""
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
     end
 
-    def create_memcached_cache_store(config)
-      DynamicLinks::MemcachedCacheStore.new(config)
-    rescue LoadError
-      raise DynamicLinks::MissingDependency, ""Please install the 'memcached' gem to use Memcached as cache store""
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584994,40.0,39,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The methods `cache_store_enabled?`, `cache_store`, `create_redis_cache_store`, and `create_memcached_cache_store` have been added. These methods handle the creation of the appropriate cache store based on the configuration. It's a good practice to encapsulate this logic within the `Configuration` class. However, please ensure that exceptions are handled appropriately in the application where these methods are used. For instance, if the cache store is not configured or an unsupported type is provided, an exception will be raised. 

One potential improvement could be to move the `require 'redis'` and `require 'memcached'` statements to the top of the file, outside the methods. This would make it clear at the beginning of the file that these dependencies are required, rather than potentially causing a `LoadError` at runtime.

```diff
+ require 'redis'
+ require 'memcached'

module DynamicLinks
  class Configuration
    # ...
    
    private

    def create_redis_cache_store(config)
-     require 'redis'
      DynamicLinks::RedisCacheStore.new(config)
    rescue LoadError
      raise DynamicLinks::MissingDependency, ""Please install the 'redis' gem to use Redis as cache store""
    end

    def create_memcached_cache_store(config)
-     require 'memcached'
      DynamicLinks::MemcachedCacheStore.new(config)
    rescue LoadError
      raise DynamicLinks::MissingDependency, ""Please install the 'memcached' gem to use Memcached as cache store""
    end
  end
end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      raise DynamicLinks::MissingDependency, ""Please install the 'memcached' gem to use Memcached as cache store""
     end","--- 

+++ 

@@ -1,16 +1,18 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
+  class Configuration
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
 
-  class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store_config
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
     DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
-    DEFAULT_CACHE_STORE_CONFIG = { type: nil, redis_config: {}, memcached_config: {} }
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
     # Usage:
     #     DynamicLinks.configure do |config|
@@ -20,9 +22,9 @@

     #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
     #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
     #       # if you use Redis
-    #       config.cache_store_config = { type: :redis, redis_config: { host: 'localhost', port: 6379 } }
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
     #       # if you use Memcached
-    #       config.cache_store_config = { type: :memcached, memcached_config: { host: 'localhost', port: 6379 } }
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
     #     end
     #
     # @return [Configuration]
@@ -31,46 +33,39 @@

       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
       @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
-      @cache_store_config = DEFAULT_CACHE_STORE_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end
 
-    def cache_store_enabled?
-      [:redis, :memcached].include?(@cache_store_config[:type])
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
     end
 
-    def cache_store
-      @cache_store ||= begin
-                         unless cache_store_enabled?
-                           raise ConfigurationError, 'Cache store is not configured'
-                         end
-
-                         case cache_store_config[:type]
-                         when :redis
-                           create_redis_cache_store(cache_store_config[:redis_config])
-                         when :memcached
-                           create_memcached_cache_store(cache_store_config[:memcached_config])
-                         else
-                           raise DynamicLinks::UnknownCacheStoreType, ""Unsupported cache store type: #{cache_store_config[:type]}""
-                         end
-                       end
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
     end
 
-    private
-
-    def create_redis_cache_store(config)
-      require 'redis'
-      DynamicLinks::RedisCacheStore.new(config)
-    rescue LoadError
-      raise DynamicLinks::MissingDependency, ""Please install the 'redis' gem to use Redis as cache store""
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
     end
 
-    def create_memcached_cache_store(config)
-      DynamicLinks::MemcachedCacheStore.new(config)
-    rescue LoadError
-      raise DynamicLinks::MissingDependency, ""Please install the 'memcached' gem to use Memcached as cache store""
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451584997,8.0,13,7f6a534d4b37815a0875c8ea5b51fec84dd7b2ec,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The Redis configuration is hard-coded in the initializer. This should be moved to environment variables for better security and flexibility across different environments.
```diff
-  config.redis_counter_config.config = {
-    host: ""redis"",
-    port: 6379,
-    db: 1,
-  }
+  config.redis_counter_config.config = {
+    host: ENV['REDIS_HOST'] || 'localhost',
+    port: ENV['REDIS_PORT'] || 6379,
+    db: ENV['REDIS_DB'] || 0,
+  }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  config.redis_counter_config.pool_size = 5
+  config.redis_counter_config.pool_timeout = 5
+  config.redis_counter_config.config = {
+    host: ""redis"",
+    port: 6379,
+    db: 1,","--- 

+++ 

@@ -2,14 +2,14 @@

   config.shortening_strategy = ENV['SHORTENING_STRATEGY'].to_sym
   config.enable_rest_api = true
   if ENV['CITUS_ENABLED'].to_s == 'true'
-    config.db_infra_strategy = :citus
+    config.db_infra_strategy = :sharding
   end
 
   config.redis_counter_config.pool_size = 5
   config.redis_counter_config.pool_timeout = 5
   config.redis_counter_config.config = {
-    host: ""redis"",
-    port: 6379,
-    db: 1,
+    host: ENV.fetch('REDIS_HOST', 'localhost'),
+    port: ENV.fetch('REDIS_PORT', 6379).to_i,
+    db: ENV.fetch('REDIS_DB', 0).to_i,
   }
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451763494,1.0,63,2754afb94cd26b0dc8a460f5d5c7b83c224267d5,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Locker` class seems to be well implemented. It provides functionality for locking and unlocking a short URL in the cache store to prevent duplicate short URL creation. The use of SHA256 for generating a unique key from the URL is a good practice. Dependency injection is also allowed for the cache store, which can improve testability. However, there's no error handling mechanism in place if the cache store operations fail. Consider adding error handling or retries for these operations.
```diff
      def lock(client, key, content)
        lock_key = generate_key(client, key)
        begin
          cache_store.set(lock_key, content, ex: 60, nx: true)
        rescue => e
          # Log the error and retry or handle it accordingly
        end
        lock_key
      end
```
This is just an example. You should adjust the error handling according to your application's needs and logging practices.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+module DynamicLinks
+  module Async
+    # This is to lock/unlock a short url into cache store
+    # to prevent duplicate short url creation
+    class Locker
+      def generate_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_key(url)}""
+      end
+
+      # allow dependency injection
+      def cache_store(store = DynamicLinks.configuration.cache_store)
+        @cache_store ||= store
+      end
+
+      def lock(client, key, content)
+        lock_key = generate_key(client, key)
+        cache_store.set(lock_key, content, ex: 60, nx: true)
+        lock_key
+      end
+
+      def locked?(key)
+        cache_store.read(key).present?
+      end
+
+      def read(key)
+        cache_store.read(key)
+      end
+
+      private
+
+      def url_to_key(url)
+        Digest::SHA256.hexdigest(url)
+      end
+    end
+  end
+end","--- 

+++ 

@@ -1,35 +1,58 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
 module DynamicLinks
   module Async
-    # This is to lock/unlock a short url into cache store
-    # to prevent duplicate short url creation
+    # @author Saiqul Haq <saiqulhaq@gmail.com>
     class Locker
-      def generate_key(client, url)
-        ""lock:shorten_url#{client.id}:#{url_to_key(url)}""
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
       end
 
-      # allow dependency injection
-      def cache_store(store = DynamicLinks.configuration.cache_store)
-        @cache_store ||= store
+      def generate_lock_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
       end
 
-      def lock(client, key, content)
-        lock_key = generate_key(client, key)
-        cache_store.set(lock_key, content, ex: 60, nx: true)
-        lock_key
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
       end
 
-      def locked?(key)
-        cache_store.read(key).present?
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
       end
 
-      def read(key)
-        cache_store.read(key)
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
       end
 
       private
 
-      def url_to_key(url)
+      def url_to_lock_key(url)
         Digest::SHA256.hexdigest(url)
       end
     end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451763495,1.0,31,2754afb94cd26b0dc8a460f5d5c7b83c224267d5,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ShortenUrlJob` class seems to be well implemented. However, there is a potential issue with the order of operations in the `perform` method. If the job fails after creating the shortened URL but before deleting the lock key, it could leave a stale lock in the cache store. This could prevent future attempts to shorten the same URL. To handle this, consider using an ensure block to delete the lock key regardless of whether the operation was successful or not.
```diff
    def perform(client, url, short_url, lock_key)
      ShortenedUrl.find_or_create(client, short_url, url)

      # delete the lock key
      DynamicLinks.configuration.cache_store.delete(lock_key)
    end
+      rescue => e
+        raise e
+      ensure
+        # Ensure the lock key is deleted even if an error occurs
+        DynamicLinks.configuration.cache_store.delete(lock_key)
  end
```
This way, the lock key will always be deleted, ensuring that a failed job doesn't leave a stale lock behind.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+
+module DynamicLinks
+  # This job is used to create a shortened url
+  class ShortenUrlJob < ApplicationJob
+    queue_as :default
+
+    def perform(client, url, short_url, lock_key)
+      ShortenedUrl.find_or_create(client, short_url, url)
+
+      # delete the lock key
+      DynamicLinks.configuration.cache_store.delete(lock_key)
+    end
+  end
+end","--- 

+++ 

@@ -1,15 +1,31 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
-
 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   # This job is used to create a shortened url
   class ShortenUrlJob < ApplicationJob
     queue_as :default
 
     def perform(client, url, short_url, lock_key)
-      ShortenedUrl.find_or_create(client, short_url, url)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
 
-      # delete the lock key
-      DynamicLinks.configuration.cache_store.delete(lock_key)
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end
+    end
+
+    private
+
+    def storage
+      @storage ||= ShortenedUrl
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451763496,1.0,52,2754afb94cd26b0dc8a460f5d5c7b83c224267d5,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortener.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten` and `shorten_async` methods are quite similar, with the only difference being how they handle the shortening process (synchronously or asynchronously). To improve maintainability and follow the DRY principle, you could extract the common parts into a separate method.

Here's an example of how you might refactor this:

```ruby
def shorten(client, url, async: false)
  short_url = strategy.shorten(url)

  if strategy.always_growing?
    storage.create!(client: client, url: url, short_url: short_url)
  else
    storage.find_or_create(client, short_url, url)
  end

  build_url(client, short_url).tap do |result|
    if async
      lock_and_enqueue(client, url, short_url)
    end
  end
end

private

def build_url(client, short_url)
  URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
end

def lock_and_enqueue(client, url, short_url)
  lock_key = locker.generate_key(client, url)

  if locker.locked?(lock_key)
    return build_url(client, locker.read(lock_key))
  end

  content = {
    url: url,
    short_url: short_url
  }

  locker.lock(client, lock_key, content)
  async_worker.perform_later(client, url, short_url, lock_key)
end
```

This way, the `shorten` method can be used for both synchronous and asynchronous shortening, depending on the `async` parameter. The locking and enqueuing logic is extracted to the `lock_and_enqueue` method, and the URL building logic is extracted to the `build_url` method. This makes the code more modular and easier to understand and maintain.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def shorten(client, url)
+      short_url = strategy.shorten(url)
+
+      if strategy.always_growing?
+        storage.create!(client: client, url: url, short_url: short_url)
+      else
+        storage.find_or_create(client, short_url, url)
+      end
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    end
+
+    def shorten_async(client, url)
+      lock_key = locker.generate_key(client, url)
+
+      if locker.locked?(lock_key)
+        short_url = locker.read(lock_key)
+        # Return the short url if it is already in the cache
+        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      end
+
+      short_url = strategy.shorten(url)
+      content = {
+        url: url,
+        short_url: short_url
+      }
+
+      locker.lock(client, lock_key, content)
+      async_worker.perform_later(client, url, short_url, lock_key)
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    end
+
+    # @api private
+    def locker(klass = DynamicLinks::Async::Locker)
+      @locker ||= klass.new
+    end
+
+    # @api private
+    def strategy(strategy_key = DynamicLinks.configuration.shortening_strategy)
+      @strategy ||= StrategyFactory.get_strategy(strategy_key)
+    end
+
+    # @api private
+    def storage(model = ShortenedUrl)
+      @shortened_url_model ||= model
+    end
+
+    # @api private
+    def async_worker(klass = ShortenUrlJob)
+      @async_worker ||= klass
+    end
+  end
+end","--- 

+++ 

@@ -1,6 +1,18 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Shortener
+    attr_reader :locker, :strategy, :storage, :async_worker
+
+    def initialize(locker: DynamicLinks::Async::Locker.new,
+                   strategy: StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy),
+                   storage: ShortenedUrl,
+                   async_worker: ShortenUrlJob)
+      @locker = locker
+      @strategy = strategy
+      @storage = storage
+      @async_worker = async_worker
+    end
+
     # @param client [Client] the client that owns the url
     # @param url [String] the url to be shortened
     # @return [String] the shortened url
@@ -10,49 +22,31 @@

       if strategy.always_growing?
         storage.create!(client: client, url: url, short_url: short_url)
       else
-        storage.find_or_create(client, short_url, url)
+        storage.find_or_create!(client, short_url, url)
       end
       URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL: #{e.message}"")
+      raise e
     end
 
+    # @param client [Client] the client that owns the url
+    # @param url [String] the url to be shortened
     def shorten_async(client, url)
-      lock_key = locker.generate_key(client, url)
+      lock_key = locker.generate_lock_key(client, url)
 
-      if locker.locked?(lock_key)
-        short_url = locker.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      locker.lock_if_absent(lock_key) do
+        short_url = strategy.shorten(url)
+        content = {
+          url: url,
+          short_url: short_url
+        }
+
+        async_worker.perform_later(client, url, short_url, lock_key)
       end
-
-      short_url = strategy.shorten(url)
-      content = {
-        url: url,
-        short_url: short_url
-      }
-
-      locker.lock(client, lock_key, content)
-      async_worker.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
-
-    # @api private
-    def locker(klass = DynamicLinks::Async::Locker)
-      @locker ||= klass.new
-    end
-
-    # @api private
-    def strategy(strategy_key = DynamicLinks.configuration.shortening_strategy)
-      @strategy ||= StrategyFactory.get_strategy(strategy_key)
-    end
-
-    # @api private
-    def storage(model = ShortenedUrl)
-      @shortened_url_model ||= model
-    end
-
-    # @api private
-    def async_worker(klass = ShortenUrlJob)
-      @async_worker ||= klass
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL asynchronously: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1451763498,1.0,38,2754afb94cd26b0dc8a460f5d5c7b83c224267d5,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code is adding a conditional block to include SimpleCov for test coverage only in the 'test' environment. This is a good practice as it prevents unnecessary overhead in other environments. The rest of the changes are just reordering and adding new require statements, which seems fine.
```diff
+ # @author Saiqul Haq <saiqulhaq@gmail.com>
+ 
+ if ENV['RAILS_ENV'] == 'test'
+   require 'simplecov'
+ 
+   SimpleCov.start do
+     load_profile ""test_frameworks""
+ 
+     add_filter %r{^/config/}
+     add_filter %r{^/db/}
+ 
+     add_group ""Controllers"", ""app/controllers""
+     add_group ""Channels"", ""app/channels""
+     add_group ""Models"", ""app/models""
+     add_group ""Mailers"", ""app/mailers""
+     add_group ""Helpers"", ""app/helpers""
+     add_group ""Jobs"", %w[app/jobs app/workers]
+     add_group ""DynamicLinks"", ""lib/""
+   end
+ end
+ 
+  require ""dynamic_links/version""
+  require ""dynamic_links/engine""
+  require ""dynamic_links/error_classes""
+  require ""dynamic_links/redis_config""
+  require ""dynamic_links/configuration""
+  require ""dynamic_links/validator""
+  require ""dynamic_links/strategy_factory""
+  require 'dynamic_links/cache_store/base_cache_store'
+  require 'dynamic_links/cache_store/redis_cache_store'
+  require 'dynamic_links/cache_store/memcached_cache_store'
+  require ""dynamic_links/shortening_strategies/base_strategy""
+  require ""dynamic_links/shortening_strategies/sha256_strategy""
+  require ""dynamic_links/shortening_strategies/md5_strategy""
+  require ""dynamic_links/shortening_strategies/crc32_strategy""
+  require ""dynamic_links/shortening_strategies/nano_id_strategy""
+  require ""dynamic_links/shortening_strategies/redis_counter_strategy""
+  require ""dynamic_links/shortening_strategies/mock_strategy""
+  require ""dynamic_links/async/locker""
+  require ""dynamic_links/shortener""
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+if ENV['RAILS_ENV'] == 'test'
+  require 'simplecov'
+
+  SimpleCov.start do
+    load_profile ""test_frameworks""
+
+    add_filter %r{^/config/}
+    add_filter %r{^/db/}
+
+    add_group ""Controllers"", ""app/controllers""
+    add_group ""Channels"", ""app/channels""
+    add_group ""Models"", ""app/models""
+    add_group ""Mailers"", ""app/mailers""
+    add_group ""Helpers"", ""app/helpers""
+    add_group ""Jobs"", %w[app/jobs app/workers]
+    add_group ""DynamicLinks"", ""lib/""
+  end
+end
+
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
 require ""dynamic_links/error_classes""
+require ""dynamic_links/redis_config""
+require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
+require 'dynamic_links/cache_store/base_cache_store'
+require 'dynamic_links/cache_store/redis_cache_store'
+require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy""
 require ""dynamic_links/shortening_strategies/crc32_strategy""
 require ""dynamic_links/shortening_strategies/nano_id_strategy""
 require ""dynamic_links/shortening_strategies/redis_counter_strategy""
 require ""dynamic_links/shortening_strategies/mock_strategy""
-require ""dynamic_links/configuration""
+require ""dynamic_links/async/locker""
+require ""dynamic_links/shortener""","--- 

+++ 

@@ -21,14 +21,12 @@

 
 require ""dynamic_links/version""
 require ""dynamic_links/engine""
+require ""dynamic_links/logger""
 require ""dynamic_links/error_classes""
 require ""dynamic_links/redis_config""
 require ""dynamic_links/configuration""
 require ""dynamic_links/validator""
 require ""dynamic_links/strategy_factory""
-require 'dynamic_links/cache_store/base_cache_store'
-require 'dynamic_links/cache_store/redis_cache_store'
-require 'dynamic_links/cache_store/memcached_cache_store'
 require ""dynamic_links/shortening_strategies/base_strategy""
 require ""dynamic_links/shortening_strategies/sha256_strategy""
 require ""dynamic_links/shortening_strategies/md5_strategy"""
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645600,26.0,33,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create` method could potentially raise an exception if the record fails to save. It would be better to handle this scenario gracefully, perhaps by returning a meaningful error message or using the `save` method instead of `save!`, which doesn't raise an exception.

```diff
-      # TODO Handle issue when failed to save record
-      record.save!
+      unless record.save
+        return { error: ""Failed to save record"", record: record }
+      end
```
This way, you can check for an error key in the returned object and handle it accordingly.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      record = find_or_initialize_by(client: client, short_url: short_url)
+      return record if record.persisted?
+
+      record.url = url
+      # TODO Handle issue when failed to save record
+      record.save!
+      record
+    end","--- 

+++ 

@@ -23,14 +23,17 @@

     validates :url, presence: true
     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
-    def self.find_or_create(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      # TODO Handle issue when failed to save record
-      record.save!
-      record
+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645602,20.0,82,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,test/models/dynamic_links/shortened_url_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The tests are well written and cover a variety of scenarios. However, there is one potential issue with the `find_or_create` method tests. The `find_or_create` method is expected to be idempotent, meaning it should have the same effect no matter how many times it's called. Currently, you're only testing the creation part once. It would be beneficial to add an assertion that calling `find_or_create` again with the same parameters doesn't increase the count of `ShortenedUrl`.

```diff
  test ""find_or_create creates and returns a new record if it doesn't exist"" do
    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
      result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
      assert_equal @client, result.client
      assert_equal @url, result.url
      assert_equal @short_url, result.short_url
+      # Call find_or_create again and ensure the count doesn't change
+      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+        result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
+      end
     end
   end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+class DynamicLinks::ShortenedUrlTest < ActiveSupport::TestCase
+  self.use_transactional_tests = true
 
-module DynamicLinks
-  class ShortenedUrlTest < ActiveSupport::TestCase
-    self.use_transactional_tests = true
-
-    setup do
-      @client = dynamic_links_clients(:one)  # Assuming you have fixtures set up
-    end
+  setup do
+    @client = dynamic_links_clients(:one)
+    @url = 'https://example.com'
+    @short_url = 'shortened_url'
+  end
 
-    test 'should not save shortened url without url' do
-      shortened_url = ShortenedUrl.new(client: @client, short_url: 'abc123')
-      assert_not shortened_url.save, 'Saved the shortened url without a url'
-    end
+  test 'should not save shortened url without url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: 'abc123')
+    assert_not shortened_url.save, 'Saved the shortened url without a url'
+  end
 
-    test 'should not save shortened url without short_url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com')
-      assert_not shortened_url.save, 'Saved the shortened url without a short_url'
-    end
+  test 'should not save shortened url without short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com')
+    assert_not shortened_url.save, 'Saved the shortened url without a short_url'
+  end
 
-    test 'should save valid shortened url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
-      assert shortened_url.save, 'Failed to save valid shortened url'
-    end
+  test 'should save valid shortened url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
+    assert shortened_url.save, 'Failed to save valid shortened url'
+  end
 
-    test 'should not save shortened url with duplicate short_url' do
-      ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
-      duplicate_url = ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
-      assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
-    end
+  test 'should not save shortened url with duplicate short_url' do
+    DynamicLinks::ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
+    assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
+  end
 
-    test 'reference to client is optional' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'abc123a')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+  test 'should allow the same short_url for different clients' do
+    client_one = dynamic_links_clients(:one)
+    client_two = dynamic_links_clients(:two)
 
-    test 'should handle urls without associated client' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'xyz789')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
+    url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
 
-    test 'should allow the same short_url for different clients' do
-      client_one = dynamic_links_clients(:one)
-      client_two = dynamic_links_clients(:two)
+    assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+  end
 
-      url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
-      url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
+  test 'should not allow the same short_url for the same client' do
+    client = dynamic_links_clients(:one)
 
-      assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
-    end
+    DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
 
-    test 'should not allow the same short_url for the same client' do
-      client = dynamic_links_clients(:one)
+    assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  end
 
-      DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
-      duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
+  test ""find_or_create returns existing record if it exists"" do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
+    assert_equal existing_record, result, ""Expected to return the existing record""
+  end
 
-      assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  test ""find_or_create creates and returns a new record if it doesn't exist"" do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
+      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+      assert_equal @client, result.client
+      assert_equal @url, result.url
+      assert_equal @short_url, result.short_url","--- 

+++ 

@@ -69,17 +69,63 @@

 
   test ""find_or_create returns existing record if it exists"" do
     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
-    result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
+    result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
     assert_equal existing_record, result, ""Expected to return the existing record""
   end
 
   test ""find_or_create creates and returns a new record if it doesn't exist"" do
     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
-      result = DynamicLinks::ShortenedUrl.find_or_create(@client, @short_url, @url)
+      result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
       assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
       assert_equal @client, result.client
       assert_equal @url, result.url
       assert_equal @short_url, result.short_url
+      # Call find_or_create again and ensure the count doesn't change
+      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+        DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      end
+
+      ActiveRecord::Base.transaction do
+        DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
+      end
+    end
+  end
+
+  test 'should validate presence of url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: @short_url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:url], ""can't be blank""
+  end
+
+  test 'should validate presence of short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: @url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:short_url], ""can't be blank""
+  end
+
+  test 'should validate uniqueness of short_url scoped to client_id' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    new_record = DynamicLinks::ShortenedUrl.new(client: @client, url: @url, short_url: @short_url)
+    assert_not new_record.valid?
+    assert_includes new_record.errors[:short_url], ""has already been taken""
+  end
+
+  test 'find_or_create! should find existing record' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, found_record
+  end
+
+  test 'find_or_create! should create new record if not exists' do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    end
+  end
+
+  test 'find_or_create! should raise error and log if save fails' do
+    DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/ShortenedUrl creation failed/))
+    assert_raises ActiveRecord::RecordInvalid do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, nil, @url)
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645606,7.0,11,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `generate_key` method is public and can be called with any client and URL. If the client or URL is not properly sanitized, it could lead to key collisions or unexpected behavior. Consider validating these inputs before using them.

```diff
+       def generate_key(client, url)
+         raise ArgumentError, 'Invalid client' unless client.valid?
+         raise ArgumentError, 'Invalid URL' unless valid_url?(url)
+         ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
+       end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      def generate_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
+      end","--- 

+++ 

@@ -1,29 +1,53 @@

 module DynamicLinks
   module Async
     # @author Saiqul Haq <saiqulhaq@gmail.com>
-    # This is to lock/unlock a short url into cache store
-    # to prevent duplicate short url creation
     class Locker
-      def generate_key(client, url)
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
         ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
       end
 
-      def lock(lock_key, content)
-        cache_store.set(lock_key, content, ex: 60, nx: true)
-        lock_key
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
       end
 
-      def locked?(lock_key)
-        cache_store.read(lock_key).present?
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
       end
 
-      def read(lock_key)
-        cache_store.read(lock_key)
-      end
-
-      # @api private
-      def cache_store(store = DynamicLinks.configuration.cache_store)
-        @cache_store ||= store
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
       end
 
       private"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645607,15.0,19,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `locked?` method checks if a lock exists by reading from the cache store. This operation could potentially return stale data if another process modifies the cache between the read and write operations. Consider using a more atomic operation if your cache store supports it.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+      def locked?(lock_key)
+        cache_store.read(lock_key).present?
+      end","--- 

+++ 

@@ -1,29 +1,53 @@

 module DynamicLinks
   module Async
     # @author Saiqul Haq <saiqulhaq@gmail.com>
-    # This is to lock/unlock a short url into cache store
-    # to prevent duplicate short url creation
     class Locker
-      def generate_key(client, url)
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
         ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
       end
 
-      def lock(lock_key, content)
-        cache_store.set(lock_key, content, ex: 60, nx: true)
-        lock_key
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
       end
 
-      def locked?(lock_key)
-        cache_store.read(lock_key).present?
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
       end
 
-      def read(lock_key)
-        cache_store.read(lock_key)
-      end
-
-      # @api private
-      def cache_store(store = DynamicLinks.configuration.cache_store)
-        @cache_store ||= store
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
       end
 
       private"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645608,23.0,54,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `cache_store` method allows for an optional parameter to set the cache store. However, this could lead to unexpected behavior if used improperly as it changes the state of the Locker instance. Consider making this method private or removing the parameter to avoid misuse.

```diff
-       # @api private
-       def cache_store(store = DynamicLinks.configuration.cache_store)
-         @cache_store ||= store
-       end
+       private
+
+       def cache_store
+         @cache_store ||= DynamicLinks.configuration.cache_store
+       end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+      # @api private
+      def cache_store(store = DynamicLinks.configuration.cache_store)
+        @cache_store ||= store
+      end","--- 

+++ 

@@ -1,29 +1,53 @@

 module DynamicLinks
   module Async
     # @author Saiqul Haq <saiqulhaq@gmail.com>
-    # This is to lock/unlock a short url into cache store
-    # to prevent duplicate short url creation
     class Locker
-      def generate_key(client, url)
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
         ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
       end
 
-      def lock(lock_key, content)
-        cache_store.set(lock_key, content, ex: 60, nx: true)
-        lock_key
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
       end
 
-      def locked?(lock_key)
-        cache_store.read(lock_key).present?
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
       end
 
-      def read(lock_key)
-        cache_store.read(lock_key)
-      end
-
-      # @api private
-      def cache_store(store = DynamicLinks.configuration.cache_store)
-        @cache_store ||= store
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
       end
 
       private"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1452645610,18.0,35,49e9c6b0d77fdf95fd6d7cdc97f9693048a0b3e9,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortener.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There is a potential race condition between checking if the lock exists and creating it. If two processes check the lock at the same time, they might both see that it doesn't exist and then both try to create it. This could lead to unexpected behavior. To avoid this, you should use a pattern like ""check-and-set"" or ""set-if-not-exists"" which can be done atomically in many key-value stores.

```diff
-       if locker.locked?(lock_key)
-         short_url = locker.read(lock_key)
-         # Return the short url if it is already in the cache
-         return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-       end
-
-       short_url = strategy.shorten(url)
-       content = {
-         url: url,
-         short_url: short_url
-       }
-
-       locker.lock(lock_key, content)
+       content = locker.lock_if_absent(lock_key) do
+          short_url = strategy.shorten(url)
+          {
+            url: url,
+            short_url: short_url
+          }
+        end
+
+        short_url = content[:short_url]
```

In this suggested change, `lock_if_absent` would be a new method on the `locker` object that takes a block. It checks if the lock exists, and if not, it sets the lock with the value returned by the block. This operation is done atomically to prevent race conditions. If the lock does exist, it simply returns the existing value.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def shorten_async(client, url)
+      lock_key = locker.generate_key(client, url)
+
+      if locker.locked?(lock_key)
+        short_url = locker.read(lock_key)
+        # Return the short url if it is already in the cache
+        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      end
+
+      short_url = strategy.shorten(url)
+      content = {
+        url: url,
+        short_url: short_url
+      }
+
+      locker.lock(lock_key, content)
+      async_worker.perform_later(client, url, short_url, lock_key)
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s","--- 

+++ 

@@ -1,6 +1,18 @@

-# @author Saiqul Haq <saiqulhaq@gmail.com>
 module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Shortener
+    attr_reader :locker, :strategy, :storage, :async_worker
+
+    def initialize(locker: DynamicLinks::Async::Locker.new,
+                   strategy: StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy),
+                   storage: ShortenedUrl,
+                   async_worker: ShortenUrlJob)
+      @locker = locker
+      @strategy = strategy
+      @storage = storage
+      @async_worker = async_worker
+    end
+
     # @param client [Client] the client that owns the url
     # @param url [String] the url to be shortened
     # @return [String] the shortened url
@@ -10,49 +22,31 @@

       if strategy.always_growing?
         storage.create!(client: client, url: url, short_url: short_url)
       else
-        storage.find_or_create(client, short_url, url)
+        storage.find_or_create!(client, short_url, url)
       end
       URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL: #{e.message}"")
+      raise e
     end
 
+    # @param client [Client] the client that owns the url
+    # @param url [String] the url to be shortened
     def shorten_async(client, url)
-      lock_key = locker.generate_key(client, url)
+      lock_key = locker.generate_lock_key(client, url)
 
-      if locker.locked?(lock_key)
-        short_url = locker.read(lock_key)
-        # Return the short url if it is already in the cache
-        return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+      locker.lock_if_absent(lock_key) do
+        short_url = strategy.shorten(url)
+        content = {
+          url: url,
+          short_url: short_url
+        }
+
+        async_worker.perform_later(client, url, short_url, lock_key)
       end
-
-      short_url = strategy.shorten(url)
-      content = {
-        url: url,
-        short_url: short_url
-      }
-
-      locker.lock(lock_key, content)
-      async_worker.perform_later(client, url, short_url, lock_key)
-      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
-
-    # @api private
-    def locker(klass = DynamicLinks::Async::Locker)
-      @locker ||= klass.new
-    end
-
-    # @api private
-    def strategy(strategy_key = DynamicLinks.configuration.shortening_strategy)
-      @strategy ||= StrategyFactory.get_strategy(strategy_key)
-    end
-
-    # @api private
-    def storage(model = ShortenedUrl)
-      @shortened_url_model ||= model
-    end
-
-    # @api private
-    def async_worker(klass = ShortenUrlJob)
-      @async_worker ||= klass
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL asynchronously: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725798,1.0,23,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/rest_api.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The benchmarking script looks good overall, but it's important to note that the `api_key` is hardcoded as ""foo"". This might not be a problem for testing purposes, but if this script is used in different environments or by different users, you may want to consider making the `api_key` configurable. You can do this by using environment variables or command-line arguments.

Here's an example of how you could use an environment variable for the `api_key`:

```diff
-            ""api_key"": ""foo"",
+            ""api_key"": os.getenv('API_KEY', 'foo'),
```

Don't forget to import the `os` module at the beginning of your script:

```python
import os
```

This way, you can set the `API_KEY` environment variable before running the script to use a different API key. If the `API_KEY` environment variable is not set, it will default to ""foo"".


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+# benchmark rest api using apache bench (ab)
+# target URL http://localhost:8000/v1/shortLinks
+# method POST
+# payload: replace {random} with random string for every run
+#   {
+#     ""api_key"": ""foo"",
+#     ""url"": ""https://example.com/{random}""
+#   }
+from locust import HttpUser, task, between
+import random
+import string
+
+class APIUser(HttpUser):
+    wait_time = between(1, 2)
+
+    @task
+    def post_url(self):
+        random_str = ''.join(random.choices(string.ascii_lowercase + string.digits, k=10))
+        self.client.post(""/v1/shortLinks"", json={
+            ""api_key"": ""foo"",
+            ""url"": f""https://example.com/{random_str}""
+        })",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725801,26.0,32,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create!` method can be simplified by using the `find_or_create_by!` method provided by ActiveRecord. This method automatically handles the finding or creating of a record based on the provided attributes.
```diff
-    def self.find_or_create!(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      record.save!
-      record
-    end
+    def self.find_or_create!(client, short_url, url)
+      find_or_create_by!(client: client, short_url: short_url, url: url)
+    end
```
Please note that this change assumes that the `url` attribute should also be used for finding existing records. If this is not the case, and `url` should only be used for creating new records, then the original implementation is correct.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.find_or_create!(client, short_url, url)
+      record = find_or_initialize_by(client: client, short_url: short_url)
+      return record if record.persisted?
+
+      record.url = url
+      record.save!
+      record","--- 

+++ 

@@ -24,12 +24,16 @@

     validates :short_url, presence: true, uniqueness: { scope: :client_id }
 
     def self.find_or_create!(client, short_url, url)
-      record = find_or_initialize_by(client: client, short_url: short_url)
-      return record if record.persisted?
-
-      record.url = url
-      record.save!
-      record
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725802,8.0,10,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The client setup for the benchmarking script is hardcoded. This might not be an issue if this script is only used in a controlled environment, but it could be problematic if the script is run in different environments or if the 'Benchmark Client' already exists with different parameters. Consider making these values configurable via environment variables or command-line arguments.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# Dummy client setup
+client = DynamicLinks::Client.find_or_create_by!(name: 'Benchmark Client', api_key: 'benchmark_key', hostname: 'example.com', scheme: 'http')
+",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725803,16.0,22,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In your benchmarking tests, you're using the `times` variable to generate unique URLs for each iteration of the test. However, the `times` variable in the `Benchmark#report` block doesn't represent a unique value for each iteration; instead, it represents the total number of times the block will be called during the benchmark. This means that you're not actually generating unique URLs for each iteration as intended. To fix this, consider using a different approach to generate unique URLs for each iteration.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  x.report(""sync shorten_url"") do |times|
+    DynamicLinks.shorten_url(""https://example.com/#{times}"", client, async: false)
+  end
+
+  x.report(""async shorten_url"") do |times|
+    DynamicLinks.shorten_url(""https://example-async.com/#{times}"", client, async: true)
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725804,26.0,64,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/ruby_api.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

These lines contain the results of the benchmarking tests. While it's useful to include these results in the PR description or commit message for context, they don't need to be included in the codebase itself. Including them in the codebase can lead to confusion as they may become outdated or incorrect as the code changes. Consider removing these lines from the codebase and including them in the PR description or commit message instead.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+# Results: 2023-01-05
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    32.036B i/100ms
+#    async shorten_url    186.494B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     68.100T (¬±17.2%) i/s -    325.931T in   4.992942s
+#    async shorten_url    411.674T (¬±19.4%) i/s -      1.841Q in   4.992430s
+
+# Comparison:
+#    async shorten_url: 411673657787738.8 i/s
+#     sync shorten_url: 68100041231802.3 i/s - 6.05x  slower
+
+# Results: 2023-01-06
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    29.439B i/100ms
+#    async shorten_url2024-01-06T11:30:31.937Z pid=6316 tid=3uo INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    216.882B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     67.021T (¬±21.4%) i/s -    313.206T in   4.993776s
+#    async shorten_url      3.887Q (¬±21.7%) i/s -     17.554Q in   4.958253s
+# Comparison:
+#    async shorten_url: 3887497582634705.5 i/s
+#     sync shorten_url: 67020687656060.7 i/s - 58.00x  slower
+
+# When the cache is exist
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#     sync shorten_url    21.002B i/100ms
+#    async shorten_url2024-01-06T11:32:12.036Z pid=6347 tid=3xj INFO: Sidekiq 7.2.0 connecting to Redis with options {:size=>10, :pool_name=>""internal"", :url=>""redis://redis:6379/2""}
+#    830.542B i/100ms
+# Calculating -------------------------------------
+#     sync shorten_url     49.498T (¬±20.3%) i/s -    232.909T in   4.993091s
+#    async shorten_url     15.310Q (¬±20.2%) i/s -     69.875Q in   4.957633s
+# Comparison:
+#    async shorten_url: 15309572098986642.0 i/s
+#     sync shorten_url: 49497616265721.5 i/s - 309.30x  slower",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725805,14.0,16,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `multi_tenant` method is being used here to handle multi-tenancy. However, it's not clear what happens if an exception occurs within the block passed to `multi_tenant`. It would be beneficial to have error handling in place to catch any exceptions that might occur during the execution of the block and respond appropriately.

```diff
      multi_tenant(client) do
        render json: DynamicLinks.generate_short_url(url, client), status: :created
      end
+    rescue => e
+      render json: { error: 'An error occurred while processing your request' }, status: :internal_server_error
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      multi_tenant(client) do
+        render json: DynamicLinks.generate_short_url(url, client), status: :created
+      end","--- 

+++ 

@@ -16,6 +16,9 @@

       end
     rescue DynamicLinks::InvalidURIError
       render json: { error: 'Invalid URL' }, status: :bad_request
+    rescue => e
+      DynamicLinks::Logger.log_error(e)
+      render json: { error: 'An error occurred while processing your request' }, status: :internal_server_error
     end
 
     private
@@ -27,7 +30,7 @@

     end
 
     def multi_tenant(client, db_infra_strategy = DynamicLinks.configuration.db_infra_strategy)
-      if db_infra_strategy == :citus
+      if db_infra_strategy == :sharding
         MultiTenant.with(client) do
           yield
         end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725809,28.0,37,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `multi_tenant` method is introduced to handle multi-tenancy based on the `db_infra_strategy` configuration. This is a good approach as it abstracts away the details of multi-tenancy from the controller action. However, it's important to ensure that the `client` parameter is always valid before passing it to `MultiTenant.with`. Consider adding a check for `client` validity.

```diff
    def multi_tenant(client, db_infra_strategy = DynamicLinks.configuration.db_infra_strategy)
+      raise ArgumentError, ""Invalid client"" unless client.valid?
      if db_infra_strategy == :citus
        MultiTenant.with(client) do
          yield
        end
      else
        yield
      end
    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    def multi_tenant(client, db_infra_strategy = DynamicLinks.configuration.db_infra_strategy)
+      if db_infra_strategy == :citus
+        MultiTenant.with(client) do
+          yield
+        end
+      else
+        yield
+      end
+    end","--- 

+++ 

@@ -16,6 +16,9 @@

       end
     rescue DynamicLinks::InvalidURIError
       render json: { error: 'Invalid URL' }, status: :bad_request
+    rescue => e
+      DynamicLinks::Logger.log_error(e)
+      render json: { error: 'An error occurred while processing your request' }, status: :internal_server_error
     end
 
     private
@@ -27,7 +30,7 @@

     end
 
     def multi_tenant(client, db_infra_strategy = DynamicLinks.configuration.db_infra_strategy)
-      if db_infra_strategy == :citus
+      if db_infra_strategy == :sharding
         MultiTenant.with(client) do
           yield
         end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725811,12.0,14,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228165744_create_dynamic_links_clients.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The conditional creation of a reference table based on the `db_infra_strategy` configuration is a good approach for supporting different database infrastructures. However, it's important to ensure that this configuration is set correctly before running the migration. If not, it could lead to inconsistent database states across different environments.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if DynamicLinks.configuration.db_infra_strategy == :citus
+      create_reference_table(:dynamic_links_clients)
+    end","--- 

+++ 

@@ -9,7 +9,7 @@

       t.timestamps
     end
 
-    if DynamicLinks.configuration.db_infra_strategy == :citus
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
       create_reference_table(:dynamic_links_clients)
     end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725814,23.0,61,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,benchmarks/create_or_find.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The benchmarking script is well written, but there's a potential issue with the `create_or_find_v2` method. It first tries to create a new record and if it fails due to uniqueness constraints or validation errors, it then tries to find an existing record. This approach can lead to race conditions where two concurrent requests might both fail to create a record and then both fail to find one. Consider using a find-or-create approach that handles this race condition.

```diff
# def self.create_or_find_v2(client, short_url, url)
#   record = ShortenedUrl.new(client: client, short_url: short_url, url: url)
#   record.save!
# rescue ActiveRecord::RecordNotUnique, ActiveRecord::RecordInvalid
#   find_by!(client: client, short_url: short_url)
# end
+ 55: # def self.create_or_find_v2(client, short_url, url)
+ 56: #   begin
+ 57: #     record = find_by!(client: client, short_url: short_url)
+ 58: #   rescue ActiveRecord::RecordNotFound
+ 59: #     record = ShortenedUrl.new(client: client, short_url: short_url, url: url)
+ 60: #     record.save!
+ 61: #   end
+ 62: #   record
+ 63: # end
```
This change ensures that the method first tries to find an existing record and only creates a new one if none exists. This should help avoid the potential race condition.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+# result 1st run, when the record doesn't exist
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#            version 1    31.375B i/100ms
+#            version 2    41.160B i/100ms
+# Calculating -------------------------------------
+#            version 1     77.901T (¬±22.4%) i/s -    361.498T in   4.992731s
+#            version 2     28.375T (¬±17.9%) i/s -    136.735T in   4.997445s
+# Comparison:
+#            version 1: 77901187338263.5 i/s
+#            version 2: 28375221927813.5 i/s - 2.75x  slower
+
+# result 2nd run, when the record already exists
+# ruby 3.2.2 (2023-03-30 revision e51014f9c0) +YJIT [x86_64-linux]
+# Warming up --------------------------------------
+#            version 1   170.252B i/100ms
+#            version 2    75.387B i/100ms
+# Calculating -------------------------------------
+#            version 1    431.022T (¬±21.6%) i/s -      2.013Q in   4.993299s
+#            version 2     52.222T (¬±17.7%) i/s -    252.019T in   4.999358s
+# Comparison:
+#            version 1: 431021525910322.2 i/s
+#            version 2: 52221834807549.5 i/s - 8.25x  slower
+
+# V1 and V2 code
+# def self.create_or_find_v1(client, short_url, url)
+#   record = find_or_initialize_by(client: client, short_url: short_url)
+#   record.url = url if record.new_record?
+#   record.save!
+# end
+
+# def self.create_or_find_v2(client, short_url, url)
+#   record = ShortenedUrl.new(client: client, short_url: short_url, url: url)
+#   record.save!
+# rescue ActiveRecord::RecordNotUnique, ActiveRecord::RecordInvalid
+#   find_by!(client: client, short_url: short_url)
+# end
+",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725815,7.0,22,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,app/jobs/dynamic_links/shorten_url_job.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `ShortenUrlJob` class has a `perform` method that is responsible for creating or finding a shortened URL. However, there are some issues with error handling and resource management in this method.

1. The `locker.unlock(lock_key)` statement is inside the `begin-rescue` block. If an exception occurs before this line, the lock will not be released, which could lead to deadlocks. To ensure that the lock is always released, you should put this statement in an `ensure` block.

2. When an error occurs, the error message is logged and then the error is re-raised. This could lead to the same error being logged multiple times in different parts of the application. Instead, consider logging the error at a higher level in the call stack where it can be handled appropriately.

Here's how you might revise the code:

```diff
      begin
        if strategy.always_growing?
          storage.create!(client: client, url: url, short_url: short_url)
        else
          storage.find_or_create!(client, short_url, url)
        end
- 17:         locker.unlock(lock_key)
- 18:         DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
      rescue => e
- 20:         DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
- 21:         raise e
+ 17:       ensure
+ 18:         locker.unlock(lock_key)
+ 19:         DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
      end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def perform(client, url, short_url, lock_key)
+      locker = DynamicLinks::Async::Locker.new
+      strategy = StrategyFactory.get_strategy(DynamicLinks.configuration.shortening_strategy)
+
+      begin
+        if strategy.always_growing?
+          storage.create!(client: client, url: url, short_url: short_url)
+        else
+          storage.find_or_create!(client, short_url, url)
+        end
+        locker.unlock(lock_key)
+        DynamicLinks::Logger.log_info(""Lock key #{lock_key} deleted after ShortenUrlJob"")
+      rescue => e
+        DynamicLinks::Logger.log_error(""Error in ShortenUrlJob: #{e.message}"")
+        raise e
+      end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725816,,10,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228175142_create_dynamic_links_shortened_urls.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The change from nullable to non-nullable for the `client` reference in the `shortened_urls` table could potentially cause issues if there are existing records with null `client_id`. If there are such records, they would violate the new non-null constraint. Please ensure that all existing records have a valid `client_id` before applying this migration.
```diff
- t.references :client, null: true, foreign_key: { to_table: :dynamic_links_clients }, type: :bigint
+ t.references :client, foreign_key: { to_table: :dynamic_links_clients }, type: :bigint
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","       end
 
-      t.references :client, null: true, foreign_key: { to_table: :dynamic_links_clients }, type: :bigint
+      t.references :client, foreign_key: { to_table: :dynamic_links_clients }, type: :bigint","--- 

+++ 

@@ -16,20 +16,5 @@

       t.datetime :expires_at
       t.timestamps
     end
-
-    if DynamicLinks.configuration.db_infra_strategy == :citus
-      # execute SQL to remove primary key constraint
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
-      SQL
-
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        ADD PRIMARY KEY (id, client_id);
-      SQL
-      create_distributed_table :dynamic_links_shortened_urls, :client_id
-    end
-    add_index :dynamic_links_shortened_urls, [:client_id, :short_url], unique: true
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725817,20.0,32,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228175142_create_dynamic_links_shortened_urls.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

This block of code is modifying the primary key of the `dynamic_links_shortened_urls` table based on the `db_infra_strategy` configuration. This could lead to potential issues if the migration is run multiple times or if the configuration changes after the migration has been run. It's generally not recommended to include conditional logic in migrations that depends on external configurations as it can lead to inconsistent database states. Consider creating separate migrations for different database strategies instead.
```diff
-    if DynamicLinks.configuration.db_infra_strategy == :citus
-      # execute SQL to remove primary key constraint
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
-      SQL
-
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        ADD PRIMARY KEY (id, client_id);
-      SQL
-      create_distributed_table :dynamic_links_shortened_urls, :client_id
-    end
+    # Create separate migrations for different db infra strategies
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if DynamicLinks.configuration.db_infra_strategy == :citus
+      # execute SQL to remove primary key constraint
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id, client_id);
+      SQL
+      create_distributed_table :dynamic_links_shortened_urls, :client_id
+    end","--- 

+++ 

@@ -16,20 +16,5 @@

       t.datetime :expires_at
       t.timestamps
     end
-
-    if DynamicLinks.configuration.db_infra_strategy == :citus
-      # execute SQL to remove primary key constraint
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
-      SQL
-
-      execute <<-SQL
-        ALTER TABLE dynamic_links_shortened_urls
-        ADD PRIMARY KEY (id, client_id);
-      SQL
-      create_distributed_table :dynamic_links_shortened_urls, :client_id
-    end
-    add_index :dynamic_links_shortened_urls, [:client_id, :short_url], unique: true
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725819,53.0,61,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of `shorten_url` method is much cleaner and easier to understand than the old one. However, it seems like you've removed the error handling for strategy initialization which was present in the old code. This could lead to unhandled exceptions if there are issues with strategy initialization. Consider adding back the error handling.

```diff
+    begin
+      shortener = Shortener.new
+      if async
+        shortener.shorten_async(client, url)
+      else
+        shortener.shorten(client, url)
+      end
+    rescue RuntimeError => e
+      # This will catch the 'Unknown strategy' error from the factory
+      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
+    rescue ArgumentError
+      raise ""#{strategy_key} strategy needs to be initialized with arguments""
+    rescue StandardError => e
+      raise ""Unexpected error while initializing the strategy: #{e.message}""
+    end
```
This way, we can ensure that any errors during strategy initialization are properly caught and handled.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  def self.shorten_url(url, client, async: DynamicLinks.configuration.async_processing)
     raise InvalidURIError, 'Invalid URL' unless Validator.valid_url?(url)
 
-    strategy_key = configuration.shortening_strategy
-
-    strategy = begin
-      StrategyFactory.get_strategy(strategy_key)
-    rescue RuntimeError => e
-      # This will catch the 'Unknown strategy' error from the factory
-      raise ""Invalid shortening strategy: #{strategy_key}. Error: #{e.message}""
-    rescue ArgumentError
-      raise ""#{strategy_key} strategy needs to be initialized with arguments""
-    rescue StandardError => e
-      raise ""Unexpected error while initializing the strategy: #{e.message}""
-    end
-
-    if strategy.always_growing?
-      short_url = strategy.shorten(url)
-
-      short_url_record = ShortenedUrl.create!(client: client, url: url, short_url: short_url)
-      return URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
-    end
-
-    # If no existing record or always growing, generate new short URL
-    short_url = strategy.shorten(url)
-    record = ShortenedUrl.find_or_initialize_by(client: client, short_url: short_url)
-    if record.new_record?
-      record.url = url
-      record.save!
-      record
+    shortener = Shortener.new
+    if async
+      shortener.shorten_async(client, url)
+    else
+      shortener.shorten(client, url)
     end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725822,14.0,20,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortening_strategies/redis_counter_strategy.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new implementation of the `initialize` method in `RedisCounterStrategy` class is more robust and maintainable. It now accepts a `redis_config` hash and uses it to create a new instance of `RedisConfig`. If no `redis_config` is provided, it falls back to the default configuration from `DynamicLinks.configuration.redis_counter_config`.

However, there's a potential issue with the way you're handling the `redis_config` parameter. If an empty hash `{}` is passed as `redis_config`, it will not be treated as `nil` and hence, the default configuration won't be used. This could lead to unexpected behavior if the caller expects that passing an empty hash would use the default configuration.

Consider checking for both `nil` and empty hash:

```diff
- configuration = redis_config.nil? ? DynamicLinks.configuration.redis_counter_config : DynamicLinks::Configuration::RedisConfig.new(redis_config)
+ configuration = redis_config.blank? ? DynamicLinks.configuration.redis_counter_config : DynamicLinks::Configuration::RedisConfig.new(redis_config)
```
This change ensures that if `redis_config` is either `nil` or an empty hash, the default configuration will be used.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      # @param redis_config [Hash]
       def initialize(redis_config = nil)
         super()
-        pool_size = DynamicLinks.configuration.redis_pool_size
-        pool_timeout = DynamicLinks.configuration.redis_pool_timeout
 
-        @redis = ConnectionPool.new(size: pool_size, timeout: pool_timeout) do
-          redis_config = redis_config.presence || DynamicLinks.configuration.redis_config
-          Redis.new(redis_config)
+        configuration = redis_config.nil? ? DynamicLinks.configuration.redis_counter_config : DynamicLinks::Configuration::RedisConfig.new(redis_config)
+        @redis = ConnectionPool.new(size: configuration.pool_size, timeout: configuration.pool_timeout) do
+          Redis.new(configuration.config)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725823,1.0,63,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Locker` class seems to be well implemented. It provides a mechanism for acquiring and releasing locks, which is crucial in an asynchronous environment to prevent race conditions. The use of cache stores like Redis or Memcached for storing the locks is a good choice as they provide fast access times. The error handling with custom exceptions `LockAcquisitionError` and `LockReleaseError` is also a good practice. However, there's a potential issue with the `lock_if_absent` method.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+module DynamicLinks
+  module Async
+    # @author Saiqul Haq <saiqulhaq@gmail.com>
+    class Locker
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
+      end
+
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
+      end
+
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, race_condition_ttl: 10.seconds, &block)
+        is_locked = false
+        begin
+          cache_store.fetch(lock_key, race_condition_ttl: race_condition_ttl) do |_key, options|
+            options.expires_in = expires_in
+            is_locked = true
+            yield if block_given?
+          end
+
+          unless is_locked
+            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
+      end
+
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
+      end
+
+      private
+
+      def url_to_lock_key(url)
+        Digest::SHA256.hexdigest(url)
+      end
+    end
+  end
+end","--- 

+++ 

@@ -25,17 +25,14 @@

       # @param [Integer] expires_in, default is 60 seconds
       # @param [Block] block, the block to be executed if lock is acquired
       # @return [Boolean]
-      def lock_if_absent(lock_key, expires_in: 60, race_condition_ttl: 10.seconds, &block)
+      def lock_if_absent(lock_key, expires_in: 60, &block)
         is_locked = false
         begin
-          cache_store.fetch(lock_key, race_condition_ttl: race_condition_ttl) do |_key, options|
-            options.expires_in = expires_in
-            is_locked = true
-            yield if block_given?
-          end
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
 
           unless is_locked
-            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
           end
         rescue => e
           DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725824,28.0,46,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `lock_if_absent` method, you're using the `fetch` method on the `cache_store` object. This method is typically used to read a value from the cache and write it if it doesn't exist. However, you're using it here to implement a locking mechanism. This could lead to unexpected behavior because the `fetch` method isn't designed to be atomic. 

Instead, consider using a method specifically designed for this purpose. For example, Redis provides the `SETNX` command (set if not exists), which can be used to implement a locking mechanism. Here's how you might do it:

```diff
-          cache_store.fetch(lock_key, race_condition_ttl: race_condition_ttl) do |_key, options|
-            options.expires_in = expires_in
-            is_locked = true
-            yield if block_given?
-          end
+          is_locked = cache_store.set(lock_key, 'LOCK', nx: true, ex: expires_in)
+          if is_locked
+            yield if block_given?
+          end
```

This code uses the `set` method with the `nx: true` option, which tells Redis to only set the key if it does not already exist. If the key is successfully set, the method will return `true`, otherwise it will return `false`. The `ex: expires_in` option sets an expiration time on the key, which is a good practice when implementing locks to prevent them from staying around forever if something goes wrong.

Please note that this suggestion assumes that your `cache_store` object has a `set` method compatible with Redis's `SETNX`. If it doesn't, you might need to adjust the code accordingly.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      def lock_if_absent(lock_key, expires_in: 60, race_condition_ttl: 10.seconds, &block)
+        is_locked = false
+        begin
+          cache_store.fetch(lock_key, race_condition_ttl: race_condition_ttl) do |_key, options|
+            options.expires_in = expires_in
+            is_locked = true
+            yield if block_given?
+          end
+
+          unless is_locked
+            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
+      end","--- 

+++ 

@@ -25,17 +25,14 @@

       # @param [Integer] expires_in, default is 60 seconds
       # @param [Block] block, the block to be executed if lock is acquired
       # @return [Boolean]
-      def lock_if_absent(lock_key, expires_in: 60, race_condition_ttl: 10.seconds, &block)
+      def lock_if_absent(lock_key, expires_in: 60, &block)
         is_locked = false
         begin
-          cache_store.fetch(lock_key, race_condition_ttl: race_condition_ttl) do |_key, options|
-            options.expires_in = expires_in
-            is_locked = true
-            yield if block_given?
-          end
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
 
           unless is_locked
-            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
           end
         rescue => e
           DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725825,15.0,31,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortener.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten` method has a potential issue. It's not clear what the `strategy.always_growing?` method does, but if it returns false, then the `storage.find_or_create!(client, short_url, url)` line is executed. This could potentially lead to race conditions where two threads are trying to find or create the same record at the same time. Consider adding some form of locking mechanism around this operation to ensure atomicity.

```diff
      if strategy.always_growing?
        storage.create!(client: client, url: url, short_url: short_url)
      else
+         ActiveRecord::Base.transaction do
          storage.find_or_create!(client, short_url, url)
+         end
      end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    # @param client [Client] the client that owns the url
+    # @param url [String] the url to be shortened
+    # @return [String] the shortened url
+    def shorten(client, url)
+      short_url = strategy.shorten(url)
+
+      if strategy.always_growing?
+        storage.create!(client: client, url: url, short_url: short_url)
+      else
+        storage.find_or_create!(client, short_url, url)
+      end
+      URI::Generic.build({scheme: client.scheme, host: client.hostname, path: ""/#{short_url}""}).to_s
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL: #{e.message}"")
+      raise e
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725826,32.0,50,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/shortener.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `shorten_async` method seems to have a similar issue as the `shorten` method. The `locker.lock_if_absent(lock_key)` method is used, which suggests that there might be a need for a lock here. However, it's not clear whether this lock is sufficient to prevent race conditions when creating new records in the database. If the `async_worker.perform_later(client, url, short_url, lock_key)` method involves creating a new record in the database, consider adding a transaction block around it to ensure atomicity.

```diff
      locker.lock_if_absent(lock_key) do
        short_url = strategy.shorten(url)
        content = {
          url: url,
          short_url: short_url
        }
+         ActiveRecord::Base.transaction do
          async_worker.perform_later(client, url, short_url, lock_key)
+         end
      end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+
+    # @param client [Client] the client that owns the url
+    # @param url [String] the url to be shortened
+    def shorten_async(client, url)
+      lock_key = locker.generate_lock_key(client, url)
+
+      locker.lock_if_absent(lock_key) do
+        short_url = strategy.shorten(url)
+        content = {
+          url: url,
+          short_url: short_url
+        }
+
+        async_worker.perform_later(client, url, short_url, lock_key)
+      end
+    rescue => e
+      DynamicLinks::Logger.log_error(""Error shortening URL asynchronously: #{e.message}"")
+      raise e
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725827,4.0,39,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The new configuration class seems to be well structured and the defaults are set in a clear manner. However, it's important to ensure that the `RedisConfig.new` object created as the default for `DEFAULT_REDIS_COUNTER_CONFIG` is properly initialized with sensible defaults. If not, this could lead to unexpected behavior or errors when trying to use Redis functionalities without explicit configuration.

Also, consider adding error handling or validation for the configuration parameters. For instance, if an invalid value is provided for `shortening_strategy`, `db_infra_strategy`, or `async_processing`, the system should either throw an error or revert to the default value. This will prevent potential issues caused by misconfiguration.

```diff
+    VALID_SHORTENING_STRATEGIES = [:MD5, :SHA1, :SHA256].freeze
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze

     def initialize
       @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING

+      validate_configurations
+
       # config for RedisCounterStrategy
       @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
       @cache_store = DEFAULT_CACHE_STORE
     end

+    private
+
+    def validate_configurations
+      unless VALID_SHORTENING_STRATEGIES.include?(@shortening_strategy)
+        raise ""Invalid shortening strategy. Valid strategies are #{VALID_SHORTENING_STRATEGIES.join(', ')}""
+      end
+
+      unless VALID_DB_INFRA_STRATEGIES.include?(@db_infra_strategy)
+        raise ""Invalid DB infrastructure strategy. Valid strategies are #{VALID_DB_INFRA_STRATEGIES.join(', ')}""
+      end
+
+      unless [true, false].include?(@async_processing)
+        raise ""Invalid async processing value. It should be either true or false.""
+      end
+    end
```
This will ensure that the configuration is valid at the time of initialization and prevent potential issues down the line.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","   class Configuration
-    attr_accessor :shortening_strategy, :redis_config,
-                  :redis_pool_size, :redis_pool_timeout,
-                  :enable_rest_api
+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store
+
+    DEFAULT_SHORTENING_STRATEGY = :MD5
+    DEFAULT_ENABLE_REST_API = true
+    DEFAULT_DB_INFRA_STRATEGY = :standard
+    DEFAULT_ASYNC_PROCESSING = false
+    DEFAULT_REDIS_COUNTER_CONFIG = RedisConfig.new
+    # use any class that extends ActiveSupport::Cache::Store, default is MemoryStore
+    DEFAULT_CACHE_STORE = ActiveSupport::Cache::MemoryStore.new
 
+    # Usage:
+    #     DynamicLinks.configure do |config|
+    #       config.shortening_strategy = :MD5 # or other strategy name, see StrategyFactory for available strategies
+    #       config.enable_rest_api = true # or false. when false, the API requests will be rejected
+    #       config.db_infra_strategy = :standard # or :sharding. if sharding is used, then xxx
+    #       config.async_processing = false # or true. if true, the shortening process will be done asynchronously using ActiveJob
+    #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
+    #       # if you use Redis
+    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       # if you use Memcached
+    #       config.cache_store = ActiveSupport::Cache::MemCacheStore.new('localhost:11211')
+    #     end
+    #
+    # @return [Configuration]
     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-      @redis_pool_size = 5  # Default pool size
-      @redis_pool_timeout = 5  # Default timeout in seconds
-      @enable_rest_api = true  # Enable REST API by default
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
+
+      # config for RedisCounterStrategy
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
     end","--- 

+++ 

@@ -1,11 +1,12 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
-
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
                   :async_processing, :redis_counter_config, :cache_store
 
-    DEFAULT_SHORTENING_STRATEGY = :MD5
+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+
+    DEFAULT_SHORTENING_STRATEGY = :md5
     DEFAULT_ENABLE_REST_API = true
     DEFAULT_DB_INFRA_STRATEGY = :standard
     DEFAULT_ASYNC_PROCESSING = false
@@ -32,10 +33,39 @@

       @enable_rest_api = DEFAULT_ENABLE_REST_API
       @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
       @async_processing = DEFAULT_ASYNC_PROCESSING
-
       # config for RedisCounterStrategy
       @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
       @cache_store = DEFAULT_CACHE_STORE
     end
+
+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
+    end
+
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
+
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
+    end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725834,8.0,13,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The Redis configuration is directly dependent on environment variables without any fallback values. This can lead to unexpected behavior if these variables are not set. Consider providing default values.
```diff
- config.redis_counter_config.config = {
-   host: ENV['REDIS_HOST'] || 'localhost',
-   port: ENV['REDIS_PORT'] || 6379,
-   db: ENV['REDIS_DB'] || 0,
- }
+ config.redis_counter_config.config = {
+   host: ENV.fetch('REDIS_HOST', 'localhost'),
+   port: ENV.fetch('REDIS_PORT', 6379).to_i,
+   db: ENV.fetch('REDIS_DB', 0).to_i,
+ }
```
Note: The `fetch` method is used here to retrieve the environment variable and provide a default value if it's not set. The `to_i` method is used to ensure the values are integers, as expected by the Redis configuration.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  config.redis_counter_config.pool_size = 5
+  config.redis_counter_config.pool_timeout = 5
+  config.redis_counter_config.config = {
+    host: ENV['REDIS_HOST'] || 'localhost',
+    port: ENV['REDIS_PORT'] || 6379,
+    db: ENV['REDIS_DB'] || 0,","--- 

+++ 

@@ -2,14 +2,14 @@

   config.shortening_strategy = ENV['SHORTENING_STRATEGY'].to_sym
   config.enable_rest_api = true
   if ENV['CITUS_ENABLED'].to_s == 'true'
-    config.db_infra_strategy = :citus
+    config.db_infra_strategy = :sharding
   end
 
   config.redis_counter_config.pool_size = 5
   config.redis_counter_config.pool_timeout = 5
   config.redis_counter_config.config = {
-    host: ENV['REDIS_HOST'] || 'localhost',
-    port: ENV['REDIS_PORT'] || 6379,
-    db: ENV['REDIS_DB'] || 0,
+    host: ENV.fetch('REDIS_HOST', 'localhost'),
+    port: ENV.fetch('REDIS_PORT', 6379).to_i,
+    db: ENV.fetch('REDIS_DB', 0).to_i,
   }
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725836,28.0,34,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,test/jobs/shorten_url_job_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a typo in the test description. It should be `strategy` instead of `startegy`.
```diff
-     test 'perform should find_or_create a shortened URL if startegy#always_growing? is false' do
+     test 'perform should find_or_create a shortened URL if strategy#always_growing? is false' do
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    test 'perform should find_or_create a shortened URL if startegy#always_growing? is false' do
+      @strategy.stubs(:always_growing?).returns(false)
+      @job.stubs(:storage).returns(@storage)
+
+      @storage.expects(:find_or_create!).with(@client, ""#{@short_url}12"", @url)
+      @job.perform(@client, @url, ""#{@short_url}12"", @lock_key)
+    end","--- 

+++ 

@@ -17,7 +17,7 @@

       @locker.cache_store.write(@lock_key, true)
     end
 
-    test 'perform should create a shortened URL if startegy#always_growing? is true' do
+    test 'perform should create a shortened URL if strategy#always_growing? is true' do
       @strategy.stubs(:always_growing?).returns(true)
       @job.stubs(:storage).returns(@storage)
 
@@ -25,7 +25,7 @@

       @job.perform(@client, @url, ""#{@short_url}11"", @lock_key)
     end
 
-    test 'perform should find_or_create a shortened URL if startegy#always_growing? is false' do
+    test 'perform should find_or_create a shortened URL if strategy#always_growing? is false' do
       @strategy.stubs(:always_growing?).returns(false)
       @job.stubs(:storage).returns(@storage)
 
@@ -44,10 +44,10 @@

 
     test 'perform should log error and re-raise exception on failure' do
       @strategy.stubs(:always_growing?).returns(true)
-      ShortenedUrl.stubs(:create!).raises(StandardError.new('Creation failed'))
+      ShortenedUrl.stubs(:create!).raises(ShorteningFailed.new('Creation failed'))
       DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error in ShortenUrlJob/))
 
-      assert_raises StandardError do
+      assert_raises ShorteningFailed do
         @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
       end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725837,45.0,55,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,test/jobs/shorten_url_job_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case for error handling seems to be missing an important aspect. When an exception occurs, it's crucial to ensure that the lock is released to prevent deadlocks. Please add an assertion to check if the lock is released when an exception is raised.
```diff
  assert_raises StandardError do
    @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
  end

+ refute @locker.locked?(@lock_key)

  assert @locker.locked?(@lock_key)
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    test 'perform should log error and re-raise exception on failure' do
+      @strategy.stubs(:always_growing?).returns(true)
+      ShortenedUrl.stubs(:create!).raises(StandardError.new('Creation failed'))
+      DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error in ShortenUrlJob/))
+
+      assert_raises StandardError do
+        @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
+      end
+
+      assert @locker.locked?(@lock_key)
+    end","--- 

+++ 

@@ -17,7 +17,7 @@

       @locker.cache_store.write(@lock_key, true)
     end
 
-    test 'perform should create a shortened URL if startegy#always_growing? is true' do
+    test 'perform should create a shortened URL if strategy#always_growing? is true' do
       @strategy.stubs(:always_growing?).returns(true)
       @job.stubs(:storage).returns(@storage)
 
@@ -25,7 +25,7 @@

       @job.perform(@client, @url, ""#{@short_url}11"", @lock_key)
     end
 
-    test 'perform should find_or_create a shortened URL if startegy#always_growing? is false' do
+    test 'perform should find_or_create a shortened URL if strategy#always_growing? is false' do
       @strategy.stubs(:always_growing?).returns(false)
       @job.stubs(:storage).returns(@storage)
 
@@ -44,10 +44,10 @@

 
     test 'perform should log error and re-raise exception on failure' do
       @strategy.stubs(:always_growing?).returns(true)
-      ShortenedUrl.stubs(:create!).raises(StandardError.new('Creation failed'))
+      ShortenedUrl.stubs(:create!).raises(ShorteningFailed.new('Creation failed'))
       DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error in ShortenUrlJob/))
 
-      assert_raises StandardError do
+      assert_raises ShorteningFailed do
         @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
       end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725838,1.0,59,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,test/lib/shortener_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The tests look well written and cover the main functionalities of the `Shortener` class. However, there's a potential issue with the way exceptions are being handled in your tests. In lines 30 and 50, you're raising a `StandardError` to simulate an error scenario. While this is fine for testing purposes, it's generally better to define custom exception classes for specific error scenarios. This makes it easier to handle these errors elsewhere in your code and also makes your tests more precise.

For example, instead of raising a `StandardError` when shortening fails, you could define a `ShorteningFailed` exception:

```ruby
class ShorteningFailed < StandardError; end
```

Then, in your test, you can raise this exception:

```diff
- @strategy.stubs(:shorten).raises(StandardError.new('shortening failed'))
+ @strategy.stubs(:shorten).raises(ShorteningFailed.new('shortening failed'))
```

And in your production code, you can rescue this specific exception:

```ruby
begin
  # code that may raise ShorteningFailed
rescue ShorteningFailed => e
  # handle the error
end
```

This approach makes your error handling more robust and your tests more precise.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+require ""test_helper""
+require ""minitest/mock""
+
+module DynamicLinks
+  # @author Saiqul Haq <saiqulhaq@gmail.com>
+  class ShortenerTest < ActiveSupport::TestCase
+    setup do
+      @client = dynamic_links_clients(:one)
+      @url = 'https://example.com'
+      @short_url = 'abc123'
+      @locker = DynamicLinks::Async::Locker.new
+      @strategy = mock('strategy')
+      @storage = ShortenedUrl
+      @async_worker = mock('async_worker')
+      @shortener = Shortener.new(locker: @locker, strategy: @strategy, storage: @storage, async_worker: @async_worker)
+    end
+
+    test 'shorten should create a shortened URL and save it' do
+      @strategy.stubs(:shorten).returns(@short_url)
+      @strategy.stubs(:always_growing?).returns(true)
+      @storage.stubs(:create!).returns(ShortenedUrl.new)
+
+      result = @shortener.shorten(@client, @url)
+
+      assert_match @short_url, result
+      assert_equal ""#{@client.scheme}://#{@client.hostname}/#{@short_url}"", result
+    end
+
+    test 'shorten should handle exceptions and log errors' do
+      @strategy.stubs(:shorten).raises(StandardError.new('shortening failed'))
+      DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error shortening URL/))
+
+      assert_raises StandardError do
+        @shortener.shorten(@client, @url)
+      end
+    end
+
+    test 'shorten_async should enqueue a job to shorten the URL' do
+      lock_key = 'lock_key'
+      @locker.stubs(:generate_lock_key).returns(lock_key)
+      @locker.stubs(:lock_if_absent).yields
+      @strategy.stubs(:shorten).returns(@short_url)
+      @async_worker.expects(:perform_later).with(@client, @url, @short_url, lock_key)
+
+      @shortener.shorten_async(@client, @url)
+    end
+
+    test 'shorten_async should handle exceptions and log errors' do
+      @locker.stubs(:generate_lock_key).returns('lock_key')
+      @locker.stubs(:lock_if_absent).raises(StandardError.new('async shortening failed'))
+      DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error shortening URL asynchronously/))
+
+      assert_raises StandardError do
+        @shortener.shorten_async(@client, @url)
+      end
+    end
+  end
+end
+","--- 

+++ 

@@ -27,10 +27,10 @@

     end
 
     test 'shorten should handle exceptions and log errors' do
-      @strategy.stubs(:shorten).raises(StandardError.new('shortening failed'))
+      @strategy.stubs(:shorten).raises(ShorteningFailed.new('shortening failed'))
       DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error shortening URL/))
 
-      assert_raises StandardError do
+      assert_raises ShorteningFailed do
         @shortener.shorten(@client, @url)
       end
     end
@@ -47,10 +47,10 @@

 
     test 'shorten_async should handle exceptions and log errors' do
       @locker.stubs(:generate_lock_key).returns('lock_key')
-      @locker.stubs(:lock_if_absent).raises(StandardError.new('async shortening failed'))
+      @locker.stubs(:lock_if_absent).raises(ShorteningFailed.new('async shortening failed'))
       DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error shortening URL asynchronously/))
 
-      assert_raises StandardError do
+      assert_raises ShorteningFailed do
         @shortener.shorten_async(@client, @url)
       end
     end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468725840,20.0,86,a71f93658310e321b930a78926b4ab31f66de4f4,67675ef2633001974317dae9fc570ecf17a48936,test/models/dynamic_links/shortened_url_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The tests look good overall, but there's a potential issue with the `find_or_create` tests. The tests are assuming that the `find_or_create!` method will always return the same instance when called multiple times with the same parameters. However, this might not be the case if there are concurrent requests trying to create the same record at the same time. This could lead to unexpected behavior or even test failures in certain scenarios.

To make the tests more robust, consider using a lock or some other form of synchronization to ensure that the `find_or_create!` method behaves as expected even under concurrent access. Here's an example of how you might do this:

```diff
  test ""find_or_create returns existing record if it exists"" do
    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
    result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
    assert_equal existing_record, result, ""Expected to return the existing record""
  end
 
  test ""find_or_create creates and returns a new record if it doesn't exist"" do
    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
      result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
      assert_equal @client, result.client
      assert_equal @url, result.url
      assert_equal @short_url, result.short_url
      # Call find_or_create again and ensure the count doesn't change
      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
        DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
      end
+      # Add a lock to ensure that the find_or_create! method behaves as expected under concurrent access
+      ActiveRecord::Base.transaction do
+        DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
+      end
     end
   end
```
This change ensures that the `find_or_create!` method is thread-safe and will behave as expected even if there are multiple threads trying to create the same record at the same time.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+class DynamicLinks::ShortenedUrlTest < ActiveSupport::TestCase
+  self.use_transactional_tests = true
 
-module DynamicLinks
-  class ShortenedUrlTest < ActiveSupport::TestCase
-    self.use_transactional_tests = true
-
-    setup do
-      @client = dynamic_links_clients(:one)  # Assuming you have fixtures set up
-    end
+  setup do
+    @client = dynamic_links_clients(:one)
+    @url = 'https://example.com'
+    @short_url = 'shortened_url'
+  end
 
-    test 'should not save shortened url without url' do
-      shortened_url = ShortenedUrl.new(client: @client, short_url: 'abc123')
-      assert_not shortened_url.save, 'Saved the shortened url without a url'
-    end
+  test 'should not save shortened url without url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: 'abc123')
+    assert_not shortened_url.save, 'Saved the shortened url without a url'
+  end
 
-    test 'should not save shortened url without short_url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com')
-      assert_not shortened_url.save, 'Saved the shortened url without a short_url'
-    end
+  test 'should not save shortened url without short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com')
+    assert_not shortened_url.save, 'Saved the shortened url without a short_url'
+  end
 
-    test 'should save valid shortened url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
-      assert shortened_url.save, 'Failed to save valid shortened url'
-    end
+  test 'should save valid shortened url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
+    assert shortened_url.save, 'Failed to save valid shortened url'
+  end
 
-    test 'should not save shortened url with duplicate short_url' do
-      ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
-      duplicate_url = ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
-      assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
-    end
+  test 'should not save shortened url with duplicate short_url' do
+    DynamicLinks::ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
+    assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
+  end
 
-    test 'reference to client is optional' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'abc123a')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+  test 'should allow the same short_url for different clients' do
+    client_one = dynamic_links_clients(:one)
+    client_two = dynamic_links_clients(:two)
 
-    test 'should handle urls without associated client' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'xyz789')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
+    url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
 
-    test 'should allow the same short_url for different clients' do
-      client_one = dynamic_links_clients(:one)
-      client_two = dynamic_links_clients(:two)
+    assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+  end
 
-      url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
-      url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
+  test 'should not allow the same short_url for the same client' do
+    client = dynamic_links_clients(:one)
 
-      assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
-    end
+    DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
 
-    test 'should not allow the same short_url for the same client' do
-      client = dynamic_links_clients(:one)
+    assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  end
 
-      DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
-      duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
+  test ""find_or_create returns existing record if it exists"" do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, result, ""Expected to return the existing record""
+  end
 
-      assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  test ""find_or_create creates and returns a new record if it doesn't exist"" do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+      assert_equal @client, result.client
+      assert_equal @url, result.url
+      assert_equal @short_url, result.short_url
+      # Call find_or_create again and ensure the count doesn't change
+      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+        DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      end","--- 

+++ 

@@ -84,6 +84,48 @@

       assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
         DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
       end
+
+      ActiveRecord::Base.transaction do
+        DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
+      end
+    end
+  end
+
+  test 'should validate presence of url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: @short_url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:url], ""can't be blank""
+  end
+
+  test 'should validate presence of short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: @url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:short_url], ""can't be blank""
+  end
+
+  test 'should validate uniqueness of short_url scoped to client_id' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    new_record = DynamicLinks::ShortenedUrl.new(client: @client, url: @url, short_url: @short_url)
+    assert_not new_record.valid?
+    assert_includes new_record.errors[:short_url], ""has already been taken""
+  end
+
+  test 'find_or_create! should find existing record' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, found_record
+  end
+
+  test 'find_or_create! should create new record if not exists' do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    end
+  end
+
+  test 'find_or_create! should raise error and log if save fails' do
+    DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/ShortenedUrl creation failed/))
+    assert_raises ActiveRecord::RecordInvalid do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, nil, @url)
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731905,24.0,25,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/client.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The regular expressions used for validating `hostname` and `scheme` are not correct. A hostname can contain alphanumeric characters, hyphens, and periods but it should not start or end with a hyphen or period. The scheme is typically 'http' or 'https', so the validation should reflect this.

```diff
- validates :hostname, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid hostname' }
- validates :scheme, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid scheme' }
+ validates :hostname, format: { with: /\A[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?)*\z/i, message: 'must be a valid hostname' }
+ validates :scheme, format: { with: /\Ahttps?\z/i, message: 'must be a valid scheme' }
```
This change ensures that the hostname starts and ends with an alphanumeric character and does not exceed the maximum length of 63 characters for each label (part between dots). The scheme validation is simplified to only allow 'http' or 'https'.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    validates :hostname, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid hostname' }
+    validates :scheme, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid scheme' }","--- 

+++ 

@@ -21,7 +21,6 @@

     validates :api_key, presence: true, uniqueness: true
     validates :hostname, presence: true, uniqueness: true
     validates :scheme, presence: true, inclusion: { in: VALID_SCHEMES }
-    validates :hostname, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid hostname' }
-    validates :scheme, format: { with: /\A[a-z0-9\-\.]+\z/i, message: 'must be a valid scheme' }
+    validates :hostname, format: { with: /\A[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?)*\z/i, message: 'must be a valid hostname' }
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731907,1.0,60,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Locker` class seems to be well implemented. It provides a mechanism for acquiring and releasing locks using a cache store, which is useful in preventing race conditions when multiple processes are trying to shorten the same URL at the same time. The use of exceptions to handle errors during lock acquisition and release is also a good practice. However, there's a potential issue with the `lock_if_absent` method. If an exception occurs before the yield statement (for example, if `cache_store.increment` raises an exception), the method will log the error and re-raise the exception, but it won't return `is_locked`. This could lead to confusion because the method's contract implies that it should always return a boolean value. To fix this, you should ensure that `is_locked` is returned even when an exception is raised.

```diff
          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
          yield if is_locked && block_given?

          unless is_locked
            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
          end
        rescue => e
          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
          raise e
+40:         ensure
+41:           return is_locked
        end

        is_locked
```
This change ensures that `is_locked` is always returned, regardless of whether an exception is raised.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+module DynamicLinks
+  module Async
+    # @author Saiqul Haq <saiqulhaq@gmail.com>
+    class Locker
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
+      end
+
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
+      end
+
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
+      end
+
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
+      end
+
+      private
+
+      def url_to_lock_key(url)
+        Digest::SHA256.hexdigest(url)
+      end
+    end
+  end
+end","--- 

+++ 

@@ -32,7 +32,7 @@

           yield if is_locked && block_given?
 
           unless is_locked
-            raise LockAcquisitionError, ""Unable to acquire lock for key: #{lock_key}""
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
           end
         rescue => e
           DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731908,1.0,17,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20240128030329_fix_citus_index.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The migration script seems to be altering the primary key of the `dynamic_links_shortened_urls` table when the database infrastructure strategy is set to `:citus`. This could potentially lead to data inconsistency issues if not handled properly. 

Ensure that there are no active connections or ongoing transactions on the `dynamic_links_shortened_urls` table before running this migration. Also, consider adding a rollback method in case the migration needs to be reversed.

Moreover, it's important to note that changing the primary key will invalidate foreign keys in other tables referencing this one. If there are any such foreign keys, they need to be dropped before this migration and recreated afterwards with the new primary key.

Lastly, ensure that the application code that interacts with this table can handle the change in the primary key structure. For instance, ORM queries based on the old primary key would fail after this migration.

Here's an example of how you might add a `down` method for rollback:

```ruby
def down
  if DynamicLinks.configuration.db_infra_strategy == :citus
    execute <<-SQL
      ALTER TABLE dynamic_links_shortened_urls
      DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
    SQL

    execute <<-SQL
      ALTER TABLE dynamic_links_shortened_urls
      ADD PRIMARY KEY (id);
    SQL
    # Add logic to revert the distributed table back to normal
  end
end
```
This `down` method reverts the changes made in the `up` method by dropping the new primary key and restoring the old one. Note that you'll also need to add logic to revert the distributed table back to a normal table if possible.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class FixCitusIndex < ActiveRecord::Migration[7.1]
+  def change
+    if DynamicLinks.configuration.db_infra_strategy == :citus
+      # execute SQL to remove primary key constraint
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id, client_id);
+      SQL
+      create_distributed_table :dynamic_links_shortened_urls, :client_id
+    end
+  end
+end","--- 

+++ 

@@ -1,6 +1,6 @@

 class FixCitusIndex < ActiveRecord::Migration[7.1]
-  def change
-    if DynamicLinks.configuration.db_infra_strategy == :citus
+  def up
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
       # execute SQL to remove primary key constraint
       execute <<-SQL
         ALTER TABLE dynamic_links_shortened_urls
@@ -14,4 +14,21 @@

       create_distributed_table :dynamic_links_shortened_urls, :client_id
     end
   end
+
+  # this code is untested
+  def down
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
+      drop_distributed_table :dynamic_links_shortened_urls, :client_id
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id);
+      SQL
+    end
+  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731909,29.0,31,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/db/schema.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The primary key of the `dynamic_links_shortened_urls` table is now a composite key consisting of `id` and `client_id`. This change could potentially affect any queries or operations that rely on the primary key. Ensure that all such instances are updated to handle this new composite key.
```diff
- create_table ""dynamic_links_shortened_urls"", force: :cascade do |t|
-    t.bigint ""client_id""
+ create_table ""dynamic_links_shortened_urls"", primary_key: [""id"", ""client_id""], force: :cascade do |t|
+     t.bigserial ""id"", null: false
+     t.bigint ""client_id"", null: false
```
Please also ensure that foreign key constraints in other tables referencing this primary key are updated accordingly.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  create_table ""dynamic_links_shortened_urls"", primary_key: [""id"", ""client_id""], force: :cascade do |t|
+    t.bigserial ""id"", null: false
+    t.bigint ""client_id"", null: false",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731910,20.0,21,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `multi_tenant` method is being called conditionally based on its existence. This could lead to unexpected behavior if the method does not exist. It would be better to ensure that the method is always available or handle the absence of the method more explicitly.
```diff
-    multi_tenant :client if respond_to?(:multi_tenant)
+    if respond_to?(:multi_tenant)
+      multi_tenant :client
+    else
+      # Handle the case when multi_tenant method is not available
+      # For example, raise an error or log a warning
+    end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    belongs_to :client
+    multi_tenant :client if respond_to?(:multi_tenant)","--- 

+++ 

@@ -25,10 +25,8 @@

 
     def self.find_or_create!(client, short_url, url)
       transaction do
-        record = find_or_initialize_by(client: client, short_url: short_url)
-        if record.new_record?
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
           record.url = url
-          record.save!
         end
         record
       end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731911,26.0,39,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create!` method has been implemented correctly and it handles the ActiveRecord::RecordInvalid exception well by logging the error and re-raising it. However, it's important to note that this method will lock the row in the database during the transaction which might affect performance if there are many concurrent requests. If this becomes a problem, consider using a different strategy for handling concurrency, such as optimistic locking or a distributed lock.

Also, the `find_or_initialize_by` method is used here, which can lead to race conditions if two threads run this code at the same time with the same parameters. One thread could initialize the record after the other has checked for its existence but before it has had a chance to save it. Consider using `find_or_create_by` instead, which is atomic and avoids this issue.
```diff
-        record = find_or_initialize_by(client: client, short_url: short_url)
+        record = find_or_create_by(client: client, short_url: short_url) do |r|
+          r.url = url
+        end
```
Please note that the above change will make lines 29-32 redundant and they should be removed.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_initialize_by(client: client, short_url: short_url)
+        if record.new_record?
+          record.url = url
+          record.save!
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
+    end","--- 

+++ 

@@ -25,10 +25,8 @@

 
     def self.find_or_create!(client, short_url, url)
       transaction do
-        record = find_or_initialize_by(client: client, short_url: short_url)
-        if record.new_record?
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
           record.url = url
-          record.save!
         end
         record
       end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731913,4.0,6,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The configuration for `db_infra_strategy` is only set when `CITUS_ENABLED` is true. If it's false or not set, `db_infra_strategy` will be nil which might cause issues later in the code. Consider setting a default value for `db_infra_strategy`.
```diff
  if ENV['CITUS_ENABLED'].to_s == 'true'
    config.db_infra_strategy = :citus
+ else
+   config.db_infra_strategy = :default_strategy
  end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if ENV['CITUS_ENABLED'].to_s == 'true'
+    config.db_infra_strategy = :citus
+  end","--- 

+++ 

@@ -2,7 +2,7 @@

   config.shortening_strategy = ENV['SHORTENING_STRATEGY'].to_sym
   config.enable_rest_api = true
   if ENV['CITUS_ENABLED'].to_s == 'true'
-    config.db_infra_strategy = :citus
+    config.db_infra_strategy = :sharding
   end
 
   config.redis_counter_config.pool_size = 5"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731915,67.0,69,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/models/dynamic_links/client_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case 'should not save client with invalid hostname' is a good addition. However, it would be more informative if you assert the specific validation error message that you expect to be raised when trying to save a client with an invalid hostname. This way, you can ensure that the record is invalid for the expected reason.
```diff
- refute Client.new(name: 'Test Client', api_key: 'test_api_key', hostname: 'invalid hostname', scheme: 'https').save
+ client = Client.new(name: 'Test Client', api_key: 'test_api_key', hostname: 'invalid hostname', scheme: 'https')
+ assert_not client.save
+ assert_includes client.errors.full_messages, ""Hostname is invalid""
```
This change ensures that the test will fail if the client is invalid for any other reason than the hostname being invalid.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    test 'should not save client with invalid hostname' do
+      refute Client.new(name: 'Test Client', api_key: 'test_api_key', hostname: 'invalid hostname', scheme: 'https').save
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731916,20.0,26,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/jobs/shorten_url_job_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a typo in the test description on line 20. It should be `strategy#always_growing?` instead of `startegy#always_growing?`.
```diff
-     test 'perform should create a shortened URL if startegy#always_growing? is true' do
+     test 'perform should create a shortened URL if strategy#always_growing? is true' do
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    test 'perform should create a shortened URL if startegy#always_growing? is true' do
+      @strategy.stubs(:always_growing?).returns(true)
+      @job.stubs(:storage).returns(@storage)
+
+      @storage.expects(:create!).with(client: @client, url: @url, short_url: ""#{@short_url}11"")
+      @job.perform(@client, @url, ""#{@short_url}11"", @lock_key)
+    end","--- 

+++ 

@@ -17,7 +17,7 @@

       @locker.cache_store.write(@lock_key, true)
     end
 
-    test 'perform should create a shortened URL if startegy#always_growing? is true' do
+    test 'perform should create a shortened URL if strategy#always_growing? is true' do
       @strategy.stubs(:always_growing?).returns(true)
       @job.stubs(:storage).returns(@storage)
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731917,45.0,55,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/jobs/shorten_url_job_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In your test case for error handling, you're not ensuring that the lock is released when an exception occurs. This could lead to deadlocks in your system. Consider adding a `ensure` block to always unlock the key regardless of whether an exception was raised or not.
```diff
      @strategy.stubs(:always_growing?).returns(true)
      ShortenedUrl.stubs(:create!).raises(ShorteningFailed.new('Creation failed'))
      DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error in ShortenUrlJob/))

      assert_raises ShorteningFailed do
        @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
      end

      assert @locker.locked?(@lock_key)
+       ensure
+         @locker.unlock(@lock_key)
    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    test 'perform should log error and re-raise exception on failure' do
+      @strategy.stubs(:always_growing?).returns(true)
+      ShortenedUrl.stubs(:create!).raises(ShorteningFailed.new('Creation failed'))
+      DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/Error in ShortenUrlJob/))
+
+      assert_raises ShorteningFailed do
+        @job.perform(@client, @url, ""#{@short_url}123"", @lock_key)
+      end
+
+      assert @locker.locked?(@lock_key)
+    end","--- 

+++ 

@@ -17,7 +17,7 @@

       @locker.cache_store.write(@lock_key, true)
     end
 
-    test 'perform should create a shortened URL if startegy#always_growing? is true' do
+    test 'perform should create a shortened URL if strategy#always_growing? is true' do
       @strategy.stubs(:always_growing?).returns(true)
       @job.stubs(:storage).returns(@storage)
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468731918,20.0,122,6a04344edc078339533770c8f620fb2067e95fbd,67675ef2633001974317dae9fc570ecf17a48936,test/models/dynamic_links/shortened_url_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The tests are well written and cover a wide range of scenarios. However, there seems to be some redundancy in the tests for `find_or_create!` method. The tests on lines 70-74, 76-87, 109-113, and 115-118 are essentially testing the same functionality of the `find_or_create!` method. It would be more efficient to combine these into two distinct tests: one for when the record already exists, and another for when it does not.

```diff
-   test ""find_or_create returns existing record if it exists"" do
-     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
-     result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     assert_equal existing_record, result, ""Expected to return the existing record""
-   end
-  
-   test ""find_or_create creates and returns a new record if it doesn't exist"" do
-     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
-       result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-       assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
-       assert_equal @client, result.client
-       assert_equal @url, result.url
-       assert_equal @short_url, result.short_url
-       # Call find_or_create again and ensure the count doesn't change
-       assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
-         DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-       end
-      end
-   end
-  
-   test 'find_or_create! should find existing record' do
-     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
-     found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     assert_equal existing_record, found_record
-   end
- 
-   test 'find_or_create! should create new record if not exists' do
-     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
-       DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     end
-   end

+   test 'find_or_create! returns existing record if it exists and creates a new one if it does not' do
+     # Test when the record already exists
+     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+     result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+     assert_equal existing_record, result, ""Expected to return the existing record""
+
+     # Test when the record does not exist
+     new_short_url = 'new_short_url'
+     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+       result = DynamicLinks::ShortenedUrl.find_or_create!(@client, new_short_url, @url)
+       assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+       assert_equal @client, result.client
+       assert_equal @url, result.url
+       assert_equal new_short_url, result.short_url
+     end
+     
+     # Call find_or_create again and ensure the count doesn't change
+     assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+       DynamicLinks::ShortenedUrl.find_or_create!(@client, new_short_url, @url)
+     end
+   end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+class DynamicLinks::ShortenedUrlTest < ActiveSupport::TestCase
+  self.use_transactional_tests = true
 
-module DynamicLinks
-  class ShortenedUrlTest < ActiveSupport::TestCase
-    self.use_transactional_tests = true
+  setup do
+    @client = dynamic_links_clients(:one)
+    @url = 'https://example.com'
+    @short_url = 'shortened_url'
+  end
 
-    setup do
-      @client = dynamic_links_clients(:one)  # Assuming you have fixtures set up
-    end
+  test 'should not save shortened url without url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: 'abc123')
+    assert_not shortened_url.save, 'Saved the shortened url without a url'
+  end
 
-    test 'should not save shortened url without url' do
-      shortened_url = ShortenedUrl.new(client: @client, short_url: 'abc123')
-      assert_not shortened_url.save, 'Saved the shortened url without a url'
-    end
+  test 'should not save shortened url without short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com')
+    assert_not shortened_url.save, 'Saved the shortened url without a short_url'
+  end
 
-    test 'should not save shortened url without short_url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com')
-      assert_not shortened_url.save, 'Saved the shortened url without a short_url'
-    end
+  test 'should save valid shortened url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
+    assert shortened_url.save, 'Failed to save valid shortened url'
+  end
 
-    test 'should save valid shortened url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
-      assert shortened_url.save, 'Failed to save valid shortened url'
-    end
+  test 'should not save shortened url with duplicate short_url' do
+    DynamicLinks::ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
+    assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
+  end
 
-    test 'should not save shortened url with duplicate short_url' do
-      ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
-      duplicate_url = ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
-      assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
-    end
+  test 'should allow the same short_url for different clients' do
+    client_one = dynamic_links_clients(:one)
+    client_two = dynamic_links_clients(:two)
 
-    test 'reference to client is optional' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'abc123a')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
+    url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
 
-    test 'should handle urls without associated client' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'xyz789')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+  end
+
+  test 'should not allow the same short_url for the same client' do
+    client = dynamic_links_clients(:one)
 
-    test 'should allow the same short_url for different clients' do
-      client_one = dynamic_links_clients(:one)
-      client_two = dynamic_links_clients(:two)
+    DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
+
+    assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  end
 
-      url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
-      url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
+  test ""find_or_create returns existing record if it exists"" do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, result, ""Expected to return the existing record""
+  end
 
-      assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+  test ""find_or_create creates and returns a new record if it doesn't exist"" do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+      assert_equal @client, result.client
+      assert_equal @url, result.url
+      assert_equal @short_url, result.short_url
+      # Call find_or_create again and ensure the count doesn't change
+      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+        DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      end
     end
+  end
 
-    test 'should not allow the same short_url for the same client' do
-      client = dynamic_links_clients(:one)
+  test 'should validate presence of url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: @short_url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:url], ""can't be blank""
+  end
 
-      DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
-      duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
+  test 'should validate presence of short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: @url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:short_url], ""can't be blank""
+  end
+
+  test 'should validate uniqueness of short_url scoped to client_id' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    new_record = DynamicLinks::ShortenedUrl.new(client: @client, url: @url, short_url: @short_url)
+    assert_not new_record.valid?
+    assert_includes new_record.errors[:short_url], ""has already been taken""
+  end
+
+  test 'find_or_create! should find existing record' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, found_record
+  end
+
+  test 'find_or_create! should create new record if not exists' do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    end
+  end
 
-      assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  test 'find_or_create! should raise error and log if save fails' do
+    DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/ShortenedUrl creation failed/))","--- 

+++ 

@@ -84,6 +84,10 @@

       assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
         DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
       end
+
+      ActiveRecord::Base.transaction do
+        DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
+      end
     end
   end
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732984,14.0,21,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,app/controllers/dynamic_links/v1/short_links_controller.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error handling here is a bit too generic. It's good that you're logging the error, but it would be better to provide more specific error messages to the client based on the type of error that occurred. This will help with debugging and also improve the user experience.
```diff
-    rescue => e
+    rescue StandardError => e
```
This change ensures that only standard errors are caught, not other types of exceptions like script errors or system exits.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      multi_tenant(client) do
+        render json: DynamicLinks.generate_short_url(url, client), status: :created
+      end
     rescue DynamicLinks::InvalidURIError
       render json: { error: 'Invalid URL' }, status: :bad_request
+    rescue => e
+      DynamicLinks::Logger.log_error(e)
+      render json: { error: 'An error occurred while processing your request' }, status: :internal_server_error",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732985,4.0,5,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `attr_accessor` method is used to create both getter and setter methods. However, you have explicitly defined setter methods for these attributes later in the code (lines 41, 46, 51, 56, 61, 66). This results in unnecessary duplication. You should use `attr_reader` instead of `attr_accessor` for these attributes to avoid this redundancy.
```diff
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
-                  :async_processing, :redis_counter_config, :cache_store
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                :async_processing, :redis_counter_config, :cache_store
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store","--- 

+++ 

@@ -1,7 +1,7 @@

 module DynamicLinks
   # @author Saiqul Haq <saiqulhaq@gmail.com>
   class Configuration
-    attr_accessor :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
                   :async_processing, :redis_counter_config, :cache_store
 
     VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732986,,2,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `SHORTENING_STRATEGY` environment variable is directly converted to a symbol without any validation. This could lead to unexpected behavior if an invalid strategy is provided. Consider adding a validation step to ensure that the provided strategy is one of the expected values.
```diff
- config.shortening_strategy = ENV['SHORTENING_STRATEGY'].to_sym
+ config.shortening_strategy = validate_shortening_strategy(ENV['SHORTENING_STRATEGY'])
```
In the above snippet, `validate_shortening_strategy` is a method that checks if the provided strategy is valid and returns it as a symbol. If the strategy is not valid, it should raise an appropriate error.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,10 +1,15 @@
 DynamicLinks.configure do |config|
-  config.shortening_strategy = :md5
+  config.shortening_strategy = ENV['SHORTENING_STRATEGY'].to_sym",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732987,4.0,6,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `db_infra_strategy` is set to `:sharding` only when `CITUS_ENABLED` is true. However, there's no default value for when `CITUS_ENABLED` is false or not set. It would be better to have a default value for `db_infra_strategy`.
```diff
  if ENV['CITUS_ENABLED'].to_s == 'true'
    config.db_infra_strategy = :sharding
+ else
+   config.db_infra_strategy = :default_strategy
  end
```
In the above snippet, `:default_strategy` should be replaced with whatever your default database infrastructure strategy is.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  if ENV['CITUS_ENABLED'].to_s == 'true'
+    config.db_infra_strategy = :sharding
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732988,8.0,13,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/dummy/config/initializers/dynamic_links.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The Redis configuration is now nested under `redis_counter_config`, which is a good change for organization. However, it seems like the old `redis_config` is completely removed. If other parts of the code are still using `redis_config`, this could cause issues. Make sure to update all references to `redis_config` or maintain backward compatibility if necessary.

Overall, these changes look good, but please consider the points mentioned above to improve the robustness and maintainability of the code.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  config.redis_counter_config.pool_size = 5
+  config.redis_counter_config.pool_timeout = 5
+  config.redis_counter_config.config = {
+    host: ENV.fetch('REDIS_HOST', 'localhost'),
+    port: ENV.fetch('REDIS_PORT', 6379).to_i,
+    db: ENV.fetch('REDIS_DB', 0).to_i,",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732989,6.0,7,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228165744_create_dynamic_links_clients.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Adding `scheme` and `hostname` fields to the `dynamic_links_clients` table is a good idea for supporting different protocols and hostnames. However, it's important to validate these fields to ensure they contain valid data. Consider adding model-level validations for these fields.

```ruby
validates :scheme, inclusion: { in: %w(http https) }
validates :hostname, format: { with: URI::DEFAULT_PARSER.make_regexp }
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      t.string :scheme, default: 'https', null: false
+      t.string :hostname, null: false",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732990,12.0,14,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228165744_create_dynamic_links_clients.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The conditional creation of the reference table based on the configuration value `db_infra_strategy` could lead to issues if the configuration changes after the migration has been run. Migrations should ideally be idempotent and not depend on external configurations. Consider creating the reference table unconditionally or handling this logic outside of the migration.



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if DynamicLinks.configuration.db_infra_strategy == :sharding
+      create_reference_table(:dynamic_links_clients)
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732991,16.0,18,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20231228165744_create_dynamic_links_clients.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Adding indexes on `name`, `api_key`, and `hostname` will improve query performance. However, please note that the `unique: true` constraint on `hostname` might cause issues if multiple clients are allowed to have the same hostname. If that's the case, consider removing the uniqueness constraint.
```diff
- add_index :dynamic_links_clients, :hostname, unique: true
+ add_index :dynamic_links_clients, :hostname
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    add_index :dynamic_links_clients, :name
     add_index :dynamic_links_clients, :api_key, unique: true
+    add_index :dynamic_links_clients, :hostname, unique: true",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732992,1.0,17,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20240128030329_fix_citus_index.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The migration script seems to be altering the primary key constraint of the `dynamic_links_shortened_urls` table based on the value of `DynamicLinks.configuration.db_infra_strategy`. However, there is no else clause or handling for when `db_infra_strategy` is not `:sharding`. This could lead to unexpected behavior if the strategy is different. Consider adding an else clause or a case statement to handle other strategies.

```diff
class FixCitusIndex < ActiveRecord::Migration[7.1]
  def change
    if DynamicLinks.configuration.db_infra_strategy == :sharding
      # execute SQL to remove primary key constraint
      execute <<-SQL
        ALTER TABLE dynamic_links_shortened_urls
        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
      SQL

      execute <<-SQL
        ALTER TABLE dynamic_links_shortened_urls
        ADD PRIMARY KEY (id, client_id);
      SQL
      create_distributed_table :dynamic_links_shortened_urls, :client_id
+15:     else
+16:       # Handle other strategies here
    end
  end
end
```
Also, it's important to note that this migration is irreversible because there's no `down` method defined. If you ever need to rollback this migration, it will fail. Consider making your migrations reversible whenever possible.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+class FixCitusIndex < ActiveRecord::Migration[7.1]
+  def change
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
+      # execute SQL to remove primary key constraint
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id, client_id);
+      SQL
+      create_distributed_table :dynamic_links_shortened_urls, :client_id
+    end
+  end
+end","--- 

+++ 

@@ -1,5 +1,5 @@

 class FixCitusIndex < ActiveRecord::Migration[7.1]
-  def change
+  def up
     if DynamicLinks.configuration.db_infra_strategy == :sharding
       # execute SQL to remove primary key constraint
       execute <<-SQL
@@ -14,4 +14,21 @@

       create_distributed_table :dynamic_links_shortened_urls, :client_id
     end
   end
+
+  # this code is untested
+  def down
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
+      drop_distributed_table :dynamic_links_shortened_urls, :client_id
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id);
+      SQL
+    end
+  end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732993,10.0,12,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/controllers/v1/short_links_controller_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Good practice to reset the configuration after each test. However, it would be better to use `ensure` block to make sure that the configuration is always reset even if an error occurs in the test.
```diff
-    teardown do
-      DynamicLinks.configuration.enable_rest_api = @original_rest_api_setting
-      DynamicLinks.configuration.db_infra_strategy = @original_db_infra_strategy
-    end
+    teardown do
+      begin
+        # Test code here...
+      ensure
+        DynamicLinks.configuration.enable_rest_api = @original_rest_api_setting
+        DynamicLinks.configuration.db_infra_strategy = @original_db_infra_strategy
+      end
+    end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","   teardown do
-    # Reset the configuration after each test
     DynamicLinks.configuration.enable_rest_api = @original_rest_api_setting
+    DynamicLinks.configuration.db_infra_strategy = @original_db_infra_strategy",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732994,54.0,60,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/controllers/v1/short_links_controller_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The test case seems fine, but it's not actually asserting anything. It's good to have a check for the response or the result of the operation.
```diff
-    ::MultiTenant.expects(:with).with(@client).once
-    url = 'https://example.com/'
-    api_key = @client.api_key
-    post '/v1/shortLinks', params: { url: url, api_key: api_key }
+    assert_difference('ShortLink.count') do
+      ::MultiTenant.expects(:with).with(@client).once
+      url = 'https://example.com/'
+      api_key = @client.api_key
+      post '/v1/shortLinks', params: { url: url, api_key: api_key }
+    end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""should use MultiTenant.with when db_infra_strategy is :sharding"" do
+    DynamicLinks.configuration.db_infra_strategy = :sharding
+    ::MultiTenant.expects(:with).with(@client).once
+    url = 'https://example.com/'
+    api_key = @client.api_key
+    post '/v1/shortLinks', params: { url: url, api_key: api_key }
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732995,62.0,68,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/controllers/v1/short_links_controller_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similar to the previous comment, this test case should also include an assertion to verify the expected outcome.
```diff
-    ::MultiTenant.expects(:with).with(@client).never
-    url = 'https://example.com/'
-    api_key = @client.api_key
-    post '/v1/shortLinks', params: { url: url, api_key: api_key }
+    assert_no_difference('ShortLink.count') do
+      ::MultiTenant.expects(:with).with(@client).never
+      url = 'https://example.com/'
+      api_key = @client.api_key
+      post '/v1/shortLinks', params: { url: url, api_key: api_key }
+    end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  test ""should not use MultiTenant.with when db_infra_strategy is not :sharding"" do
+    DynamicLinks.configuration.db_infra_strategy = :standard
+    url = 'https://example.com/'
+    api_key = @client.api_key
+    ::MultiTenant.expects(:with).with(@client).never
+    post '/v1/shortLinks', params: { url: url, api_key: api_key }
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468732996,20.0,128,e05aa587bfb9b9b6df12263b94eda4c473794ee6,67675ef2633001974317dae9fc570ecf17a48936,test/models/dynamic_links/shortened_url_test.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The tests are well written and cover a wide range of scenarios. However, there seems to be some redundancy in the test cases for `find_or_create!` method. The tests at lines 70-74, 76-92, 113-116, and 119-123 seem to be testing the same functionality with slight variations. Consider consolidating these tests to avoid redundancy and improve maintainability.
```diff
-   test ""find_or_create returns existing record if it exists"" do
-     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
-     result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     assert_equal existing_record, result, ""Expected to return the existing record""
-   end
- 
-   test ""find_or_create creates and returns a new record if it doesn't exist"" do
-     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
-       result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-       assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
-       assert_equal @client, result.client
-       assert_equal @url, result.url
-       assert_equal @short_url, result.short_url
-       # Call find_or_create again and ensure the count doesn't change
-       assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
-         DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-       end
- 
-       ActiveRecord::Base.transaction do
-         DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
-       end
-      end
-   end
- 
-   test 'find_or_create! should find existing record' do
-     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
-     found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     assert_equal existing_record, found_record
-   end
- 
-   test 'find_or_create! should create new record if not exists' do
-     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
-       DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
-     end
-   end

+   test 'find_or_create! should handle both existing and non-existing records' do
+     # Test for existing record
+     existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+     found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+     assert_equal existing_record, found_record, ""Expected to return the existing record""
+
+     # Test for non-existing record
+     new_short_url = 'new_short_url'
+     assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+       result = DynamicLinks::ShortenedUrl.find_or_create!(@client, new_short_url, @url)
+       assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+       assert_equal @client, result.client
+       assert_equal @url, result.url
+       assert_equal new_short_url, result.short_url
+     end
+
+     # Call find_or_create again and ensure the count doesn't change
+     assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+       DynamicLinks::ShortenedUrl.find_or_create!(@client, new_short_url, @url)
+     end
+   end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+# @author Saiqul Haq <saiqulhaq@gmail.com>
+class DynamicLinks::ShortenedUrlTest < ActiveSupport::TestCase
+  self.use_transactional_tests = true
 
-module DynamicLinks
-  class ShortenedUrlTest < ActiveSupport::TestCase
-    self.use_transactional_tests = true
+  setup do
+    @client = dynamic_links_clients(:one)
+    @url = 'https://example.com'
+    @short_url = 'shortened_url'
+  end
 
-    setup do
-      @client = dynamic_links_clients(:one)  # Assuming you have fixtures set up
-    end
+  test 'should not save shortened url without url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: 'abc123')
+    assert_not shortened_url.save, 'Saved the shortened url without a url'
+  end
 
-    test 'should not save shortened url without url' do
-      shortened_url = ShortenedUrl.new(client: @client, short_url: 'abc123')
-      assert_not shortened_url.save, 'Saved the shortened url without a url'
-    end
+  test 'should not save shortened url without short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com')
+    assert_not shortened_url.save, 'Saved the shortened url without a short_url'
+  end
 
-    test 'should not save shortened url without short_url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com')
-      assert_not shortened_url.save, 'Saved the shortened url without a short_url'
-    end
+  test 'should save valid shortened url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
+    assert shortened_url.save, 'Failed to save valid shortened url'
+  end
 
-    test 'should save valid shortened url' do
-      shortened_url = ShortenedUrl.new(client: @client, url: 'https://example.com', short_url: 'abc123ss')
-      assert shortened_url.save, 'Failed to save valid shortened url'
-    end
+  test 'should not save shortened url with duplicate short_url' do
+    DynamicLinks::ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
+    assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
+  end
 
-    test 'should not save shortened url with duplicate short_url' do
-      ShortenedUrl.create!(client: @client, url: 'https://example.com', short_url: 'abc123b')
-      duplicate_url = ShortenedUrl.new(client: @client, url: 'https://example.com/another', short_url: 'abc123b')
-      assert_not duplicate_url.save, 'Saved the shortened url with a duplicate short_url'
-    end
+  test 'should allow the same short_url for different clients' do
+    client_one = dynamic_links_clients(:one)
+    client_two = dynamic_links_clients(:two)
 
-    test 'reference to client is optional' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'abc123a')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
+    url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
 
-    test 'should handle urls without associated client' do
-      shortened_url = ShortenedUrl.new(url: 'https://example.com', short_url: 'xyz789')
-      assert shortened_url.save, 'Failed to save shortened url without an associated client'
-    end
+    assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+  end
 
-    test 'should allow the same short_url for different clients' do
-      client_one = dynamic_links_clients(:one)
-      client_two = dynamic_links_clients(:two)
+  test 'should not allow the same short_url for the same client' do
+    client = dynamic_links_clients(:one)
 
-      url_one = DynamicLinks::ShortenedUrl.create!(client: client_one, url: 'https://example.com', short_url: 'foobar')
-      url_two = DynamicLinks::ShortenedUrl.new(client: client_two, url: 'https://example.org', short_url: 'foobar')
+    DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
+    duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
 
-      assert url_two.valid?, 'ShortenedUrl with duplicate short_url but different client should be valid'
+    assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  end
+
+  test ""find_or_create returns existing record if it exists"" do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, result, ""Expected to return the existing record""
+  end
+
+  test ""find_or_create creates and returns a new record if it doesn't exist"" do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      result = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      assert_not_nil result, ""Expected a new ShortenedUrl record to be created""
+      assert_equal @client, result.client
+      assert_equal @url, result.url
+      assert_equal @short_url, result.short_url
+      # Call find_or_create again and ensure the count doesn't change
+      assert_no_difference 'DynamicLinks::ShortenedUrl.count' do
+        DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+      end
+
+      ActiveRecord::Base.transaction do
+        DynamicLinks::ShortenedUrl.lock.find_or_create!(@client, @short_url, @url)
+      end
     end
+  end
 
-    test 'should not allow the same short_url for the same client' do
-      client = dynamic_links_clients(:one)
+  test 'should validate presence of url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, short_url: @short_url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:url], ""can't be blank""
+  end
 
-      DynamicLinks::ShortenedUrl.create!(client: client, url: 'https://example.com', short_url: 'xyz789')
-      duplicate_url = DynamicLinks::ShortenedUrl.new(client: client, url: 'https://example.org', short_url: 'xyz789')
+  test 'should validate presence of short_url' do
+    shortened_url = DynamicLinks::ShortenedUrl.new(client: @client, url: @url)
+    assert_not shortened_url.valid?
+    assert_includes shortened_url.errors[:short_url], ""can't be blank""
+  end
+
+  test 'should validate uniqueness of short_url scoped to client_id' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    new_record = DynamicLinks::ShortenedUrl.new(client: @client, url: @url, short_url: @short_url)
+    assert_not new_record.valid?
+    assert_includes new_record.errors[:short_url], ""has already been taken""
+  end
+
+  test 'find_or_create! should find existing record' do
+    existing_record = DynamicLinks::ShortenedUrl.create!(client: @client, url: @url, short_url: @short_url)
+    found_record = DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    assert_equal existing_record, found_record
+  end
+
+  test 'find_or_create! should create new record if not exists' do
+    assert_difference 'DynamicLinks::ShortenedUrl.count', 1 do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, @short_url, @url)
+    end
+  end
 
-      assert_not duplicate_url.valid?, 'ShortenedUrl with duplicate short_url for the same client should not be valid'
+  test 'find_or_create! should raise error and log if save fails' do
+    DynamicLinks::Logger.expects(:log_error).with(regexp_matches(/ShortenedUrl creation failed/))
+    assert_raises ActiveRecord::RecordInvalid do
+      DynamicLinks::ShortenedUrl.find_or_create!(@client, nil, @url)",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734074,18.0,33,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,db/migrate/20240128030329_fix_citus_index.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `down` migration method is marked as untested. It's crucial to ensure that all migrations can be rolled back without issues, especially when dealing with database constraints and sharding strategies. Please add tests for the `down` method to verify its functionality.
```diff
  # this code is untested
  def down
    if DynamicLinks.configuration.db_infra_strategy == :sharding
      drop_distributed_table :dynamic_links_shortened_urls, :client_id

      execute <<-SQL
        ALTER TABLE dynamic_links_shortened_urls
        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
      SQL

      execute <<-SQL
        ALTER TABLE dynamic_links_shortened_urls
        ADD PRIMARY KEY (id);
      SQL
    end
  end
end
```
Please consider adding a test case to validate the rollback operation.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  # this code is untested
+  def down
+    if DynamicLinks.configuration.db_infra_strategy == :sharding
+      drop_distributed_table :dynamic_links_shortened_urls, :client_id
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        DROP CONSTRAINT dynamic_links_shortened_urls_pkey;
+      SQL
+
+      execute <<-SQL
+        ALTER TABLE dynamic_links_shortened_urls
+        ADD PRIMARY KEY (id);
+      SQL
+    end
+  end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734075,24.0,25,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/client.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The hostname and scheme validations are good additions for ensuring data integrity. However, the regular expression used for validating the hostname is quite complex and may not cover all edge cases. Consider using a library or built-in method to validate hostnames if available. Also, the `VALID_SCHEMES` constant already restricts the scheme to 'http' or 'https', so the format validation on line 25 seems redundant.
```diff
- validates :hostname, format: { with: /\A[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?)*\z/i, message: 'must be a valid hostname' }
+ validates :hostname, format: { with: URI::DEFAULT_PARSER.make_regexp(%w[http https]), message: 'must be a valid hostname' }

- validates :scheme, format: { with: /\Ahttps?\z/i, message: 'must be a valid scheme' }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    validates :hostname, format: { with: /\A[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?)*\z/i, message: 'must be a valid hostname' }
+    validates :scheme, format: { with: /\Ahttps?\z/i, message: 'must be a valid scheme' }","--- 

+++ 

@@ -22,6 +22,5 @@

     validates :hostname, presence: true, uniqueness: true
     validates :scheme, presence: true, inclusion: { in: VALID_SCHEMES }
     validates :hostname, format: { with: /\A[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?(\.[a-z0-9]([a-z0-9\-]{,61}[a-z0-9])?)*\z/i, message: 'must be a valid hostname' }
-    validates :scheme, format: { with: /\Ahttps?\z/i, message: 'must be a valid scheme' }
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734076,26.0,37,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,app/models/dynamic_links/shortened_url.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `find_or_create!` method could potentially lead to race conditions. If two threads try to create the same record at the same time, one of them will fail with a `RecordInvalid` exception due to the uniqueness validation on `short_url`. This is because `find_or_create_by!` is not atomic. It first tries to find the record, and if it doesn't exist, it attempts to create it. Between these two operations, another thread could create the record, leading to an exception when the first thread tries to create it.

To avoid this, you can use `create_or_find_by!` instead, which first tries to create the record, and if it fails due to a uniqueness constraint violation, it then tries to find the existing record. This method is atomic and avoids the race condition.

```diff
-        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+        record = create_or_find_by!(client: client, short_url: short_url) do |record|
           record.url = url
         end
```
Please note that `create_or_find_by!` was added in Rails 6.0. If you're using an older version of Rails, you'll need to handle the `ActiveRecord::RecordNotUnique` exception manually.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def self.find_or_create!(client, short_url, url)
+      transaction do
+        record = find_or_create_by!(client: client, short_url: short_url) do |record|
+          record.url = url
+        end
+        record
+      end
+    rescue ActiveRecord::RecordInvalid => e
+      # Log the error and re-raise if needed or return a meaningful error message
+      DynamicLinks::Logger.log_error(""ShortenedUrl creation failed: #{e.message}"")
+      raise e
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734077,1.0,60,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/async/locker.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Locker` class seems to be well implemented. It provides a mechanism for acquiring and releasing locks, which is crucial in an asynchronous environment to prevent race conditions. The use of a cache store (like Redis or Memcached) for storing the locks is a good choice as these stores are designed for high-speed operations. The error handling and logging are also properly done.

However, there's a potential issue with the `lock_if_absent` method. If an exception occurs within the provided block (line 32), the lock won't be released. This could lead to a situation where a lock is held indefinitely, blocking other processes from acquiring it. To avoid this, you should ensure that the lock is released even if an error occurs during the execution of the block.

Here's a suggested change:

```diff
      def lock_if_absent(lock_key, expires_in: 60, &block)
        is_locked = false
        begin
          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
-32:           yield if is_locked && block_given?
+32:           yield if is_locked && block_given?

          unless is_locked
            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
          end
        rescue => e
          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
          raise e
        ensure
+41:           unlock(lock_key) if is_locked
        end

        is_locked
      end
```

This ensures that the lock is released in the `ensure` block, which will always execute regardless of whether an exception was raised. Note that this will change the behavior of your method - the lock will now be released after the block is executed, not when the job is done as mentioned in the comments. If you need to keep the lock until the job is done, you might need to handle the lock release within the job itself, ensuring it's released even if an error occurs.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+module DynamicLinks
+  module Async
+    # @author Saiqul Haq <saiqulhaq@gmail.com>
+    class Locker
+      LockAcquisitionError = Class.new(StandardError)
+      LockReleaseError = Class.new(StandardError)
+      attr_reader :cache_store
+
+      def initialize(cache_store = DynamicLinks.configuration.cache_store)
+        @cache_store = cache_store
+      end
+
+      def generate_lock_key(client, url)
+        ""lock:shorten_url#{client.id}:#{url_to_lock_key(url)}""
+      end
+
+      def locked?(lock_key)
+        cache_store.exist?(lock_key)
+      end
+
+      # Acquires a lock for the given key and executes the block if lock is acquired.
+      # This method won't release the lock after block execution.
+      # We release the lock in the job after the job is done.
+      # @param [String] lock_key, it's better to use generate_lock_key method to generate lock_key
+      # @param [Integer] expires_in, default is 60 seconds
+      # @param [Block] block, the block to be executed if lock is acquired
+      # @return [Boolean]
+      def lock_if_absent(lock_key, expires_in: 60, &block)
+        is_locked = false
+        begin
+          is_locked = cache_store.increment(lock_key, 1, expires_in: expires_in) == 1
+          yield if is_locked && block_given?
+
+          unless is_locked
+            DynamicLinks::Logger.log_info ""Unable to acquire lock for key: #{lock_key}""
+          end
+        rescue => e
+          DynamicLinks::Logger.log_error(""Locking error: #{e.message}"")
+          raise e
+        end
+
+        is_locked
+      end
+
+      # Deletes an entry in the cache. Returns true if an entry is deleted and false otherwise.
+      # @return [Boolean]
+      def unlock(lock_key)
+        deleted = cache_store.delete(lock_key)
+        raise LockReleaseError, ""Unable to release lock for key: #{lock_key}"" unless deleted
+        deleted
+      end
+
+      private
+
+      def url_to_lock_key(url)
+        Digest::SHA256.hexdigest(url)
+      end
+    end
+  end
+end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734078,4.0,5,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `attr_reader` method is used here to define getter methods for the instance variables. This is a good practice as it encapsulates the instance variables and provides control over their values and visibility.

However, I noticed that you removed the `redis_pool_size` and `redis_pool_timeout` attributes from the old hunk. If these configurations are still needed in other parts of your application, removing them might cause issues. If they are not needed anymore, then this change is fine.

```diff
- attr_accessor :shortening_strategy, :redis_config,
-               :redis_pool_size, :redis_pool_timeout,
-               :enable_rest_api
+ attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+             :async_processing, :redis_counter_config, :cache_store
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    attr_reader :shortening_strategy, :enable_rest_api, :db_infra_strategy,
+                  :async_processing, :redis_counter_config, :cache_store",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734079,7.0,8,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Defining constants for valid strategies is a good practice as it makes the code more maintainable and less error-prone. It's also easier to add or remove strategies in the future.

```diff
- # No equivalent line in old hunk
+ VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    VALID_DB_INFRA_STRATEGIES = [:standard, :sharding].freeze
+",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734080,31.0,39,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `initialize` method has been updated to set default values for the new configuration options. This is a good practice as it ensures that all configuration options have a default value, reducing the chance of runtime errors due to unset configuration options.

```diff
- def initialize
-  @shortening_strategy = :MD5  # Default strategy
-  @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-  @redis_pool_size = 5  # Default pool size
-  @redis_pool_timeout = 5  # Default timeout in seconds
-  @enable_rest_api = true  # Enable REST API by default
- end
+ def initialize
+  @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+  @enable_rest_api = DEFAULT_ENABLE_REST_API
+  @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+  @async_processing = DEFAULT_ASYNC_PROCESSING
+  # config for RedisCounterStrategy
+  @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+  @cache_store = DEFAULT_CACHE_STORE
+ end
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","     def initialize
-      @shortening_strategy = :MD5  # Default strategy
-      @redis_config = {}  # Default to an empty hash, can be overridden in configuration
-      @redis_pool_size = 5  # Default pool size
-      @redis_pool_timeout = 5  # Default timeout in seconds
-      @enable_rest_api = true  # Enable REST API by default
+      @shortening_strategy = DEFAULT_SHORTENING_STRATEGY
+      @enable_rest_api = DEFAULT_ENABLE_REST_API
+      @db_infra_strategy = DEFAULT_DB_INFRA_STRATEGY
+      @async_processing = DEFAULT_ASYNC_PROCESSING
+      # config for RedisCounterStrategy
+      @redis_counter_config = DEFAULT_REDIS_COUNTER_CONFIG
+      @cache_store = DEFAULT_CACHE_STORE
+    end",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1468734081,41.0,69,cb45762abe756fcf955be1c98765739f4d8e6f67,67675ef2633001974317dae9fc570ecf17a48936,lib/dynamic_links/configuration.rb,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The setter methods have been updated to include validation checks. This is a good practice as it ensures that the values being set are valid and reduces the chance of runtime errors due to invalid configuration options.

```diff
- # No equivalent lines in old hunk
+ def shortening_strategy=(strategy)
+   raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+   @shortening_strategy = strategy
+ end
+
+ def enable_rest_api=(value)
+   raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+   @enable_rest_api = value
+ end
+
+ def db_infra_strategy=(strategy)
+   raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+   @db_infra_strategy = strategy
+ end
+
+ def async_processing=(value)
+   raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+   @async_processing = value
+ end
+
+ def redis_counter_config=(config)
+   raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+   @redis_counter_config = config
+ end
+
+ def cache_store=(store)
+   raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+   @cache_store = store
+ end
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    def shortening_strategy=(strategy)
+      raise ArgumentError, ""Invalid shortening strategy"" unless StrategyFactory::VALID_SHORTENING_STRATEGIES.include?(strategy)
+      @shortening_strategy = strategy
+    end
+
+    def enable_rest_api=(value)
+      raise ArgumentError, ""enable_rest_api must be a boolean"" unless [true, false].include?(value)
+      @enable_rest_api = value
+    end
+
+    def db_infra_strategy=(strategy)
+      raise ArgumentError, ""Invalid DB infra strategy"" unless VALID_DB_INFRA_STRATEGIES.include?(strategy)
+      @db_infra_strategy = strategy
+    end
+
+    def async_processing=(value)
+      raise ArgumentError, ""async_processing must be a boolean"" unless [true, false].include?(value)
+      @async_processing = value
+    end
+
+    def redis_counter_config=(config)
+      raise ArgumentError, ""redis_counter_config must be an instance of RedisConfig"" unless config.is_a?(RedisConfig)
+      @redis_counter_config = config
+    end
+
+    def cache_store=(store)
+      raise ArgumentError, ""cache_store must be an instance of ActiveSupport::Cache::Store"" unless store.is_a?(ActiveSupport::Cache::Store)
+      @cache_store = store
     end",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1834997230,161.0,167,c19e559cd35d607cec57893456f4e9443148da95,8d58a421d62b626afbf1de632a3e5c86481776fa,docker/dd-extension/ui/src/components/Snapshot.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

To improve maintainability and avoid potential issues with undefined values, consider using optional chaining when accessing properties of `snapshotData`. This ensures that if any property is missing, it won't cause an error.
```diff
           localStorage.setItem('snapshot', JSON.stringify({
             token: snapshotData?.token,
             license: snapshotData?.license,
             restartPolicy: snapshotData?.restartPolicy,
             curlPort: snapshotData?.curlPort,
             country: snapshotData?.country,
             architecture: snapshotData?.architecture            
           }));
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          localStorage.setItem('snapshot', JSON.stringify({
+            token:token,
+            license:licenseKey,
+            restartPolicy:restartPolicy,
+            curlPort:curlPort,
+            country:country,
+            architecture:architecture            ","--- 

+++ 

@@ -132,12 +132,12 @@

     const storedData = localStorage.getItem('snapshot');
     if (storedData) {
       const snapshotData = JSON.parse(storedData);
-      setToken(snapshotData.token);
-      setLicenseKey(snapshotData.license);
-      setRestartPolicy(snapshotData.restartPolicy);
-      setCurlPort(snapshotData.curlPort);
-      setCountry(snapshotData.country);
-      setArchitecture(snapshotData.architecture);
+      setToken(snapshotData?.token);
+      setLicenseKey(snapshotData?.license);
+      setRestartPolicy(snapshotData?.restartPolicy);
+      setCurlPort(snapshotData?.curlPort);
+      setCountry(snapshotData?.country);
+      setArchitecture(snapshotData?.architecture);
     }
   }, []);
 "
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1834997232,72.0,77,c19e559cd35d607cec57893456f4e9443148da95,8d58a421d62b626afbf1de632a3e5c86481776fa,docker/dd-extension/ui/src/components/Stream.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Storing sensitive information like tokens in `localStorage` can pose security risks as it is accessible via JavaScript and can be exploited by malicious scripts. Consider using more secure storage mechanisms or encrypting the data before storing it.

```diff
+ // Consider encrypting sensitive data before storing
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+          localStorage.setItem('stream', JSON.stringify({
+            token:token,
+            streamPath:streamPath,
+            restartPolicy:restartPolicy,
+            license:license,
+          }));","--- 

+++ 

@@ -98,10 +98,10 @@

     const storedData = localStorage.getItem('stream');
     if (storedData) {
       const streamData = JSON.parse(storedData);
-      setToken(streamData.token);
-      setStreamPath(streamData.streamPath);
-      setRestartPolicy(streamData.restartPolicy);
-      setLicense(streamData.license);
+      setToken(streamData?.token);
+      setStreamPath(streamData?.streamPath);
+      setRestartPolicy(streamData?.restartPolicy);
+      setLicense(streamData?.license);
     }
   }, []);
 "
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1929972014,51.0,54,55d3744af73ac0094d7aa96136d695f5266e2bcf,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/soap.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logging statements should include more context about the failure, such as the reason for the failure if available. Consider capturing and logging any exception details or response content that might indicate why the request failed.

```diff
     response = client.service.PostImage(**request_data)
 
     if response:
         logging.info(f""Vehicle: {plate}. SOAP request successful."")
         return ""SOAP request successful."", 200
     else:
-        logging.error(f""Vehicle: {plate}. SOAP request failed."")
+        logging.error(f""Vehicle: {plate}. SOAP request failed. Response: {response}"")
         return ""SOAP request failed."", 400
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        logging.info(f""Vehicle: {plate}. SOAP request successful."")
         return ""SOAP request successful."", 200
     else:
+        logging.error(f""Vehicle: {plate}. SOAP request failed."")","--- 

+++ 

@@ -51,7 +51,7 @@

         logging.info(f""Vehicle: {plate}. SOAP request successful."")
         return ""SOAP request successful."", 200
     else:
-        logging.error(f""Vehicle: {plate}. SOAP request failed."")
+        logging.error(f""Vehicle: {plate}. SOAP request failed. Response: {response}"")
         return ""SOAP request failed."", 400
 
 "
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1929972015,72.0,74,55d3744af73ac0094d7aa96136d695f5266e2bcf,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/openeye.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The log message should include more context about the request, such as the URL or payload, to improve traceability and debugging.
```diff
-        logging.info(
-            f""Vehicle:{plate}. Response sent successfully: {response.status_code}""
-        )
+        logging.info(
+            f""Vehicle:{plate}, URL:{url}. Response sent successfully with status code: {response.status_code}""
+        )
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        logging.info(
+            f""Vehicle:{plate}. Response sent successfully: {response.status_code}""
+        )","--- 

+++ 

@@ -70,9 +70,9 @@

         response = requests.post(url, headers=headers, data=payload)
         response.raise_for_status()
         logging.info(
-            f""Vehicle:{plate}. Response sent successfully: {response.status_code}""
+            f""Vehicle:{plate}, URL:{url}. Response sent successfully with status code: {response.status_code}""
         )
         return ""Request sent successfully."", response.status_code
     except requests.exceptions.HTTPError as err:
-        logging.error(f""Vehicle:{plate}. Failed to send request: {err}"")
+        logging.error(f""Vehicle:{plate}, URL:{url}. Failed to send request: {err}"")
         return f""Failed to send request: {err}"", 400"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1929972018,75.0,77,55d3744af73ac0094d7aa96136d695f5266e2bcf,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/openeye.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

To enhance error handling, consider logging additional details like the URL or payload in the error message for better traceability.
```diff
-        logging.error(f""Vehicle:{plate}. Failed to send request: {err}"")
+        logging.error(f""Vehicle:{plate}, URL:{url}. Failed to send request: {err}"")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         return ""Request sent successfully."", response.status_code
     except requests.exceptions.HTTPError as err:
-        print(err)
+        logging.error(f""Vehicle:{plate}. Failed to send request: {err}"")","--- 

+++ 

@@ -70,9 +70,9 @@

         response = requests.post(url, headers=headers, data=payload)
         response.raise_for_status()
         logging.info(
-            f""Vehicle:{plate}. Response sent successfully: {response.status_code}""
+            f""Vehicle:{plate}, URL:{url}. Response sent successfully with status code: {response.status_code}""
         )
         return ""Request sent successfully."", response.status_code
     except requests.exceptions.HTTPError as err:
-        logging.error(f""Vehicle:{plate}. Failed to send request: {err}"")
+        logging.error(f""Vehicle:{plate}, URL:{url}. Failed to send request: {err}"")
         return f""Failed to send request: {err}"", 400"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1929972021,43.0,44,55d3744af73ac0094d7aa96136d695f5266e2bcf,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/synology_rest.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's an indentation issue on line 43. The `return` statement should be aligned with the `if` block.
```diff
-          return ""REST request successful."", response.status_code
+         return ""REST request successful."", response.status_code
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","         return ""REST request successful."", response.status_code
     else:",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1847761311,62.0,67,c11df0b86e1c729ffab5f43a17f162194dab28cc,0a8b126ddb2ad5a17023d3182d4fd26eefa4010b,snapshot-middleware/README.md,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [5-5]

The check for `Content-Length` being less than or equal to zero is a good addition. However, consider handling cases where `Content-Length` might not be present in the headers, which could lead to `parseInt` returning `NaN`. You can add a default value or an additional check.

```diff
-    if (contentType !== 'application/json' || parseInt(contentLength, 10) <= 0) {
+    if (contentType !== 'application/json' || !contentLength || parseInt(contentLength, 10) <= 0) {
```


<!-- This is an auto-generated reply by OSS CodeRabbit -->"," - Open a browser tab at http://localhost:8787/ to see your worker in action
 - Test with curl
     ```shell
-  	curl -vX POST http://localhost:8787/ -d @Survision.txt --header ""Content-Type: application/json"" --header ""survision-serial-number: sv1-searial-1""
+  	curl -vX POST http://localhost:8787/ -d @test/Survision.txt --header ""Content-Type: application/json"" --header ""survision-serial-number: sv1-searial-1""
+    curl -vX POST http://localhost:8787/ -d @test/Genetec.txt --header ""Content-Type: application/json""
     ```",
https://api.github.com/repos/Taiki130/pill_notify/pulls/comments/1487800467,12.0,13,adc15c999c94645f90656c286c1c6b1e6660337c,ad5391124f2ba41e080645b27079076e8b5d14de,.github/workflows/notify.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

LGTM!


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      - uses: actions/checkout@v4.1.1
+      - uses: actions/setup-go@v5",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1800851777,14.0,19,78bade31e089a24dea4e795fb211bc7110277d9c,48e3faf1f476cc97c697b374adb305fa5d5ed15f,.github/workflows/code-review.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The concurrency group definition is complex and may lead to unexpected behavior if not carefully managed. Consider simplifying the logic or adding comments for clarity.

```yaml
concurrency:
  group: ${{ github.repository }}-${{ github.event.number || github.head_ref || github.sha }}-${{ github.workflow }}-${{ github.event_name == 'pull_request_review_comment' && 'pr_comment' || 'pr' }}
  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+concurrency:
+  group:
+    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1827454482,,56,7742696fbb6f08fa319e1903ee13db48ab477d95,3f881a7931d59aeb7801361e09303985ed5f6559,docker/dd-extension/ui/src/components/ShowCommand.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

There's a typo in the error message. ""Failled"" should be corrected to ""Failed"".
```diff
-       ddClient.desktopUI.toast.error(`Run Command Failled: ${e.stderr.trim()}`);
+       ddClient.desktopUI.toast.error(`Run Command Failed: ${e.stderr.trim()}`);
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","       setRunningCommand(false);
       console.error(e);
-      ddClient.desktopUI.toast.error('Failed Run Command.');
+      ddClient.desktopUI.toast.error(`Run Command Failled: ${e.stderr.trim()}`);","--- 

+++ 

@@ -53,7 +53,7 @@

     }).catch(e => {
       setRunningCommand(false);
       console.error(e);
-      ddClient.desktopUI.toast.error(`Run Command Failled: ${e.stderr.trim()}`);
+      ddClient.desktopUI.toast.error(`Run Command Failed: ${e.stderr.trim()}`);
     })
   }
 "
https://api.github.com/repos/xun082/create-neat/pulls/comments/1554996741,,1,cc106b06877462e9650f7735ab93a15298eb2770,614a69b7a29e61e2b2ae649af8fe93660701dd44,packages/@plugin/plugin-babel/generator/index.cjs,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

`module.exports = (generatorAPI) => {` ËøôÈáåÂ∞Ü `export default` Êîπ‰∏∫‰∫Ü `module.exports`ÔºåËøôÊòØÊ≠£Á°ÆÁöÑÊõ¥Êîπ„ÄÇ



<!-- This is an auto-generated comment by OSS CodeRabbit -->","@@ -1,8 +1,8 @@
-export default (generatorAPI) => {
+module.exports = (generatorAPI) => {",
https://api.github.com/repos/xun082/create-neat/pulls/comments/1528386594,10.0,11,f2be23f1ae686199b9484b03542b92985cf21fff,4598fc72268b0d4692e7a29c4f5668b4113b85b5,.github/ISSUE_TEMPLATE/Regression.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Âú® `Is there an existing issue that is already proposing this?` ÈÄâÈ°π‰∏≠ÔºåÁº∫Â∞ë‰∫Ü `required: true` Â±ûÊÄß„ÄÇ
```diff
-        - label: ""I have searched the existing issues""
-          required: true
+        - label: ""I have searched the existing issues"" required: true
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        - label: ""I have searched the existing issues""
+          required: true",
https://api.github.com/repos/xun082/create-neat/pulls/comments/1528419651,39.0,67,81f540db2555909188e1df100702bfb760088c8d,4598fc72268b0d4692e7a29c4f5668b4113b85b5,packages/core/src/utils/dependenciesInstall.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Âú®ÂÆâË£ÖdevDependenciesÊó∂ÔºåÊ∑ªÂä†‰∫ÜÂºÇÂ∏∏Â§ÑÁêÜÁöÑ‰ª£Á†ÅÂùó„ÄÇËøôÊòØ‰∏Ä‰∏™ÂæàÂ•ΩÁöÑÂÆûË∑µÔºå‰ª•Á°Æ‰øùÂú®ÂÆâË£ÖËøáÁ®ã‰∏≠Âá∫Áé∞ÈîôËØØÊó∂ËÉΩÂ§üËøõË°åÈÄÇÂΩìÁöÑÂ§ÑÁêÜ„ÄÇÁÑ∂ËÄåÔºåÂú®ÊçïËé∑ÂºÇÂ∏∏ÂêéÔºåÂè™ÊòØÁÆÄÂçïÂú∞ÊâìÂç∞‰∫ÜÈîôËØØÊ∂àÊÅØÔºåÂπ∂Ê≤°ÊúâÈááÂèñ‰ªª‰ΩïÂÖ∂‰ªñÊé™ÊñΩÊù•Â§ÑÁêÜÈîôËØØÊàñÊèê‰æõÊõ¥ËØ¶ÁªÜÁöÑÈîôËØØ‰ø°ÊÅØ„ÄÇÂª∫ËÆÆÂú®ËøôÈáåÊ∑ªÂä†ÈÄÇÂΩìÁöÑÈîôËØØÂ§ÑÁêÜÈÄªËæëÔºå‰æãÂ¶ÇËÆ∞ÂΩïÈîôËØØÊó•Âøó„ÄÅÂêëÁî®Êà∑ÊòæÁ§∫ÈîôËØØÊ∂àÊÅØÊàñÊâßË°åÂÖ∂‰ªñÂøÖË¶ÅÁöÑÊìç‰Ωú„ÄÇ

```diff
        try {
          const pm = spawn(
            packageManager + (process.platform === ""win32"" ? "".cmd"" : """"),
            [installCommand[packageManager], installParams[packageManager], ...devDepsArray],
            {
              stdio: ""ignore"",
              cwd: packageJsonFile,
            },
          );
 
          // ÁõëÂê¨ÂÆâË£ÖÂëΩ‰ª§ÁöÑËæìÂá∫
          pm.on(""close"", (code) => {
            if (code === 0) {
              // code‰∏∫0‰ª£Ë°®ÂÆâË£ÖÊàêÂäü
              resolve(""devDependencies installed successfully."");
              console.log(""devDependencies installed successfully."");
            } else {
              console.error(
                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
              );
              reject(
                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
              );
            }
          });
        } catch (err) {
          console.log(""Installing devDependencies failed: "", err);
        }
      } else {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+        try {
+          const pm = spawn(
+            packageManager + (process.platform === ""win32"" ? "".cmd"" : """"),
+            [installCommand[packageManager], installParams[packageManager], ...devDepsArray],
+            {
+              stdio: ""ignore"",
+              cwd: packageJsonFile,
+            },
+          );
 
-        // ÁõëÂê¨ÂÆâË£ÖÂëΩ‰ª§ÁöÑËæìÂá∫
-        pm.on(""close"", (code) => {
-          if (code === 0) {
-            // code‰∏∫0‰ª£Ë°®ÂÆâË£ÖÊàêÂäü
-            resolve(""devDependencies installed successfully."");
-            console.log(""devDependencies installed successfully."");
-          } else {
-            console.error(
-              `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
-            );
-            reject(`${packageManager} ${installCommand[packageManager]} exited with code ${code}`);
-          }
-        });
-      }catch(err){
-         console.log(""Installing devDependencies failed: "",err)
-      }} else {
+          // ÁõëÂê¨ÂÆâË£ÖÂëΩ‰ª§ÁöÑËæìÂá∫
+          pm.on(""close"", (code) => {
+            if (code === 0) {
+              // code‰∏∫0‰ª£Ë°®ÂÆâË£ÖÊàêÂäü
+              resolve(""devDependencies installed successfully."");
+              console.log(""devDependencies installed successfully."");
+            } else {
+              console.error(
+                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
+              );
+              reject(
+                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
+              );
+            }
+          });
+        } catch (err) {
+          console.log(""Installing devDependencies failed: "", err);
+        }
+      } else {",
https://api.github.com/repos/xun082/create-neat/pulls/comments/1528419658,36.0,70,81f540db2555909188e1df100702bfb760088c8d,4598fc72268b0d4692e7a29c4f5668b4113b85b5,packages/core/src/utils/dependenciesInstall.ts,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [79-79]

Êñá‰ª∂Êú´Â∞æÁº∫Â∞ëÊç¢Ë°åÁ¨¶„ÄÇÂª∫ËÆÆÂú®Êñá‰ª∂Êú´Â∞æÊ∑ªÂä†‰∏Ä‰∏™Êç¢Ë°åÁ¨¶„ÄÇ

```diff
export default dependenciesInstall;
```

‰ª•‰∏äÊòØÂØπ `packages/core/src/utils/dependenciesInstall.ts` Êñá‰ª∂Êõ¥ÊîπÁöÑËØÑËÆ∫„ÄÇ


<!-- This is an auto-generated comment by OSS CodeRabbit -->","           ([dep, version]) => `${dep}@${version}`,
         );
         // ÊâßË°åÂÖ∑‰ΩìÂëΩ‰ª§
-       try{const pm = spawn(
-          packageManager + (process.platform === ""win32"" ? "".cmd"" : """"),
-          [installCommand[packageManager], installParams[packageManager], ...devDepsArray],
-          {
-            stdio: ""ignore"",
-            cwd: packageJsonFile,
-          },
-        );
+        try {
+          const pm = spawn(
+            packageManager + (process.platform === ""win32"" ? "".cmd"" : """"),
+            [installCommand[packageManager], installParams[packageManager], ...devDepsArray],
+            {
+              stdio: ""ignore"",
+              cwd: packageJsonFile,
+            },
+          );
 
-        // ÁõëÂê¨ÂÆâË£ÖÂëΩ‰ª§ÁöÑËæìÂá∫
-        pm.on(""close"", (code) => {
-          if (code === 0) {
-            // code‰∏∫0‰ª£Ë°®ÂÆâË£ÖÊàêÂäü
-            resolve(""devDependencies installed successfully."");
-            console.log(""devDependencies installed successfully."");
-          } else {
-            console.error(
-              `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
-            );
-            reject(`${packageManager} ${installCommand[packageManager]} exited with code ${code}`);
-          }
-        });
-      }catch(err){
-         console.log(""Installing devDependencies failed: "",err)
-      }} else {
+          // ÁõëÂê¨ÂÆâË£ÖÂëΩ‰ª§ÁöÑËæìÂá∫
+          pm.on(""close"", (code) => {
+            if (code === 0) {
+              // code‰∏∫0‰ª£Ë°®ÂÆâË£ÖÊàêÂäü
+              resolve(""devDependencies installed successfully."");
+              console.log(""devDependencies installed successfully."");
+            } else {
+              console.error(
+                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
+              );
+              reject(
+                `${packageManager} ${installCommand[packageManager]} exited with code ${code}`,
+              );
+            }
+          });
+        } catch (err) {
+          console.log(""Installing devDependencies failed: "", err);
+        }
+      } else {
         console.log(""No devDependencies found in package.json."");
         reject(""No devDependencies found in package.json."");
       }",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196744,7.0,18,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/eslint.config.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The ESLint configuration is missing the `rules` property, which is essential for defining custom linting rules or overriding recommended ones. Consider adding a `rules` section to customize the linting behavior according to your project's needs.

```diff
18a19
+   {
+     rules: {
+       // Add custom rules or overrides here
+     },
+   }
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  {
+    files: [""**/*.{js,mjs,cjs,ts,jsx,tsx}""],
+  },
+  {
+    languageOptions: {
+      globals: globals.browser,
+    },
+  },
+  pluginJs.configs.recommended,
+  ...tseslint.configs.recommended,
+  pluginReact.configs.flat.recommended,
+];","--- 

+++ 

@@ -12,6 +12,14 @@

       globals: globals.browser,
     },
   },
+  {
+    ignores: [""build/""],
+  },
+  {
+    rules: {
+      // ""@typescript-eslint/no-explicit-any"": ""off"",
+    },
+  },
   pluginJs.configs.recommended,
   ...tseslint.configs.recommended,
   pluginReact.configs.flat.recommended,"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196749,1.0,2,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/requirements.txt,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The specified version for Flask, `3.0.3`, does not exist as of the last known updates. The latest stable release is `2.1.x`. Please verify the correct version number.
```diff
- Flask==3.0.3
+ Flask==2.1.3
```
Additionally, ensure that there is a newline at the end of the file to avoid potential issues with some text editors and tools.
```diff
gunicorn==23.0.0
\ No newline at end of file
+ 
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+Flask==3.0.3
+gunicorn==23.0.0",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196752,23.0,31,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/main.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `verify_token_license` function does not handle cases where the JSON payload might be missing or malformed, which could lead to a server error. It's important to validate the input before accessing it.

```python
@app.route(""/verify-token"", methods=[""POST""])
def verify_token_license():
    if not request.is_json:
        lgr.error(""Request data is not JSON"")
        return {""valid"": False, ""message"": ""Invalid input format""}, 400

    data = request.get_json()
    token = data.get(""token"")
    license = data.get(""license"")

    if not token or not license:
        lgr.error(""Missing token or license in request data"")
        return {""valid"": False, ""message"": ""Missing token or license""}, 400

    valid, message = verify_token(token, license, ""port"" not in data)
    lgr.info(f""verify result: {valid} - {message}"")

    return {""valid"": valid, ""message"": message}
```

This change ensures that the function checks for JSON content and required fields, returning appropriate error messages and status codes when necessary.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+@app.route(""/verify-token"", methods=[""POST""])
+def verify_token_license():
+    lgr.debug(f""verify token data: {request.data}"")
+    token = request.json.get(""token"")
+    license = request.json.get(""license"")
+    valid, message = verify_token(token, license, ""port"" not in request.json)
+    lgr.info(f""verify result: {valid} - {message}"")
+
+    return {""valid"": valid, ""message"": message}","--- 

+++ 

@@ -25,7 +25,12 @@

     lgr.debug(f""verify token data: {request.data}"")
     token = request.json.get(""token"")
     license = request.json.get(""license"")
-    valid, message = verify_token(token, license, ""port"" not in request.json)
+    try:
+        valid, message = verify_token(token, license, ""port"" not in request.json)
+    except ValueError as e:
+        valid = False
+        message = str(e)
+
     lgr.info(f""verify result: {valid} - {message}"")
 
     return {""valid"": valid, ""message"": message}"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196756,,21,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Uninstall.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Avoid using `any` as the type for the event parameter. Instead, specify a more precise type to improve type safety and maintainability.
```diff
-   const handleUninstall = (e: any) => {
+   const handleUninstall = (e: React.MouseEvent<HTMLButtonElement>) => {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const [isLoading, setLoading] = useState(false);
+  const ddClient = useDockerDesktopClient();
+
+  const handleUninstall = (e: any) => {","--- 

+++ 

@@ -18,7 +18,7 @@

   const [isLoading, setLoading] = useState(false);
   const ddClient = useDockerDesktopClient();
 
-  const handleUninstall = (e: any) => {
+  const handleUninstall = () => {
     setLoading(true);
     ddClient.docker.cli
       .exec(""ps"", ["
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196760,,25,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Update.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Avoid using `any` as the type for the event parameter. Instead, specify a more precise type to improve type safety and maintainability.
```diff
-   const handleUpdateImage = (e: any) => {
+   const handleUpdateImage = (e: React.MouseEvent<HTMLButtonElement>) => {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  const [isLoading, setLoading] = useState(false);
+  const ddClient = useDockerDesktopClient();
+
+  const handleUpdateImage = (e: any) => {","--- 

+++ 

@@ -22,11 +22,12 @@

   const [isLoading, setLoading] = useState(false);
   const ddClient = useDockerDesktopClient();
 
-  const handleUpdateImage = (e: any) => {
+  const handleUpdateImage = () => {
     setLoading(true);
     ddClient.docker.cli
       .exec(""pull"", [image])
       .then((result) => {
+        console.debug(result)
         setLoading(false);
       })
       .catch((err) => {"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196765,,22,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/Makefile,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `push-extension` target uses a combination of `docker pull` and `echo` to check if a tag already exists. This approach can be improved for clarity and reliability. Consider using a more explicit check with `docker manifest inspect`, which is specifically designed for this purpose.

```diff
-	docker pull $(IMAGE):$(TAG) && echo ""Failure: Tag already exists"" || docker buildx build --push --builder=$(BUILDER) --platform=linux/amd64,linux/arm64 --build-arg TAG=$(TAG) --tag=$(IMAGE):$(TAG) .
+	docker manifest inspect $(IMAGE):$(TAG) > /dev/null && echo ""Failure: Tag already exists"" || docker buildx build --push --builder=$(BUILDER) --platform=linux/amd64,linux/arm64 --build-arg TAG=$(TAG) --tag=$(IMAGE):$(TAG) .
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	docker buildx inspect $(BUILDER) || docker buildx create --name=$(BUILDER) --driver=docker-container --driver-opt=network=host
+
+push-extension: prepare-buildx ## Build & Upload extension image to hub. Do not push if tag already exists: make push-extension tag=0.1
+	docker pull $(IMAGE):$(TAG) && echo ""Failure: Tag already exists"" || docker buildx build --push --builder=$(BUILDER) --platform=linux/amd64,linux/arm64 --build-arg TAG=$(TAG) --tag=$(IMAGE):$(TAG) .",
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196770,9.0,11,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/utils.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error message in the `ValueError` should be more consistent in terms of plurality. Consider changing ""is"" to ""are"" for grammatical correctness.
```diff
-        raise ValueError(""API token and license key is required."")
+        raise ValueError(""API token and license key are required."")
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+def verify_token(token, license_key, is_stream=True):
+    if not token or not license_key:
+        raise ValueError(""API token and license key is required."")","--- 

+++ 

@@ -1,9 +1,12 @@

+import logging
+
 try:
     from urllib.error import URLError
     from urllib.request import Request, urlopen
 except ImportError:
     from urllib2 import Request, URLError, urlopen  # type: ignore
-from ssl import SSLError
+
+lgr = logging.getLogger(__name__)
 
 
 def verify_token(token, license_key, is_stream=True):
@@ -18,15 +21,6 @@

         req.add_header(""Authorization"", f""Token {token.strip()}"")
         urlopen(req).read()
         return True, None
-
-    except SSLError:
-        req = Request(
-            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
-        )
-        req.add_header(""Authorization"", f""Token {token.strip()}"")
-        urlopen(req).read()
-        return True, None
-
     except URLError as e:
         if ""404"" in str(e):
             return (
@@ -38,3 +32,6 @@

 
         else:
             return True, None
+    except Exception as e:
+        lgr.exception(e)
+        return False, str(e)"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196777,22.0,28,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/utils.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Handling SSL errors by switching to HTTP is insecure. Instead, consider logging the error or notifying the user without downgrading security protocols. Here's a safer approach:

```diff
-    except SSLError:
-        req = Request(
-            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
-        )
-        req.add_header(""Authorization"", f""Token {token.strip()}"")
-        urlopen(req).read()
-        return True, None
+    except SSLError as e:
+        return False, f""SSL Error occurred: {e}. Please check your network settings.""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    except SSLError:
+        req = Request(
+            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
+        )
+        req.add_header(""Authorization"", f""Token {token.strip()}"")
+        urlopen(req).read()
+        return True, None","--- 

+++ 

@@ -1,9 +1,12 @@

+import logging
+
 try:
     from urllib.error import URLError
     from urllib.request import Request, urlopen
 except ImportError:
     from urllib2 import Request, URLError, urlopen  # type: ignore
-from ssl import SSLError
+
+lgr = logging.getLogger(__name__)
 
 
 def verify_token(token, license_key, is_stream=True):
@@ -18,15 +21,6 @@

         req.add_header(""Authorization"", f""Token {token.strip()}"")
         urlopen(req).read()
         return True, None
-
-    except SSLError:
-        req = Request(
-            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
-        )
-        req.add_header(""Authorization"", f""Token {token.strip()}"")
-        urlopen(req).read()
-        return True, None
-
     except URLError as e:
         if ""404"" in str(e):
             return (
@@ -38,3 +32,6 @@

 
         else:
             return True, None
+    except Exception as e:
+        lgr.exception(e)
+        return False, str(e)"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196780,30.0,40,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/utils.py,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The logic for handling URLError could be improved by explicitly checking the HTTP status code instead of relying on string matching, which can be unreliable. Consider parsing the error response if possible.

```diff
-    except URLError as e:
-        if ""404"" in str(e):
+    except URLError as e:
+        if hasattr(e, 'code') and e.code == 404:
             return (
                 False,
                 ""The License Key cannot be found. Please use the correct License Key."",
             )
-        elif str(403) in str(e):
+        elif hasattr(e, 'code') and e.code == 403:
             return False, ""The API Token cannot be found. Please use the correct Token.""
        else:
            return False, f""URL Error occurred: {e}""
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    except URLError as e:
+        if ""404"" in str(e):
+            return (
+                False,
+                ""The License Key cannot be found. Please use the correct License Key."",
+            )
+        elif str(403) in str(e):
+            return False, ""The API Token cannot be found. Please use the correct Token.""
+
+        else:
+            return True, None","--- 

+++ 

@@ -1,9 +1,12 @@

+import logging
+
 try:
     from urllib.error import URLError
     from urllib.request import Request, urlopen
 except ImportError:
     from urllib2 import Request, URLError, urlopen  # type: ignore
-from ssl import SSLError
+
+lgr = logging.getLogger(__name__)
 
 
 def verify_token(token, license_key, is_stream=True):
@@ -18,15 +21,6 @@

         req.add_header(""Authorization"", f""Token {token.strip()}"")
         urlopen(req).read()
         return True, None
-
-    except SSLError:
-        req = Request(
-            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
-        )
-        req.add_header(""Authorization"", f""Token {token.strip()}"")
-        urlopen(req).read()
-        return True, None
-
     except URLError as e:
         if ""404"" in str(e):
             return (
@@ -38,3 +32,6 @@

 
         else:
             return True, None
+    except Exception as e:
+        lgr.exception(e)
+        return False, str(e)"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196785,,27,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/ShowCommand.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Avoid using `any` as the type for the event parameter. Instead, specify a more precise type to improve type safety.
```diff
-   function copyToClipboard(e: any) {
+   function copyToClipboard(e: React.MouseEvent<HTMLButtonElement>) {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  }
+  const ddClient = useDockerDesktopClient();
+
+  function copyToClipboard(e: any) {","--- 

+++ 

@@ -24,7 +24,7 @@

   }
   const ddClient = useDockerDesktopClient();
 
-  function copyToClipboard(e: any) {
+  function copyToClipboard() {
     navigator.clipboard
       .writeText(command)
       .then(() => {
@@ -35,11 +35,11 @@

       });
   }
 
-  function runCommand(e: any){
+  function runCommand(){
     setRunningCommand(true);
     // Generate list of run options
     console.debug(command);
-    const cmd:any = command.match(/[^ ]+/g)?.slice(2);
+    const cmd:Array<string> = command.match(/[^ ]+/g)?.slice(2) || [];
     // Run in the background
     if (!cmd.includes('-d')){
       cmd.unshift('-d')"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196788,,38,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/ShowCommand.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Similarly, specify a more precise type for the event parameter in the `runCommand` function instead of using `any`.
```diff
-   function runCommand(e: any){
+   function runCommand(e: React.MouseEvent<HTMLButtonElement>){
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+      });
+  }
+
+  function runCommand(e: any){","--- 

+++ 

@@ -24,7 +24,7 @@

   }
   const ddClient = useDockerDesktopClient();
 
-  function copyToClipboard(e: any) {
+  function copyToClipboard() {
     navigator.clipboard
       .writeText(command)
       .then(() => {
@@ -35,11 +35,11 @@

       });
   }
 
-  function runCommand(e: any){
+  function runCommand(){
     setRunningCommand(true);
     // Generate list of run options
     console.debug(command);
-    const cmd:any = command.match(/[^ ]+/g)?.slice(2);
+    const cmd:Array<string> = command.match(/[^ ]+/g)?.slice(2) || [];
     // Run in the background
     if (!cmd.includes('-d')){
       cmd.unshift('-d')"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196792,,42,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/ShowCommand.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The use of `any` for the `cmd` variable should be avoided. You can infer the type from the context or use a more specific type like `string[]`.
```diff
-     const cmd:any = command.match(/[^ ]+/g)?.slice(2);
+     const cmd: string[] = command.match(/[^ ]+/g)?.slice(2) || [];
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    setRunningCommand(true);
+    // Generate list of run options
+    console.debug(command);
+    const cmd:any = command.match(/[^ ]+/g)?.slice(2);","--- 

+++ 

@@ -24,7 +24,7 @@

   }
   const ddClient = useDockerDesktopClient();
 
-  function copyToClipboard(e: any) {
+  function copyToClipboard() {
     navigator.clipboard
       .writeText(command)
       .then(() => {
@@ -35,11 +35,11 @@

       });
   }
 
-  function runCommand(e: any){
+  function runCommand(){
     setRunningCommand(true);
     // Generate list of run options
     console.debug(command);
-    const cmd:any = command.match(/[^ ]+/g)?.slice(2);
+    const cmd:Array<string> = command.match(/[^ ]+/g)?.slice(2) || [];
     // Run in the background
     if (!cmd.includes('-d')){
       cmd.unshift('-d')"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1806196798,44.0,46,e6cb426871faa979f4dbf2a97150a7c32798fe61,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/ShowCommand.tsx,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Ensure that `cmd` is not null before calling `includes` and `unshift`. The current code assumes `command.match` always returns an array, which might not be the case if the regex doesn't match anything.
```diff
-     if (!cmd.includes('-d')){
-       cmd.unshift('-d')
+     if (cmd && !cmd.includes('-d')) {
+       cmd.unshift('-d');
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+    if (!cmd.includes('-d')){
+      cmd.unshift('-d')
+    }","--- 

+++ 

@@ -24,7 +24,7 @@

   }
   const ddClient = useDockerDesktopClient();
 
-  function copyToClipboard(e: any) {
+  function copyToClipboard() {
     navigator.clipboard
       .writeText(command)
       .then(() => {
@@ -35,11 +35,11 @@

       });
   }
 
-  function runCommand(e: any){
+  function runCommand(){
     setRunningCommand(true);
     // Generate list of run options
     console.debug(command);
-    const cmd:any = command.match(/[^ ]+/g)?.slice(2);
+    const cmd:Array<string> = command.match(/[^ ]+/g)?.slice(2) || [];
     // Run in the background
     if (!cmd.includes('-d')){
       cmd.unshift('-d')"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477938,5.0,6,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/snapshot.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The error message could be more descriptive to help users understand the context better. Consider specifying that the token is required for API authentication.

```diff
-			throw new Error(""Snapshot TOKEN is required if using Cloud API"");
+			throw new Error(""Snapshot API token is required for authentication."");
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if (token === null) {
+			throw new Error(""Snapshot TOKEN is required if using Cloud API"");","--- 

+++ 

@@ -1,34 +1,52 @@

 import { fetchWithRetry } from ""./utils"";
 
 export class SnapshotApi {
-	constructor(token, sdkUrl = null) {
-		if (token === null) {
-			throw new Error(""Snapshot TOKEN is required if using Cloud API"");
-		} else {
-			this.token = token;
-		}
-		if (sdkUrl) {
-			this.apiBase = sdkUrl;
-		} else {
-			this.apiBase = ""https://api.platerecognizer.com"";
-		}
-		console.debug(""Api Base: "" + this.apiBase);
-	}
+  constructor(token, sdkUrl = null) {
+    if (token === null) {
+      throw new Error(""Snapshot API token is required for authentication."");
+    } else {
+      this.token = token;
+    }
+    if (sdkUrl) {
+      this.apiBase = sdkUrl;
+    } else {
+      this.apiBase = ""https://api.platerecognizer.com"";
+    }
+    console.debug(""Api Base: "" + this.apiBase);
+  }
 
-	async uploadBase64(encodedImage, camera, timestamp) {
-		const endpoint = ""/v1/plate-reader/"";
-		const body = new FormData();
-		body.set(""camera_id"", camera);
-		body.set(""upload"", encodedImage);
-		body.set(""timestamp"", timestamp);
-		let init = {
-			body: body,
-			method: ""POST"",
-			headers: {
-				Authorization: ""Token "" + this.token,
-			},
-		};
-		const url = this.apiBase + endpoint;
-		return fetchWithRetry(url, init).then((response) => response.json());
-	}
+  async uploadBase64(encodedImage, camera, timestamp, params) {
+    console.debug(params);
+    const endpoint = ""/v1/plate-reader/"";
+    const body = new FormData();
+    body.set(""upload"", encodedImage);
+    body.set(""timestamp"", timestamp);
+
+    if (params[""camera_id""]) {
+      body.set(""camera_id"", params[""camera_id""]);
+    } else {
+      body.set(""camera_id"", camera);
+    }
+    if (params[""mmc""]) {
+      body.set(""mmc"", params[""mmc""]);
+    } else {
+      body.set(""mmc"", ""true"");
+    }
+    if (params[""regions""]) {
+      body.set(""regions"", params[""regions""]);
+    }
+    if (params[""config""]) {
+      body.set(""config"", params[""config""]);
+    }
+
+    let init = {
+      body: body,
+      method: ""POST"",
+      headers: {
+        Authorization: ""Token "" + this.token,
+      },
+    };
+    const url = this.apiBase + endpoint;
+    return fetchWithRetry(url, init).then((response) => response.json());
+  }
 }"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477939,5.0,36,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/utils.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The retry logic in `fetchWithRetry` could be improved by adding exponential backoff to avoid overwhelming the server with requests. Here's a suggested change:

```diff
- const delay = 2000;
+ const delay = Math.pow(2, 3 - tries) * 1000; // Exponential backoff
```

This change will increase the delay between retries exponentially, which is a common practice for handling rate limits and server errors more gracefully.


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const fetchWithRetry = (url, init, tries = 3) =>
+	fetch(url, init)
+		.then(async (response) => {
+			if (response.ok) {
+				return response;
+			} else {
+				// 1. throw a new exception
+				if (response.status === 429)
+					throw new Error429(""Rate Limited"", response);
+				if (response.status >= 500 && response.status <= 599)
+					throw new Error5xx(`Server Error - ${response.status}`, response);
+
+				console.error(await response.text());
+				// 2. reject instead of throw, preferred
+				const errorMessage = `Unexpected Response: ${response.status}`;
+				return Promise.reject(new Error(errorMessage));
+			}
+		})
+		.catch((error) => {
+			// Retry network error or 5xx errors
+			if (
+				(error instanceof Error429 || error instanceof Error5xx) &&
+				tries > 0
+			) {
+				console.error(`Retry Response status: ${error.message}`);
+				// if the rate limit is reached or exceeded,
+				const delay = 2000;
+				return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
+			} else {
+				throw error;
+			}
+		});","--- 

+++ 

@@ -2,35 +2,36 @@

 
 const wait = (delay) => new Promise((resolve) => setTimeout(resolve, delay));
 
-export const fetchWithRetry = (url, init, tries = 3) =>
-	fetch(url, init)
-		.then(async (response) => {
-			if (response.ok) {
-				return response;
-			} else {
-				// 1. throw a new exception
-				if (response.status === 429)
-					throw new Error429(""Rate Limited"", response);
-				if (response.status >= 500 && response.status <= 599)
-					throw new Error5xx(`Server Error - ${response.status}`, response);
+export function fetchWithRetry(url, init, tries = 3) {
+  return fetch(url, init)
+    .then(async (response) => {
+      if (response.ok) {
+        return response;
+      } else {
+        // 1. throw a new exception
+        if (response.status === 429)
+          throw new Error429(""Rate Limited"", response);
+        if (response.status >= 500 && response.status <= 599)
+          throw new Error5xx(`Server Error - ${response.status}`, response);
 
-				console.error(await response.text());
-				// 2. reject instead of throw, preferred
-				const errorMessage = `Unexpected Response: ${response.status}`;
-				return Promise.reject(new Error(errorMessage));
-			}
-		})
-		.catch((error) => {
-			// Retry network error or 5xx errors
-			if (
-				(error instanceof Error429 || error instanceof Error5xx) &&
-				tries > 0
-			) {
-				console.error(`Retry Response status: ${error.message}`);
-				// if the rate limit is reached or exceeded,
-				const delay = 2000;
-				return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
-			} else {
-				throw error;
-			}
-		});
+        console.error(await response.text());
+        // 2. reject instead of throw, preferred
+        const errorMessage = `Unexpected Response: ${response.status}`;
+        return Promise.reject(new Error(errorMessage));
+      }
+    })
+    .catch((error) => {
+      // Retry network error or 5xx errors
+      if (
+        (error instanceof Error429 || error instanceof Error5xx) &&
+        tries > 0
+      ) {
+        console.log(`Retrying request: ${tries}`);
+        // if the rate limit is reached or exceeded,
+        const delay = 2000;
+        return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
+      } else {
+        throw error;
+      }
+    });
+}"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477940,21.0,23,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/test/index.spec.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `request` and `ctx` variables are not defined in this test case. You should define them similarly to the previous test case to ensure the test runs correctly.
```diff
	it(""responds with Hello World! (integration style)"", async () => {
		const request = new Request(""http://example.com"");
		const ctx = createExecutionContext();
		const response = await SELF.fetch(request, env, ctx);
		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
	});
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	it(""responds with Hello World! (integration style)"", async () => {
+		const response = await SELF.fetch(request, env, ctx);
+		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);","--- 

+++ 

@@ -1,25 +1,28 @@

 import {
-	env,
-	createExecutionContext,
-	waitOnExecutionContext,
-	SELF,
+  env,
+  createExecutionContext,
+  waitOnExecutionContext,
+  SELF,
 } from ""cloudflare:test"";
 import { describe, it, expect } from ""vitest"";
 import worker from ""../src"";
 
 describe(""Hello World worker"", () => {
-	it(""responds with Hello World! (unit style)"", async () => {
-		const request = new Request(""http://example.com"");
-		// Create an empty context to pass to `worker.fetch()`.
-		const ctx = createExecutionContext();
-		const response = await worker.fetch(request, env, ctx);
-		// Wait for all `Promise`s passed to `ctx.waitUntil()` to settle before running test assertions
-		await waitOnExecutionContext(ctx);
-		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
-	});
+  it(""responds with Hello World! (unit style)"", async () => {
+    const request = new Request(""http://example.com"");
+    // Create an empty context to pass to `worker.fetch()`.
+    const ctx = createExecutionContext();
+    const response = await worker.fetch(request, env, ctx);
+    // Wait for all `Promise`s passed to `ctx.waitUntil()` to settle before running test assertions
+    await waitOnExecutionContext(ctx);
+    expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
+  });
 
-	it(""responds with Hello World! (integration style)"", async () => {
-		const response = await SELF.fetch(request, env, ctx);
-		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
-	});
+  it(""responds with Hello World! (integration style)"", async () => {
+    const request = new Request(""http://example.com"");
+    // Create an empty context to pass to `worker.fetch()`.
+    const ctx = createExecutionContext();
+    const response = await SELF.fetch(request, env, ctx);
+    expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
+  });
 });"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477941,6.0,14,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Frame` function could be improved for clarity and maintainability by using object destructuring directly in the parameter list. This makes it clear what properties are expected from `stackFrame`.

```diff
- function Frame(stackFrame) {
+ function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
    const data = {};
-   data.filename = stackFrame.fileName;
-   data.lineno = stackFrame.lineNumber;
-   data.colno = stackFrame.columnNumber;
-   data.method = stackFrame.functionName;
-   data.args = stackFrame.args;
+   data.filename = fileName;
+   data.lineno = lineNumber;
+   data.colno = columnNumber;
+   data.method = functionName;
+   data.args = args;
    return data;
}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+function Frame(stackFrame) {
+	const data = {};
+	data.filename = stackFrame.fileName;
+	data.lineno = stackFrame.lineNumber;
+	data.colno = stackFrame.columnNumber;
+	data.method = stackFrame.functionName;
+	data.args = stackFrame.args;
+	return data;
+}","--- 

+++ 

@@ -3,109 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame(stackFrame) {
-	const data = {};
-	data.filename = stackFrame.fileName;
-	data.lineno = stackFrame.lineNumber;
-	data.colno = stackFrame.columnNumber;
-	data.method = stackFrame.functionName;
-	data.args = stackFrame.args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = [];
-		for (const exception of exceptions) {
-			traceChain.push(this.createTrace(description, exception));
-		}
-
-		const telemetry = [];
-		for (const eventLog of eventLogs) {
-			telemetry.push(this.createTelemetry(eventLog));
-		}
-
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477942,49.0,89,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `error` method can be optimized by using `map` instead of manually pushing to arrays. This will make the code more concise and functional.

```diff
- const traceChain = [];
- for (const exception of exceptions) {
-     traceChain.push(this.createTrace(description, exception));
- }
+ const traceChain = exceptions.map(exception => this.createTrace(description, exception));

- const telemetry = [];
- for (const eventLog of eventLogs) {
-     telemetry.push(this.createTelemetry(eventLog));
- }
+ const telemetry = eventLogs.map(eventLog => this.createTelemetry(eventLog));
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+		const traceChain = [];
+		for (const exception of exceptions) {
+			traceChain.push(this.createTrace(description, exception));
+		}
+
+		const telemetry = [];
+		for (const eventLog of eventLogs) {
+			telemetry.push(this.createTelemetry(eventLog));
+		}
+
+		const rollbarData = {
+			environment: this.environment,
+			body: { telemetry: telemetry, trace_chain: traceChain },
+			timestamp: timestamp,
+			code_version: codeVersion,
+			language: ""javascript"",
+		};
+
+		if (""request"" in event) {
+			const request = event[""request""];
+			const params = request[""cf""];
+			delete params[""tlsClientAuth""];
+			delete params[""tlsExportedAuthenticator""];
+			rollbarData[""request""] = {
+				url: request[""url""],
+				method: request[""method""],
+				headers: request[""headers""],
+				params: params,
+			};
+		}
+
+		let init = {
+			method: ""POST"",
+			body: JSON.stringify({ data: rollbarData }),
+			headers: {
+				""Content-type"": ""application/json"",
+				""X-Rollbar-Access-Token"": this.token,
+			},
+		};
+		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());","--- 

+++ 

@@ -3,109 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame(stackFrame) {
-	const data = {};
-	data.filename = stackFrame.fileName;
-	data.lineno = stackFrame.lineNumber;
-	data.colno = stackFrame.columnNumber;
-	data.method = stackFrame.functionName;
-	data.args = stackFrame.args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = [];
-		for (const exception of exceptions) {
-			traceChain.push(this.createTrace(description, exception));
-		}
-
-		const telemetry = [];
-		for (const eventLog of eventLogs) {
-			telemetry.push(this.createTelemetry(eventLog));
-		}
-
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477943,4.0,52,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `fetch` method currently processes POST requests with JSON content. However, it lacks error handling for potential issues such as malformed JSON or missing fields in the data object. Consider adding try-catch blocks to handle these scenarios gracefully.

```diff
	async fetch(request, env, ctx) {
		if (request.method === ""POST"") {
			const contentType = request.headers.get(""content-type"");
			if (contentType.includes(""application/json"")) {
				try {
					const data = await request.json();
					console.debug(data);
					let cameraId = null;
					let imageBase64 = null;
					let createdDate = null;
					const survisionSerialNumber = request.headers.get(
						""survision-serial-number"",
					);
					if (survisionSerialNumber) {
						cameraId = survisionSerialNumber;
						createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
						imageBase64 = data[""anpr""][""decision""][""jpeg""];
					} else {
						cameraId = data[""CameraName""];
						imageBase64 = data[""ContextImage""];
						// ""10/01/2022"", Format DD/MM/YYYY
						let [day, month, year] = data[""DateUtc""].split(""/"");
						//  ""11:49:22"", Format HH/MM/SS
						let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
						createdDate = new Date(
							parseInt(year),
							parseInt(month),
							parseInt(day),
							parseInt(hours),
							parseInt(seconds),
						).toISOString();
					}
					ctx.waitUntil(
						env.INCOMING_WEBHOOKS.send({
							image: imageBase64,
							cameraId: cameraId,
							timestamp: createdDate,
						}),
					);
					return new Response(""OK!"");
				} catch (error) {
					console.error(""Error processing request:"", error);
					return new Response(""Error - Invalid JSON or Missing Fields"", { status: 400 });
				}
			} else {
				return new Response(
					""Error - Invalid Content Type, Expected application/json "",
					{ status: 400 },
				);
			}
		} else {
			return new Response(""Error - Required POST"", { status: 400 });
		}
	},
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async fetch(request, env, ctx) {
+		if (request.method === ""POST"") {
+			const contentType = request.headers.get(""content-type"");
+			if (contentType.includes(""application/json"")) {
+				const data = await request.json();
+				console.debug(data);
+				let cameraId = null;
+				let imageBase64 = null;
+				let createdDate = null;
+				const survisionSerialNumber = request.headers.get(
+					""survision-serial-number"",
+				);
+				if (survisionSerialNumber) {
+					cameraId = survisionSerialNumber;
+					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
+					imageBase64 = data[""anpr""][""decision""][""jpeg""];
+				} else {
+					cameraId = data[""CameraName""];
+					imageBase64 = data[""ContextImage""];
+					// ""10/01/2022"", Format DD/MM/YYYY
+					let [day, month, year] = data[""DateUtc""].split(""/"");
+					//  ""11:49:22"", Format HH/MM/SS
+					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+					createdDate = new Date(
+						parseInt(year),
+						parseInt(month),
+						parseInt(day),
+						parseInt(hours),
+						parseInt(minutes),
+						parseInt(seconds),
+					).toISOString();
+				}
+				ctx.waitUntil(
+					env.INCOMING_WEBHOOKS.send({
+						image: imageBase64,
+						cameraId: cameraId,
+						timestamp: createdDate,
+					}),
+				);
+				return new Response(""OK!"");
+			} else {
+				return new Response(
+					""Error - Invalid Content Type, Expected application/json "",
+					{ status: 400 },
+				);
+			}
+		} else {
+			return new Response(""Error - Required POST"", { status: 400 });
+		}","--- 

+++ 

@@ -1,70 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
+function validGenetecEvent(data) {
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
+}
+
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807477944,57.0,69,42f0016b41535fdb9769e9948fe0139596842150,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `queue` method, the loop processes each message and acknowledges it after uploading. If an error occurs during the upload, the message will not be acknowledged, potentially causing retries. Consider adding error handling to log errors and decide on a retry strategy.

```diff
	async queue(batch, env) {
		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
		for (const message of batch.messages) {
			try {
				const result = await snapshot.uploadBase64(
					message.body[""image""],
					message.body[""cameraId""],
					message.body[""timestamp""],
				);
				console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
				// Explicitly acknowledge the message as delivered
				message.ack();
			} catch (error) {
				console.error(""Error uploading image:"", error);
				// Optionally implement a retry mechanism or dead-letter queue
			}
		}
	},
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async queue(batch, env) {
+		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+		for (const message of batch.messages) {
+			const result = await snapshot.uploadBase64(
+				message.body[""image""],
+				message.body[""cameraId""],
+				message.body[""timestamp""],
+			);
+			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+			// Explicitly acknowledge the message as delivered
+			message.ack();
+		}
+	},","--- 

+++ 

@@ -1,70 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
+function validGenetecEvent(data) {
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
+}
+
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807928839,6.0,14,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Frame` function is defined using a regular function declaration. To improve consistency and modernize the code, consider using an arrow function instead.

```diff
- function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
+ const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
+	const data = {};
+	data.filename = fileName;
+	data.lineno = lineNumber;
+	data.colno = columnNumber;
+	data.method = functionName;
+	data.args = args;
+	return data;
+}","--- 

+++ 

@@ -3,105 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
-	const data = {};
-	data.filename = fileName;
-	data.lineno = lineNumber;
-	data.colno = columnNumber;
-	data.method = functionName;
-	data.args = args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = exceptions.map((exception) =>
-			this.createTrace(description, exception),
-		);
-		const telemetry = eventLogs.map((eventLog) =>
-			this.createTelemetry(eventLog),
-		);
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807928841,18.0,19,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Returning early from the constructor if no token is provided can lead to instances of `Rollbar` being created without proper initialization. Instead, consider throwing an error or setting a flag to indicate invalid initialization.

```diff
- if (!token) {
-     return;
+ if (!token) {
+     throw new Error(""Token is required for Rollbar initialization"");
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if (!token) {
+			return;","--- 

+++ 

@@ -3,105 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
-	const data = {};
-	data.filename = fileName;
-	data.lineno = lineNumber;
-	data.colno = columnNumber;
-	data.method = functionName;
-	data.args = args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = exceptions.map((exception) =>
-			this.createTrace(description, exception),
-		);
-		const telemetry = eventLogs.map((eventLog) =>
-			this.createTelemetry(eventLog),
-		);
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1807928846,13.0,66,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `fetch` function currently processes POST requests and checks for JSON content type. However, it lacks error handling for potential issues such as JSON parsing errors or missing headers. Consider adding try-catch blocks to handle these scenarios gracefully.

```diff
	async fetch(request, env, ctx) {
		if (request.method === ""POST"") {
			const contentType = request.headers.get(""content-type"");
			if (contentType && contentType.includes(""application/json"")) {
				try {
					const data = await request.json();
					console.debug(data);
					let cameraId = null;
					let imageBase64 = null;
					let createdDate = null;
					const survisionSerialNumber = request.headers.get(
						""survision-serial-number"",
					);
					if (survisionSerialNumber) {
						cameraId = survisionSerialNumber;
						createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
						imageBase64 = data[""anpr""][""decision""][""jpeg""];
					} else if (validGenetecEvent(data)) {
						cameraId = data[""CameraName""];
						imageBase64 = data[""ContextImage""];
						// ""10/01/2022"", Format DD/MM/YYYY
						let [day, month, year] = data[""DateUtc""].split(""/"");
						//  ""11:49:22"", Format HH/MM/SS
						let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
						createdDate = new Date(
							parseInt(year),
							parseInt(month),
							parseInt(day),
							parseInt(hours),
							parseInt(minutes),
							parseInt(seconds),
						).toISOString();
					} else {
						return new Response(""Error - Invalid Request Content"", {
							status: 400,
						});
					}
					ctx.waitUntil(
						env.INCOMING_WEBHOOKS.send({
							image: imageBase64,
							cameraId: cameraId,
							timestamp: createdDate,
						}),
					);
					return new Response(""OK!"");
				} catch (error) {
					console.error(""Error processing request:"", error);
					return new Response(""Error - Unable to process request"", { status: 500 });
				}
			} else {
				return new Response(
					""Error - Invalid Content Type, Expected application/json "",
					{ status: 400 },
				);
			}
		} else {
			return new Response(""Error - Required POST"", { status: 400 });
		}
	},
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async fetch(request, env, ctx) {
+		if (request.method === ""POST"") {
+			const contentType = request.headers.get(""content-type"");
+			if (contentType.includes(""application/json"")) {
+				const data = await request.json();
+				console.debug(data);
+				let cameraId = null;
+				let imageBase64 = null;
+				let createdDate = null;
+				const survisionSerialNumber = request.headers.get(
+					""survision-serial-number"",
+				);
+				if (survisionSerialNumber) {
+					cameraId = survisionSerialNumber;
+					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
+					imageBase64 = data[""anpr""][""decision""][""jpeg""];
+				} else if (validGenetecEvent(data)) {
+					cameraId = data[""CameraName""];
+					imageBase64 = data[""ContextImage""];
+					// ""10/01/2022"", Format DD/MM/YYYY
+					let [day, month, year] = data[""DateUtc""].split(""/"");
+					//  ""11:49:22"", Format HH/MM/SS
+					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+					createdDate = new Date(
+						parseInt(year),
+						parseInt(month),
+						parseInt(day),
+						parseInt(hours),
+						parseInt(minutes),
+						parseInt(seconds),
+					).toISOString();
+				} else {
+					return new Response(""Error - Invalid Request Content"", {
+						status: 400,
+					});
+				}
+				ctx.waitUntil(
+					env.INCOMING_WEBHOOKS.send({
+						image: imageBase64,
+						cameraId: cameraId,
+						timestamp: createdDate,
+					}),
+				);
+				return new Response(""OK!"");
+			} else {
+				return new Response(
+					""Error - Invalid Content Type, Expected application/json "",
+					{ status: 400 },
+				);
+			}
+		} else {
+			return new Response(""Error - Required POST"", { status: 400 });
+		}
+	},","--- 

+++ 

@@ -1,83 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463509,5.0,6,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/snapshot.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The check for a `null` token should also handle undefined or empty string cases to ensure robust validation.

```diff
- 		if (token === null) {
+ 		if (!token) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+		if (token === null) {
+			throw new Error(""Snapshot API token is required for authentication."");","--- 

+++ 

@@ -1,34 +1,52 @@

 import { fetchWithRetry } from ""./utils"";
 
 export class SnapshotApi {
-	constructor(token, sdkUrl = null) {
-		if (token === null) {
-			throw new Error(""Snapshot API token is required for authentication."");
-		} else {
-			this.token = token;
-		}
-		if (sdkUrl) {
-			this.apiBase = sdkUrl;
-		} else {
-			this.apiBase = ""https://api.platerecognizer.com"";
-		}
-		console.debug(""Api Base: "" + this.apiBase);
-	}
+  constructor(token, sdkUrl = null) {
+    if (token === null) {
+      throw new Error(""Snapshot API token is required for authentication."");
+    } else {
+      this.token = token;
+    }
+    if (sdkUrl) {
+      this.apiBase = sdkUrl;
+    } else {
+      this.apiBase = ""https://api.platerecognizer.com"";
+    }
+    console.debug(""Api Base: "" + this.apiBase);
+  }
 
-	async uploadBase64(encodedImage, camera, timestamp) {
-		const endpoint = ""/v1/plate-reader/"";
-		const body = new FormData();
-		body.set(""camera_id"", camera);
-		body.set(""upload"", encodedImage);
-		body.set(""timestamp"", timestamp);
-		let init = {
-			body: body,
-			method: ""POST"",
-			headers: {
-				Authorization: ""Token "" + this.token,
-			},
-		};
-		const url = this.apiBase + endpoint;
-		return fetchWithRetry(url, init).then((response) => response.json());
-	}
+  async uploadBase64(encodedImage, camera, timestamp, params) {
+    console.debug(params);
+    const endpoint = ""/v1/plate-reader/"";
+    const body = new FormData();
+    body.set(""upload"", encodedImage);
+    body.set(""timestamp"", timestamp);
+
+    if (params[""camera_id""]) {
+      body.set(""camera_id"", params[""camera_id""]);
+    } else {
+      body.set(""camera_id"", camera);
+    }
+    if (params[""mmc""]) {
+      body.set(""mmc"", params[""mmc""]);
+    } else {
+      body.set(""mmc"", ""true"");
+    }
+    if (params[""regions""]) {
+      body.set(""regions"", params[""regions""]);
+    }
+    if (params[""config""]) {
+      body.set(""config"", params[""config""]);
+    }
+
+    let init = {
+      body: body,
+      method: ""POST"",
+      headers: {
+        Authorization: ""Token "" + this.token,
+      },
+    };
+    const url = this.apiBase + endpoint;
+    return fetchWithRetry(url, init).then((response) => response.json());
+  }
 }"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463511,6.0,13,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `Frame` function could be improved by using object shorthand syntax to make the code more concise.

```diff
function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
	return {
		filename: fileName,
		lineno: lineNumber,
		colno: columnNumber,
		method: functionName,
		args,
	};
}
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
+	const data = {};
+	data.filename = fileName;
+	data.lineno = lineNumber;
+	data.colno = columnNumber;
+	data.method = functionName;
+	data.args = args;
+	return data;","--- 

+++ 

@@ -3,105 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
-	const data = {};
-	data.filename = fileName;
-	data.lineno = lineNumber;
-	data.colno = columnNumber;
-	data.method = functionName;
-	data.args = args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = exceptions.map((exception) =>
-			this.createTrace(description, exception),
-		);
-		const telemetry = eventLogs.map((eventLog) =>
-			this.createTelemetry(eventLog),
-		);
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463517,49.0,85,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

In the `error` method, consider adding error handling for the `fetchWithRetry` call to manage potential network or API errors gracefully.

```javascript
return fetchWithRetry(rollbarUrl, init)
    .then((response) => response.json())
    .catch((error) => {
        console.error(""Failed to send data to Rollbar:"", error);
        throw error; // Re-throw if you want to handle it further up the chain
    });
```



<!-- This is an auto-generated reply by OSS CodeRabbit -->","+	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+		const traceChain = exceptions.map((exception) =>
+			this.createTrace(description, exception),
+		);
+		const telemetry = eventLogs.map((eventLog) =>
+			this.createTelemetry(eventLog),
+		);
+		const rollbarData = {
+			environment: this.environment,
+			body: { telemetry: telemetry, trace_chain: traceChain },
+			timestamp: timestamp,
+			code_version: codeVersion,
+			language: ""javascript"",
+		};
+
+		if (""request"" in event) {
+			const request = event[""request""];
+			const params = request[""cf""];
+			delete params[""tlsClientAuth""];
+			delete params[""tlsExportedAuthenticator""];
+			rollbarData[""request""] = {
+				url: request[""url""],
+				method: request[""method""],
+				headers: request[""headers""],
+				params: params,
+			};
+		}
+
+		let init = {
+			method: ""POST"",
+			body: JSON.stringify({ data: rollbarData }),
+			headers: {
+				""Content-type"": ""application/json"",
+				""X-Rollbar-Access-Token"": this.token,
+			},
+		};
+		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());","--- 

+++ 

@@ -3,105 +3,119 @@

 
 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
-function Frame({ fileName, lineNumber, columnNumber, functionName, args }) {
-	const data = {};
-	data.filename = fileName;
-	data.lineno = lineNumber;
-	data.colno = columnNumber;
-	data.method = functionName;
-	data.args = args;
-	return data;
+const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
+};
+
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
+
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
+
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
+
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
+
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
+
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
 }
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			return;
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
-
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
-
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
-
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = exceptions.map((exception) =>
-			this.createTrace(description, exception),
-		);
-		const telemetry = eventLogs.map((eventLog) =>
-			this.createTelemetry(eventLog),
-		);
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
-
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
-
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
+export default {
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };
-
-export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
-};"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463524,,16,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

To avoid potential runtime errors, ensure that `contentType` is not null before calling `includes`. Consider using optional chaining.

```diff
-            if (contentType.includes(""application/json"")) {
+            if (contentType?.includes(""application/json"")) {
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async fetch(request, env, ctx) {
+		if (request.method === ""POST"") {
+			const contentType = request.headers.get(""content-type"");
+			if (contentType.includes(""application/json"")) {","--- 

+++ 

@@ -1,83 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463529,,27,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `parseInt` function should specify the radix to prevent unexpected behavior. Use `parseInt(data[""anpr""][""@date""], 10)` for clarity and consistency.

```diff
-                    createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
+                    createdDate = new Date(parseInt(data[""anpr""][""@date""], 10)).toISOString(); // sample 1729206290098
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+				);
+				if (survisionSerialNumber) {
+					cameraId = survisionSerialNumber;
+					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098","--- 

+++ 

@@ -1,83 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463530,37.0,42,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

When using `parseInt`, it's a good practice to specify the radix to avoid unexpected results. Update each call to `parseInt` with a radix of 10.

```diff
-                        parseInt(year),
-                        parseInt(month),
-                        parseInt(day),
-                        parseInt(hours),
-                        parseInt(minutes),
-                        parseInt(seconds),
+                        parseInt(year, 10),
+                        parseInt(month, 10),
+                        parseInt(day, 10),
+                        parseInt(hours, 10),
+                        parseInt(minutes, 10),
+                        parseInt(seconds, 10),
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+						parseInt(year),
+						parseInt(month),
+						parseInt(day),
+						parseInt(hours),
+						parseInt(minutes),
+						parseInt(seconds),","--- 

+++ 

@@ -1,83 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463535,70.0,81,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

Consider adding error handling for the `uploadBase64` method to manage potential failures during the upload process. This will improve robustness and provide better feedback in case of errors.

```javascript
try {
    const result = await snapshot.uploadBase64(
        message.body[""image""],
        message.body[""cameraId""],
        message.body[""timestamp""],
    );
    console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
    // Explicitly acknowledge the message as delivered
    message.ack();
} catch (error) {
    console.error(`Failed to log vehicle: ${error.message}`);
    // Optionally handle retries or other logic here
}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async queue(batch, env) {
+		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+		for (const message of batch.messages) {
+			const result = await snapshot.uploadBase64(
+				message.body[""image""],
+				message.body[""cameraId""],
+				message.body[""timestamp""],
+			);
+			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+			// Explicitly acknowledge the message as delivered
+			message.ack();
+		}","--- 

+++ 

@@ -1,83 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					createdDate = new Date(parseInt(data[""anpr""][""@date""])).toISOString(); // sample 1729206290098
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year),
-						parseInt(month),
-						parseInt(day),
-						parseInt(hours),
-						parseInt(minutes),
-						parseInt(seconds),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808463539,5.0,35,3eb603bac63a7a24dc7b49f43190d857c2223350,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/utils.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `fetchWithRetry` function could be improved for better readability and maintainability by using async/await syntax instead of chaining promises. This would make the code easier to follow and reduce nesting.

```diff
5c5,6
< export const fetchWithRetry = (url, init, tries = 3) =>


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+export const fetchWithRetry = (url, init, tries = 3) =>
+	fetch(url, init)
+		.then(async (response) => {
+			if (response.ok) {
+				return response;
+			} else {
+				// 1. throw a new exception
+				if (response.status === 429)
+					throw new Error429(""Rate Limited"", response);
+				if (response.status >= 500 && response.status <= 599)
+					throw new Error5xx(`Server Error - ${response.status}`, response);
+
+				console.error(await response.text());
+				// 2. reject instead of throw, preferred
+				const errorMessage = `Unexpected Response: ${response.status}`;
+				return Promise.reject(new Error(errorMessage));
+			}
+		})
+		.catch((error) => {
+			// Retry network error or 5xx errors
+			if (
+				(error instanceof Error429 || error instanceof Error5xx) &&
+				tries > 0
+			) {
+				console.error(`Retry Response status: ${error.message}`);
+				// if the rate limit is reached or exceeded,
+				const delay = 2000;
+				return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
+			} else {
+				throw error;
+			}","--- 

+++ 

@@ -2,35 +2,36 @@

 
 const wait = (delay) => new Promise((resolve) => setTimeout(resolve, delay));
 
-export const fetchWithRetry = (url, init, tries = 3) =>
-	fetch(url, init)
-		.then(async (response) => {
-			if (response.ok) {
-				return response;
-			} else {
-				// 1. throw a new exception
-				if (response.status === 429)
-					throw new Error429(""Rate Limited"", response);
-				if (response.status >= 500 && response.status <= 599)
-					throw new Error5xx(`Server Error - ${response.status}`, response);
+export function fetchWithRetry(url, init, tries = 3) {
+  return fetch(url, init)
+    .then(async (response) => {
+      if (response.ok) {
+        return response;
+      } else {
+        // 1. throw a new exception
+        if (response.status === 429)
+          throw new Error429(""Rate Limited"", response);
+        if (response.status >= 500 && response.status <= 599)
+          throw new Error5xx(`Server Error - ${response.status}`, response);
 
-				console.error(await response.text());
-				// 2. reject instead of throw, preferred
-				const errorMessage = `Unexpected Response: ${response.status}`;
-				return Promise.reject(new Error(errorMessage));
-			}
-		})
-		.catch((error) => {
-			// Retry network error or 5xx errors
-			if (
-				(error instanceof Error429 || error instanceof Error5xx) &&
-				tries > 0
-			) {
-				console.error(`Retry Response status: ${error.message}`);
-				// if the rate limit is reached or exceeded,
-				const delay = 2000;
-				return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
-			} else {
-				throw error;
-			}
-		});
+        console.error(await response.text());
+        // 2. reject instead of throw, preferred
+        const errorMessage = `Unexpected Response: ${response.status}`;
+        return Promise.reject(new Error(errorMessage));
+      }
+    })
+    .catch((error) => {
+      // Retry network error or 5xx errors
+      if (
+        (error instanceof Error429 || error instanceof Error5xx) &&
+        tries > 0
+      ) {
+        console.log(`Retrying request: ${tries}`);
+        // if the rate limit is reached or exceeded,
+        const delay = 2000;
+        return wait(delay).then(() => fetchWithRetry(url, init, tries - 1));
+      } else {
+        throw error;
+      }
+    });
+}"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808510632,3.0,7,810055517fbd417e34f2e6f832f3812d2c4640b4,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,.github/workflows/code-review.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `pull_request` event configuration is missing the `types` key, which specifies the types of activity that trigger the workflow. This could lead to unintended behavior if not specified.

```diff
   pull_request:
+      types: [opened, reopened, review_requested]
     branches: [master]
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->"," on:
   pull_request:
-    types: [opened, reopened, review_requested]
     branches: [master]
   pull_request_review_comment:
     types: [ created ]","--- 

+++ 

@@ -2,6 +2,7 @@

 
 on:
   pull_request:
+    types: [opened, reopened]
     branches: [master]
   pull_request_review_comment:
     types: [ created ]"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808510636,13.0,69,810055517fbd417e34f2e6f832f3812d2c4640b4,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The code does not handle potential exceptions that might occur during the JSON parsing or date conversion processes. This could lead to unhandled promise rejections or runtime errors.

```diff
				try {
					const data = await request.json();
				} catch (error) {
					return new Response(""Error - Invalid JSON"", { status: 400 });
				}

				console.debug(data);
				let cameraId = null;
				let imageBase64 = null;
				let createdDate = null;
				const survisionSerialNumber = request.headers.get(
					""survision-serial-number"",
				);
				if (survisionSerialNumber) {
					cameraId = survisionSerialNumber;
					try {
						createdDate = new Date(
							parseInt(data[""anpr""][""@date""], 10),
						).toISOString();
					} catch (error) {
						return new Response(""Error - Invalid Date Format"", { status: 400 });
					}
					imageBase64 = data[""anpr""][""decision""][""jpeg""];
				} else if (validGenetecEvent(data)) {
					cameraId = data[""CameraName""];
					imageBase64 = data[""ContextImage""];
					try {
						let [day, month, year] = data[""DateUtc""].split(""/"");
						let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
						createdDate = new Date(
							parseInt(year, 10),
							parseInt(month, 10),
							parseInt(day, 10),
							parseInt(hours, 10),
							parseInt(minutes, 10),
							parseInt(seconds, 10),
						).toISOString();
					} catch (error) {
						return new Response(""Error - Invalid Date/Time Format"", { status: 400 });
					}
				} else {
					return new Response(""Error - Invalid Request Content"", {
						status: 400,
					});
				}
```



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	async fetch(request, env, ctx) {
+		if (request.method === ""POST"") {
+			const contentType = request.headers.get(""content-type"");
+			if (contentType?.includes(""application/json"")) {
+				const data = await request.json();
+				console.debug(data);
+				let cameraId = null;
+				let imageBase64 = null;
+				let createdDate = null;
+				const survisionSerialNumber = request.headers.get(
+					""survision-serial-number"",
+				);
+				if (survisionSerialNumber) {
+					cameraId = survisionSerialNumber;
+					// sample 1729206290098
+					createdDate = new Date(
+						parseInt(data[""anpr""][""@date""], 10),
+					).toISOString();
+					imageBase64 = data[""anpr""][""decision""][""jpeg""];
+				} else if (validGenetecEvent(data)) {
+					cameraId = data[""CameraName""];
+					imageBase64 = data[""ContextImage""];
+					// ""10/01/2022"", Format DD/MM/YYYY
+					let [day, month, year] = data[""DateUtc""].split(""/"");
+					//  ""11:49:22"", Format HH/MM/SS
+					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+					createdDate = new Date(
+						parseInt(year, 10),
+						parseInt(month, 10),
+						parseInt(day, 10),
+						parseInt(hours, 10),
+						parseInt(minutes, 10),
+						parseInt(seconds, 10),
+					).toISOString();
+				} else {
+					return new Response(""Error - Invalid Request Content"", {
+						status: 400,
+					});
+				}
+				ctx.waitUntil(
+					env.INCOMING_WEBHOOKS.send({
+						image: imageBase64,
+						cameraId: cameraId,
+						timestamp: createdDate,
+					}),
+				);
+				return new Response(""OK!"");
+			} else {
+				return new Response(
+					""Error - Invalid Content Type, Expected application/json "",
+					{ status: 400 },
+				);
+			}
+		} else {
+			return new Response(""Error - Required POST"", { status: 400 });
+		}
+	},","--- 

+++ 

@@ -1,86 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType?.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					// sample 1729206290098
-					createdDate = new Date(
-						parseInt(data[""anpr""][""@date""], 10),
-					).toISOString();
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year, 10),
-						parseInt(month, 10),
-						parseInt(day, 10),
-						parseInt(hours, 10),
-						parseInt(minutes, 10),
-						parseInt(seconds, 10),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808510642,71.0,85,810055517fbd417e34f2e6f832f3812d2c4640b4,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/index.js,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

The `queue` function should include error handling for the asynchronous operations within the loop to ensure that any issues with uploading images do not cause the entire batch processing to fail.

```diff
		for (const message of batch.messages) {
			try {
				const result = await snapshot.uploadBase64(
					message.body[""image""],
					message.body[""cameraId""],
					message.body[""timestamp""],
				);
				console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
				// Explicitly acknowledge the message as delivered
				message.ack();
			} catch (error) {
				console.error(`Failed to process message: ${error.message}`);
				// Optionally, you can choose to not acknowledge the message here
			}
		}
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+	// The queue handler is invoked when a batch of messages is ready to be delivered
+	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+	async queue(batch, env) {
+		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+		for (const message of batch.messages) {
+			const result = await snapshot.uploadBase64(
+				message.body[""image""],
+				message.body[""cameraId""],
+				message.body[""timestamp""],
+			);
+			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+			// Explicitly acknowledge the message as delivered
+			message.ack();
+		}
+	},","--- 

+++ 

@@ -1,86 +1,116 @@

 import { SnapshotApi } from ""./snapshot"";
 
 function validGenetecEvent(data) {
-	return (
-		""CameraName"" in data &&
-		""ContextImage"" in data &&
-		""DateUtc"" in data &&
-		""TimeUtc"" in data
-	);
+  return (
+    ""CameraName"" in data &&
+    ""ContextImage"" in data &&
+    ""DateUtc"" in data &&
+    ""TimeUtc"" in data
+  );
+}
+
+function validInt(i) {
+  if (isNaN(i)) {
+    throw new Error(`Invalid value for time - ${i}`);
+  }
+  return parseInt(i, 10);
+}
+
+function requestParams(request) {
+  const { searchParams } = new URL(request.url);
+  return {
+    mmc: searchParams.get(""mmc""),
+    camera_id: searchParams.get(""camera_id""),
+    regions: searchParams.get(""regions""),
+    config: searchParams.get(""config""),
+  };
 }
 
 export default {
-	async fetch(request, env, ctx) {
-		if (request.method === ""POST"") {
-			const contentType = request.headers.get(""content-type"");
-			if (contentType?.includes(""application/json"")) {
-				const data = await request.json();
-				console.debug(data);
-				let cameraId = null;
-				let imageBase64 = null;
-				let createdDate = null;
-				const survisionSerialNumber = request.headers.get(
-					""survision-serial-number"",
-				);
-				if (survisionSerialNumber) {
-					cameraId = survisionSerialNumber;
-					// sample 1729206290098
-					createdDate = new Date(
-						parseInt(data[""anpr""][""@date""], 10),
-					).toISOString();
-					imageBase64 = data[""anpr""][""decision""][""jpeg""];
-				} else if (validGenetecEvent(data)) {
-					cameraId = data[""CameraName""];
-					imageBase64 = data[""ContextImage""];
-					// ""10/01/2022"", Format DD/MM/YYYY
-					let [day, month, year] = data[""DateUtc""].split(""/"");
-					//  ""11:49:22"", Format HH/MM/SS
-					let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
-					createdDate = new Date(
-						parseInt(year, 10),
-						parseInt(month, 10),
-						parseInt(day, 10),
-						parseInt(hours, 10),
-						parseInt(minutes, 10),
-						parseInt(seconds, 10),
-					).toISOString();
-				} else {
-					return new Response(""Error - Invalid Request Content"", {
-						status: 400,
-					});
-				}
-				ctx.waitUntil(
-					env.INCOMING_WEBHOOKS.send({
-						image: imageBase64,
-						cameraId: cameraId,
-						timestamp: createdDate,
-					}),
-				);
-				return new Response(""OK!"");
-			} else {
-				return new Response(
-					""Error - Invalid Content Type, Expected application/json "",
-					{ status: 400 },
-				);
-			}
-		} else {
-			return new Response(""Error - Required POST"", { status: 400 });
-		}
-	},
+  async fetch(request, env, ctx) {
+    if (request.method === ""POST"") {
+      const contentType = request.headers.get(""content-type"");
+      if (contentType?.includes(""application/json"")) {
+        const data = await request.json();
+        let cameraId = null;
+        let imageBase64 = null;
+        let createdDate = null;
+        const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+        const survisionSerialNumber = request.headers.get(
+          ""survision-serial-number"",
+        );
+        if (survisionSerialNumber) {
+          cameraId = survisionSerialNumber;
+          // sample 1729206290098
+          createdDate = new Date(
+            validInt(data[""anpr""][""@date""], 10),
+          ).toISOString();
+          imageBase64 = data[""anpr""][""decision""][""jpeg""];
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else if (validGenetecEvent(data)) {
+          cameraId = data[""CameraName""];
+          imageBase64 = data[""ContextImage""];
+          // ""10/01/2022"", Format DD/MM/YYYY
+          let [month, day, year] = data[""DateUtc""].split(""/"");
+          //  ""11:49:22"", Format HH/MM/SS
+          let [hours, minutes, seconds] = data[""TimeUtc""].split("":"");
+          createdDate = new Date(
+            validInt(year, 10),
+            validInt(month, 10) - 1,
+            validInt(day, 10),
+            validInt(hours, 10),
+            validInt(minutes, 10),
+            validInt(seconds, 10),
+          ).toISOString();
+          // Gentec camera data is larger than the queue limit (128 KB), we send directly
+          ctx.waitUntil(
+            snapshot.uploadBase64(
+              imageBase64,
+              cameraId,
+              createdDate,
+              requestParams(request),
+            ),
+          );
+        } else {
+          return new Response(""Error - Invalid Request Content"", {
+            status: 400,
+          });
+        }
+        return new Response(""OK!"");
+      } else {
+        return new Response(
+          ""Error - Invalid Content Type, Expected application/json "",
+          { status: 400 },
+        );
+      }
+    } else {
+      return new Response(""Error - Required POST"", { status: 400 });
+    }
+  },
 
-	// The queue handler is invoked when a batch of messages is ready to be delivered
-	// https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
-	async queue(batch, env) {
-		const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
-		for (const message of batch.messages) {
-			const result = await snapshot.uploadBase64(
-				message.body[""image""],
-				message.body[""cameraId""],
-				message.body[""timestamp""],
-			);
-			console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
-			// Explicitly acknowledge the message as delivered
-			message.ack();
-		}
-	},
+  // The queue handler is invoked when a batch of messages is ready to be delivered
+  // https://developers.cloudflare.com/queues/platform/javascript-apis/#messagebatch
+  async queue(batch, env) {
+    const snapshot = new SnapshotApi(env.SNAPSHOT_TOKEN, env.SNAPSHOT_URL);
+    for (const message of batch.messages) {
+      console.info(""Processing Queue Message:"");
+      console.info(message.body[""cameraId""]);
+      const result = await snapshot.uploadBase64(
+        message.body[""image""],
+        message.body[""cameraId""],
+        message.body[""timestamp""],
+        message.body[""params""],
+      );
+      console.info(`Logged Vehicle: ${JSON.stringify(result)}`);
+      // Explicitly acknowledge the message as delivered
+      message.ack();
+    }
+  },
 };"
https://api.github.com/repos/imajoriri/tokeru/pulls/comments/1485482147,1.0,6,b4e0af1fae43a84de11f5ec68149d3bcbb9ea2e2,838fca20fc1260fca9bf5cda5931fe15f36cc4fb,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

LGTM!



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+name: Code Review
+
+permissions:
+  contents: read
+  pull-requests: write
+",
https://api.github.com/repos/imajoriri/tokeru/pulls/comments/1485482148,8.0,10,b4e0af1fae43a84de11f5ec68149d3bcbb9ea2e2,838fca20fc1260fca9bf5cda5931fe15f36cc4fb,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

LGTM!



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+  pull_request:
+  pull_request_review_comment:
+    types: [created]",
https://api.github.com/repos/imajoriri/tokeru/pulls/comments/1485482149,12.0,17,b4e0af1fae43a84de11f5ec68149d3bcbb9ea2e2,838fca20fc1260fca9bf5cda5931fe15f36cc4fb,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

LGTM!



<!-- This is an auto-generated comment by OSS CodeRabbit -->","+concurrency:
+  group:
+    ${{ github.repository }}-${{ github.event.number || github.head_ref ||
+    github.sha }}-${{ github.workflow }}-${{ github.event_name ==
+    'pull_request_review_comment' && 'pr_comment' || 'pr' }}
+  cancel-in-progress: ${{ github.event_name != 'pull_request_review_comment' }}",
https://api.github.com/repos/imajoriri/tokeru/pulls/comments/1485482150,19.0,33,b4e0af1fae43a84de11f5ec68149d3bcbb9ea2e2,838fca20fc1260fca9bf5cda5931fe15f36cc4fb,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

LGTM!


<!-- This is an auto-generated comment by OSS CodeRabbit -->","+jobs:
+  review:
+    runs-on: ubuntu-latest
+    steps:
+      - uses: coderabbitai/ai-pr-reviewer@latest
+        env:
+          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
+          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
+        with:
+          debug: false
+          review_simple_changes: true
+          review_comment_lgtm: true
+          language: ja-JP
+          openai_light_model: gpt-3.5-turbo
+          openai_heavy_model: gpt-3.5-turbo",
https://api.github.com/repos/Dev-FE-1/Toy_Project_3_team4/pulls/comments/1720613303,28.0,33,4d4c8af98a34064b8ed276ba373b62750b3d8656,00fc9f4fb1dffa49ba18d3f95d81a267618fe44b,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, so it was mapped to the patch with the greatest overlap. Original lines [33-34]

`openai_heavy_model` Í∞íÏóê Î∂àÌïÑÏöîÌïú Í≥µÎ∞±Ïù¥ Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§. Ïù¥Î•º Ï†úÍ±∞ÌïòÏó¨ ÏùºÍ¥ÄÏÑ±ÏùÑ Ïú†ÏßÄÌï¥Ïïº Ìï©ÎãàÎã§.
```diff
-           openai_heavy_model: gpt-4o-2024-05-13 
+           openai_heavy_model: gpt-4o-2024-05-13
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","           review_comment_lgtm: false
           openai_light_model: gpt-3.5-turbo
-          openai_heavy_model: gpt-4o-2024-05-13
-          language: ko-KR
\ No newline at end of file
+          openai_heavy_model: gpt-4o-2024-05-13 
+          language: ko-KR","--- 

+++ 

@@ -29,5 +29,5 @@

           review_simple_changes: false
           review_comment_lgtm: false
           openai_light_model: gpt-3.5-turbo
-          openai_heavy_model: gpt-4o-2024-05-13 
+          openai_heavy_model: gpt-4o-2024-05-13
           language: ko-KR"
https://api.github.com/repos/Dev-FE-1/Toy_Project_3_team4/pulls/comments/1720613724,29.0,33,f8fc51bc8ec56f006944ec866be114aa45863d96,00fc9f4fb1dffa49ba18d3f95d81a267618fe44b,.github/workflows/ai-pr-reviewer.yml,,"<img src=""https://avatars.githubusercontent.com/in/347564?s=41"" alt=""Image description"" width=""20"" height=""20"">   CodeRabbit

> Note: This review was outside of the patch, but no patch was found that overlapped with it. Original lines [34-34]

`language` ÏÑ§Ï†ïÏùò Îì§Ïó¨Ïì∞Í∏∞Í∞Ä ÎßûÏßÄ ÏïäÏäµÎãàÎã§. YAML ÌååÏùºÏóêÏÑú Îì§Ïó¨Ïì∞Í∏∞Îäî Ï§ëÏöîÌïòÎØÄÎ°ú ÏàòÏ†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.
```diff
-34:           language: ko-KR
+34:          language: ko-KR
```


<!-- This is an auto-generated comment by OSS CodeRabbit -->","           openai_light_model: gpt-3.5-turbo
           openai_heavy_model: gpt-4o-2024-05-13
-          language: ko-KR
\ No newline at end of file
+          language: ko-KR",
