Comment_URL,Review_Start_Line,Review_End_Line,Original_Commit_id,Merge_Commit_id,Diff_path,New_path,Body,Diff_hunk,Change_Until_Merged
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1476796216,,77,6745de069ef16939630ce0229a847a5f53842d54,804c77fe29ecbd7a11b467f1fe890fdc31fc9758,aeon/dj_pipeline/populate/worker.py,,why not just make this default to -1?,"     db_prefix=db_prefix,
     run_duration=-1,
-    sleep_duration=10,
+    max_idled_cycle=500,","--- 

+++ 

@@ -23,7 +23,6 @@

 # ---- Some constants ----
 logger = dj.logger
 worker_schema_name = db_prefix + ""worker""
-WORKER_MAX_IDLED_CYCLE = 3
 
 # ---- Manage experiments for automated ingestion ----
 
@@ -59,8 +58,8 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
-    sleep_duration=120,
+    max_idled_cycle=6,
+    sleep_duration=1200,
 )
 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.Environment)
@@ -74,7 +73,7 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=500,
+    max_idled_cycle=400,
     sleep_duration=30,
 )
 
@@ -89,7 +88,7 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
+    max_idled_cycle=3,
     sleep_duration=10,
 )
 
@@ -106,9 +105,9 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
-    sleep_duration=60,
+    max_idled_cycle=6,
+    sleep_duration=1200,
 )
 
-analysis_worker(block_analysis.BlockAnalysis, max_calls=4)
-analysis_worker(block_analysis.BlockPlots, max_calls=4)
+analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockPlots, max_calls=6)"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1476803084,,114,6745de069ef16939630ce0229a847a5f53842d54,804c77fe29ecbd7a11b467f1fe890fdc31fc9758,aeon/dj_pipeline/populate/worker.py,,no BlockDetection?," 
-analysis_worker(block_analysis.BlockAnalysis)
+analysis_worker(block_analysis.BlockAnalysis, max_calls=4)
+analysis_worker(block_analysis.BlockPlots, max_calls=4)","--- 

+++ 

@@ -23,7 +23,6 @@

 # ---- Some constants ----
 logger = dj.logger
 worker_schema_name = db_prefix + ""worker""
-WORKER_MAX_IDLED_CYCLE = 3
 
 # ---- Manage experiments for automated ingestion ----
 
@@ -59,8 +58,8 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
-    sleep_duration=120,
+    max_idled_cycle=6,
+    sleep_duration=1200,
 )
 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.Environment)
@@ -74,7 +73,7 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=500,
+    max_idled_cycle=400,
     sleep_duration=30,
 )
 
@@ -89,7 +88,7 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
+    max_idled_cycle=3,
     sleep_duration=10,
 )
 
@@ -106,9 +105,9 @@

     worker_schema_name=worker_schema_name,
     db_prefix=db_prefix,
     run_duration=-1,
-    max_idled_cycle=WORKER_MAX_IDLED_CYCLE,
-    sleep_duration=60,
+    max_idled_cycle=6,
+    sleep_duration=1200,
 )
 
-analysis_worker(block_analysis.BlockAnalysis, max_calls=4)
-analysis_worker(block_analysis.BlockPlots, max_calls=4)
+analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockPlots, max_calls=6)"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1664288098,,78,5f64e9043bfc0bc09752157e362a426bbca0b70c,abdbb918a3afb802b49bcb5a1d8e1dbb5e482387,pyproject.toml,,"Apparently this pattern can also be used. Not sure if one is preferred, / if the * wildcard is necessary?

```
[tool.setuptools.packages.find]
where = [""aeon""]
```

"," 
-[tool.setuptools]
-packages = [""aeon""]
+[tool.setuptools.packages.find]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1756902017,,13,aa03752a0c063d0a04cef6d9592c1dfe99ee4d7e,9fedabc731d2efbfd76db9b4e51c4562c0bc7e6f,aeon/schema/social_03.py,,"Pulling out specific attributes from the JSON into columns might be a common enough operation that we could provide it at the level of the general reader, i.e. provide a `columns` property to specify which values to pull out from each record.","         super().__init__(_reader.Pose(f""{path}_202_*""))
+
+
+class EnvActiveConfigReader(_reader.JsonList):","--- 

+++ 

@@ -10,17 +10,7 @@

         super().__init__(_reader.Pose(f""{path}_202_*""))
 
 
-class EnvActiveConfigReader(_reader.JsonList):
-    def __init__(self, pattern):
-        super().__init__(pattern)
-
-    def read(self, file):
-        data = super().read(file)
-        data[""name""] = data[""value""].apply(lambda x: x[""name""])
-        return data
-
-
-class ActiveConfiguration(Stream):
+class EnvironmentActiveConfiguration(Stream):
 
     def __init__(self, path):
-        super().__init__(EnvActiveConfigReader(f""{path}_ActiveConfiguration_*""))
+        super().__init__(_reader.JsonList(f""{path}_ActiveConfiguration_*"", columns=[""name""]))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1756904873,,13,aa03752a0c063d0a04cef6d9592c1dfe99ee4d7e,9fedabc731d2efbfd76db9b4e51c4562c0bc7e6f,aeon/schema/social_03.py,,"I suggest changing
 ```
+class EnvActiveConfigReader(_reader.JsonList):
```
 to
```
+class ActiveConfigurationReader(_reader.JsonList):
```

The names of the reader and stream don't line up, since one is `EnvActiveConfig` and the other is `ActiveConfiguration`. Why not just rename the reader as `ActiveConfigurationReader`? I think the `Env` prefix might be unnecessary in this context.","         super().__init__(_reader.Pose(f""{path}_202_*""))
+
+
+class EnvActiveConfigReader(_reader.JsonList):","--- 

+++ 

@@ -10,17 +10,7 @@

         super().__init__(_reader.Pose(f""{path}_202_*""))
 
 
-class EnvActiveConfigReader(_reader.JsonList):
-    def __init__(self, pattern):
-        super().__init__(pattern)
-
-    def read(self, file):
-        data = super().read(file)
-        data[""name""] = data[""value""].apply(lambda x: x[""name""])
-        return data
-
-
-class ActiveConfiguration(Stream):
+class EnvironmentActiveConfiguration(Stream):
 
     def __init__(self, path):
-        super().__init__(EnvActiveConfigReader(f""{path}_ActiveConfiguration_*""))
+        super().__init__(_reader.JsonList(f""{path}_ActiveConfiguration_*"", columns=[""name""]))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1756906917,,567,aa03752a0c063d0a04cef6d9592c1dfe99ee4d7e,9fedabc731d2efbfd76db9b4e51c4562c0bc7e6f,aeon/dj_pipeline/acquisition.py,,Would be nice if we can agree on whether to use `Config` or `Configuration` as a common suffix for all classes dealing with these files. I think I am fine either way.," 
 
+@schema
+class EnvironmentActiveConfig(dj.Imported):","--- 

+++ 

@@ -564,7 +564,7 @@

 
 
 @schema
-class EnvironmentActiveConfig(dj.Imported):
+class EnvironmentActiveConfiguration(dj.Imported):
     definition = """"""  # Environment Active Configuration
     -> Chunk
     """"""
@@ -588,7 +588,7 @@

             ),
         )
         device = devices_schema.Environment
-        stream_reader = device.ActiveConfiguration  # expecting columns: time, name, value
+        stream_reader = device.EnvironmentActiveConfiguration  # expecting columns: time, name, value
         stream_data = io_api.load(
             root=data_dirs,
             reader=stream_reader,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1756965535,,23,aa03752a0c063d0a04cef6d9592c1dfe99ee4d7e,9fedabc731d2efbfd76db9b4e51c4562c0bc7e6f,aeon/schema/social_03.py,,would actually add `Env` here too,"+        return data
+
+
+class ActiveConfiguration(Stream):","--- 

+++ 

@@ -10,17 +10,7 @@

         super().__init__(_reader.Pose(f""{path}_202_*""))
 
 
-class EnvActiveConfigReader(_reader.JsonList):
-    def __init__(self, pattern):
-        super().__init__(pattern)
-
-    def read(self, file):
-        data = super().read(file)
-        data[""name""] = data[""value""].apply(lambda x: x[""name""])
-        return data
-
-
-class ActiveConfiguration(Stream):
+class EnvironmentActiveConfiguration(Stream):
 
     def __init__(self, path):
-        super().__init__(EnvActiveConfigReader(f""{path}_ActiveConfiguration_*""))
+        super().__init__(_reader.JsonList(f""{path}_ActiveConfiguration_*"", columns=[""name""]))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1757543203,,13,a1f46009b2f176d422b6713c5a2f49268d94818a,9fedabc731d2efbfd76db9b4e51c4562c0bc7e6f,aeon/schema/social_03.py,,"All looks great, I would just rename this stream to be called `EnvironmentActiveConfiguration` rather than the shorthand version, but not critical either way.","         super().__init__(_reader.Pose(f""{path}_202_*""))
+
+
+class EnvActiveConfiguration(Stream):","--- 

+++ 

@@ -10,7 +10,7 @@

         super().__init__(_reader.Pose(f""{path}_202_*""))
 
 
-class EnvActiveConfiguration(Stream):
+class EnvironmentActiveConfiguration(Stream):
 
     def __init__(self, path):
         super().__init__(_reader.JsonList(f""{path}_ActiveConfiguration_*"", columns=[""name""]))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1296956552,,4,c86d7b09f7074a615cb9737d94857461ffefd9e1,f1ab685e3701d9a830524ec16350cbd6f56cca32,.pre-commit-config.yaml,,"Since we're doing a clean-up, do we also want to update the repo revs? add a few more pre-commit hooks (e.g., check-docstring-first, check-merge-conflict, check-toml, mixed-line-ending, trailing-whitespace)","+  python: python3.11
 
-default_stages: [commit, push]
+#default_stages: [commit, push]","--- 

+++ 

@@ -1,7 +1,8 @@

+# For info on running pre-commit manually, see `pre-commit run --help`
+
 default_language_version:
   python: python3.11
 
-#default_stages: [commit, push]
 files: ""^(docker|aeon\/dj_pipeline)\/.*$""
 repos:
   - repo: meta
@@ -9,64 +10,41 @@

       - id: identity
 
   - repo: https://github.com/pre-commit/pre-commit-hooks
-    rev: v4.1.0
+    rev: v4.4.0
     hooks:
+      - id: check-json
       - id: check-yaml
+      - id: check-toml
+      - id: check-xml
+      - id: check-merge-conflict
       - id: detect-private-key
       - id: end-of-file-fixer
         exclude: LICENSE
       - id: no-commit-to-branch
+      - id: mixed-line-ending
+      - id: trailing-whitespace
+        args: [--markdown-linebreak-ext=md]
 
   - repo: https://github.com/psf/black
-    rev: 22.1.0
+    rev: 23.7.0
     hooks:
       - id: black
-        args:
-          - ""--config""
-          - ""./pyproject.toml""
+        args: [--check, --config, ./pyproject.toml]
+  
+  - repo: https://github.com/astral-sh/ruff-pre-commit
+    rev: v0.0.286
+    hooks:
+      - id: ruff
+        args: [--fix, --exit-non-zero-on-fix, --config, ./pyproject.toml]
 
-  - repo: https://github.com/pycqa/isort
-    rev: 5.10.1
+  - repo: https://github.com/RobertCraigie/pyright-python
+    rev: v1.1.324
     hooks:
-      - id: isort
-        name: isort (python)
-        args:
-          - ""--settings-file""
-          - ""./pyproject.toml""
-  
-  - repo: https://github.com/asottile/pyupgrade
-    rev: v3.1.0
-    hooks:
-      - id: pyupgrade
+    - id: pyright
+      args: [--level, error, --project, ./pyproject.toml]
 
-  - repo: https://github.com/pycqa/flake8
-      rev: 3.9.2
-      hooks:
-        - id: flake8
-          entry: pflake8
-          additional_dependencies: [pyproject-flake8]
-  
-  - repo: https://github.com/pre-commit/mirrors-mypy
-    rev: v0.982
-    hooks:
-      - id: mypy
-        args:
-          - ""--config-file""
-          - ""./pyproject.toml""
 
-  # Bandit and Pytest are expensive, so we show their set-up but leave them commented out.
-  # - repo: https://github.com/PyCQA/bandit
-  #   rev: 1.7.4
-  #   hooks:
-  #     - id: bandit
-  #       args:
-  #         - ""--configfile""
-  #         - ""./pyproject.toml""
-  #         - ""--recursive""
-  #         - ""-ll""
-  #       additional_dependencies:
-  #         - ""bandit[toml]""
-
+  # Pytest is expensive, so we show its set-up but leave it commented out.
   # - repo: local
   #   hooks:
   #     - id: pytest-py"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1296989343,,42,c86d7b09f7074a615cb9737d94857461ffefd9e1,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,different python versions? ,"+    name: Build env and run tests on ${{ matrix.os }}
+    runs-on: ${{ matrix.os }}
+    strategy:
+      matrix:","--- 

+++ 

@@ -1,20 +1,47 @@

-# Builds the aeon environment, flakes via flake8, checks type annotations via mypy, tests via pytest, 
-# reports test coverage via pytest-cov and codecov, and reports security vulnerabilities via bandit.
+# Builds the aeon environment; lints formatting and smells via ruff; checks type annotations via pyright;
+# tests via pytest; reports test coverage via pytest-cov and codecov.
 
 name: build_env_run_tests
 on:
-  push:
-    branches: [ main, reorg, jai_dev, config ]
   pull_request:
     branches: [ main ]
-  workflow_dispatch:  # Allows running manually from Github's 'Actions' tab
+    types: [opened, reopened, synchronize]
+  workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
-  build_env_run_tests:
+  build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
+    name: Build env using pip and pyproject.toml
+    runs-on: ubuntu-latest
+    strategy:
+      matrix:
+        os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
+      fail-fast: false
+    defaults:
+      run:
+        shell: bash -l {0}  # reset shell for each step
+    steps:
+      - name: Checkout code
+        uses: actions/checkout@v2
+      - name: Set up Python
+        uses: actions/setup-python@v2
+        with:
+          python-version: ${{ matrix.python-version }}
+      - name: Create venv and install dependencies
+        run: |
+          python -m venv .venv
+          source .venv/bin/activate
+          pip install -e .[dev]
+          pip list
+          .venv/bin/python -c ""import aeon""
+  
+  build_env_run_tests:  # checks for building env using mamba and runs codebase checks and tests
     name: Build env and run tests on ${{ matrix.os }}
     runs-on: ${{ matrix.os }}
     strategy:
       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
+        python-version: [3.11]
       fail-fast: false
     defaults:
       run:
@@ -23,28 +50,32 @@

       - name: checkout repo
         uses: actions/checkout@v2
       - name: set up conda env
-        uses: conda-incubator/setup-miniconda@v2  # from github marketplace
+        uses: conda-incubator/setup-miniconda@v2
         with:
-          activate-environment: aeon
-          environment-file: ./env_config/env.yml
           use-mamba: true
           miniforge-variant: Mambaforge
+          python-version: ${{ matrix.python-version }}
+          environment-file: ./env_config/env.yml
+          activate-environment: aeon
       - name: Update conda env with dev reqs
         run: mamba env update -f ./env_config/env_dev.yml
-      - name: flake
-        run: python -m flake8 .
-      - name: mypy
-        run: python -m mypy .
-      - name: bandit
-        run: python -m bandit . -r -ll
+
+      # Only run codebase checks and tests for ubuntu.
+      - name: ruff
+        if: matrix.os == 'ubuntu-latest'
+        run: python -m ruff check --config ./pyproject.toml .
+      - name: pyright
+        if: matrix.os == 'ubuntu-latest'
+        run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
-      # Only run coverage report job and upload for ubuntu.
+
       - name: generate test coverage report
         if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
-          python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
+          #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
         if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1310024090,,67,7937d07eac10f40c7bcdadde3c92043fc7088be8,f1ab685e3701d9a830524ec16350cbd6f56cca32,.github/workflows/build_env_run_tests.yml,,"Could we replace all instances of string `3.11` below with an environment variable to make it easier to change the python version for all the steps in the future?

It should be possible to do this using workflow environment variables: https://docs.github.com/en/actions/learn-github-actions/variables#defining-environment-variables-for-a-single-workflow","+
+      # Only run codebase checks and tests for ubuntu.
+      - name: ruff
+        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'","--- 

+++ 

@@ -7,6 +7,7 @@

     branches: [ main ]
     types: [opened, reopened, synchronize]
   workflow_dispatch:  # allows running manually from Github's 'Actions' tab
+
 jobs:
   build_env_pip_pyproject:  # checks only for building env using pip and pyproject.toml
     name: Build env using pip and pyproject.toml
@@ -15,9 +16,6 @@

       matrix:
         os: [ubuntu-latest, windows-latest, macos-latest]
         python-version: [3.11]
-        #include:  # test other python versions only on ubuntu
-          #- os: ubuntu-latest
-          #  python-version: [3.9, 3.10]
       fail-fast: false
     defaults:
       run:
@@ -64,22 +62,22 @@

 
       # Only run codebase checks and tests for ubuntu.
       - name: ruff
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m ruff check --config ./pyproject.toml .
       - name: pyright
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pyright --level error --project ./pyproject.toml . 
       - name: pytest
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: python -m pytest tests/
 
       - name: generate test coverage report
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         run: |
           python -m pytest --cov=aeon ./tests/ --cov-report=xml:./tests/test_coverage/test_coverage_report.xml
           #python -m pytest --cov=aeon ./tests/ --cov-report=html:./tests/test_coverage/test_coverage_report_html
       - name: upload test coverage report to codecov
-        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
+        if: matrix.os == 'ubuntu-latest'
         uses: codecov/codecov-action@v2
         with:
           token: ${{ secrets.CODECOV_TOKEN }}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1310038617,,24,7937d07eac10f40c7bcdadde3c92043fc7088be8,f1ab685e3701d9a830524ec16350cbd6f56cca32,docs/env_setup/local/miniconda_conda_local_setup.md,,Large indentation?,"+conda env update --file env_config/env_dev.yml
+```
+4. Using the virtual environment:
+	- `conda activate aeon`: activates the virtual environment; any commands now run within this terminal will take place within the virtual environment.",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1310056003,,5,7937d07eac10f40c7bcdadde3c92043fc7088be8,f1ab685e3701d9a830524ec16350cbd6f56cca32,docs/devs/readme.md,,"Can we elaborate here on what we are supposed to do with the output of `pyan3`? I can see how call graphs might be useful when reading the code base in some cases, but do we need to generate them when developing? Or is it more of a general tooling recommendation?","+
+- Ensure dev dependencies are installed in your Aeon Python environment
+- Ensure pre-commit hooks are installed in your Aeon Python environment: run `pre-commit install` in the activated environment
+- Use [pyan3](https://github.com/Technologicat/pyan) for generating callgraphs",File_Deleted
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1311261540,,15,8b661bd56fae560f4420350417e785f5712be191,f1ab685e3701d9a830524ec16350cbd6f56cca32,pyproject.toml,,Can you change my email here to g.lopes AT neurogears.org?,"+'''
 authors = [
   { name = ""Jai Bhagat"", email = ""jkbhagatio@gmail.com"" },
   { name = ""Goncalo Lopes"", email = ""goncaloclopes@gmail.com"" },","--- 

+++ 

@@ -12,7 +12,7 @@

 '''
 authors = [
   { name = ""Jai Bhagat"", email = ""jkbhagatio@gmail.com"" },
-  { name = ""Goncalo Lopes"", email = ""goncaloclopes@gmail.com"" },
+  { name = ""Goncalo Lopes"", email = ""g.lopes@neurogears.org"" },
   { name = ""Thinh Nguyen"", email = ""thinh@datajoint.com"" },
   { name = ""Joseph Burling"", email = ""joseph@datajoint.com"" },
   { name = ""Chang Huan Lo"", email = ""changhuan.lo@ucl.ac.uk"" },
@@ -51,20 +51,17 @@

 [project.optional-dependencies]
 dev = [
   ""bandit"",
-  ""black"",
-  ""isort"",
-  ""flake8"",
-  ""flake8-bugbear"",
-  ""flake8-docstrings"",
-  ""flake8-pyproject"",
+  ""black[jupyter]"",
   ""gh"",
   ""ipdb"",
-  ""mypy"",
-  ""pre"",
+  ""pre-commit"",
   ""pyan3 @ git+https://github.com/Technologicat/pyan.git"",
+  ""pydantic"",
+  ""pyright"",
   ""pytest"",
   ""pytest-cov"",
-  ""setuptools"",
+  ""sphinx"",
+  ""ruff"",
   ""tox"",
 ]
 gpu = ["
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1519962330,573.0,584,edff5012b2332cb67c63fa25ecc7368bd34b1f58,cefffb50fe9bb73c279a4e58a2eae4314763a029,aeon/dj_pipeline/webapps/sciviz/specsheet.yaml,,"I think you can use `.aggr()` with ""GROUP_CONCAT()` to achieve the same thing here, simplify this a bit further ","+                    df = tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"").fetch(format=""frame"")
+                    df = df.groupby('experiment_name')['identity_name'].unique().reset_index()
+
+                    subject_query = None
+
+                    for exp in query.fetch(""experiment_name""):
+                        # get identity names for each experiment
+                        identities = df[df['experiment_name'] == exp]['identity_name'].values[0]
+                        if not subject_query:
+                            subject_query = dj.U(""experiment_name"", ""subject"") & (query & f""experiment_name = '{exp}'"").proj(subject=f""CONCAT('{', '.join(identities)}')"")
+                        else:
+                            subject_query += dj.U(""experiment_name"", ""subject"") & (query & f""experiment_name = '{exp}'"").proj(subject=f""CONCAT('{', '.join(identities)}')"")","--- 

+++ 

@@ -563,27 +563,18 @@

               dj_query: >
                 def dj_query(aeon_acquisition, aeon_block_analysis, aeon_tracking):
 
-                    import pandas as pd
                     acquisition = aeon_acquisition
                     block_analysis = aeon_block_analysis
                     tracking = aeon_tracking
 
                     query = acquisition.Experiment.aggr(block_analysis.Block, block_count=""COUNT(experiment_name)"") + acquisition.Experiment.aggr(acquisition.Chunk, chunk_count=""COUNT(experiment_name)"", latest_chunk_start=""MAX(chunk_start)"")
 
-                    df = tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"").fetch(format=""frame"")
-                    df = df.groupby('experiment_name')['identity_name'].unique().reset_index()
-
-                    subject_query = None
-
-                    for exp in query.fetch(""experiment_name""):
-                        # get identity names for each experiment
-                        identities = df[df['experiment_name'] == exp]['identity_name'].values[0]
-                        if not subject_query:
-                            subject_query = dj.U(""experiment_name"", ""subject"") & (query & f""experiment_name = '{exp}'"").proj(subject=f""CONCAT('{', '.join(identities)}')"")
-                        else:
-                            subject_query += dj.U(""experiment_name"", ""subject"") & (query & f""experiment_name = '{exp}'"").proj(subject=f""CONCAT('{', '.join(identities)}')"")
-
-                    return {'query': query.join(subject_query, left=True).proj(..., participants=""subject""), 'fetch_args': []}
+                    query = query.join(acquisition.Experiment.aggr(
+                        tracking.SLEAPTracking.PoseIdentity,
+                        participants=""GROUP_CONCAT(DISTINCT identity_name SEPARATOR ', ')""
+                    ), left=True)
+
+                    return {'query': query, 'fetch_args': []}
 
     BlockAnalysis:
       route: /block_analysis"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1767284005,72.0,81,ddd59037fe8586917b4f9625e5f6df62369148e7,81bbfa19e54beddb53ffb6740ef2da1797ed9919,.github/workflows/build_env_run_tests.yml,,I guess this partly answers my earlier comment. Would it work if we just use `--use-pep517` to setup the environment for all OSes?,"+          environment-file: ./env_config/env_macos.yml
+          activate-environment: aeon
+          architecture: arm64
+          miniconda-version: ""latest""
+
+      - name: Install datajoint wheel build with pip flag (macOS)
+        if: ${{ matrix.os == 'macos-latest' }}
+        run: |
+          source $CONDA/bin/activate aeon
+          pip install --use-pep517 datajoint git+https://github.com/datajoint-company/datajoint-utilities.git",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1379092988,,69,0ffaa1fd1cd5b3975f0582c9b1659c066e747e6b,6a52fe200702bdb375f403cbe7689414f079197e,aeon/dj_pipeline/subject.py,,"I suggest changing
 ```
+        if not animal_resp:
```
 to
```
+        if len(animal_resp) == 0:
```

I think this more explicitly filters ""no animal"" conditions since response may return an error dictionary.","         animal_resp = get_pyrat_data(endpoint=f""animals"", params=params)
-        assert len(animal_resp) == 1, f""Found {len(animal_resp)} with eartag {eartag_or_id}, expect one""
-        animal_resp = animal_resp[0]
+        if not animal_resp:","--- 

+++ 

@@ -66,7 +66,7 @@

             ""eartag"": eartag_or_id,
         }
         animal_resp = get_pyrat_data(endpoint=f""animals"", params=params)
-        if not animal_resp:
+        if len(animal_resp) == 0:
             if self & key:
                 self.update1(
                     {
@@ -159,14 +159,6 @@

     origin: varchar(200)
     content=null: varchar(1000)
     attributes: varchar(1000)
-    """"""
-
-
-@schema
-class ExperimentSubject(dj.Manual):
-    definition = """"""
-    -> Subject
-    experiment_name: varchar(32)  # e.g. social-AEON3
     """"""
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1379093094,,169,0ffaa1fd1cd5b3975f0582c9b1659c066e747e6b,6a52fe200702bdb375f403cbe7689414f079197e,aeon/dj_pipeline/subject.py,,would having this in a separate table make it less error-prone?,"+class ExperimentSubject(dj.Manual):
+    definition = """"""
+    -> Subject
+    experiment_name: varchar(32)  # e.g. social-AEON3","--- 

+++ 

@@ -66,7 +66,7 @@

             ""eartag"": eartag_or_id,
         }
         animal_resp = get_pyrat_data(endpoint=f""animals"", params=params)
-        if not animal_resp:
+        if len(animal_resp) == 0:
             if self & key:
                 self.update1(
                     {
@@ -159,14 +159,6 @@

     origin: varchar(200)
     content=null: varchar(1000)
     attributes: varchar(1000)
-    """"""
-
-
-@schema
-class ExperimentSubject(dj.Manual):
-    definition = """"""
-    -> Subject
-    experiment_name: varchar(32)  # e.g. social-AEON3
     """"""
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1768752783,,865,7f5b51a7ac8f131cbe4fc51ca796d6f0c4821891,775e0cabc19bd070b372d6f3d1856acdbbd66aeb,aeon/dj_pipeline/analysis/block_analysis.py,,these should be shifted up by `win_len`,"+        )
+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
+        # Discretize into foraging bouts
+        bout_start_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == 1)[0]","--- 

+++ 

@@ -1,15 +1,17 @@

 import json
+from datetime import datetime
+
 import datajoint as dj
 import numpy as np
 import pandas as pd
 import plotly.express as px
 import plotly.graph_objs as go
 from matplotlib import path as mpl_path
-from datetime import datetime
 
 from aeon.analysis import utils as analysis_utils
 from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
 from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
+from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
 logger = dj.logger
@@ -22,6 +24,7 @@

     block_start: datetime(6)
     ---
     block_end=null: datetime(6)
+    block_duration_hr=null: decimal(6, 3)  # (hour)
     """"""
 
 
@@ -32,72 +35,59 @@

     """"""
 
     def make(self, key):
-        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.""""""
-        # find the 0s
+        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
+
+        High level logic
+        1. Find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
+        2. Remove any double 0s (0s within 1 second of each other) (pick the first 0)
+        3. Calculate block end_times (use due_time) and durations
+        4. Insert into Block table
+        """"""
+        # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
-        # In the BlockState data - if the 0 is the first index - look back at the previous chunk
-        #   if the previous timestamp belongs to a previous epoch -> block_end is the previous timestamp
-        #   else block_end is the timestamp of this 0
+
         chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
-        # only consider the time period between the last block and the current chunk
-        previous_block = Block & exp_key & f""block_start <= '{chunk_start}'""
-        if previous_block:
-            previous_block_key = previous_block.fetch(""KEY"", limit=1, order_by=""block_start DESC"")[0]
-            previous_block_start = previous_block_key[""block_start""]
-        else:
-            previous_block_key = None
-            previous_block_start = (acquisition.Chunk & exp_key).fetch(
-                ""chunk_start"", limit=1, order_by=""chunk_start""
-            )[0]
 
         chunk_restriction = acquisition.create_chunk_restriction(
-            key[""experiment_name""], previous_block_start, chunk_end
-        )
-
-        # detecting block end times
-        # pellet count reset - find 0s in BlockState
+            key[""experiment_name""], chunk_start, chunk_end
+        )
 
         block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
+        if block_state_df.empty:
+            self.insert1(key)
+            return
+
         block_state_df.index = block_state_df.index.round(
             ""us""
         )  # timestamp precision in DJ is only at microseconds
         block_state_df = block_state_df.loc[
-            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+            (block_state_df.index > chunk_start) & (block_state_df.index <= chunk_end)
         ]
 
-        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
-        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
         # find the indices of the 2nd 0s and remove
         double_0s = double_0s.shift(-1).fillna(False)
-        block_ends = block_ends[~double_0s]
+        blocks_df = blocks_df[~double_0s]
 
         block_entries = []
-        for idx, block_end in enumerate(block_ends.index):
-            if idx == 0:
-                if previous_block_key:
-                    # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
-                    previous_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{previous_pellet_time}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-                    current_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{block_end}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-
-                    previous_block_key[""block_end""] = (
-                        block_end if current_epoch == previous_epoch else previous_pellet_time
-                    )
-                    Block.update1(previous_block_key)
-            else:
-                block_entries[-1][""block_end""] = block_end
-            block_entries.append({**exp_key, ""block_start"": block_end, ""block_end"": None})
+        if not blocks_df.empty:
+            # calculate block end_times (use due_time) and durations
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
+
+            for _, row in blocks_df.iterrows():
+                block_entries.append(
+                    {
+                        **exp_key,
+                        ""block_start"": row.name,
+                        ""block_end"": row.end_time,
+                        ""block_duration_hr"": row.duration,
+                    }
+                )
 
         Block.insert(block_entries, skip_duplicates=True)
         self.insert1(key)
@@ -112,9 +102,17 @@

     -> Block
     ---
     block_duration: float  # (hour)
+    patch_count=null: int  # number of patches in the block
+    subject_count=null: int  # number of subjects in the block
     """"""
 
-    key_source = Block & ""block_end IS NOT NULL""
+    @property
+    def key_source(self):
+        # Ensure that the chunk ingestion has caught up with this block before processing
+        # (there exists a chunk that ends after the block end time)
+        ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
+        ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
+        return ks
 
     class Patch(dj.Part):
         definition = """"""
@@ -162,7 +160,7 @@

         chunk_keys = (acquisition.Chunk & key & chunk_restriction).fetch(""KEY"")
         streams_tables = (
             streams.UndergroundFeederDepletionState,
-            streams.UndergroundFeederBeamBreak,
+            streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
             tracking.SLEAPTracking,
         )
@@ -172,11 +170,9 @@

                     f""BlockAnalysis Not Ready - {streams_table.__name__} not yet fully ingested for block: {key}. Skipping (to retry later)...""
                 )
 
-        self.insert1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
-
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample by 50x - 10Hz
-        wheel_downsampling_factor = 50
+        # For wheel data, downsample to 10Hz
+        final_encoder_fs = 10
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -188,37 +184,14 @@

         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")
 
+        block_patch_entries = []
         for patch_key, patch_name in zip(patch_keys, patch_names):
             # pellet delivery and patch threshold data
-            beam_break_df = fetch_stream(
-                streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-            )[block_start:block_end]
             depletion_state_df = fetch_stream(
                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
-            # remove NaNs from threshold column
-            depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-            # identify & remove invalid indices where the time difference is less than 1 second
-            invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
-            depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
-
-            # find pellet times associated with each threshold update
-            #   for each threshold, find the time of the next threshold update,
-            #   find the closest beam break after this update time,
-            #   and use this beam break time as the delivery time for the initial threshold
-            pellet_ts_threshold_df = depletion_state_df.copy()
-            pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
-            for threshold_idx in range(len(pellet_ts_threshold_df) - 1):
-                if np.isnan(pellet_ts_threshold_df.threshold.iloc[threshold_idx]):
-                    continue
-                next_threshold_time = pellet_ts_threshold_df.index[threshold_idx + 1]
-                post_thresh_pellet_ts = beam_break_df.index[beam_break_df.index > next_threshold_time]
-                if post_thresh_pellet_ts.empty:
-                    break
-                next_beam_break = post_thresh_pellet_ts[np.searchsorted(post_thresh_pellet_ts, next_threshold_time)]
-                pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = next_beam_break
-            # remove NaNs from pellet_timestamp column (last row)
-            pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
+
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
             encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
@@ -241,10 +214,10 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
             if depletion_state_df.empty:
                 raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
+
+            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
 
             if len(depletion_state_df.rate.unique()) > 1:
                 # multiple patch rates per block is unexpected, log a note and pick the first rate to move forward
@@ -261,7 +234,12 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            self.Patch.insert1(
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
+            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+
+            block_patch_entries.append(
                 {
                     **key,
                     ""patch_name"": patch_name,
@@ -279,8 +257,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,11 +275,12 @@

             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
             pos_query = (
                 streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", anchor_part=""part_name"")
+                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
                 * tracking.SLEAPTracking.Part
                 & key
                 & {
@@ -327,7 +305,7 @@

             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
-            self.Subject.insert1(
+            block_subject_entries.append(
                 {
                     **key,
                     ""subject_name"": subject_name,
@@ -342,11 +320,20 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
+
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
+        self.Patch.insert(block_patch_entries)
+        self.Subject.insert(block_subject_entries)
 
         if block_end != (Block & key).fetch1(""block_end""):
-            Block.update1({**key, ""block_end"": block_end})
             self.update1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
 
 
@@ -366,6 +353,7 @@

         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
+        patch_threshold: longblob  # patch threshold value at each pellet delivery
         patch_threshold: longblob  # patch threshold value at each pellet delivery
         wheel_cumsum_distance_travelled: longblob  # wheel's cumulative distance travelled
         """"""
@@ -506,9 +494,9 @@

                 )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -516,16 +504,16 @@

 
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        patch_threshold=subj_patch_thresh,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""patch_threshold"": subj_patch_thresh,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -552,20 +540,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -677,10 +665,10 @@

 
     def make(self, key):
         from aeon.analysis.block_plotting import (
+            gen_hex_grad,
+            patch_markers,
+            patch_markers_linestyles,
             subject_colors,
-            patch_markers_linestyles,
-            patch_markers,
-            gen_hex_grad,
         )
 
         patch_names, subject_names = (BlockSubjectAnalysis.Preference & key).fetch(
@@ -734,11 +722,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -756,13 +744,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -774,7 +762,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -800,6 +788,115 @@

     """"""
 
 
+# ---- Helper Functions ----
+
+
+def get_threshold_associated_pellets(patch_key, start, end):
+    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+
+    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
+        - Remove all events within 1 second of each other
+        - Remove all events without threshold value (NaN)
+    2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
+    3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
+        - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
+    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
+    5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
+    Args:
+        patch_key (dict): primary key for the patch
+        start (datetime): start timestamp
+        end (datetime): end timestamp
+    Returns:
+        pd.DataFrame: DataFrame with the following columns:
+        - threshold_update_timestamp (index)
+        - pellet_timestamp
+        - beam_break_timestamp
+        - offset
+        - rate
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
+    delivered_pellet_df = fetch_stream(
+        streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
+    )[start:end]
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
+    # i.e. nearest pellet delivery within 100ms before or after threshold update
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+
+    # Clean up the df
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
+        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
+    )
+    return pellet_ts_threshold_df
+
+
 """"""Foraging bout function.""""""
 
 
@@ -807,12 +904,12 @@

     key: dict,
     min_pellets: int = 3,
     max_inactive_time: pd.Timedelta | None = None,  # seconds
-    min_wheel_movement: float = 10,  # cm
+    min_wheel_movement: float = 5,  # cm
 ) -> pd.DataFrame:
     """"""Gets foraging bouts for all subjects across all patches within a block.
 
     Args:
-        key: Block key - dict containing keys for 'experiment_name', 'block_start', 'block_end'.
+        key: Block key - dict containing keys for 'experiment_name' and 'block_start'.
         min_pellets: Minimum number of pellets for a foraging bout.
         max_inactive_time: Maximum time between `min_wheel_movement`s for a foraging bout.
         min_wheel_movement: Minimum wheel movement for a foraging bout.
@@ -821,51 +918,64 @@

         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
     max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
+    if subject_patch_data.empty:
+        return bout_data
     subject_patch_data.reset_index(level=[""experiment_name""], drop=True, inplace=True)
-    subject_names = subject_patch_data.index.get_level_values(""subject_name"").unique()
-    wheel_ts = (BlockAnalysis.Patch & key).fetch(""wheel_timestamps"")[0]
+    wheel_ts = (BlockAnalysis.Patch() & key).fetch(""wheel_timestamps"")[0]
     # For each subject:
     #   - Create cumulative wheel distance spun sum df combining all patches
     #     - Columns: timestamp, wheel distance, patch
     #   - Discretize into 'possible foraging events' based on `max_inactive_time`, and `min_wheel_movement`
+    #       - Look ahead by `max_inactive_time` and compare with current wheel distance;
+    #         if the wheel will have moved by `min_wheel_movement`, then it is a foraging event
+    #       - Because we ""looked ahead"" (shifted), we need to readjust the start time of a foraging bout
+    #       - For the foraging bout end time, we need to account for the final pellet delivery time
     #   - Filter out events with < `min_pellets`
     #   - For final events, get: duration, n_pellets, cum_wheel_distance -> add to returned DF
-    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
-    for subject in subject_names:
+    for subject in subject_patch_data.index.unique(""subject_name""):
         cur_subject_data = subject_patch_data.xs(subject, level=""subject_name"")
-        # Create combined cumulative wheel distance spun: ensure equal length wheel vals across patches
+        n_pels = sum([arr.size for arr in cur_subject_data[""pellet_timestamps""].values])
+        if n_pels < min_pellets:
+            continue
+        # Create combined cumulative wheel distance spun
         wheel_vals = cur_subject_data[""wheel_cumsum_distance_travelled""].values
-        min_len = min(len(arr) for arr in wheel_vals)
-        comb_cum_wheel_dist = np.sum([arr[:min_len] for arr in wheel_vals], axis=0)
-        wheel_ts, comb_cum_wheel_dist = (  # ensure equal length wheel vals and wheel ts
-            arr[: min(len(wheel_ts), len(comb_cum_wheel_dist))] for arr in [wheel_ts, comb_cum_wheel_dist]
-        )
-        # For each wheel_ts, get the correspdoning patch that was spun
-        patch_spun = np.empty(len(wheel_ts), dtype=""<U20"")
-        patch_spun[:] = """"
+        # Ensure equal length wheel_vals across patches and wheel_ts
+        min_len = min(*(len(arr) for arr in wheel_vals), len(wheel_ts))
+        wheel_vals = [arr[:min_len] for arr in wheel_vals]
+        comb_cum_wheel_dist = np.vstack(wheel_vals).sum(axis=0)
+        wheel_ts = wheel_ts[:min_len]
+        # Ensure monotically increasing wheel dist
+        comb_cum_wheel_dist = np.maximum.accumulate(comb_cum_wheel_dist)
+        # For each wheel_ts, get the corresponding patch that was spun
+        patch_spun = np.full(len(wheel_ts), """", dtype=""<U20"")
+        patch_names = cur_subject_data.index.get_level_values(1)
         wheel_spun_thresh = 0.03  # threshold for wheel movement (cm)
-        for _, row in cur_subject_data.iterrows():
-            patch_name = row.name[1]
-            diff = np.diff(row[""wheel_cumsum_distance_travelled""], prepend=0)
-            spun_indices = np.where(diff > wheel_spun_thresh)[0]
-            patch_spun[spun_indices] = patch_name
-        patch_spun_df = pd.DataFrame(index=wheel_ts, columns=[""cum_wheel_dist"", ""patch_spun""])
-        patch_spun_df[""cum_wheel_dist""] = comb_cum_wheel_dist
-        patch_spun_df[""patch_spun""] = patch_spun
+        diffs = np.diff(np.stack(wheel_vals), axis=1, prepend=0)
+        spun_indices = np.where(diffs > wheel_spun_thresh)
+        patch_spun[spun_indices[1]] = patch_names[spun_indices[0]]
+        patch_spun_df = pd.DataFrame(
+            {""cum_wheel_dist"": comb_cum_wheel_dist, ""patch_spun"": patch_spun}, index=wheel_ts
+        )
         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
-        win_len = int(max_inactive_time / wheel_s_r)
+        max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""]
-            .shift(-(win_len - 1))
-            .rolling(window=win_len, min_periods=1)
-            .max()
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
         foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == 1)[0]
-        bout_end_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == -1)[0]
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
+        n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
+        bout_end_indxs = (
+            np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
+            + (max_inactive_win_len - 1)
+            + n_samples_in_1s
+        )
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
+        # Remove bout that starts at block end
+        if bout_start_indxs[-1] >= len(wheel_ts):
+            bout_start_indxs = bout_start_indxs[:-1]
+            bout_end_indxs = bout_end_indxs[:-1]
         assert len(bout_start_indxs) == len(bout_end_indxs)
         bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1768761688,,866,7f5b51a7ac8f131cbe4fc51ca796d6f0c4821891,775e0cabc19bd070b372d6f3d1856acdbbd66aeb,aeon/dj_pipeline/analysis/block_analysis.py,,Need to account for final pellet delivery in bout: time for bonsai to deliver pellet / beambreak to occur after threshold crossing,"+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
+        # Discretize into foraging bouts
+        bout_start_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == 1)[0]
+        bout_end_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == -1)[0]","--- 

+++ 

@@ -1,15 +1,17 @@

 import json
+from datetime import datetime
+
 import datajoint as dj
 import numpy as np
 import pandas as pd
 import plotly.express as px
 import plotly.graph_objs as go
 from matplotlib import path as mpl_path
-from datetime import datetime
 
 from aeon.analysis import utils as analysis_utils
 from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
 from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
+from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
 logger = dj.logger
@@ -22,6 +24,7 @@

     block_start: datetime(6)
     ---
     block_end=null: datetime(6)
+    block_duration_hr=null: decimal(6, 3)  # (hour)
     """"""
 
 
@@ -32,72 +35,59 @@

     """"""
 
     def make(self, key):
-        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.""""""
-        # find the 0s
+        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
+
+        High level logic
+        1. Find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
+        2. Remove any double 0s (0s within 1 second of each other) (pick the first 0)
+        3. Calculate block end_times (use due_time) and durations
+        4. Insert into Block table
+        """"""
+        # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
-        # In the BlockState data - if the 0 is the first index - look back at the previous chunk
-        #   if the previous timestamp belongs to a previous epoch -> block_end is the previous timestamp
-        #   else block_end is the timestamp of this 0
+
         chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
-        # only consider the time period between the last block and the current chunk
-        previous_block = Block & exp_key & f""block_start <= '{chunk_start}'""
-        if previous_block:
-            previous_block_key = previous_block.fetch(""KEY"", limit=1, order_by=""block_start DESC"")[0]
-            previous_block_start = previous_block_key[""block_start""]
-        else:
-            previous_block_key = None
-            previous_block_start = (acquisition.Chunk & exp_key).fetch(
-                ""chunk_start"", limit=1, order_by=""chunk_start""
-            )[0]
 
         chunk_restriction = acquisition.create_chunk_restriction(
-            key[""experiment_name""], previous_block_start, chunk_end
-        )
-
-        # detecting block end times
-        # pellet count reset - find 0s in BlockState
+            key[""experiment_name""], chunk_start, chunk_end
+        )
 
         block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
+        if block_state_df.empty:
+            self.insert1(key)
+            return
+
         block_state_df.index = block_state_df.index.round(
             ""us""
         )  # timestamp precision in DJ is only at microseconds
         block_state_df = block_state_df.loc[
-            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+            (block_state_df.index > chunk_start) & (block_state_df.index <= chunk_end)
         ]
 
-        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
-        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
         # find the indices of the 2nd 0s and remove
         double_0s = double_0s.shift(-1).fillna(False)
-        block_ends = block_ends[~double_0s]
+        blocks_df = blocks_df[~double_0s]
 
         block_entries = []
-        for idx, block_end in enumerate(block_ends.index):
-            if idx == 0:
-                if previous_block_key:
-                    # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
-                    previous_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{previous_pellet_time}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-                    current_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{block_end}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-
-                    previous_block_key[""block_end""] = (
-                        block_end if current_epoch == previous_epoch else previous_pellet_time
-                    )
-                    Block.update1(previous_block_key)
-            else:
-                block_entries[-1][""block_end""] = block_end
-            block_entries.append({**exp_key, ""block_start"": block_end, ""block_end"": None})
+        if not blocks_df.empty:
+            # calculate block end_times (use due_time) and durations
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
+
+            for _, row in blocks_df.iterrows():
+                block_entries.append(
+                    {
+                        **exp_key,
+                        ""block_start"": row.name,
+                        ""block_end"": row.end_time,
+                        ""block_duration_hr"": row.duration,
+                    }
+                )
 
         Block.insert(block_entries, skip_duplicates=True)
         self.insert1(key)
@@ -112,9 +102,17 @@

     -> Block
     ---
     block_duration: float  # (hour)
+    patch_count=null: int  # number of patches in the block
+    subject_count=null: int  # number of subjects in the block
     """"""
 
-    key_source = Block & ""block_end IS NOT NULL""
+    @property
+    def key_source(self):
+        # Ensure that the chunk ingestion has caught up with this block before processing
+        # (there exists a chunk that ends after the block end time)
+        ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
+        ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
+        return ks
 
     class Patch(dj.Part):
         definition = """"""
@@ -162,7 +160,7 @@

         chunk_keys = (acquisition.Chunk & key & chunk_restriction).fetch(""KEY"")
         streams_tables = (
             streams.UndergroundFeederDepletionState,
-            streams.UndergroundFeederBeamBreak,
+            streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
             tracking.SLEAPTracking,
         )
@@ -172,11 +170,9 @@

                     f""BlockAnalysis Not Ready - {streams_table.__name__} not yet fully ingested for block: {key}. Skipping (to retry later)...""
                 )
 
-        self.insert1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
-
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample by 50x - 10Hz
-        wheel_downsampling_factor = 50
+        # For wheel data, downsample to 10Hz
+        final_encoder_fs = 10
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -188,37 +184,14 @@

         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")
 
+        block_patch_entries = []
         for patch_key, patch_name in zip(patch_keys, patch_names):
             # pellet delivery and patch threshold data
-            beam_break_df = fetch_stream(
-                streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-            )[block_start:block_end]
             depletion_state_df = fetch_stream(
                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
-            # remove NaNs from threshold column
-            depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-            # identify & remove invalid indices where the time difference is less than 1 second
-            invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
-            depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
-
-            # find pellet times associated with each threshold update
-            #   for each threshold, find the time of the next threshold update,
-            #   find the closest beam break after this update time,
-            #   and use this beam break time as the delivery time for the initial threshold
-            pellet_ts_threshold_df = depletion_state_df.copy()
-            pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
-            for threshold_idx in range(len(pellet_ts_threshold_df) - 1):
-                if np.isnan(pellet_ts_threshold_df.threshold.iloc[threshold_idx]):
-                    continue
-                next_threshold_time = pellet_ts_threshold_df.index[threshold_idx + 1]
-                post_thresh_pellet_ts = beam_break_df.index[beam_break_df.index > next_threshold_time]
-                if post_thresh_pellet_ts.empty:
-                    break
-                next_beam_break = post_thresh_pellet_ts[np.searchsorted(post_thresh_pellet_ts, next_threshold_time)]
-                pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = next_beam_break
-            # remove NaNs from pellet_timestamp column (last row)
-            pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
+
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
             encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
@@ -241,10 +214,10 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
             if depletion_state_df.empty:
                 raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
+
+            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
 
             if len(depletion_state_df.rate.unique()) > 1:
                 # multiple patch rates per block is unexpected, log a note and pick the first rate to move forward
@@ -261,7 +234,12 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            self.Patch.insert1(
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
+            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+
+            block_patch_entries.append(
                 {
                     **key,
                     ""patch_name"": patch_name,
@@ -279,8 +257,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,11 +275,12 @@

             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
             pos_query = (
                 streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", anchor_part=""part_name"")
+                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
                 * tracking.SLEAPTracking.Part
                 & key
                 & {
@@ -327,7 +305,7 @@

             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
-            self.Subject.insert1(
+            block_subject_entries.append(
                 {
                     **key,
                     ""subject_name"": subject_name,
@@ -342,11 +320,20 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
+
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
+        self.Patch.insert(block_patch_entries)
+        self.Subject.insert(block_subject_entries)
 
         if block_end != (Block & key).fetch1(""block_end""):
-            Block.update1({**key, ""block_end"": block_end})
             self.update1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
 
 
@@ -366,6 +353,7 @@

         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
+        patch_threshold: longblob  # patch threshold value at each pellet delivery
         patch_threshold: longblob  # patch threshold value at each pellet delivery
         wheel_cumsum_distance_travelled: longblob  # wheel's cumulative distance travelled
         """"""
@@ -506,9 +494,9 @@

                 )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -516,16 +504,16 @@

 
                 self.Patch.insert1(
                     key
-                    | dict(
-                        patch_name=patch[""patch_name""],
-                        subject_name=subject_name,
-                        in_patch_timestamps=subject_in_patch.index.values,
-                        in_patch_time=subject_in_patch_cum_time[-1],
-                        pellet_count=len(subj_pellets),
-                        pellet_timestamps=subj_pellets.index.values,
-                        patch_threshold=subj_patch_thresh,
-                        wheel_cumsum_distance_travelled=cum_wheel_dist_subj_df[subject_name].values,
-                    )
+                    | {
+                        ""patch_name"": patch[""patch_name""],
+                        ""subject_name"": subject_name,
+                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_time"": subject_in_patch_cum_time[-1],
+                        ""pellet_count"": len(subj_pellets),
+                        ""pellet_timestamps"": subj_pellets.index.values,
+                        ""patch_threshold"": subj_patch_thresh,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
+                    }
                 )
 
         # Now that we have computed all individual patch and subject values, we iterate again through
@@ -552,20 +540,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -677,10 +665,10 @@

 
     def make(self, key):
         from aeon.analysis.block_plotting import (
+            gen_hex_grad,
+            patch_markers,
+            patch_markers_linestyles,
             subject_colors,
-            patch_markers_linestyles,
-            patch_markers,
-            gen_hex_grad,
         )
 
         patch_names, subject_names = (BlockSubjectAnalysis.Preference & key).fetch(
@@ -734,11 +722,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -756,13 +744,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -774,7 +762,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -800,6 +788,115 @@

     """"""
 
 
+# ---- Helper Functions ----
+
+
+def get_threshold_associated_pellets(patch_key, start, end):
+    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+
+    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
+        - Remove all events within 1 second of each other
+        - Remove all events without threshold value (NaN)
+    2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
+    3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
+        - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
+    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
+    5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
+    Args:
+        patch_key (dict): primary key for the patch
+        start (datetime): start timestamp
+        end (datetime): end timestamp
+    Returns:
+        pd.DataFrame: DataFrame with the following columns:
+        - threshold_update_timestamp (index)
+        - pellet_timestamp
+        - beam_break_timestamp
+        - offset
+        - rate
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
+    delivered_pellet_df = fetch_stream(
+        streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
+    )[start:end]
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
+    # i.e. nearest pellet delivery within 100ms before or after threshold update
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+
+    # Clean up the df
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
+        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
+    )
+    return pellet_ts_threshold_df
+
+
 """"""Foraging bout function.""""""
 
 
@@ -807,12 +904,12 @@

     key: dict,
     min_pellets: int = 3,
     max_inactive_time: pd.Timedelta | None = None,  # seconds
-    min_wheel_movement: float = 10,  # cm
+    min_wheel_movement: float = 5,  # cm
 ) -> pd.DataFrame:
     """"""Gets foraging bouts for all subjects across all patches within a block.
 
     Args:
-        key: Block key - dict containing keys for 'experiment_name', 'block_start', 'block_end'.
+        key: Block key - dict containing keys for 'experiment_name' and 'block_start'.
         min_pellets: Minimum number of pellets for a foraging bout.
         max_inactive_time: Maximum time between `min_wheel_movement`s for a foraging bout.
         min_wheel_movement: Minimum wheel movement for a foraging bout.
@@ -821,51 +918,64 @@

         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
     max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
+    if subject_patch_data.empty:
+        return bout_data
     subject_patch_data.reset_index(level=[""experiment_name""], drop=True, inplace=True)
-    subject_names = subject_patch_data.index.get_level_values(""subject_name"").unique()
-    wheel_ts = (BlockAnalysis.Patch & key).fetch(""wheel_timestamps"")[0]
+    wheel_ts = (BlockAnalysis.Patch() & key).fetch(""wheel_timestamps"")[0]
     # For each subject:
     #   - Create cumulative wheel distance spun sum df combining all patches
     #     - Columns: timestamp, wheel distance, patch
     #   - Discretize into 'possible foraging events' based on `max_inactive_time`, and `min_wheel_movement`
+    #       - Look ahead by `max_inactive_time` and compare with current wheel distance;
+    #         if the wheel will have moved by `min_wheel_movement`, then it is a foraging event
+    #       - Because we ""looked ahead"" (shifted), we need to readjust the start time of a foraging bout
+    #       - For the foraging bout end time, we need to account for the final pellet delivery time
     #   - Filter out events with < `min_pellets`
     #   - For final events, get: duration, n_pellets, cum_wheel_distance -> add to returned DF
-    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
-    for subject in subject_names:
+    for subject in subject_patch_data.index.unique(""subject_name""):
         cur_subject_data = subject_patch_data.xs(subject, level=""subject_name"")
-        # Create combined cumulative wheel distance spun: ensure equal length wheel vals across patches
+        n_pels = sum([arr.size for arr in cur_subject_data[""pellet_timestamps""].values])
+        if n_pels < min_pellets:
+            continue
+        # Create combined cumulative wheel distance spun
         wheel_vals = cur_subject_data[""wheel_cumsum_distance_travelled""].values
-        min_len = min(len(arr) for arr in wheel_vals)
-        comb_cum_wheel_dist = np.sum([arr[:min_len] for arr in wheel_vals], axis=0)
-        wheel_ts, comb_cum_wheel_dist = (  # ensure equal length wheel vals and wheel ts
-            arr[: min(len(wheel_ts), len(comb_cum_wheel_dist))] for arr in [wheel_ts, comb_cum_wheel_dist]
-        )
-        # For each wheel_ts, get the correspdoning patch that was spun
-        patch_spun = np.empty(len(wheel_ts), dtype=""<U20"")
-        patch_spun[:] = """"
+        # Ensure equal length wheel_vals across patches and wheel_ts
+        min_len = min(*(len(arr) for arr in wheel_vals), len(wheel_ts))
+        wheel_vals = [arr[:min_len] for arr in wheel_vals]
+        comb_cum_wheel_dist = np.vstack(wheel_vals).sum(axis=0)
+        wheel_ts = wheel_ts[:min_len]
+        # Ensure monotically increasing wheel dist
+        comb_cum_wheel_dist = np.maximum.accumulate(comb_cum_wheel_dist)
+        # For each wheel_ts, get the corresponding patch that was spun
+        patch_spun = np.full(len(wheel_ts), """", dtype=""<U20"")
+        patch_names = cur_subject_data.index.get_level_values(1)
         wheel_spun_thresh = 0.03  # threshold for wheel movement (cm)
-        for _, row in cur_subject_data.iterrows():
-            patch_name = row.name[1]
-            diff = np.diff(row[""wheel_cumsum_distance_travelled""], prepend=0)
-            spun_indices = np.where(diff > wheel_spun_thresh)[0]
-            patch_spun[spun_indices] = patch_name
-        patch_spun_df = pd.DataFrame(index=wheel_ts, columns=[""cum_wheel_dist"", ""patch_spun""])
-        patch_spun_df[""cum_wheel_dist""] = comb_cum_wheel_dist
-        patch_spun_df[""patch_spun""] = patch_spun
+        diffs = np.diff(np.stack(wheel_vals), axis=1, prepend=0)
+        spun_indices = np.where(diffs > wheel_spun_thresh)
+        patch_spun[spun_indices[1]] = patch_names[spun_indices[0]]
+        patch_spun_df = pd.DataFrame(
+            {""cum_wheel_dist"": comb_cum_wheel_dist, ""patch_spun"": patch_spun}, index=wheel_ts
+        )
         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
-        win_len = int(max_inactive_time / wheel_s_r)
+        max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""]
-            .shift(-(win_len - 1))
-            .rolling(window=win_len, min_periods=1)
-            .max()
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
         foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == 1)[0]
-        bout_end_indxs = np.where(np.diff(foraging_mask.astype(int), prepend=0) == -1)[0]
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
+        n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
+        bout_end_indxs = (
+            np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
+            + (max_inactive_win_len - 1)
+            + n_samples_in_1s
+        )
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
+        # Remove bout that starts at block end
+        if bout_start_indxs[-1] >= len(wheel_ts):
+            bout_start_indxs = bout_start_indxs[:-1]
+            bout_end_indxs = bout_end_indxs[:-1]
         assert len(bout_start_indxs) == len(bout_end_indxs)
         bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1776930394,,870,c98a16310cfebaf8255c828aa7afe45876fa7a41,775e0cabc19bd070b372d6f3d1856acdbbd66aeb,aeon/dj_pipeline/analysis/block_analysis.py,,"You may want to add something like this to avoid the last bout end being out of range of the wheel ts df
```python
bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)
```","         bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (win_len - 1)
-        n_samples_in_1s = int(1 / ((wheel_ts[1] - wheel_ts[0]).astype(int) / 1e9))
+        n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
         bout_end_indxs = np.where(np.diff(foraging_mask, prepend=0) == -1)[0] + n_samples_in_1s","--- 

+++ 

@@ -11,6 +11,7 @@

 from aeon.analysis import utils as analysis_utils
 from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
 from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
+from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
 logger = dj.logger
@@ -23,6 +24,7 @@

     block_start: datetime(6)
     ---
     block_end=null: datetime(6)
+    block_duration_hr=null: decimal(6, 3)  # (hour)
     """"""
 
 
@@ -33,72 +35,59 @@

     """"""
 
     def make(self, key):
-        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.""""""
-        # find the 0s
+        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
+
+        High level logic
+        1. Find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
+        2. Remove any double 0s (0s within 1 second of each other) (pick the first 0)
+        3. Calculate block end_times (use due_time) and durations
+        4. Insert into Block table
+        """"""
+        # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
-        # In the BlockState data - if the 0 is the first index - look back at the previous chunk
-        #   if the previous timestamp belongs to a previous epoch -> block_end is the previous timestamp
-        #   else block_end is the timestamp of this 0
+
         chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
-        # only consider the time period between the last block and the current chunk
-        previous_block = Block & exp_key & f""block_start <= '{chunk_start}'""
-        if previous_block:
-            previous_block_key = previous_block.fetch(""KEY"", limit=1, order_by=""block_start DESC"")[0]
-            previous_block_start = previous_block_key[""block_start""]
-        else:
-            previous_block_key = None
-            previous_block_start = (acquisition.Chunk & exp_key).fetch(
-                ""chunk_start"", limit=1, order_by=""chunk_start""
-            )[0]
 
         chunk_restriction = acquisition.create_chunk_restriction(
-            key[""experiment_name""], previous_block_start, chunk_end
-        )
-
-        # detecting block end times
-        # pellet count reset - find 0s in BlockState
+            key[""experiment_name""], chunk_start, chunk_end
+        )
 
         block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
+        if block_state_df.empty:
+            self.insert1(key)
+            return
+
         block_state_df.index = block_state_df.index.round(
             ""us""
         )  # timestamp precision in DJ is only at microseconds
         block_state_df = block_state_df.loc[
-            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+            (block_state_df.index > chunk_start) & (block_state_df.index <= chunk_end)
         ]
 
-        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
-        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
         # find the indices of the 2nd 0s and remove
         double_0s = double_0s.shift(-1).fillna(False)
-        block_ends = block_ends[~double_0s]
+        blocks_df = blocks_df[~double_0s]
 
         block_entries = []
-        for idx, block_end in enumerate(block_ends.index):
-            if idx == 0:
-                if previous_block_key:
-                    # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
-                    previous_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{previous_pellet_time}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-                    current_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{block_end}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-
-                    previous_block_key[""block_end""] = (
-                        block_end if current_epoch == previous_epoch else previous_pellet_time
-                    )
-                    Block.update1(previous_block_key)
-            else:
-                block_entries[-1][""block_end""] = block_end
-            block_entries.append({**exp_key, ""block_start"": block_end, ""block_end"": None})
+        if not blocks_df.empty:
+            # calculate block end_times (use due_time) and durations
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
+
+            for _, row in blocks_df.iterrows():
+                block_entries.append(
+                    {
+                        **exp_key,
+                        ""block_start"": row.name,
+                        ""block_end"": row.end_time,
+                        ""block_duration_hr"": row.duration,
+                    }
+                )
 
         Block.insert(block_entries, skip_duplicates=True)
         self.insert1(key)
@@ -113,9 +102,17 @@

     -> Block
     ---
     block_duration: float  # (hour)
+    patch_count=null: int  # number of patches in the block
+    subject_count=null: int  # number of subjects in the block
     """"""
 
-    key_source = Block & ""block_end IS NOT NULL""
+    @property
+    def key_source(self):
+        # Ensure that the chunk ingestion has caught up with this block before processing
+        # (there exists a chunk that ends after the block end time)
+        ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
+        ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
+        return ks
 
     class Patch(dj.Part):
         definition = """"""
@@ -163,7 +160,7 @@

         chunk_keys = (acquisition.Chunk & key & chunk_restriction).fetch(""KEY"")
         streams_tables = (
             streams.UndergroundFeederDepletionState,
-            streams.UndergroundFeederBeamBreak,
+            streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
             tracking.SLEAPTracking,
         )
@@ -173,11 +170,9 @@

                     f""BlockAnalysis Not Ready - {streams_table.__name__} not yet fully ingested for block: {key}. Skipping (to retry later)...""
                 )
 
-        self.insert1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
-
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample by 50x - 10Hz
-        wheel_downsampling_factor = 50
+        # For wheel data, downsample to 10Hz
+        final_encoder_fs = 10
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -189,41 +184,14 @@

         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")
 
+        block_patch_entries = []
         for patch_key, patch_name in zip(patch_keys, patch_names):
             # pellet delivery and patch threshold data
-            beam_break_df = fetch_stream(
-                streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-            )[block_start:block_end]
             depletion_state_df = fetch_stream(
                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
-            # remove NaNs from threshold column
-            depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-            # identify & remove invalid indices where the time difference is less than 1 second
-            invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[
-                0
-            ]
-            depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
-
-            # find pellet times associated with each threshold update
-            #   for each threshold, find the time of the next threshold update,
-            #   find the closest beam break after this update time,
-            #   and use this beam break time as the delivery time for the initial threshold
-            pellet_ts_threshold_df = depletion_state_df.copy()
-            pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
-            for threshold_idx in range(len(pellet_ts_threshold_df) - 1):
-                if np.isnan(pellet_ts_threshold_df.threshold.iloc[threshold_idx]):
-                    continue
-                next_threshold_time = pellet_ts_threshold_df.index[threshold_idx + 1]
-                post_thresh_pellet_ts = beam_break_df.index[beam_break_df.index > next_threshold_time]
-                if post_thresh_pellet_ts.empty:
-                    break
-                next_beam_break = post_thresh_pellet_ts[
-                    np.searchsorted(post_thresh_pellet_ts, next_threshold_time)
-                ]
-                pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = next_beam_break
-            # remove NaNs from pellet_timestamp column (last row)
-            pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
+
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
             encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
@@ -246,10 +214,10 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
             if depletion_state_df.empty:
                 raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
+
+            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
 
             if len(depletion_state_df.rate.unique()) > 1:
                 # multiple patch rates per block is unexpected, log a note and pick the first rate to move forward
@@ -266,7 +234,12 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            self.Patch.insert1(
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
+            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+
+            block_patch_entries.append(
                 {
                     **key,
                     ""patch_name"": patch_name,
@@ -302,11 +275,12 @@

             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
             pos_query = (
                 streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", anchor_part=""part_name"")
+                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
                 * tracking.SLEAPTracking.Part
                 & key
                 & {
@@ -331,7 +305,7 @@

             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
-            self.Subject.insert1(
+            block_subject_entries.append(
                 {
                     **key,
                     ""subject_name"": subject_name,
@@ -348,8 +322,18 @@

             # update block_end if last timestamp of pos_df is before the current block_end
             block_end = min(pos_df.index[-1], block_end)
 
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
+        self.Patch.insert(block_patch_entries)
+        self.Subject.insert(block_subject_entries)
+
         if block_end != (Block & key).fetch1(""block_end""):
-            Block.update1({**key, ""block_end"": block_end})
             self.update1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
 
 
@@ -369,6 +353,7 @@

         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
+        patch_threshold: longblob  # patch threshold value at each pellet delivery
         patch_threshold: longblob  # patch threshold value at each pellet delivery
         wheel_cumsum_distance_travelled: longblob  # wheel's cumulative distance travelled
         """"""
@@ -803,6 +788,115 @@

     """"""
 
 
+# ---- Helper Functions ----
+
+
+def get_threshold_associated_pellets(patch_key, start, end):
+    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+
+    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
+        - Remove all events within 1 second of each other
+        - Remove all events without threshold value (NaN)
+    2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
+    3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
+        - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
+    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
+    5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
+    Args:
+        patch_key (dict): primary key for the patch
+        start (datetime): start timestamp
+        end (datetime): end timestamp
+    Returns:
+        pd.DataFrame: DataFrame with the following columns:
+        - threshold_update_timestamp (index)
+        - pellet_timestamp
+        - beam_break_timestamp
+        - offset
+        - rate
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
+    delivered_pellet_df = fetch_stream(
+        streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
+    )[start:end]
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
+    # i.e. nearest pellet delivery within 100ms before or after threshold update
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+
+    # Clean up the df
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
+        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
+    )
+    return pellet_ts_threshold_df
+
+
 """"""Foraging bout function.""""""
 
 
@@ -824,7 +918,10 @@

         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
     max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
+    if subject_patch_data.empty:
+        return bout_data
     subject_patch_data.reset_index(level=[""experiment_name""], drop=True, inplace=True)
     wheel_ts = (BlockAnalysis.Patch() & key).fetch(""wheel_timestamps"")[0]
     # For each subject:
@@ -837,9 +934,11 @@

     #       - For the foraging bout end time, we need to account for the final pellet delivery time
     #   - Filter out events with < `min_pellets`
     #   - For final events, get: duration, n_pellets, cum_wheel_distance -> add to returned DF
-    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     for subject in subject_patch_data.index.unique(""subject_name""):
         cur_subject_data = subject_patch_data.xs(subject, level=""subject_name"")
+        n_pels = sum([arr.size for arr in cur_subject_data[""pellet_timestamps""].values])
+        if n_pels < min_pellets:
+            continue
         # Create combined cumulative wheel distance spun
         wheel_vals = cur_subject_data[""wheel_cumsum_distance_travelled""].values
         # Ensure equal length wheel_vals across patches and wheel_ts
@@ -847,27 +946,36 @@

         wheel_vals = [arr[:min_len] for arr in wheel_vals]
         comb_cum_wheel_dist = np.vstack(wheel_vals).sum(axis=0)
         wheel_ts = wheel_ts[:min_len]
+        # Ensure monotically increasing wheel dist
+        comb_cum_wheel_dist = np.maximum.accumulate(comb_cum_wheel_dist)
         # For each wheel_ts, get the corresponding patch that was spun
         patch_spun = np.full(len(wheel_ts), """", dtype=""<U20"")
         patch_names = cur_subject_data.index.get_level_values(1)
         wheel_spun_thresh = 0.03  # threshold for wheel movement (cm)
-        diffs = np.diff(
-            np.stack(cur_subject_data[""wheel_cumsum_distance_travelled""].values), axis=1, prepend=0
-        )
+        diffs = np.diff(np.stack(wheel_vals), axis=1, prepend=0)
         spun_indices = np.where(diffs > wheel_spun_thresh)
         patch_spun[spun_indices[1]] = patch_names[spun_indices[0]]
         patch_spun_df = pd.DataFrame(
             {""cum_wheel_dist"": comb_cum_wheel_dist, ""patch_spun"": patch_spun}, index=wheel_ts
         )
         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
-        win_len = int(max_inactive_time / wheel_s_r)
+        max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(win_len - 1)).ffill()
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
         foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (win_len - 1)
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
         n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
-        bout_end_indxs = np.where(np.diff(foraging_mask, prepend=0) == -1)[0] + n_samples_in_1s
+        bout_end_indxs = (
+            np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
+            + (max_inactive_win_len - 1)
+            + n_samples_in_1s
+        )
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
+        # Remove bout that starts at block end
+        if bout_start_indxs[-1] >= len(wheel_ts):
+            bout_start_indxs = bout_start_indxs[:-1]
+            bout_end_indxs = bout_end_indxs[:-1]
         assert len(bout_start_indxs) == len(bout_end_indxs)
         bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1736222201,805.0,807,e3337f705867f45aa3ae4abe125e7d920e32dc8e,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+    # identify & remove invalid indices where the time difference is less than 1 second
+    invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
+    depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
```
 to
```
+    # remove invalid rows where the time difference is less than 1 second
+    depletion_state_df = depletion_state_df[~(depletion_state_df.index.diff().total_seconds() < 1)]
```
If the DateTimeIndex at `invalid_indices` are duplicated,  `depletion_state_df.index[invalid_indices]` (i.e. dropping by DateTimeIndex)  will remove all rows having the same DateTimeIndex - we want to keep at least the first occurrence. For instance, all rows with `Timestamp(""2024-02-02 11:09:57.012000084"")` will be removed in the dataframe below:

![image](https://github.com/user-attachments/assets/623d9edb-8e81-4539-9d70-bd568544147f)","+    # identify & remove invalid indices where the time difference is less than 1 second
+    invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
+    depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])","--- 

+++ 

@@ -7,6 +7,7 @@

 from matplotlib import path as mpl_path
 from datetime import datetime
 
+from aeon.io import api as io_api
 from aeon.analysis import utils as analysis_utils
 from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
 from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
@@ -22,6 +23,7 @@

     block_start: datetime(6)
     ---
     block_end=null: datetime(6)
+    block_duration_hr=null: decimal(6, 3)  # (hour)
     """"""
 
 
@@ -32,72 +34,59 @@

     """"""
 
     def make(self, key):
-        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.""""""
-        # find the 0s
+        """"""
+        On a per-chunk basis, check for the presence of new block, insert into Block table.
+        High level logic
+        1. Find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
+        2. Remove any double 0s (0s within 1 second of each other) (pick the first 0)
+        3. Calculate block end_times (use due_time) and durations
+        4. Insert into Block table
+        """"""
+        # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
-        # In the BlockState data - if the 0 is the first index - look back at the previous chunk
-        #   if the previous timestamp belongs to a previous epoch -> block_end is the previous timestamp
-        #   else block_end is the timestamp of this 0
+
         chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
-        # only consider the time period between the last block and the current chunk
-        previous_block = Block & exp_key & f""block_start <= '{chunk_start}'""
-        if previous_block:
-            previous_block_key = previous_block.fetch(""KEY"", limit=1, order_by=""block_start DESC"")[0]
-            previous_block_start = previous_block_key[""block_start""]
-        else:
-            previous_block_key = None
-            previous_block_start = (acquisition.Chunk & exp_key).fetch(
-                ""chunk_start"", limit=1, order_by=""chunk_start""
-            )[0]
 
         chunk_restriction = acquisition.create_chunk_restriction(
-            key[""experiment_name""], previous_block_start, chunk_end
-        )
-
-        # detecting block end times
-        # pellet count reset - find 0s in BlockState
+            key[""experiment_name""], chunk_start, chunk_end
+        )
 
         block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
+        if block_state_df.empty:
+            self.insert1(key)
+            return
+
         block_state_df.index = block_state_df.index.round(
             ""us""
         )  # timestamp precision in DJ is only at microseconds
         block_state_df = block_state_df.loc[
-            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+            (block_state_df.index > chunk_start) & (block_state_df.index <= chunk_end)
         ]
 
-        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
-        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
         # find the indices of the 2nd 0s and remove
         double_0s = double_0s.shift(-1).fillna(False)
-        block_ends = block_ends[~double_0s]
+        blocks_df = blocks_df[~double_0s]
 
         block_entries = []
-        for idx, block_end in enumerate(block_ends.index):
-            if idx == 0:
-                if previous_block_key:
-                    # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
-                    previous_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{previous_pellet_time}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-                    current_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{block_end}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-
-                    previous_block_key[""block_end""] = (
-                        block_end if current_epoch == previous_epoch else previous_pellet_time
-                    )
-                    Block.update1(previous_block_key)
-            else:
-                block_entries[-1][""block_end""] = block_end
-            block_entries.append({**exp_key, ""block_start"": block_end, ""block_end"": None})
+        if not blocks_df.empty:
+            # calculate block end_times (use due_time) and durations
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
+
+            for _, row in blocks_df.iterrows():
+                block_entries.append(
+                    {
+                        **exp_key,
+                        ""block_start"": row.name,
+                        ""block_end"": row.end_time,
+                        ""block_duration_hr"": row.duration,
+                    }
+                )
 
         Block.insert(block_entries, skip_duplicates=True)
         self.insert1(key)
@@ -112,9 +101,17 @@

     -> Block
     ---
     block_duration: float  # (hour)
+    patch_count=null: int  # number of patches in the block
+    subject_count=null: int  # number of subjects in the block
     """"""
 
-    key_source = Block & ""block_end IS NOT NULL""
+    @property
+    def key_source(self):
+        # Ensure that the chunk ingestion has caught up with this block before processing
+        # (there exists a chunk that ends after the block end time)
+        ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
+        ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
+        return ks
 
     class Patch(dj.Part):
         definition = """"""
@@ -172,11 +169,9 @@

                     f""BlockAnalysis Not Ready - {streams_table.__name__} not yet fully ingested for block: {key}. Skipping (to retry later)...""
                 )
 
-        self.insert1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
-
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample by 50x - 10Hz
-        wheel_downsampling_factor = 50
+        # For wheel data, downsample to 10Hz
+        final_encoder_fs = 10
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -188,6 +183,7 @@

         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")
 
+        block_patch_entries = []
         for patch_key, patch_name in zip(patch_keys, patch_names):
             # pellet delivery and patch threshold data
             depletion_state_df = fetch_stream(
@@ -237,7 +233,12 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            self.Patch.insert1(
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
+            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+
+            block_patch_entries.append(
                 {
                     **key,
                     ""patch_name"": patch_name,
@@ -255,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -274,6 +274,7 @@

             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
             pos_query = (
@@ -303,7 +304,7 @@

             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
-            self.Subject.insert1(
+            block_subject_entries.append(
                 {
                     **key,
                     ""subject_name"": subject_name,
@@ -318,11 +319,20 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
+
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
+        self.Patch.insert(block_patch_entries)
+        self.Subject.insert(block_subject_entries)
 
         if block_end != (Block & key).fetch1(""block_end""):
-            Block.update1({**key, ""block_end"": block_end})
             self.update1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
 
 
@@ -528,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -710,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -732,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -750,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -775,6 +785,7 @@

     note: varchar(3000)
     """"""
 
+
 # ---- Helper Functions ----
 
 
@@ -785,46 +796,98 @@

         - Remove all events within 1 second of each other
         - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
+    Args:
+        patch_key (dict): primary key for the patch
+        start (datetime): start timestamp
+        end (datetime): end timestamp
+    Returns:
+        pd.DataFrame: DataFrame with the following columns:
+        - threshold_update_timestamp (index)
+        - pellet_timestamp
+        - beam_break_timestamp
+        - offset
+        - rate
     """"""
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
-    # pellet delivery and patch threshold data
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
     )[start:end]
-    # remove NaNs from threshold column
+    # Remove NaNs
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # identify & remove invalid indices where the time difference is less than 1 second
-    invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
-    depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
-
-    # find pellet times approximately coincide with each threshold update
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
-    delivered_pellet_ts = delivered_pellet_df.index
-    pellet_ts_threshold_df = depletion_state_df.copy()
-    pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
-    for threshold_idx in range(len(pellet_ts_threshold_df)):
-        threshold_time = pellet_ts_threshold_df.index[threshold_idx]
-        within_range_pellet_ts = np.logical_and(delivered_pellet_ts >= threshold_time - pd.Timedelta(milliseconds=100),
-                                                delivered_pellet_ts <= threshold_time + pd.Timedelta(milliseconds=100))
-        if not within_range_pellet_ts.any():
-            continue
-        pellet_time = delivered_pellet_ts[within_range_pellet_ts][-1]
-        pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = pellet_time
-
-    # remove rows of threshold updates without corresponding pellet times from i.e. pellet_timestamp is NaN
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+
+    # Clean up the df
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
-
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
     return pellet_ts_threshold_df"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1736276782,811.0,824,e3337f705867f45aa3ae4abe125e7d920e32dc8e,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+    delivered_pellet_ts = delivered_pellet_df.index
+    pellet_ts_threshold_df = depletion_state_df.copy()
+    pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
+    for threshold_idx in range(len(pellet_ts_threshold_df)):
+        threshold_time = pellet_ts_threshold_df.index[threshold_idx]
+        within_range_pellet_ts = np.logical_and(delivered_pellet_ts >= threshold_time - pd.Timedelta(milliseconds=100),
+                                                delivered_pellet_ts <= threshold_time + pd.Timedelta(milliseconds=100))
+        if not within_range_pellet_ts.any():
+            continue
+        pellet_time = delivered_pellet_ts[within_range_pellet_ts][-1]
+        pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = pellet_time
+
+    # remove rows of threshold updates without corresponding pellet times from i.e. pellet_timestamp is NaN
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
```
 to
```
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            delivered_pellet_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event""])
```","+    delivered_pellet_ts = delivered_pellet_df.index
+    pellet_ts_threshold_df = depletion_state_df.copy()
+    pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
+    for threshold_idx in range(len(pellet_ts_threshold_df)):
+        threshold_time = pellet_ts_threshold_df.index[threshold_idx]
+        within_range_pellet_ts = np.logical_and(delivered_pellet_ts >= threshold_time - pd.Timedelta(milliseconds=100),
+                                                delivered_pellet_ts <= threshold_time + pd.Timedelta(milliseconds=100))
+        if not within_range_pellet_ts.any():
+            continue
+        pellet_time = delivered_pellet_ts[within_range_pellet_ts][-1]
+        pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = pellet_time
+
+    # remove rows of threshold updates without corresponding pellet times from i.e. pellet_timestamp is NaN
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])","--- 

+++ 

@@ -7,6 +7,7 @@

 from matplotlib import path as mpl_path
 from datetime import datetime
 
+from aeon.io import api as io_api
 from aeon.analysis import utils as analysis_utils
 from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
 from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
@@ -22,6 +23,7 @@

     block_start: datetime(6)
     ---
     block_end=null: datetime(6)
+    block_duration_hr=null: decimal(6, 3)  # (hour)
     """"""
 
 
@@ -32,72 +34,59 @@

     """"""
 
     def make(self, key):
-        """"""On a per-chunk basis, check for the presence of new block, insert into Block table.""""""
-        # find the 0s
+        """"""
+        On a per-chunk basis, check for the presence of new block, insert into Block table.
+        High level logic
+        1. Find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
+        2. Remove any double 0s (0s within 1 second of each other) (pick the first 0)
+        3. Calculate block end_times (use due_time) and durations
+        4. Insert into Block table
+        """"""
+        # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
-        # In the BlockState data - if the 0 is the first index - look back at the previous chunk
-        #   if the previous timestamp belongs to a previous epoch -> block_end is the previous timestamp
-        #   else block_end is the timestamp of this 0
+
         chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
-        # only consider the time period between the last block and the current chunk
-        previous_block = Block & exp_key & f""block_start <= '{chunk_start}'""
-        if previous_block:
-            previous_block_key = previous_block.fetch(""KEY"", limit=1, order_by=""block_start DESC"")[0]
-            previous_block_start = previous_block_key[""block_start""]
-        else:
-            previous_block_key = None
-            previous_block_start = (acquisition.Chunk & exp_key).fetch(
-                ""chunk_start"", limit=1, order_by=""chunk_start""
-            )[0]
 
         chunk_restriction = acquisition.create_chunk_restriction(
-            key[""experiment_name""], previous_block_start, chunk_end
-        )
-
-        # detecting block end times
-        # pellet count reset - find 0s in BlockState
+            key[""experiment_name""], chunk_start, chunk_end
+        )
 
         block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
+        if block_state_df.empty:
+            self.insert1(key)
+            return
+
         block_state_df.index = block_state_df.index.round(
             ""us""
         )  # timestamp precision in DJ is only at microseconds
         block_state_df = block_state_df.loc[
-            (block_state_df.index > previous_block_start) & (block_state_df.index <= chunk_end)
+            (block_state_df.index > chunk_start) & (block_state_df.index <= chunk_end)
         ]
 
-        block_ends = block_state_df[block_state_df.pellet_ct == 0]
+        blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
-        double_0s = block_ends.index.to_series().diff().dt.total_seconds() < 1
+        double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
         # find the indices of the 2nd 0s and remove
         double_0s = double_0s.shift(-1).fillna(False)
-        block_ends = block_ends[~double_0s]
+        blocks_df = blocks_df[~double_0s]
 
         block_entries = []
-        for idx, block_end in enumerate(block_ends.index):
-            if idx == 0:
-                if previous_block_key:
-                    # if there is a previous block - insert ""block_end"" for the previous block
-                    previous_pellet_time = block_state_df[:block_end].index[-1]
-                    previous_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{previous_pellet_time}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-                    current_epoch = (
-                        acquisition.Epoch.join(acquisition.EpochEnd, left=True)
-                        & exp_key
-                        & f""'{block_end}' BETWEEN epoch_start AND IFNULL(epoch_end, '2200-01-01')""
-                    ).fetch1(""KEY"")
-
-                    previous_block_key[""block_end""] = (
-                        block_end if current_epoch == previous_epoch else previous_pellet_time
-                    )
-                    Block.update1(previous_block_key)
-            else:
-                block_entries[-1][""block_end""] = block_end
-            block_entries.append({**exp_key, ""block_start"": block_end, ""block_end"": None})
+        if not blocks_df.empty:
+            # calculate block end_times (use due_time) and durations
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
+
+            for _, row in blocks_df.iterrows():
+                block_entries.append(
+                    {
+                        **exp_key,
+                        ""block_start"": row.name,
+                        ""block_end"": row.end_time,
+                        ""block_duration_hr"": row.duration,
+                    }
+                )
 
         Block.insert(block_entries, skip_duplicates=True)
         self.insert1(key)
@@ -112,9 +101,17 @@

     -> Block
     ---
     block_duration: float  # (hour)
+    patch_count=null: int  # number of patches in the block
+    subject_count=null: int  # number of subjects in the block
     """"""
 
-    key_source = Block & ""block_end IS NOT NULL""
+    @property
+    def key_source(self):
+        # Ensure that the chunk ingestion has caught up with this block before processing
+        # (there exists a chunk that ends after the block end time)
+        ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
+        ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
+        return ks
 
     class Patch(dj.Part):
         definition = """"""
@@ -172,11 +169,9 @@

                     f""BlockAnalysis Not Ready - {streams_table.__name__} not yet fully ingested for block: {key}. Skipping (to retry later)...""
                 )
 
-        self.insert1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
-
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample by 50x - 10Hz
-        wheel_downsampling_factor = 50
+        # For wheel data, downsample to 10Hz
+        final_encoder_fs = 10
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -188,6 +183,7 @@

         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")
 
+        block_patch_entries = []
         for patch_key, patch_name in zip(patch_keys, patch_names):
             # pellet delivery and patch threshold data
             depletion_state_df = fetch_stream(
@@ -237,7 +233,12 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            self.Patch.insert1(
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
+            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+
+            block_patch_entries.append(
                 {
                     **key,
                     ""patch_name"": patch_name,
@@ -255,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -274,6 +274,7 @@

             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
             pos_query = (
@@ -303,7 +304,7 @@

             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
-            self.Subject.insert1(
+            block_subject_entries.append(
                 {
                     **key,
                     ""subject_name"": subject_name,
@@ -318,11 +319,20 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
+
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
+        self.Patch.insert(block_patch_entries)
+        self.Subject.insert(block_subject_entries)
 
         if block_end != (Block & key).fetch1(""block_end""):
-            Block.update1({**key, ""block_end"": block_end})
             self.update1({**key, ""block_duration"": (block_end - block_start).total_seconds() / 3600})
 
 
@@ -528,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -710,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -732,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -750,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -775,6 +785,7 @@

     note: varchar(3000)
     """"""
 
+
 # ---- Helper Functions ----
 
 
@@ -785,46 +796,98 @@

         - Remove all events within 1 second of each other
         - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
+    Args:
+        patch_key (dict): primary key for the patch
+        start (datetime): start timestamp
+        end (datetime): end timestamp
+    Returns:
+        pd.DataFrame: DataFrame with the following columns:
+        - threshold_update_timestamp (index)
+        - pellet_timestamp
+        - beam_break_timestamp
+        - offset
+        - rate
     """"""
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
-    # pellet delivery and patch threshold data
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
     )[start:end]
-    # remove NaNs from threshold column
+    # Remove NaNs
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # identify & remove invalid indices where the time difference is less than 1 second
-    invalid_indices = np.where(depletion_state_df.index.to_series().diff().dt.total_seconds() < 1)[0]
-    depletion_state_df = depletion_state_df.drop(depletion_state_df.index[invalid_indices])
-
-    # find pellet times approximately coincide with each threshold update
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
-    delivered_pellet_ts = delivered_pellet_df.index
-    pellet_ts_threshold_df = depletion_state_df.copy()
-    pellet_ts_threshold_df[""pellet_timestamp""] = pd.NaT
-    for threshold_idx in range(len(pellet_ts_threshold_df)):
-        threshold_time = pellet_ts_threshold_df.index[threshold_idx]
-        within_range_pellet_ts = np.logical_and(delivered_pellet_ts >= threshold_time - pd.Timedelta(milliseconds=100),
-                                                delivered_pellet_ts <= threshold_time + pd.Timedelta(milliseconds=100))
-        if not within_range_pellet_ts.any():
-            continue
-        pellet_time = delivered_pellet_ts[within_range_pellet_ts][-1]
-        pellet_ts_threshold_df.pellet_timestamp.iloc[threshold_idx] = pellet_time
-
-    # remove rows of threshold updates without corresponding pellet times from i.e. pellet_timestamp is NaN
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    pellet_ts_threshold_df = (
+        pd.merge_asof(
+            depletion_state_df.reset_index(),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
+            left_on=""time"",
+            right_on=""pellet_timestamp"",
+            tolerance=pd.Timedelta(""100ms""),
+            direction=""nearest"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""pellet_timestamp""])
+    )
+
+    # Clean up the df
+    pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp""])
-
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
     return pellet_ts_threshold_df"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1764812030,,790,b05bc0788de0e6f0d99f8d245ef148c3d4ef104e,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,,"We need to also remove all manual pellet delivery events
","+def get_threshold_associated_pellets(patch_key, start, end):
+    """"""
+    Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""","--- 

+++ 

@@ -233,7 +233,9 @@

             # handles patch rate value being INF
             patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
 
-            encoder_fs = 1 / encoder_df.index.to_series().diff().dt.total_seconds().median()  # mean or median?
+            encoder_fs = (
+                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
+            )  # mean or median?
             wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
 
             block_patch_entries.append(
@@ -254,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -318,13 +319,16 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
-
-        self.insert1({**key,
-                      ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                      ""patch_count"": len(patch_keys),
-                      ""subject_count"": len(subject_names)})
+            block_end = min(pos_df.index[-1], block_end)
+
+        self.insert1(
+            {
+                **key,
+                ""block_duration"": (block_end - block_start).total_seconds() / 3600,
+                ""patch_count"": len(patch_keys),
+                ""subject_count"": len(subject_names),
+            }
+        )
         self.Patch.insert(block_patch_entries)
         self.Subject.insert(block_subject_entries)
 
@@ -534,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -716,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -738,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -756,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -781,6 +785,7 @@

     note: varchar(3000)
     """"""
 
+
 # ---- Helper Functions ----
 
 
@@ -791,17 +796,15 @@

         - Remove all events within 1 second of each other
         - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 500ms after each pellet delivery
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
-
     Args:
         patch_key (dict): primary key for the patch
         start (datetime): start timestamp
         end (datetime): end timestamp
-
     Returns:
         pd.DataFrame: DataFrame with the following columns:
         - threshold_update_timestamp (index)
@@ -810,42 +813,63 @@

         - offset
         - rate
     """"""
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
-    # pellet delivery and beam break data
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
+
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
-    beam_break_df = fetch_stream(
-        streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
     )[start:end]
-
-    if delivered_pellet_df.empty or beam_break_df.empty:
-        return acquisition.io_api._empty([""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""])
-
-    # patch threshold data
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
+        return acquisition.io_api._empty(
+            [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
+        )
+
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
+    pellet_beam_break_df = (
+        pd.merge_asof(
+            delivered_pellet_df.reset_index(),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            left_on=""time"",
+            right_on=""beam_break_timestamp"",
+            tolerance=pd.Timedelta(""1.2s""),
+            direction=""forward"",
+        )
+        .set_index(""time"")
+        .dropna(subset=[""beam_break_timestamp""])
+    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
+
+    # Get patch threshold data
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
     )[start:end]
-    # remove NaNs from threshold column
+    # Remove NaNs
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # remove invalid rows where the time difference is less than 1 second
+    # Remove invalid rows where the time difference is less than 1 second
     invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
     depletion_state_df = depletion_state_df[~invalid_rows]
 
-    # find pellet times with matching beam break times (within 500ms after pellet times)
-    pellet_beam_break_df = pd.merge_asof(
-        delivered_pellet_df.reset_index(),
-        beam_break_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
-        left_on=""time"",
-        right_on=""beam_break_timestamp"",
-        tolerance=pd.Timedelta(""1.2s""),
-        direction=""forward"",
-    ).set_index(""time"").dropna(subset=[""beam_break_timestamp""])
-    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
-
-    # find pellet times approximately coincide with each threshold update
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
@@ -859,11 +883,11 @@

         .set_index(""time"")
         .dropna(subset=[""pellet_timestamp""])
     )
+
+    # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
     pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
     pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
-
     return pellet_ts_threshold_df"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1765179469,,845,773996bee952dc45e9821266872b15a348af62dc,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,,"Should we remove near-in-time pellet delivery events before doing this `merge_asof`, to reduce the tolerance specified here? Does it make a difference?

I'll look at this briefly","+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # find pellet times with matching beam break times (within 500ms after pellet times)
+    pellet_beam_break_df = (","--- 

+++ 

@@ -256,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,8 +319,7 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
 
         self.insert1(
             {
@@ -540,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -722,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -744,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -762,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -798,17 +796,15 @@

         - Remove all events within 1 second of each other
         - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 500ms after each pellet delivery
+        - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
-
     Args:
         patch_key (dict): primary key for the patch
         start (datetime): start timestamp
         end (datetime): end timestamp
-
     Returns:
         pd.DataFrame: DataFrame with the following columns:
         - threshold_update_timestamp (index)
@@ -818,34 +814,41 @@

         - rate
     """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
-    # pellet delivery and beam break data
+
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
-    beam_break_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
         start:end
     ]
-
-    if delivered_pellet_df.empty or beam_break_df.empty:
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
+    manual_delivery_df = fetch_stream(
+        streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
+    )[start:end]
+    delivered_pellet_df = delivered_pellet_df.loc[
+        delivered_pellet_df.index.difference(manual_delivery_df.index)
+    ]
+
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
         return acquisition.io_api._empty(
             [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
         )
 
-    # patch threshold data
-    depletion_state_df = fetch_stream(
-        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
-    )[start:end]
-    # remove NaNs from threshold column
-    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # remove invalid rows where the time difference is less than 1 second
-    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
-    depletion_state_df = depletion_state_df[~invalid_rows]
-
-    # find pellet times with matching beam break times (within 500ms after pellet times)
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
     pellet_beam_break_df = (
         pd.merge_asof(
             delivered_pellet_df.reset_index(),
-            beam_break_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
             tolerance=pd.Timedelta(""1.2s""),
@@ -856,7 +859,17 @@

     )
     pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
-    # find pellet times approximately coincide with each threshold update
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
@@ -870,13 +883,11 @@

         .set_index(""time"")
         .dropna(subset=[""pellet_timestamp""])
     )
+
+    # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
     pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
-        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
-    )
-
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
     return pellet_ts_threshold_df"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1773764875,,184,c69b78ea3a84abff51985f2f506670f854571b04,07fe4c2a54d737cbf6a2ae06a3fbbd3e268dfd1d,aeon/dj_pipeline/analysis/block_analysis.py,,Should we exclude Dummy patches here or is this already handled elsewhere and we can assume dummy patches will never be fetched?,"@@ -188,37 +183,14 @@ def make(self, key):
         )
         patch_keys, patch_names = patch_query.fetch(""KEY"", ""underground_feeder_name"")","--- 

+++ 

@@ -256,8 +256,7 @@

             )
 
             # update block_end if last timestamp of encoder_df is before the current block_end
-            if encoder_df.index[-1] < block_end:
-                block_end = encoder_df.index[-1]
+            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,8 +319,7 @@

             )
 
             # update block_end if last timestamp of pos_df is before the current block_end
-            if pos_df.index[-1] < block_end:
-                block_end = pos_df.index[-1]
+            block_end = min(pos_df.index[-1], block_end)
 
         self.insert1(
             {
@@ -540,20 +538,20 @@

 
                 self.Preference.insert1(
                     key
-                    | dict(
-                        patch_name=patch_name,
-                        subject_name=subject_name,
-                        cumulative_preference_by_time=cum_pref_time,
-                        cumulative_preference_by_wheel=cum_pref_dist,
-                        final_preference_by_time=cum_pref_time[-1],
-                        final_preference_by_wheel=cum_pref_dist[-1],
-                    )
+                    | {
+                        ""patch_name"": patch_name,
+                        ""subject_name"": subject_name,
+                        ""cumulative_preference_by_time"": cum_pref_time,
+                        ""cumulative_preference_by_wheel"": cum_pref_dist,
+                        ""final_preference_by_time"": cum_pref_time[-1],
+                        ""final_preference_by_wheel"": cum_pref_dist[-1],
+                    }
                 )
 
 
 @schema
 class BlockPlots(dj.Computed):
-    definition = """""" 
+    definition = """"""
     -> BlockAnalysis
     ---
     subject_positions_plot: longblob
@@ -722,11 +720,11 @@

                             x=wheel_ts,
                             y=cum_pref,
                             mode=""lines"",  # +  markers"",
-                            line=dict(
-                                width=2,
-                                color=subject_colors[subj_i],
-                                dash=patch_markers_linestyles[patch_i],
-                            ),
+                            line={
+                                ""width"": 2,
+                                ""color"": subject_colors[subj_i],
+                                ""dash"": patch_markers_linestyles[patch_i],
+                            },
                             name=f""{subj} - {p}: μ: {patch_mean}"",
                         )
                     )
@@ -744,13 +742,13 @@

                                 x=cur_cum_pel_ct[""time""],
                                 y=cur_cum_pel_ct[""cum_pref""],
                                 mode=""markers"",
-                                marker=dict(
-                                    symbol=patch_markers[patch_i],
-                                    color=gen_hex_grad(
+                                marker={
+                                    ""symbol"": patch_markers[patch_i],
+                                    ""color"": gen_hex_grad(
                                         subject_colors[-1], cur_cum_pel_ct[""norm_thresh_val""]
                                     ),
-                                    size=8,
-                                ),
+                                    ""size"": 8,
+                                },
                                 showlegend=False,
                                 customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                                 hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
@@ -762,7 +760,7 @@

                 title=f""Cumulative Patch Preference - {title}"",
                 xaxis_title=""Time"",
                 yaxis_title=""Pref Index"",
-                yaxis=dict(tickvals=np.arange(0, 1.1, 0.1)),
+                yaxis={""tickvals"": np.arange(0, 1.1, 0.1)},
             )
 
         # Insert figures as json-formatted plotly plots
@@ -803,12 +801,10 @@

         - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
-
     Args:
         patch_key (dict): primary key for the patch
         start (datetime): start timestamp
         end (datetime): end timestamp
-
     Returns:
         pd.DataFrame: DataFrame with the following columns:
         - threshold_update_timestamp (index)
@@ -819,42 +815,40 @@

     """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
-    # pellet delivery and beam break data
+    # Get pellet delivery trigger data
     delivered_pellet_df = fetch_stream(
         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
-    beam_break_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+    # Remove invalid rows where the time difference is less than 1.2 seconds
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    delivered_pellet_df = delivered_pellet_df[~invalid_rows]
+
+    # Get beambreak data
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
         start:end
     ]
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < 1
+    beambreak_df = beambreak_df[~invalid_rows]
+    # Exclude manual deliveries
     manual_delivery_df = fetch_stream(
         streams.UndergroundFeederManualDelivery & patch_key & chunk_restriction
     )[start:end]
-
-    # exclude ManualDelivery from the pellet delivery (take the not intersecting part)
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    if delivered_pellet_df.empty or beam_break_df.empty:
+    # Return empty if no pellets
+    if delivered_pellet_df.empty or beambreak_df.empty:
         return acquisition.io_api._empty(
             [""threshold"", ""offset"", ""rate"", ""pellet_timestamp"", ""beam_break_timestamp""]
         )
 
-    # patch threshold data
-    depletion_state_df = fetch_stream(
-        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
-    )[start:end]
-    # remove NaNs from threshold column
-    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    # remove invalid rows where the time difference is less than 1 second
-    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
-    depletion_state_df = depletion_state_df[~invalid_rows]
-
-    # find pellet times with matching beam break times (within 1.2s after pellet times)
+    # Find pellet delivery triggers with matching beambreaks within 1.2s after each pellet delivery
     pellet_beam_break_df = (
         pd.merge_asof(
             delivered_pellet_df.reset_index(),
-            beam_break_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
+            beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
             tolerance=pd.Timedelta(""1.2s""),
@@ -865,7 +859,17 @@

     )
     pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
-    # find pellet times approximately coincide with each threshold update
+    # Get patch threshold data
+    depletion_state_df = fetch_stream(
+        streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
+    )[start:end]
+    # Remove NaNs
+    depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
+    # Remove invalid rows where the time difference is less than 1 second
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < 1
+    depletion_state_df = depletion_state_df[~invalid_rows]
+
+    # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
@@ -879,13 +883,11 @@

         .set_index(""time"")
         .dropna(subset=[""pellet_timestamp""])
     )
+
+    # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
-    # shift back the pellet_timestamp values by 1 to match the pellet_timestamp with the previous threshold update
+    # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
     pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
     pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    # remove NaNs from pellet_timestamp column (last row)
-    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
-        subset=[""pellet_timestamp"", ""beam_break_timestamp""]
-    )
-
+    pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(subset=[""pellet_timestamp"", ""beam_break_timestamp""])
     return pellet_ts_threshold_df"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1787760107,,1516,747232bc21d3b9c88f11f59bcca42b15545c6796,5dacb450d6db83e0fb8158c8c76476a4ffb6a828,aeon/dj_pipeline/analysis/block_analysis.py,,I'm happy to remove duration here and in the foraging_bouts function as it is trivial to recompute from the start and end times,"+        bout_start: datetime(6)
+        ---
+        bout_end: datetime(6)
+        bout_duration: float  # (seconds)","--- 

+++ 

@@ -1513,7 +1513,6 @@

         bout_start: datetime(6)
         ---
         bout_end: datetime(6)
-        bout_duration: float  # (seconds)
         pellet_count: int  # number of pellets consumed during the bout
         cum_wheel_dist: float  # cumulative distance travelled during the bout
         """"""
@@ -1525,7 +1524,6 @@

                 ""subject_name"": ""subject"",
                 ""bout_start"": ""start"",
                 ""bout_end"": ""end"",
-                ""bout_duration"": ""duration"",
                 ""pellet_count"": ""n_pellets"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
@@ -1781,7 +1779,6 @@

                     {
                         ""start"": bout_starts_ends[:, 0],
                         ""end"": bout_starts_ends[:, 1],
-                        ""duration"": bout_durations,
                         ""n_pellets"": bout_pellets,
                         ""cum_wheel_dist"": bout_cum_wheel_dist,
                         ""subject"": subject,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1782530630,,315,6b32583f40753edc082637943750e831d0bd1c71,83cd9056b5434b830384c61211752035990e1738,aeon/io/reader.py,,"Are we sure this will always work? I think there was a reason I was counting dirs backwards from the end of the file, maybe if locations change it's important? 

But you can confirm?","-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[1:])","--- 

+++ 

@@ -304,15 +304,22 @@

     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[1:])
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
 
         # Check if model directory exists in local or shared directories.
         # Local directory is prioritized over shared directory."
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1581104089,,61,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,tests/conftest.py,,Can we drop the `return` statements in the fixtures that do not return anything?,"+    dj.config[""custom""][
+        ""database.prefix""
+    ] = f""u_{dj.config['database.user']}_testsuite_""
+    return","--- 

+++ 

@@ -58,7 +58,6 @@

     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_""
-    return
 
 
 def load_pipeline():
@@ -137,8 +136,6 @@

         }
     )
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def epoch_chunk_ingestion(test_params, pipeline, experiment_creation):
@@ -154,8 +151,6 @@

 
     acquisition.Chunk.ingest_chunks(experiment_name=test_params[""experiment_name""])
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def experimentlog_ingestion(pipeline):
@@ -166,20 +161,14 @@

     acquisition.SubjectEnterExit.populate(**_populate_settings)
     acquisition.SubjectWeight.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_qc_ingestion(pipeline, epoch_chunk_ingestion):
     qc = pipeline[""qc""]
     qc.CameraQC.populate(**_populate_settings)
 
-    return
-
 
 @pytest.fixture(scope=""session"")
 def camera_tracking_ingestion(pipeline, camera_qc_ingestion):
     tracking = pipeline[""tracking""]
     tracking.CameraTracking.populate(**_populate_settings)
-
-    return"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1581121186,,51,5276cc1a4e96add9c54da83ecccdebd38d487001,25cc4b724d0451bf1171a70791bc3a3ed7d8d698,aeon/dj_pipeline/populate/worker.py,,"```suggestion
```","+    """"""Extract and insert complete visits for experiments specified in AutomatedExperimentIngestion.""""""
+    experiment_names = AutomatedExperimentIngestion.fetch(""experiment_name"")
+    # analysis.ingest_environment_visits(experiment_names)
+    pass","--- 

+++ 

@@ -64,7 +64,7 @@

 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
 # acquisition_worker(ingest_environment_visits)
-# acquisition_worker(block_analysis.BlockDetection)
+acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync
 pyrat_worker = DataJointWorker(
@@ -108,7 +108,8 @@

 
 analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
 analysis_worker(block_analysis.BlockPlots, max_calls=6)
-
+analysis_worker(block_analysis.BlockSubjectAnalysis, max_calls=6)
+analysis_worker(block_analysis.BlockSubjectPlots, max_calls=6)
 
 def get_workflow_operation_overview():
     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1609815493,,25,275ffeb2971e2e36f84b6ac3cffb46911772ceb9,b185949d02319db80a4db7af6d853351edf0b0e4,lib/dynamic_links/configuration.rb,,nice one ,"     #       config.redis_counter_config = RedisConfig.new # see RedisConfig documentation for more details
     #       # if you use Redis
-    #       config.cache_store = ActiveSupport::Cache::RedisStore.new('redis://localhost:6379/0/cache')
+    #       config.cache_store = ActiveSupport::Cache::RedisCacheStore.new(url: 'redis://localhost:6379/0/cache')",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1822847464,,43,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/__init__.py,,Defaults to `True`? ,"+    Args:
+        query (datajoint.Query): A query object containing data from a Stream table
+        drop_pk (bool, optional): Drop primary key columns. Defaults to True.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to False.","--- 

+++ 

@@ -40,7 +40,7 @@

     Args:
         query (datajoint.Query): A query object containing data from a Stream table
         drop_pk (bool, optional): Drop primary key columns. Defaults to True.
-        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to False.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to True.
             (this is important as timestamps in mysql is only accurate to microseconds)
     """"""
     df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823047548,,204,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/analysis/block_analysis.py,,"I think we actually want this to be 50 hz, not 10 hz","         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
         # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        final_encoder_hz = 10","--- 

+++ 

@@ -200,8 +200,8 @@

                 use_blob_position = True
 
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_hz = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
         freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
@@ -251,6 +251,7 @@

             if encoder_df.empty:
                 encoder_df[""distance_travelled""] = 0
             else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
                 encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
                 encoder_df = encoder_df.resample(f""{freq}ms"").first()
 
@@ -327,7 +328,7 @@

                 )
                 pos_df = fetch_stream(pos_query)[block_start:block_end]
                 pos_df[""likelihood""] = np.nan
-                # keep only rows with area between 0 and 1000
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
                 pos_df = pos_df[(pos_df.area > 0) & (pos_df.area < 1000)]
             else:
                 pos_query = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823050082,,254,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/analysis/block_analysis.py,,"maybe add a comment saying something like -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder value?","+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)","--- 

+++ 

@@ -200,8 +200,8 @@

                 use_blob_position = True
 
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_hz = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
         freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
@@ -251,6 +251,7 @@

             if encoder_df.empty:
                 encoder_df[""distance_travelled""] = 0
             else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
                 encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
                 encoder_df = encoder_df.resample(f""{freq}ms"").first()
 
@@ -327,7 +328,7 @@

                 )
                 pos_df = fetch_stream(pos_query)[block_start:block_end]
                 pos_df[""likelihood""] = np.nan
-                # keep only rows with area between 0 and 1000
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
                 pos_df = pos_df[(pos_df.area > 0) & (pos_df.area < 1000)]
             else:
                 pos_query = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823053362,,271,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/analysis/block_analysis.py,,is it actually an issue if patch rate is inf? Does it cause some downstream issue? We do this as default when no env is loaded.,"+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate","--- 

+++ 

@@ -200,8 +200,8 @@

                 use_blob_position = True
 
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_hz = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
         freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
@@ -251,6 +251,7 @@

             if encoder_df.empty:
                 encoder_df[""distance_travelled""] = 0
             else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
                 encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
                 encoder_df = encoder_df.resample(f""{freq}ms"").first()
 
@@ -327,7 +328,7 @@

                 )
                 pos_df = fetch_stream(pos_query)[block_start:block_end]
                 pos_df[""likelihood""] = np.nan
-                # keep only rows with area between 0 and 1000
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
                 pos_df = pos_df[(pos_df.area > 0) & (pos_df.area < 1000)]
             else:
                 pos_query = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823057400,,302,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/analysis/block_analysis.py,,"sometimes we use other, non ""Test"" subjects as test subjects. Maybe the check should be, if the subject does not begin with 'baa' (can str.lower to check for regardless of case) ?","             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]","--- 

+++ 

@@ -200,8 +200,8 @@

                 use_blob_position = True
 
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_hz = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
         freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
@@ -251,6 +251,7 @@

             if encoder_df.empty:
                 encoder_df[""distance_travelled""] = 0
             else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
                 encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
                 encoder_df = encoder_df.resample(f""{freq}ms"").first()
 
@@ -327,7 +328,7 @@

                 )
                 pos_df = fetch_stream(pos_query)[block_start:block_end]
                 pos_df[""likelihood""] = np.nan
-                # keep only rows with area between 0 and 1000
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
                 pos_df = pos_df[(pos_df.area > 0) & (pos_df.area < 1000)]
             else:
                 pos_query = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823059889,,330,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/dj_pipeline/analysis/block_analysis.py,,"is this because areas of > 1000 is likely an experimenter, or some other artifact? Maybe specify that in the comment?","+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000","--- 

+++ 

@@ -200,8 +200,8 @@

                 use_blob_position = True
 
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_hz = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
         freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
@@ -251,6 +251,7 @@

             if encoder_df.empty:
                 encoder_df[""distance_travelled""] = 0
             else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
                 encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
                 encoder_df = encoder_df.resample(f""{freq}ms"").first()
 
@@ -327,7 +328,7 @@

                 )
                 pos_df = fetch_stream(pos_query)[block_start:block_end]
                 pos_df[""likelihood""] = np.nan
-                # keep only rows with area between 0 and 1000
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
                 pos_df = pos_df[(pos_df.area > 0) & (pos_df.area < 1000)]
             else:
                 pos_query = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823086530,,82,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/io/reader.py,,This I guess will be fixed in the next PR targeting a new Encoder reader in ingestion_schemas.py ?,"+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1823096837,,51,907808593960317fa64d5e5eda43986de1c2c058,c2e90b6890583ec01418c5c5e2b88ed7fa0cdb58,aeon/schema/social_02.py,,made add a comment that this is necessary due to changing registers for the pose streams for social02 in particular? And that 03 corresponds to the fact that this is because this pattern is what we're going with for social03 and moving forward? Or call this class something else?,"         super().__init__(_reader.Pose(f""{path}_test-node1*""))
 
 
+class Pose03(Stream):",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1868322747,,42,a8528bce77a1d556960d22d83496d78c1813fc07,bf4e56c57d2085797cbc74984a05cbd89e6e7c03,aeon/dj_pipeline/docs/PIPELINE_LOCAL_DEPLOYMENT.md,,The download link will be updated once we have the new dataset released for the paper,"+
+## Download the data
+
+The released data for Project Aeon can be downloaded from the data repository [here](https://zenodo.org/records/13881885)","--- 

+++ 

@@ -46,7 +46,7 @@

 
 ### Installation Instructions
 
-In order to run the pipeline, follow the instruction to install this codebase in the ""Local set-up"" section
+In order to run the pipeline, follow the instruction to install this codebase in the [Local set-up](../../../README.md#local-set-up) section
 
 ### Configuration Instructions
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1869244679,,49,a8528bce77a1d556960d22d83496d78c1813fc07,bf4e56c57d2085797cbc74984a05cbd89e6e7c03,aeon/dj_pipeline/docs/PIPELINE_LOCAL_DEPLOYMENT.md,,"We can add directly the link here for reference. Something similar to this: 

`[""Local set-up"" section](../../README.md#Local-set-up)`","+
+### Installation Instructions
+
+In order to run the pipeline, follow the instruction to install this codebase in the ""Local set-up"" section","--- 

+++ 

@@ -46,7 +46,7 @@

 
 ### Installation Instructions
 
-In order to run the pipeline, follow the instruction to install this codebase in the ""Local set-up"" section
+In order to run the pipeline, follow the instruction to install this codebase in the [Local set-up](../../../README.md#local-set-up) section
 
 ### Configuration Instructions
 "
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1665791828,,15,76acfb1a8d3d58a0ad32483f607beb8c74c187f8,8cd8984b1205ca62cbd111fe6cd0ee8395c6f814,Gemfile,,"it should be in the gemspec file
because it's a dependency of this gem"," 
 gem 'sprockets-rails'
 
+gem 'ahoy_matey'",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1665798860,,8,76acfb1a8d3d58a0ad32483f607beb8c74c187f8,8cd8984b1205ca62cbd111fe6cd0ee8395c6f814,app/controllers/dynamic_links/redirects_controller.rb,,"need to track more metrics, maybe like this

```
ahoy.track ""ShortenedUrl Visit"", {
  shortened_url: short_url,
  user_agent: request.user_agent,
  referrer: request.referrer,
  ip: request.ip,
  device_type: ahoy.request.device_type,
  os: ahoy.request.os,
  browser: ahoy.request.browser,
  utm_source: params[:utm_source],
  utm_medium: params[:utm_medium],
  utm_campaign: params[:utm_campaign],
  landing_page: request.original_url,
}
```","       link = ShortenedUrl.find_by(short_url: short_url)
 
       if link
+        ahoy.track ""ShortenedUrl Visit"", {","--- 

+++ 

@@ -1,20 +1,20 @@

 module DynamicLinks
   class RedirectsController < ApplicationController
+
+    # Rails will return a 404 if the record is not found
     def show
       short_url = params[:short_url]
-      link = ShortenedUrl.find_by(short_url: short_url)
+      link = ShortenedUrl.find_by!(short_url: short_url)
 
-      if link
-        ahoy.track ""ShortenedUrl Visit"", {
-          shortened_url: short_url,
-          user_agent: request.user_agent,
-          referrer: request.referrer
-        }
+      raise ActiveRecord::RecordNotFound if link.expired?
 
-        redirect_to link.url, status: :found, allow_other_host: true
-      else
-        raise ActiveRecord::RecordNotFound
-      end
+      ahoy.track ""ShortenedUrl Visit"", {
+        shortened_url: short_url,
+        user_agent: request.user_agent,
+        referrer: request.referrer
+      }
+
+      redirect_to link.url, status: :found, allow_other_host: true
     end
   end
 end"
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1665801004,,7,76acfb1a8d3d58a0ad32483f607beb8c74c187f8,8cd8984b1205ca62cbd111fe6cd0ee8395c6f814,app/models/ahoy/event.rb,,"why there is a relation to `user`?
I think we don't have `user` model","+  self.table_name = ""ahoy_events""
+
+  belongs_to :visit
+  belongs_to :user, optional: true",
https://api.github.com/repos/saiqulhaq/dynamic_links/pulls/comments/1665802061,,11,76acfb1a8d3d58a0ad32483f607beb8c74c187f8,8cd8984b1205ca62cbd111fe6cd0ee8395c6f814,db/migrate/20240529173554_create_ahoy_visits_and_events.rb,,"there is no `user` object in this gem
![image](https://github.com/saiqulhaq/dynamic_links/assets/1275215/185963ae-b4ba-4afa-bd93-248cbc651255)
","+      # simply remove any you don't want
+
+      # user
+      t.references :user",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1868119732,,58,60f33dd71ff0bca2a59d3d64ead36afd8ba16b4f,6ed933aef30449bd8d50ead67d82a2e7a5be8927,aeon/dj_pipeline/README.md,,"This is good.
Please expand more on the 2 part tables
- `PoseIdentity` - identified Subject (i.e. Identity) and stores the name of the body part used as ""anchor_part""
- `Part` - inferred x,y position over time for all body parts from SLEAP model ","++ `qc.CameraQC` - quality control procedure applied to each `ExperimentCamera` (e.g. missing frame, etc.)
 
-10. `WheelState` - wheel states (threshold, d1, delta) associated with a given `ExperimentFoodPatch`
++ `tracking.SLEAPTracking` - position tracking for object(s), from a particular `VideoSource` per chunk","--- 

+++ 

@@ -38,7 +38,7 @@

 
 #### Experiment and data acquisition
 
-+ `Experiment` - the `aquisition.Experiment` table stores meta information about the experiments
++ `Experiment` - The `aquisition.Experiment` table stores meta information about the experiments
 done in Project Aeon, with secondary information such as the lab/room the experiment is carried out,
 which animals participating, the directory storing the raw data, etc.
 
@@ -46,22 +46,24 @@

 The `aquisition.Epoch` table records all acquisition epochs and their associated configuration for
 any particular experiment (in the above `aquisition.Experiment` table).
 
-+ `Chunk` - the raw data are acquired by Bonsai and stored as
++ `Chunk` - The raw data are acquired by Bonsai and stored as
 a collection of files every one hour - we call this one-hour a time chunk.
 The `aquisition.Chunk` table records all time chunks and their associated raw data files for
 any particular experiment (in the above `aquisition.Experiment` table). A chunk must belong to one epoch.
 
 #### Position data
 
-+ `qc.CameraQC` - quality control procedure applied to each `ExperimentCamera` (e.g. missing frame, etc.)
++ `qc.CameraQC` - A quality control procedure applied to each `ExperimentCamera` (e.g. missing frame, etc.)
 
-+ `tracking.SLEAPTracking` - position tracking for object(s), from a particular `VideoSource` per chunk
++ `tracking.SLEAPTracking` - Position tracking of object(s) from a particular `VideoSource` per chunk. Key tables include:
+    - `PoseIdentity` - Identifies the Subject (i.e. Identity) and stores the name of the body part used as ""anchor_part"".
+    - `Part` - Contains the inferred x,y positions over time for all body parts, as derived from the SLEAP model.
 
 #### Standard analyses
 
-+ `Visit` - a `Visit` is defined as a period of time during which a particular animal remains at a specific place.
++ `Visit` - A `Visit` is defined as a period of time during which a particular animal remains at a specific place.
 
-+ `Block` - a `Block` refers to a specific period of time, typically lasting around 3 hours, during which the reward rate for each patch is predefined to facilitate certain animal behaviors.
++ `Block` - A `Block` refers to a specific period of time, typically lasting around 3 hours, during which the reward rate for each patch is predefined to facilitate certain animal behaviors.
 
 + `BlockAnalysis` - A higher-level aggregation of events and metrics occurring within a defined block of time during an experiment.This analysis computes patch-related and subject-related metrics separately, without combining or cross-correlating data between them. It provides an overview of behavior and environmental interactions at a broader level, integrating data from multiple subjects and patches.
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1766872761,,47,6bacc43e93826f9a3ffa8e1c8c9189abc0bf4c14,a889dba13c07c7eb6142a8265b8d8de8c60cef9c,aeon/dj_pipeline/populate/worker.py,,Remove also https://github.com/SainsburyWellcomeCentre/aeon_mecha/blob/6bacc43e93826f9a3ffa8e1c8c9189abc0bf4c14/aeon/dj_pipeline/populate/worker.py#L59,"         acquisition.Chunk.ingest_chunks(experiment_name)
 
 
-def ingest_environment_visits():","--- 

+++ 

@@ -56,7 +56,6 @@

 acquisition_worker(ingest_epochs_chunks)
 acquisition_worker(acquisition.EpochConfig)
 acquisition_worker(acquisition.Environment)
-# acquisition_worker(ingest_environment_visits)
 acquisition_worker(block_analysis.BlockDetection)
 
 # configure a worker to handle pyrat sync"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820592131,,32,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/analysis/block_plotting.py,," E741: ambiguous variable name -> Renamed the variable from `l` to `ll` in `gen_hex_grad` in `aeon/analysis/block_plotting.py`.
","     """"""Generates an array of hex color values based on a gradient defined by unit-normalized values.""""""
     # Convert hex to rgb to hls
-    h, l, s = rgb_to_hls(*[int(hex_col.lstrip(""#"")[i : i + 2], 16) / 255 for i in (0, 2, 4)])  # noqa: E741
+    h, ll, s = rgb_to_hls(","--- 

+++ 

@@ -26,19 +26,19 @@

 patch_markers_linestyles = [""solid"", ""dash"", ""dot"", ""dashdot"", ""longdashdot""]
 
 
-def gen_hex_grad(hex_col, vals, min_l=0.3):
+def gen_hex_grad(hex_col, vals, min_lightness=0.3):
     """"""Generates an array of hex color values based on a gradient defined by unit-normalized values.""""""
     # Convert hex to rgb to hls
-    h, ll, s = rgb_to_hls(
+    hue, lightness, saturation = rgb_to_hls(
         *[int(hex_col.lstrip(""#"")[i : i + 2], 16) / 255 for i in (0, 2, 4)]
     )
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        cur_l = (ll * val) + (
-            min_l * (1 - val)
+        cur_lightness = (lightness * val) + (
+            min_lightness * (1 - val)
         )  # get cur lightness relative to `hex_col`
-        cur_l = max(min(cur_l, ll), min_l)  # set min, max bounds
-        cur_rgb_col = hls_to_rgb(h, cur_l, s)  # convert to rgb
+        cur_lightness = max(min(cur_lightness, lightness), min_lightness)  # set min, max bounds
+        cur_rgb_col = hls_to_rgb(hue, cur_lightness, saturation)  # convert to rgb
         cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
             *tuple(int(c * 255) for c in cur_rgb_col)
         )  # convert to hex
@@ -63,13 +63,15 @@

 
 
 def gen_patch_style_dict(patch_names):
-    """"""Based on a list of patches, generates a dictionary of the following items.
+    """"""Generates a dictionary of patch styles given a list of patch_names.
 
-    - patch_colors_dict: patch name to color
-    - patch_markers_dict: patch name to marker
-    - patch_symbols_dict: patch name to symbol
-    - patch_linestyles_dict: patch name to linestyle
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
 
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
     """"""
     return {
         ""colors"": dict(zip(patch_names, patch_colors, strict=False)),"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820593406,,62,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/analysis/block_plotting.py,,Refactored to dict," def gen_subject_colors_dict(subject_names):
     """"""Generates a dictionary of subject colors based on a list of subjects.""""""
-    return {s: c for s, c in zip(subject_names, subject_colors)}
+    return dict(zip(subject_names, subject_colors, strict=False))","--- 

+++ 

@@ -26,19 +26,19 @@

 patch_markers_linestyles = [""solid"", ""dash"", ""dot"", ""dashdot"", ""longdashdot""]
 
 
-def gen_hex_grad(hex_col, vals, min_l=0.3):
+def gen_hex_grad(hex_col, vals, min_lightness=0.3):
     """"""Generates an array of hex color values based on a gradient defined by unit-normalized values.""""""
     # Convert hex to rgb to hls
-    h, ll, s = rgb_to_hls(
+    hue, lightness, saturation = rgb_to_hls(
         *[int(hex_col.lstrip(""#"")[i : i + 2], 16) / 255 for i in (0, 2, 4)]
     )
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        cur_l = (ll * val) + (
-            min_l * (1 - val)
+        cur_lightness = (lightness * val) + (
+            min_lightness * (1 - val)
         )  # get cur lightness relative to `hex_col`
-        cur_l = max(min(cur_l, ll), min_l)  # set min, max bounds
-        cur_rgb_col = hls_to_rgb(h, cur_l, s)  # convert to rgb
+        cur_lightness = max(min(cur_lightness, lightness), min_lightness)  # set min, max bounds
+        cur_rgb_col = hls_to_rgb(hue, cur_lightness, saturation)  # convert to rgb
         cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
             *tuple(int(c * 255) for c in cur_rgb_col)
         )  # convert to hex
@@ -63,13 +63,15 @@

 
 
 def gen_patch_style_dict(patch_names):
-    """"""Based on a list of patches, generates a dictionary of the following items.
+    """"""Generates a dictionary of patch styles given a list of patch_names.
 
-    - patch_colors_dict: patch name to color
-    - patch_markers_dict: patch name to marker
-    - patch_symbols_dict: patch name to symbol
-    - patch_linestyles_dict: patch name to linestyle
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
 
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
     """"""
     return {
         ""colors"": dict(zip(patch_names, patch_colors, strict=False)),"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820593778,,75,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/analysis/block_plotting.py,,Refactored to dict.,"-        ""markers"": {p: m for p, m in zip(patch_names, patch_markers)},
-        ""symbols"": {p: s for p, s in zip(patch_names, patch_markers_symbols)},
-        ""linestyles"": {p: ls for p, ls in zip(patch_names, patch_markers_linestyles)},
+        ""colors"": dict(zip(patch_names, patch_colors, strict=False)),","--- 

+++ 

@@ -26,19 +26,19 @@

 patch_markers_linestyles = [""solid"", ""dash"", ""dot"", ""dashdot"", ""longdashdot""]
 
 
-def gen_hex_grad(hex_col, vals, min_l=0.3):
+def gen_hex_grad(hex_col, vals, min_lightness=0.3):
     """"""Generates an array of hex color values based on a gradient defined by unit-normalized values.""""""
     # Convert hex to rgb to hls
-    h, ll, s = rgb_to_hls(
+    hue, lightness, saturation = rgb_to_hls(
         *[int(hex_col.lstrip(""#"")[i : i + 2], 16) / 255 for i in (0, 2, 4)]
     )
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        cur_l = (ll * val) + (
-            min_l * (1 - val)
+        cur_lightness = (lightness * val) + (
+            min_lightness * (1 - val)
         )  # get cur lightness relative to `hex_col`
-        cur_l = max(min(cur_l, ll), min_l)  # set min, max bounds
-        cur_rgb_col = hls_to_rgb(h, cur_l, s)  # convert to rgb
+        cur_lightness = max(min(cur_lightness, lightness), min_lightness)  # set min, max bounds
+        cur_rgb_col = hls_to_rgb(hue, cur_lightness, saturation)  # convert to rgb
         cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
             *tuple(int(c * 255) for c in cur_rgb_col)
         )  # convert to hex
@@ -63,13 +63,15 @@

 
 
 def gen_patch_style_dict(patch_names):
-    """"""Based on a list of patches, generates a dictionary of the following items.
+    """"""Generates a dictionary of patch styles given a list of patch_names.
 
-    - patch_colors_dict: patch name to color
-    - patch_markers_dict: patch name to marker
-    - patch_symbols_dict: patch name to symbol
-    - patch_linestyles_dict: patch name to linestyle
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
 
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
     """"""
     return {
         ""colors"": dict(zip(patch_names, patch_colors, strict=False)),"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820595194,,30,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/__init__.py,,Security: Replaced `hashlib.md5` with `hashlib.sha256` due to known vulnerabilities in the MD5 hashing algorithm. ," def dict_to_uuid(key) -> uuid.UUID:
     """"""Given a dictionary `key`, returns a hash string as UUID.""""""
-    hashed = hashlib.md5()
+    hashed = hashlib.sha256()","--- 

+++ 

@@ -1,10 +1,13 @@

 """"""DataJoint pipeline for Aeon.""""""
 
 import hashlib
+import logging
 import os
 import uuid
 
 import datajoint as dj
+
+logger = dj.logger
 
 _default_database_prefix = os.getenv(""DJ_DB_PREFIX"") or ""aeon_""
 _default_repository_config = {""ceph_aeon"": ""/ceph/aeon""}
@@ -15,9 +18,7 @@

 
 db_prefix = dj.config[""custom""].get(""database.prefix"", _default_database_prefix)
 
-repository_config = dj.config[""custom""].get(
-    ""repository_config"", _default_repository_config
-)
+repository_config = dj.config[""custom""].get(""repository_config"", _default_repository_config)
 
 
 def get_schema_name(name) -> str:
@@ -27,24 +28,28 @@

 
 def dict_to_uuid(key) -> uuid.UUID:
     """"""Given a dictionary `key`, returns a hash string as UUID.""""""
-    hashed = hashlib.sha256()
+    hashed = hashlib.md5()
     for k, v in sorted(key.items()):
         hashed.update(str(k).encode())
         hashed.update(str(v).encode())
-    return uuid.UUID(hex=hashed.hexdigest()[:32])
+    return uuid.UUID(hex=hashed.hexdigest())
 
 
-def fetch_stream(query, drop_pk=True):
+def fetch_stream(query, drop_pk=True, round_microseconds=True):
     """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
 
     Provided a query containing data from a Stream table,
     fetch and aggregate the data into one DataFrame indexed by ""time""
+
+    Args:
+        query (datajoint.Query): A query object containing data from a Stream table
+        drop_pk (bool, optional): Drop primary key columns. Defaults to True.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to True.
+            (this is important as timestamps in mysql is only accurate to microseconds)
     """"""
     df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
     cols2explode = [
-        c
-        for c in query.heading.secondary_attributes
-        if query.heading.attributes[c].type == ""longblob""
+        c for c in query.heading.secondary_attributes if query.heading.attributes[c].type == ""longblob""
     ]
     df = df.explode(column=cols2explode)
     cols2drop = [""sample_count""] + (query.primary_key if drop_pk else [])
@@ -56,8 +61,12 @@

         convert_string=False,
         convert_integer=False,
         convert_boolean=False,
-        convert_floating=False,
+        convert_floating=False
     )
+    if not df.empty and round_microseconds:
+        logging.warning(""Rounding timestamps to microseconds is now enabled by default.""
+                        "" To disable, set round_microseconds=False."")
+        df.index = df.index.round(""us"")
     return df
 
 
@@ -68,5 +77,5 @@

         from .utils import streams_maker
 
         streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
-    except ImportError:
-        pass
+    except Exception as e:
+        logger.debug(f""Could not import streams module: {e}"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820596662,,71,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/__init__.py,,bare-except (E722) fixed," 
         streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
-    except:
+    except ImportError:","--- 

+++ 

@@ -1,10 +1,13 @@

 """"""DataJoint pipeline for Aeon.""""""
 
 import hashlib
+import logging
 import os
 import uuid
 
 import datajoint as dj
+
+logger = dj.logger
 
 _default_database_prefix = os.getenv(""DJ_DB_PREFIX"") or ""aeon_""
 _default_repository_config = {""ceph_aeon"": ""/ceph/aeon""}
@@ -15,9 +18,7 @@

 
 db_prefix = dj.config[""custom""].get(""database.prefix"", _default_database_prefix)
 
-repository_config = dj.config[""custom""].get(
-    ""repository_config"", _default_repository_config
-)
+repository_config = dj.config[""custom""].get(""repository_config"", _default_repository_config)
 
 
 def get_schema_name(name) -> str:
@@ -27,24 +28,28 @@

 
 def dict_to_uuid(key) -> uuid.UUID:
     """"""Given a dictionary `key`, returns a hash string as UUID.""""""
-    hashed = hashlib.sha256()
+    hashed = hashlib.md5()
     for k, v in sorted(key.items()):
         hashed.update(str(k).encode())
         hashed.update(str(v).encode())
-    return uuid.UUID(hex=hashed.hexdigest()[:32])
+    return uuid.UUID(hex=hashed.hexdigest())
 
 
-def fetch_stream(query, drop_pk=True):
+def fetch_stream(query, drop_pk=True, round_microseconds=True):
     """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
 
     Provided a query containing data from a Stream table,
     fetch and aggregate the data into one DataFrame indexed by ""time""
+
+    Args:
+        query (datajoint.Query): A query object containing data from a Stream table
+        drop_pk (bool, optional): Drop primary key columns. Defaults to True.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to True.
+            (this is important as timestamps in mysql is only accurate to microseconds)
     """"""
     df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
     cols2explode = [
-        c
-        for c in query.heading.secondary_attributes
-        if query.heading.attributes[c].type == ""longblob""
+        c for c in query.heading.secondary_attributes if query.heading.attributes[c].type == ""longblob""
     ]
     df = df.explode(column=cols2explode)
     cols2drop = [""sample_count""] + (query.primary_key if drop_pk else [])
@@ -56,8 +61,12 @@

         convert_string=False,
         convert_integer=False,
         convert_boolean=False,
-        convert_floating=False,
+        convert_floating=False
     )
+    if not df.empty and round_microseconds:
+        logging.warning(""Rounding timestamps to microseconds is now enabled by default.""
+                        "" To disable, set round_microseconds=False."")
+        df.index = df.index.round(""us"")
     return df
 
 
@@ -68,5 +77,5 @@

         from .utils import streams_maker
 
         streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
-    except ImportError:
-        pass
+    except Exception as e:
+        logger.debug(f""Could not import streams module: {e}"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820597621,,149,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/acquisition.py,,Replaced assertions with exceptions to ensure expected behavior in all scenarios.,"         dir_path = pathlib.Path(dir_path)
         if dir_path.exists():
-            assert dir_path.is_relative_to(paths.get_repository_path(repo_name))
+            if not dir_path.is_relative_to(paths.get_repository_path(repo_name)):","--- 

+++ 

@@ -8,14 +8,16 @@

 import datajoint as dj
 import pandas as pd
 
-from aeon.dj_pipeline import get_schema_name
+from aeon.dj_pipeline import get_schema_name, lab, subject
 from aeon.dj_pipeline.utils import paths
 from aeon.io import api as io_api
 from aeon.io import reader as io_reader
-from aeon.schema import schemas as aeon_schemas
+from aeon.schema import ingestion_schemas as aeon_schemas
+
+schema = dj.schema(get_schema_name(""acquisition""))
 
 logger = dj.logger
-schema = dj.schema(get_schema_name(""acquisition""))
+
 
 # ------------------- Some Constants --------------------------
 
@@ -147,7 +149,7 @@

         dir_path = pathlib.Path(dir_path)
         if dir_path.exists():
             if not dir_path.is_relative_to(paths.get_repository_path(repo_name)):
-                raise ValueError(f""f{dir_path} is not relative to the repository path."")
+                raise ValueError(f""{dir_path} is not relative to the repository path."")
             data_directory = dir_path
         else:
             data_directory = paths.get_repository_path(repo_name) / dir_path
@@ -165,11 +167,26 @@

         return [
             d
             for dir_type in directory_types
-            if (
-                d := cls.get_data_directory(experiment_key, dir_type, as_posix=as_posix)
-            )
-            is not None
+            if (d := cls.get_data_directory(experiment_key, dir_type, as_posix=as_posix)) is not None
         ]
+
+
+@schema
+class ExperimentTimeline(dj.Manual):
+    definition = """"""  # different parts of an experiment timeline
+    -> Experiment
+    name: varchar(32)  # e.g. presocial, social, postsocial
+    ---
+    start: datetime
+    end: datetime
+    note='': varchar(1000)
+    """"""
+
+    class Subject(dj.Part):
+        definition = """"""  # the subjects participating in this part of the experiment timeline
+        -> master
+        -> Experiment.Subject
+        """"""
 
 
 # ------------------- ACQUISITION EPOCH --------------------
@@ -196,9 +213,7 @@

         for i, (_, chunk) in enumerate(all_chunks.iterrows()):
             chunk_rep_file = pathlib.Path(chunk.path)
             epoch_dir = pathlib.Path(chunk_rep_file.as_posix().split(device_name)[0])
-            epoch_start = datetime.datetime.strptime(
-                epoch_dir.name, ""%Y-%m-%dT%H-%M-%S""
-            )
+            epoch_start = datetime.datetime.strptime(epoch_dir.name, ""%Y-%m-%dT%H-%M-%S"")
             # --- insert to Epoch ---
             epoch_key = {""experiment_name"": experiment_name, ""epoch_start"": epoch_start}
 
@@ -217,15 +232,11 @@

             if i > 0:
                 previous_chunk = all_chunks.iloc[i - 1]
                 previous_chunk_path = pathlib.Path(previous_chunk.path)
-                previous_epoch_dir = pathlib.Path(
-                    previous_chunk_path.as_posix().split(device_name)[0]
-                )
+                previous_epoch_dir = pathlib.Path(previous_chunk_path.as_posix().split(device_name)[0])
                 previous_epoch_start = datetime.datetime.strptime(
                     previous_epoch_dir.name, ""%Y-%m-%dT%H-%M-%S""
                 )
-                previous_chunk_end = previous_chunk.name + datetime.timedelta(
-                    hours=io_api.CHUNK_DURATION
-                )
+                previous_chunk_end = previous_chunk.name + datetime.timedelta(hours=io_api.CHUNK_DURATION)
                 previous_epoch_end = min(previous_chunk_end, epoch_start)
                 previous_epoch_key = {
                     ""experiment_name"": experiment_name,
@@ -254,9 +265,7 @@

                         {
                             **previous_epoch_key,
                             ""epoch_end"": previous_epoch_end,
-                            ""epoch_duration"": (
-                                previous_epoch_end - previous_epoch_start
-                            ).total_seconds()
+                            ""epoch_duration"": (previous_epoch_end - previous_epoch_start).total_seconds()
                             / 3600,
                         }
                     )
@@ -297,7 +306,7 @@

         -> master
         ---
         bonsai_workflow: varchar(36)
-        commit: varchar(64) # e.g., git commit hash of aeon_experiment used to generate this epoch
+        commit: varchar(64) # e.g. git commit hash of aeon_experiment used to generate this epoch
         source='': varchar(16)  # e.g. aeon_experiment or aeon_acquisition (or others)
         metadata: longblob
         metadata_file_path: varchar(255)  # path of the file, relative to the experiment repository
@@ -329,23 +338,17 @@

         experiment_name = key[""experiment_name""]
         devices_schema = getattr(
             aeon_schemas,
-            (Experiment.DevicesSchema & {""experiment_name"": experiment_name}).fetch1(
-                ""devices_schema_name""
-            ),
+            (Experiment.DevicesSchema & {""experiment_name"": experiment_name}).fetch1(""devices_schema_name""),
         )
 
         dir_type, epoch_dir = (Epoch & key).fetch1(""directory_type"", ""epoch_dir"")
         data_dir = Experiment.get_data_directory(key, dir_type)
         metadata_yml_filepath = data_dir / epoch_dir / ""Metadata.yml""
 
-        epoch_config = extract_epoch_config(
-            experiment_name, devices_schema, metadata_yml_filepath
-        )
+        epoch_config = extract_epoch_config(experiment_name, devices_schema, metadata_yml_filepath)
         epoch_config = {
             **epoch_config,
-            ""metadata_file_path"": metadata_yml_filepath.relative_to(
-                data_dir
-            ).as_posix(),
+            ""metadata_file_path"": metadata_yml_filepath.relative_to(data_dir).as_posix(),
         }
 
         # Insert new entries for streams.DeviceType, streams.Device.
@@ -356,20 +359,15 @@

         # Define and instantiate new devices/stream tables under `streams` schema
         streams_maker.main()
         # Insert devices' installation/removal/settings
-        epoch_device_types = ingest_epoch_metadata(
-            experiment_name, devices_schema, metadata_yml_filepath
-        )
+        epoch_device_types = ingest_epoch_metadata(experiment_name, devices_schema, metadata_yml_filepath)
 
         self.insert1(key)
         self.Meta.insert1(epoch_config)
-        self.DeviceType.insert(
-            key | {""device_type"": n} for n in epoch_device_types or {}
-        )
+        self.DeviceType.insert(key | {""device_type"": n} for n in epoch_device_types or {})
         with metadata_yml_filepath.open(""r"") as f:
             metadata = json.load(f)
         self.ActiveRegion.insert(
-            {**key, ""region_name"": k, ""region_data"": v}
-            for k, v in metadata[""ActiveRegion""].items()
+            {**key, ""region_name"": k, ""region_data"": v} for k, v in metadata[""ActiveRegion""].items()
         )
 
 
@@ -408,9 +406,7 @@

         for _, chunk in all_chunks.iterrows():
             chunk_rep_file = pathlib.Path(chunk.path)
             epoch_dir = pathlib.Path(chunk_rep_file.as_posix().split(device_name)[0])
-            epoch_start = datetime.datetime.strptime(
-                epoch_dir.name, ""%Y-%m-%dT%H-%M-%S""
-            )
+            epoch_start = datetime.datetime.strptime(epoch_dir.name, ""%Y-%m-%dT%H-%M-%S"")
 
             epoch_key = {""experiment_name"": experiment_name, ""epoch_start"": epoch_start}
             if not (Epoch & epoch_key):
@@ -418,9 +414,7 @@

                 continue
 
             chunk_start = chunk.name
-            chunk_start = max(
-                chunk_start, epoch_start
-            )  # first chunk of the epoch starts at epoch_start
+            chunk_start = max(chunk_start, epoch_start)  # first chunk of the epoch starts at epoch_start
             chunk_end = chunk_start + datetime.timedelta(hours=io_api.CHUNK_DURATION)
 
             if EpochEnd & epoch_key:
@@ -440,12 +434,8 @@

             )
 
             chunk_starts.append(chunk_key[""chunk_start""])
-            chunk_list.append(
-                {**chunk_key, **directory, ""chunk_end"": chunk_end, **epoch_key}
-            )
-            file_name_list.append(
-                chunk_rep_file.name
-            )  # handle duplicated files in different folders
+            chunk_list.append({**chunk_key, **directory, ""chunk_end"": chunk_end, **epoch_key})
+            file_name_list.append(chunk_rep_file.name)  # handle duplicated files in different folders
 
             # -- files --
             file_datetime_str = chunk_rep_file.stem.replace(f""{device_name}_"", """")
@@ -562,9 +552,9 @@

         data_dirs = Experiment.get_data_directories(key)
         devices_schema = getattr(
             aeon_schemas,
-            (
-                Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}
-            ).fetch1(""devices_schema_name""),
+            (Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
         )
         device = devices_schema.Environment
 
@@ -624,14 +614,12 @@

         data_dirs = Experiment.get_data_directories(key)
         devices_schema = getattr(
             aeon_schemas,
-            (
-                Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}
-            ).fetch1(""devices_schema_name""),
+            (Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
         )
         device = devices_schema.Environment
-        stream_reader = (
-            device.EnvironmentActiveConfiguration
-        )  # expecting columns: time, name, value
+        stream_reader = device.EnvironmentActiveConfiguration  # expecting columns: time, name, value
         stream_data = io_api.load(
             root=data_dirs,
             reader=stream_reader,
@@ -655,18 +643,14 @@

     directory_types = [""quality-control"", ""raw""]
     raw_data_dirs = {
         dir_type: Experiment.get_data_directory(
-            experiment_key={""experiment_name"": experiment_name},
-            directory_type=dir_type,
-            as_posix=False,
+            experiment_key={""experiment_name"": experiment_name}, directory_type=dir_type, as_posix=False
         )
         for dir_type in directory_types
     }
     raw_data_dirs = {k: v for k, v in raw_data_dirs.items() if v}
 
     if not raw_data_dirs:
-        raise ValueError(
-            f""No raw data directory found for experiment: {experiment_name}""
-        )
+        raise ValueError(f""No raw data directory found for experiment: {experiment_name}"")
 
     chunkdata = io_api.load(
         root=list(raw_data_dirs.values()),
@@ -688,19 +672,21 @@

             repo_path = paths.get_repository_path(directory.pop(""repository_name""))
             break
     else:
-        raise FileNotFoundError(
-            f""Unable to identify the directory"" f"" where this chunk is from: {path}""
-        )
+        raise FileNotFoundError(f""Unable to identify the directory"" f"" where this chunk is from: {path}"")
 
     return raw_data_dir, directory, repo_path
 
 
 def create_chunk_restriction(experiment_name, start_time, end_time):
     """"""Create a time restriction string for the chunks between the specified ""start"" and ""end"" times.""""""
+    exp_key = {""experiment_name"": experiment_name}
     start_restriction = f'""{start_time}"" BETWEEN chunk_start AND chunk_end'
     end_restriction = f'""{end_time}"" BETWEEN chunk_start AND chunk_end'
-    start_query = Chunk & {""experiment_name"": experiment_name} & start_restriction
-    end_query = Chunk & {""experiment_name"": experiment_name} & end_restriction
+    start_query = Chunk & exp_key & start_restriction
+    end_query = Chunk & exp_key & end_restriction
+    if not end_query:
+        # No chunk contains the end time, so we need to find the last chunk that starts before the end time
+        end_query = Chunk & exp_key & f'chunk_end BETWEEN ""{start_time}"" AND ""{end_time}""'
     if not (start_query and end_query):
         raise ValueError(f""No Chunk found between {start_time} and {end_time}"")
     time_restriction = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820602066,,271,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,Fix deprecated `datetime.utcnow()`,"                 AnalysisNote.insert1(
                     {
-                        ""note_timestamp"": datetime.utcnow(),
+                        ""note_timestamp"": datetime.now(timezone.utc),","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -67,18 +60,14 @@

         # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
 
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
 
         chunk_restriction = acquisition.create_chunk_restriction(
             key[""experiment_name""], chunk_start, chunk_end
         )
 
-        block_state_query = (
-            acquisition.Environment.BlockState & exp_key & chunk_restriction
-        )
+        block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
         if block_state_df.empty:
             self.insert1(key)
@@ -94,19 +83,14 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
         if not blocks_df.empty:
             # calculate block end_times (use due_time) and durations
-            blocks_df[""end_time""] = blocks_df[""due_time""].apply(
-                lambda x: io_api.aeon(x)
-            )
-            blocks_df[""duration""] = (
-                blocks_df[""end_time""] - blocks_df.index
-            ).dt.total_seconds() / 3600
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
 
             for _, row in blocks_df.iterrows():
                 block_entries.append(
@@ -137,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time)."""""" # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -153,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -172,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -192,30 +182,36 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
-            if len(streams_table & chunk_keys) < len(
-                streams_table.key_source & chunk_keys
-            ):
+            if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
                 raise ValueError(
                     f""BlockAnalysis Not Ready - {streams_table.__name__}""
                     f""not yet fully ingested for block: {key}.""
                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
-
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], block_start, block_end
-        )
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
+
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
         patch_query = (
-            streams.UndergroundFeeder.join(
-                streams.UndergroundFeeder.RemovalTime, left=True
-            )
+            streams.UndergroundFeeder.join(streams.UndergroundFeeder.RemovalTime, left=True)
             & key
             & f'""{block_start}"" >= underground_feeder_install_time'
             & f'""{block_end}"" < IFNULL(underground_feeder_removal_time, ""2200-01-01"")'
@@ -229,14 +225,12 @@

                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
 
-            pellet_ts_threshold_df = get_threshold_associated_pellets(
-                patch_key, block_start, block_end
-            )
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
-            encoder_df = fetch_stream(
-                streams.UndergroundFeederEncoder & patch_key & chunk_restriction
-            )[block_start:block_end]
+            encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
+                block_start:block_end
+            ]
             # filter out maintenance period based on logs
             pellet_ts_threshold_df = filter_out_maintenance_periods(
                 pellet_ts_threshold_df,
@@ -254,39 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(
-                    f""No depletion state data found for block {key} - patch: {patch_name}""
-                )
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(
-                encoder_df.angle
-            )
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -294,21 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[
-                        ::wheel_downsampling_factor
-                    ],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,46 +309,65 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(
-                    ""identity_name"", part_name=""anchor_part""
-                )
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
-            pos_df = filter_out_maintenance_periods(
-                pos_df, maintenance_period, block_end
-            )
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
+            pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
                 continue
 
             position_diff = np.sqrt(
-                np.square(np.diff(pos_df.x.astype(float)))
-                + np.square(np.diff(pos_df.y.astype(float)))
+                np.square(np.diff(pos_df.x.astype(float))) + np.square(np.diff(pos_df.y.astype(float)))
             )
             cumsum_distance_travelled = np.concatenate([[0], np.cumsum(position_diff)])
 
             # weights
-            weight_query = (
-                acquisition.Environment.SubjectWeight & key & chunk_restriction
-            )
+            weight_query = acquisition.Environment.SubjectWeight & key & chunk_restriction
             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
@@ -384,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -412,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -447,10 +455,7 @@

         subjects_positions_df = pd.concat(
             [
                 pd.DataFrame(
-                    {
-                        ""subject_name"": [s[""subject_name""]]
-                        * len(s[""position_timestamps""])
-                    }
+                    {""subject_name"": [s[""subject_name""]] * len(s[""position_timestamps""])}
                     | {
                         k: s[k]
                         for k in (
@@ -466,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -478,8 +498,7 @@

             ""cum_pref_time"",
         ]
         all_subj_patch_pref_dict = {
-            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names}
-            for p in patch_names
+            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names} for p in patch_names
         }
 
         for patch in block_patches:
@@ -502,15 +521,11 @@

             ).fetch1(""attribute_value"")
             patch_center = (int(patch_center[""X""]), int(patch_center[""Y""]))
             subjects_xy = subjects_positions_df[[""position_x"", ""position_y""]].values
-            dist_to_patch = np.sqrt(
-                np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float)
-            )
+            dist_to_patch = np.sqrt(np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float))
             dist_to_patch_df = subjects_positions_df[[""subject_name""]].copy()
             dist_to_patch_df[""dist_to_patch""] = dist_to_patch
 
-            dist_to_patch_wheel_ts_id_df = pd.DataFrame(
-                index=cum_wheel_dist.index, columns=subject_names
-            )
+            dist_to_patch_wheel_ts_id_df = pd.DataFrame(index=cum_wheel_dist.index, columns=subject_names)
             dist_to_patch_pel_ts_id_df = pd.DataFrame(
                 index=patch[""pellet_timestamps""], columns=subject_names
             )
@@ -518,12 +533,10 @@

                 # Find closest match between pose_df indices and wheel indices
                 if not dist_to_patch_wheel_ts_id_df.empty:
                     dist_to_patch_wheel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_wheel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_wheel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -532,18 +545,16 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""100ms""),
                     )
-                    dist_to_patch_wheel_ts_id_df[subject_name] = (
-                        dist_to_patch_wheel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_wheel_ts_id_df[subject_name] = dist_to_patch_wheel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
                 # Find closest match between pose_df indices and pel indices
                 if not dist_to_patch_pel_ts_id_df.empty:
                     dist_to_patch_pel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_pel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_pel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -552,9 +563,9 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""200ms""),
                     )
-                    dist_to_patch_pel_ts_id_df[subject_name] = (
-                        dist_to_patch_pel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_pel_ts_id_df[subject_name] = dist_to_patch_pel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
 
             # Get closest subject to patch at each pellet timestep
             closest_subjects_pellet_ts = dist_to_patch_pel_ts_id_df.idxmin(axis=1)
@@ -566,12 +577,8 @@

             wheel_dist = cum_wheel_dist.diff().fillna(cum_wheel_dist.iloc[0])
             # Assign wheel dist to closest subject for each wheel timestep
             for subject_name in subject_names:
-                subj_idxs = cum_wheel_dist_subj_df[
-                    closest_subjects_wheel_ts == subject_name
-                ].index
-                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[
-                    subj_idxs
-                ]
+                subj_idxs = cum_wheel_dist_subj_df[closest_subjects_wheel_ts == subject_name].index
+                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[subj_idxs]
             cum_wheel_dist_subj_df = cum_wheel_dist_subj_df.cumsum(axis=0)
 
             # In patch time
@@ -579,14 +586,14 @@

             dt = np.median(np.diff(cum_wheel_dist.index)).astype(int) / 1e9  # s
             # Fill in `all_subj_patch_pref`
             for subject_name in subject_names:
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_dist""
-                ] = cum_wheel_dist_subj_df[subject_name].values
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_dist""] = (
+                    cum_wheel_dist_subj_df[subject_name].values
+                )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -597,14 +604,12 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
                         ""patch_threshold"": subj_patch_thresh,
-                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[
-                            subject_name
-                        ].values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
                     }
                 )
 
@@ -613,75 +618,49 @@

         for subject_name in subject_names:
             # Get sum of subj cum wheel dists and cum in patch time
             all_cum_dist = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1]
-                    for p in patch_names
-                ]
+                [all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1] for p in patch_names]
             )
             all_cum_time = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1]
-                    for p in patch_names
-                ]
-            )
+                [all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1] for p in patch_names]
+            )
+
+            CUM_PREF_DIST_MIN = 1e-3
+
             for patch_name in patch_names:
                 cum_pref_dist = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""]
-                    / all_cum_dist
-                )
-                CUM_PREF_DIST_MIN = 1e-3
-                cum_pref_dist = np.where(
-                    cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ] = cum_pref_dist
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""] / all_cum_dist
+                )
+                cum_pref_dist = np.where(cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist)
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""] = cum_pref_dist
 
                 cum_pref_time = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""]
-                    / all_cum_time
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ] = cum_pref_time
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""] / all_cum_time
+                )
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""] = cum_pref_time
 
             # sum pref at each ts across patches for each subject
             total_dist_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""] for p in patch_names]
                 ),
                 axis=0,
             )
             total_time_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""] for p in patch_names]
                 ),
                 axis=0,
             )
             for patch_name in patch_names:
-                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_dist_pref""
-                ] = np.divide(
+                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_dist_pref""] = np.divide(
                     cum_pref_dist,
                     total_dist_pref,
                     out=np.zeros_like(cum_pref_dist),
                     where=total_dist_pref != 0,
                 )
-                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_time_pref""
-                ] = np.divide(
+                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_time_pref""] = np.divide(
                     cum_pref_time,
                     total_time_pref,
                     out=np.zeros_like(cum_pref_time),
@@ -693,24 +672,12 @@

             | {
                 ""patch_name"": p,
                 ""subject_name"": s,
-                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ],
-                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ],
-                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""running_time_pref""
-                ],
-                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""running_dist_pref""
-                ],
-                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ][-1],
-                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ][-1],
+                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""],
+                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""],
+                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][""running_time_pref""],
+                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""running_dist_pref""],
+                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""][-1],
+                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""][-1],
             }
             for p, s in itertools.product(patch_names, subject_names)
         )
@@ -734,9 +701,7 @@

     def make(self, key):
         """"""Compute and plot various block-level statistics and visualizations.""""""
         # Define subject colors and patch styling for plotting
-        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(
-            ""subject"", order_by=""subject""
-        )
+        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(""subject"", order_by=""subject"")
         if not len(exp_subject_names):
             raise ValueError(
                 ""No subjects found in the `acquisition.Experiment.Subject`, missing a manual insert step?.""
@@ -755,10 +720,7 @@

         # Figure 1 - Patch stats: patch means and pellet threshold boxplots
         # ---
         subj_patch_info = (
-            (
-                BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"")
-                & key
-            )
+            (BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"") & key)
             .fetch(format=""frame"")
             .reset_index()
         )
@@ -772,46 +734,28 @@

             [""patch_name"", ""subject_name"", ""pellet_timestamps"", ""patch_threshold""]
         ]
         min_subj_patch_info = (
-            min_subj_patch_info.explode(
-                [""pellet_timestamps"", ""patch_threshold""], ignore_index=True
-            )
+            min_subj_patch_info.explode([""pellet_timestamps"", ""patch_threshold""], ignore_index=True)
             .dropna()
             .reset_index(drop=True)
         )
         # Rename and reindex columns
         min_subj_patch_info.columns = [""patch"", ""subject"", ""time"", ""threshold""]
-        min_subj_patch_info = min_subj_patch_info.reindex(
-            columns=[""time"", ""patch"", ""threshold"", ""subject""]
-        )
+        min_subj_patch_info = min_subj_patch_info.reindex(columns=[""time"", ""patch"", ""threshold"", ""subject""])
         # Add patch mean values and block-normalized delivery times to pellet info
         n_patches = len(patch_info)
-        patch_mean_info = pd.DataFrame(
-            index=np.arange(n_patches), columns=min_subj_patch_info.columns
-        )
+        patch_mean_info = pd.DataFrame(index=np.arange(n_patches), columns=min_subj_patch_info.columns)
         patch_mean_info[""subject""] = ""mean""
         patch_mean_info[""patch""] = [d[""patch_name""] for d in patch_info]
-        patch_mean_info[""threshold""] = [
-            ((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info
-        ]
+        patch_mean_info[""threshold""] = [((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info]
         patch_mean_info[""time""] = subj_patch_info[""block_start""][0]
-        min_subj_patch_info_plus = pd.concat(
-            (patch_mean_info, min_subj_patch_info)
-        ).reset_index(drop=True)
+        min_subj_patch_info_plus = pd.concat((patch_mean_info, min_subj_patch_info)).reset_index(drop=True)
         min_subj_patch_info_plus[""norm_time""] = (
-            (
-                min_subj_patch_info_plus[""time""]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
-            / (
-                min_subj_patch_info_plus[""time""].iloc[-1]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
+            (min_subj_patch_info_plus[""time""] - min_subj_patch_info_plus[""time""].iloc[0])
+            / (min_subj_patch_info_plus[""time""].iloc[-1] - min_subj_patch_info_plus[""time""].iloc[0])
         ).round(3)
 
         # Plot it
-        box_colors = [""#0A0A0A""] + list(
-            subject_colors_dict.values()
-        )  # subject colors + mean color
+        box_colors = [""#0A0A0A""] + list(subject_colors_dict.values())  # subject colors + mean color
         patch_stats_fig = px.box(
             min_subj_patch_info_plus.sort_values(""patch""),
             x=""patch"",
@@ -841,9 +785,7 @@

             .dropna()
             .reset_index(drop=True)
         )
-        weights_block.drop(
-            columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore""
-        )
+        weights_block.drop(columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore"")
         weights_block.rename(columns={""weight_timestamps"": ""time""}, inplace=True)
         weights_block.set_index(""time"", inplace=True)
         weights_block.sort_index(inplace=True)
@@ -867,17 +809,13 @@

         # Figure 3 - Cumulative pellet count: over time, per subject, markered by patch
         # ---
         # Create dataframe with cumulative pellet count per subject
-        cum_pel_ct = (
-            min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
-        )
+        cum_pel_ct = min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
         patch_means = cum_pel_ct.loc[0:3][[""patch"", ""threshold""]].rename(
             columns={""threshold"": ""mean_thresh""}
         )
         patch_means[""mean_thresh""] = patch_means[""mean_thresh""].astype(float).round(1)
         cum_pel_ct = cum_pel_ct.merge(patch_means, on=""patch"", how=""left"")
-        cum_pel_ct = cum_pel_ct[
-            ~cum_pel_ct[""subject""].str.contains(""mean"")
-        ].reset_index(drop=True)
+        cum_pel_ct = cum_pel_ct[~cum_pel_ct[""subject""].str.contains(""mean"")].reset_index(drop=True)
         cum_pel_ct = (
             cum_pel_ct.groupby(""subject"", group_keys=False)
             .apply(lambda group: group.assign(counter=np.arange(len(group)) + 1))
@@ -887,9 +825,7 @@

         make_float_cols = [""threshold"", ""mean_thresh"", ""norm_time""]
         cum_pel_ct[make_float_cols] = cum_pel_ct[make_float_cols].astype(float)
         cum_pel_ct[""patch_label""] = (
-            cum_pel_ct[""patch""]
-            + "" μ: ""
-            + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
+            cum_pel_ct[""patch""] + "" μ: "" + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
         )
         cum_pel_ct[""norm_thresh_val""] = (
             (cum_pel_ct[""threshold""] - cum_pel_ct[""threshold""].min())
@@ -919,9 +855,7 @@

                     mode=""markers"",
                     marker={
                         ""symbol"": patch_markers_dict[patch_grp[""patch""].iloc[0]],
-                        ""color"": gen_hex_grad(
-                            pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                        ),
+                        ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                         ""size"": 8,
                     },
                     name=patch_val,
@@ -941,9 +875,7 @@

         cum_pel_per_subject_fig = go.Figure()
         for id_val, id_grp in cum_pel_ct.groupby(""subject""):
             for patch_val, patch_grp in id_grp.groupby(""patch""):
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][""mean_thresh""].values[0]
                 cur_p = patch_val.replace(""Patch"", ""P"")
                 cum_pel_per_subject_fig.add_trace(
                     go.Scatter(
@@ -958,9 +890,7 @@

                         # line=dict(width=2, color=subject_colors_dict[id_val]),
                         marker={
                             ""symbol"": patch_markers_dict[patch_val],
-                            ""color"": gen_hex_grad(
-                                pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                            ),
+                            ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                             ""size"": 8,
                         },
                         name=f""{id_val} - {cur_p} - μ: {cur_p_mean}"",
@@ -977,9 +907,7 @@

         # Figure 5 - Cumulative wheel distance: over time, per subject-patch
         # ---
         # Get wheel timestamps for each patch
-        wheel_ts = (BlockAnalysis.Patch & key).fetch(
-            ""patch_name"", ""wheel_timestamps"", as_dict=True
-        )
+        wheel_ts = (BlockAnalysis.Patch & key).fetch(""patch_name"", ""wheel_timestamps"", as_dict=True)
         wheel_ts = {d[""patch_name""]: d[""wheel_timestamps""] for d in wheel_ts}
         # Get subject patch data
         subj_wheel_cumsum_dist = (BlockSubjectAnalysis.Patch & key).fetch(
@@ -999,9 +927,7 @@

         for subj in subject_names:
             for patch_name in patch_names:
                 cur_cum_wheel_dist = subj_wheel_cumsum_dist[(subj, patch_name)]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 cum_wheel_dist_fig.add_trace(
                     go.Scatter(
@@ -1018,10 +944,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1040,15 +963,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1062,14 +981,10 @@

         # ---
         # Get and format a dataframe with preference data
         patch_pref = (BlockSubjectAnalysis.Preference & key).fetch(format=""frame"")
-        patch_pref.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        patch_pref.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         # Replace small vals with 0
         small_pref_thresh = 1e-3
-        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[
-            ""cumulative_preference_by_wheel""
-        ].apply(
+        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[""cumulative_preference_by_wheel""].apply(
             lambda arr: np.where(np.array(arr) < small_pref_thresh, 0, np.array(arr))
         )
 
@@ -1077,18 +992,14 @@

             # Sum pref at each ts
             total_pref = np.sum(np.vstack(group[pref_col].values), axis=0)
             # Calculate running pref
-            group[out_col] = group[pref_col].apply(
-                lambda x: np.nan_to_num(x / total_pref, 0.0)
-            )
+            group[out_col] = group[pref_col].apply(lambda x: np.nan_to_num(x / total_pref, 0.0))
             return group
 
         patch_pref = (
             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1108,12 +1019,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_wheel""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_wheel""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_wheel_plot.add_trace(
                     go.Scatter(
@@ -1130,10 +1037,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1152,15 +1056,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1176,12 +1076,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_time""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_time""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_patch_fig.add_trace(
                     go.Scatter(
@@ -1198,10 +1094,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1220,15 +1113,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1242,9 +1131,7 @@

         # Figure 8 - Weighted patch preference: weighted by 'wheel_dist_spun : pel_ct' ratio
         # ---
         # Create multi-indexed dataframe with weighted distance for each subject-patch pair
-        pel_patches = [
-            p for p in patch_names if ""dummy"" not in p.lower()
-        ]  # exclude dummy patches
+        pel_patches = [p for p in patch_names if ""dummy"" not in p.lower()]  # exclude dummy patches
         data = []
         for patch in pel_patches:
             for subject in subject_names:
@@ -1257,16 +1144,12 @@

                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
         subj_wheel_pel_weighted_dist[""weighted_dist""] = np.nan
 
         # Calculate weighted distance
         subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
-        subject_patch_data.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        subject_patch_data.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         subj_wheel_pel_weighted_dist = defaultdict(lambda: defaultdict(dict))
         for s in subject_names:
             for p in pel_patches:
@@ -1274,14 +1157,11 @@

                 cur_wheel_cum_dist_df = pd.DataFrame(columns=[""time"", ""cum_wheel_dist""])
                 cur_wheel_cum_dist_df[""time""] = wheel_ts[p]
                 cur_wheel_cum_dist_df[""cum_wheel_dist""] = (
-                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""]
-                    + 1
+                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""] + 1
                 )
                 # Get cumulative pellet count
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)],
                     cur_wheel_cum_dist_df.sort_values(""time""),
                     on=""time"",
                     direction=""forward"",
@@ -1300,9 +1180,7 @@

                         on=""time"",
                         direction=""forward"",
                     )
-                    max_weight = (
-                        cur_cum_pel_ct.iloc[-1][""counter""] + 1
-                    )  # for values after last pellet
+                    max_weight = cur_cum_pel_ct.iloc[-1][""counter""] + 1  # for values after last pellet
                     merged_df[""counter""] = merged_df[""counter""].fillna(max_weight)
                     merged_df[""weighted_cum_wheel_dist""] = (
                         merged_df.groupby(""counter"")
@@ -1313,9 +1191,7 @@

                 else:
                     weighted_dist = cur_wheel_cum_dist_df[""cum_wheel_dist""].values
                 # Assign to dict
-                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[
-                    ""time""
-                ].values
+                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[""time""].values
                 subj_wheel_pel_weighted_dist[p][s][""weighted_dist""] = weighted_dist
         # Convert back to dataframe
         data = []
@@ -1326,15 +1202,11 @@

                         ""patch_name"": p,
                         ""subject_name"": s,
                         ""time"": subj_wheel_pel_weighted_dist[p][s][""time""],
-                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][
-                            ""weighted_dist""
-                        ],
+                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][""weighted_dist""],
                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
 
         # Calculate normalized weighted value
         def norm_inv_norm(group):
@@ -1343,28 +1215,20 @@

             inv_norm_dist = 1 / norm_dist
             inv_norm_dist = inv_norm_dist / (np.sum(inv_norm_dist, axis=0))
             # Map each inv_norm_dist back to patch name.
-            return pd.Series(
-                inv_norm_dist.tolist(), index=group.index, name=""norm_value""
-            )
+            return pd.Series(inv_norm_dist.tolist(), index=group.index, name=""norm_value"")
 
         subj_wheel_pel_weighted_dist[""norm_value""] = (
             subj_wheel_pel_weighted_dist.groupby(""subject_name"")
             .apply(norm_inv_norm)
             .reset_index(level=0, drop=True)
         )
-        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[
-            ""running_preference_by_wheel""
-        ]
+        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[""running_preference_by_wheel""]
 
         # Plot it
         weighted_patch_pref_fig = make_subplots(
             rows=len(pel_patches),
             cols=len(subject_names),
-            subplot_titles=[
-                f""{patch} - {subject}""
-                for patch in pel_patches
-                for subject in subject_names
-            ],
+            subplot_titles=[f""{patch} - {subject}"" for patch in pel_patches for subject in subject_names],
             specs=[[{""secondary_y"": True}] * len(subject_names)] * len(pel_patches),
             shared_xaxes=True,
             vertical_spacing=0.1,
@@ -1546,9 +1410,7 @@

         for id_val, id_grp in centroid_df.groupby(""identity_name""):
             # Add counts of x,y points to a grid that will be used for heatmap
             img_grid = np.zeros((max_x + 1, max_y + 1))
-            points, counts = np.unique(
-                id_grp[[""x"", ""y""]].values, return_counts=True, axis=0
-            )
+            points, counts = np.unique(id_grp[[""x"", ""y""]].values, return_counts=True, axis=0)
             for point, count in zip(points, counts, strict=True):
                 img_grid[point[0], point[1]] = count
             img_grid /= img_grid.max()  # normalize
@@ -1557,9 +1419,7 @@

             # so 45 cm/frame ~= 9 px/frame
             win_sz = 9  # in pixels  (ensure odd for centering)
             kernel = np.ones((win_sz, win_sz)) / win_sz**2  # moving avg kernel
-            img_grid_p = np.pad(
-                img_grid, win_sz // 2, mode=""edge""
-            )  # pad for full output from convolution
+            img_grid_p = np.pad(img_grid, win_sz // 2, mode=""edge"")  # pad for full output from convolution
             img_grid_smooth = conv2d(img_grid_p, kernel)
             heatmaps.append((id_val, img_grid_smooth))
 
@@ -1588,17 +1448,11 @@

         # Figure 3 - Position ethogram
         # ---
         # Get Active Region (ROI) locations
-        epoch_query = acquisition.Epoch & (
-            acquisition.Chunk & key & chunk_restriction
-        ).proj(""epoch_start"")
+        epoch_query = acquisition.Epoch & (acquisition.Chunk & key & chunk_restriction).proj(""epoch_start"")
         active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query
-        roi_locs = dict(
-            zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True)
-        )
+        roi_locs = dict(zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True))
         # get RFID reader locations
-        recent_rfid_query = (
-            acquisition.Experiment.proj() * streams.Device.proj() & key
-        ).aggr(
+        recent_rfid_query = (acquisition.Experiment.proj() * streams.Device.proj() & key).aggr(
             streams.RfidReader & f""rfid_reader_install_time <= '{block_start}'"",
             rfid_reader_install_time=""max(rfid_reader_install_time)"",
         )
@@ -1608,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1636,30 +1487,18 @@

 
         # For each ROI, compute if within ROI
         for roi in rois:
-            if (
-                roi == ""Corridor""
-            ):  # special case for corridor, based on between inner and outer radius
+            if roi == ""Corridor"":  # special case for corridor, based on between inner and outer radius
                 dist = np.linalg.norm(
                     (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - arena_center,
                     axis=1,
                 )
-                pos_eth_df[roi] = (dist >= arena_inner_radius) & (
-                    dist <= arena_outer_radius
-                )
+                pos_eth_df[roi] = (dist >= arena_inner_radius) & (dist <= arena_outer_radius)
             elif roi == ""Nest"":  # special case for nest, based on 4 corners
                 nest_corners = roi_locs[""NestRegion""][""ArrayOfPoint""]
-                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(
-                    nest_corners[0][""Y""]
-                )
-                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(
-                    nest_corners[1][""Y""]
-                )
-                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(
-                    nest_corners[2][""Y""]
-                )
-                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(
-                    nest_corners[3][""Y""]
-                )
+                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(nest_corners[0][""Y""])
+                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(nest_corners[1][""Y""])
+                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(nest_corners[2][""Y""])
+                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(nest_corners[3][""Y""])
                 pos_eth_df[roi] = (
                     (centroid_df[""x""] <= nest_br_x)
                     & (centroid_df[""y""] >= nest_br_y)
@@ -1673,13 +1512,10 @@

             else:
                 roi_radius = gate_radius if roi == ""Gate"" else patch_radius
                 # Get ROI coords
-                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(
-                    rfid_locs[roi + ""Rfid""][""Y""]
-                )
+                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(rfid_locs[roi + ""Rfid""][""Y""])
                 # Check if in ROI
                 dist = np.linalg.norm(
-                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T)
-                    - (roi_x, roi_y),
+                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - (roi_x, roi_y),
                     axis=1,
                 )
                 pos_eth_df[roi] = dist < roi_radius
@@ -1755,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1774,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1785,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1806,15 +1644,14 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data
     # pellet delivery trigger
@@ -1822,9 +1659,9 @@

         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
     # beambreak
-    beambreak_df = fetch_stream(
-        streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-    )[start:end]
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
     # patch threshold
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
@@ -1841,29 +1678,22 @@

         )
 
     # Step 2 - Remove invalid rows (back-to-back events)
-    BTB_TIME_DIFF = (
-        1.2  # pellet delivery trigger - time difference is less than 1.2 seconds
-    )
-    invalid_rows = (
-        delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_TIME_DIFF
-    )
+    BTB_MIN_TIME_DIFF = 1.2  # pellet delivery trigger - time diff is less than 1.2 seconds
+    BB_MIN_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
+    PT_MIN_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
+
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_MIN_TIME_DIFF
     delivered_pellet_df = delivered_pellet_df[~invalid_rows]
     # exclude manual deliveries
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    BB_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
-    invalid_rows = (
-        beambreak_df.index.to_series().diff().dt.total_seconds() < BB_TIME_DIFF
-    )
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < BB_MIN_TIME_DIFF
     beambreak_df = beambreak_df[~invalid_rows]
 
-    PT_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    invalid_rows = (
-        depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_TIME_DIFF
-    )
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_MIN_TIME_DIFF
     depletion_state_df = depletion_state_df[~invalid_rows]
 
     # Return empty if no data
@@ -1880,24 +1710,20 @@

             beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
-            tolerance=pd.Timedelta(""{BTB_TIME_DIFF}s""),
+            tolerance=pd.Timedelta(""{BTB_MIN_TIME_DIFF}s""),
             direction=""forward"",
         )
         .set_index(""time"")
         .dropna(subset=[""beam_break_timestamp""])
     )
-    pellet_beam_break_df.drop_duplicates(
-        subset=""beam_break_timestamp"", keep=""last"", inplace=True
-    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
     # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
             depletion_state_df.reset_index(),
-            pellet_beam_break_df.reset_index().rename(
-                columns={""time"": ""pellet_timestamp""}
-            ),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
             left_on=""time"",
             right_on=""pellet_timestamp"",
             tolerance=pd.Timedelta(""100ms""),
@@ -1910,12 +1736,8 @@

     # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
     # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
-    pellet_ts_threshold_df.pellet_timestamp = (
-        pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    )
-    pellet_ts_threshold_df.beam_break_timestamp = (
-        pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    )
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
     pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
         subset=[""pellet_timestamp"", ""beam_break_timestamp""]
     )
@@ -1942,12 +1764,8 @@

     Returns:
         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
-    max_inactive_time = (
-        pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
-    )
-    bout_data = pd.DataFrame(
-        columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""]
-    )
+    max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
     if subject_patch_data.empty:
         return bout_data
@@ -1991,52 +1809,34 @@

         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
         max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
-        )
-        foraging_mask = max_windowed_wheel_vals > (
-            patch_spun_df[""cum_wheel_dist""] + min_wheel_movement
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (
-            max_inactive_win_len - 1
-        )
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
         n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
         bout_end_indxs = (
             np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
             + (max_inactive_win_len - 1)
             + n_samples_in_1s
         )
-        bout_end_indxs[-1] = min(
-            bout_end_indxs[-1], len(wheel_ts) - 1
-        )  # ensure last bout ends in block
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
         # Remove bout that starts at block end
         if bout_start_indxs[-1] >= len(wheel_ts):
             bout_start_indxs = bout_start_indxs[:-1]
             bout_end_indxs = bout_end_indxs[:-1]
         if len(bout_start_indxs) != len(bout_end_indxs):
-            raise ValueError(
-                ""Mismatch between the lengths of bout_start_indxs and bout_end_indxs.""
-            )
-        bout_durations = (
-            wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]
-        ).astype(  # in seconds
+            raise ValueError(""Mismatch between the lengths of bout_start_indxs and bout_end_indxs."")
+        bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]""
-        ).astype(
-            float
-        ) / 1e9
+        ).astype(float) / 1e9
         bout_starts_ends = np.array(
             [
                 (wheel_ts[start_idx], wheel_ts[end_idx])
-                for start_idx, end_idx in zip(
-                    bout_start_indxs, bout_end_indxs, strict=True
-                )
+                for start_idx, end_idx in zip(bout_start_indxs, bout_end_indxs, strict=True)
             ]
         )
         all_pel_ts = np.sort(
-            np.concatenate(
-                [arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0]
-            )
+            np.concatenate([arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0])
         )
         bout_pellets = np.array(
             [
@@ -2050,8 +1850,7 @@

         bout_pellets = bout_pellets[bout_pellets >= min_pellets]
         bout_cum_wheel_dist = np.array(
             [
-                patch_spun_df.loc[end, ""cum_wheel_dist""]
-                - patch_spun_df.loc[start, ""cum_wheel_dist""]
+                patch_spun_df.loc[end, ""cum_wheel_dist""] - patch_spun_df.loc[start, ""cum_wheel_dist""]
                 for start, end in bout_starts_ends
             ]
         )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820604679,,632,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,," PLR2004: Replaced magic values with constant variables.
","+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""]
+                    / all_cum_dist
+                )
+                CUM_PREF_DIST_MIN = 1e-3","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -67,18 +60,14 @@

         # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
 
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
 
         chunk_restriction = acquisition.create_chunk_restriction(
             key[""experiment_name""], chunk_start, chunk_end
         )
 
-        block_state_query = (
-            acquisition.Environment.BlockState & exp_key & chunk_restriction
-        )
+        block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
         if block_state_df.empty:
             self.insert1(key)
@@ -94,19 +83,14 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
         if not blocks_df.empty:
             # calculate block end_times (use due_time) and durations
-            blocks_df[""end_time""] = blocks_df[""due_time""].apply(
-                lambda x: io_api.aeon(x)
-            )
-            blocks_df[""duration""] = (
-                blocks_df[""end_time""] - blocks_df.index
-            ).dt.total_seconds() / 3600
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
 
             for _, row in blocks_df.iterrows():
                 block_entries.append(
@@ -137,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time)."""""" # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -153,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -172,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -192,30 +182,36 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
-            if len(streams_table & chunk_keys) < len(
-                streams_table.key_source & chunk_keys
-            ):
+            if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
                 raise ValueError(
                     f""BlockAnalysis Not Ready - {streams_table.__name__}""
                     f""not yet fully ingested for block: {key}.""
                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
-
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], block_start, block_end
-        )
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
+
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
         patch_query = (
-            streams.UndergroundFeeder.join(
-                streams.UndergroundFeeder.RemovalTime, left=True
-            )
+            streams.UndergroundFeeder.join(streams.UndergroundFeeder.RemovalTime, left=True)
             & key
             & f'""{block_start}"" >= underground_feeder_install_time'
             & f'""{block_end}"" < IFNULL(underground_feeder_removal_time, ""2200-01-01"")'
@@ -229,14 +225,12 @@

                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
 
-            pellet_ts_threshold_df = get_threshold_associated_pellets(
-                patch_key, block_start, block_end
-            )
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
-            encoder_df = fetch_stream(
-                streams.UndergroundFeederEncoder & patch_key & chunk_restriction
-            )[block_start:block_end]
+            encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
+                block_start:block_end
+            ]
             # filter out maintenance period based on logs
             pellet_ts_threshold_df = filter_out_maintenance_periods(
                 pellet_ts_threshold_df,
@@ -254,39 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(
-                    f""No depletion state data found for block {key} - patch: {patch_name}""
-                )
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(
-                encoder_df.angle
-            )
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -294,21 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[
-                        ::wheel_downsampling_factor
-                    ],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,46 +309,65 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(
-                    ""identity_name"", part_name=""anchor_part""
-                )
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
-            pos_df = filter_out_maintenance_periods(
-                pos_df, maintenance_period, block_end
-            )
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
+            pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
                 continue
 
             position_diff = np.sqrt(
-                np.square(np.diff(pos_df.x.astype(float)))
-                + np.square(np.diff(pos_df.y.astype(float)))
+                np.square(np.diff(pos_df.x.astype(float))) + np.square(np.diff(pos_df.y.astype(float)))
             )
             cumsum_distance_travelled = np.concatenate([[0], np.cumsum(position_diff)])
 
             # weights
-            weight_query = (
-                acquisition.Environment.SubjectWeight & key & chunk_restriction
-            )
+            weight_query = acquisition.Environment.SubjectWeight & key & chunk_restriction
             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
@@ -384,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -412,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -447,10 +455,7 @@

         subjects_positions_df = pd.concat(
             [
                 pd.DataFrame(
-                    {
-                        ""subject_name"": [s[""subject_name""]]
-                        * len(s[""position_timestamps""])
-                    }
+                    {""subject_name"": [s[""subject_name""]] * len(s[""position_timestamps""])}
                     | {
                         k: s[k]
                         for k in (
@@ -466,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -478,8 +498,7 @@

             ""cum_pref_time"",
         ]
         all_subj_patch_pref_dict = {
-            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names}
-            for p in patch_names
+            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names} for p in patch_names
         }
 
         for patch in block_patches:
@@ -502,15 +521,11 @@

             ).fetch1(""attribute_value"")
             patch_center = (int(patch_center[""X""]), int(patch_center[""Y""]))
             subjects_xy = subjects_positions_df[[""position_x"", ""position_y""]].values
-            dist_to_patch = np.sqrt(
-                np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float)
-            )
+            dist_to_patch = np.sqrt(np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float))
             dist_to_patch_df = subjects_positions_df[[""subject_name""]].copy()
             dist_to_patch_df[""dist_to_patch""] = dist_to_patch
 
-            dist_to_patch_wheel_ts_id_df = pd.DataFrame(
-                index=cum_wheel_dist.index, columns=subject_names
-            )
+            dist_to_patch_wheel_ts_id_df = pd.DataFrame(index=cum_wheel_dist.index, columns=subject_names)
             dist_to_patch_pel_ts_id_df = pd.DataFrame(
                 index=patch[""pellet_timestamps""], columns=subject_names
             )
@@ -518,12 +533,10 @@

                 # Find closest match between pose_df indices and wheel indices
                 if not dist_to_patch_wheel_ts_id_df.empty:
                     dist_to_patch_wheel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_wheel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_wheel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -532,18 +545,16 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""100ms""),
                     )
-                    dist_to_patch_wheel_ts_id_df[subject_name] = (
-                        dist_to_patch_wheel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_wheel_ts_id_df[subject_name] = dist_to_patch_wheel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
                 # Find closest match between pose_df indices and pel indices
                 if not dist_to_patch_pel_ts_id_df.empty:
                     dist_to_patch_pel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_pel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_pel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -552,9 +563,9 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""200ms""),
                     )
-                    dist_to_patch_pel_ts_id_df[subject_name] = (
-                        dist_to_patch_pel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_pel_ts_id_df[subject_name] = dist_to_patch_pel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
 
             # Get closest subject to patch at each pellet timestep
             closest_subjects_pellet_ts = dist_to_patch_pel_ts_id_df.idxmin(axis=1)
@@ -566,12 +577,8 @@

             wheel_dist = cum_wheel_dist.diff().fillna(cum_wheel_dist.iloc[0])
             # Assign wheel dist to closest subject for each wheel timestep
             for subject_name in subject_names:
-                subj_idxs = cum_wheel_dist_subj_df[
-                    closest_subjects_wheel_ts == subject_name
-                ].index
-                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[
-                    subj_idxs
-                ]
+                subj_idxs = cum_wheel_dist_subj_df[closest_subjects_wheel_ts == subject_name].index
+                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[subj_idxs]
             cum_wheel_dist_subj_df = cum_wheel_dist_subj_df.cumsum(axis=0)
 
             # In patch time
@@ -579,14 +586,14 @@

             dt = np.median(np.diff(cum_wheel_dist.index)).astype(int) / 1e9  # s
             # Fill in `all_subj_patch_pref`
             for subject_name in subject_names:
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_dist""
-                ] = cum_wheel_dist_subj_df[subject_name].values
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_dist""] = (
+                    cum_wheel_dist_subj_df[subject_name].values
+                )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -597,14 +604,12 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
                         ""patch_threshold"": subj_patch_thresh,
-                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[
-                            subject_name
-                        ].values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
                     }
                 )
 
@@ -613,75 +618,49 @@

         for subject_name in subject_names:
             # Get sum of subj cum wheel dists and cum in patch time
             all_cum_dist = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1]
-                    for p in patch_names
-                ]
+                [all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1] for p in patch_names]
             )
             all_cum_time = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1]
-                    for p in patch_names
-                ]
-            )
+                [all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1] for p in patch_names]
+            )
+
+            CUM_PREF_DIST_MIN = 1e-3
+
             for patch_name in patch_names:
                 cum_pref_dist = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""]
-                    / all_cum_dist
-                )
-                CUM_PREF_DIST_MIN = 1e-3
-                cum_pref_dist = np.where(
-                    cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ] = cum_pref_dist
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""] / all_cum_dist
+                )
+                cum_pref_dist = np.where(cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist)
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""] = cum_pref_dist
 
                 cum_pref_time = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""]
-                    / all_cum_time
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ] = cum_pref_time
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""] / all_cum_time
+                )
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""] = cum_pref_time
 
             # sum pref at each ts across patches for each subject
             total_dist_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""] for p in patch_names]
                 ),
                 axis=0,
             )
             total_time_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""] for p in patch_names]
                 ),
                 axis=0,
             )
             for patch_name in patch_names:
-                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_dist_pref""
-                ] = np.divide(
+                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_dist_pref""] = np.divide(
                     cum_pref_dist,
                     total_dist_pref,
                     out=np.zeros_like(cum_pref_dist),
                     where=total_dist_pref != 0,
                 )
-                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_time_pref""
-                ] = np.divide(
+                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_time_pref""] = np.divide(
                     cum_pref_time,
                     total_time_pref,
                     out=np.zeros_like(cum_pref_time),
@@ -693,24 +672,12 @@

             | {
                 ""patch_name"": p,
                 ""subject_name"": s,
-                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ],
-                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ],
-                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""running_time_pref""
-                ],
-                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""running_dist_pref""
-                ],
-                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ][-1],
-                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ][-1],
+                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""],
+                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""],
+                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][""running_time_pref""],
+                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""running_dist_pref""],
+                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""][-1],
+                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""][-1],
             }
             for p, s in itertools.product(patch_names, subject_names)
         )
@@ -734,9 +701,7 @@

     def make(self, key):
         """"""Compute and plot various block-level statistics and visualizations.""""""
         # Define subject colors and patch styling for plotting
-        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(
-            ""subject"", order_by=""subject""
-        )
+        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(""subject"", order_by=""subject"")
         if not len(exp_subject_names):
             raise ValueError(
                 ""No subjects found in the `acquisition.Experiment.Subject`, missing a manual insert step?.""
@@ -755,10 +720,7 @@

         # Figure 1 - Patch stats: patch means and pellet threshold boxplots
         # ---
         subj_patch_info = (
-            (
-                BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"")
-                & key
-            )
+            (BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"") & key)
             .fetch(format=""frame"")
             .reset_index()
         )
@@ -772,46 +734,28 @@

             [""patch_name"", ""subject_name"", ""pellet_timestamps"", ""patch_threshold""]
         ]
         min_subj_patch_info = (
-            min_subj_patch_info.explode(
-                [""pellet_timestamps"", ""patch_threshold""], ignore_index=True
-            )
+            min_subj_patch_info.explode([""pellet_timestamps"", ""patch_threshold""], ignore_index=True)
             .dropna()
             .reset_index(drop=True)
         )
         # Rename and reindex columns
         min_subj_patch_info.columns = [""patch"", ""subject"", ""time"", ""threshold""]
-        min_subj_patch_info = min_subj_patch_info.reindex(
-            columns=[""time"", ""patch"", ""threshold"", ""subject""]
-        )
+        min_subj_patch_info = min_subj_patch_info.reindex(columns=[""time"", ""patch"", ""threshold"", ""subject""])
         # Add patch mean values and block-normalized delivery times to pellet info
         n_patches = len(patch_info)
-        patch_mean_info = pd.DataFrame(
-            index=np.arange(n_patches), columns=min_subj_patch_info.columns
-        )
+        patch_mean_info = pd.DataFrame(index=np.arange(n_patches), columns=min_subj_patch_info.columns)
         patch_mean_info[""subject""] = ""mean""
         patch_mean_info[""patch""] = [d[""patch_name""] for d in patch_info]
-        patch_mean_info[""threshold""] = [
-            ((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info
-        ]
+        patch_mean_info[""threshold""] = [((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info]
         patch_mean_info[""time""] = subj_patch_info[""block_start""][0]
-        min_subj_patch_info_plus = pd.concat(
-            (patch_mean_info, min_subj_patch_info)
-        ).reset_index(drop=True)
+        min_subj_patch_info_plus = pd.concat((patch_mean_info, min_subj_patch_info)).reset_index(drop=True)
         min_subj_patch_info_plus[""norm_time""] = (
-            (
-                min_subj_patch_info_plus[""time""]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
-            / (
-                min_subj_patch_info_plus[""time""].iloc[-1]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
+            (min_subj_patch_info_plus[""time""] - min_subj_patch_info_plus[""time""].iloc[0])
+            / (min_subj_patch_info_plus[""time""].iloc[-1] - min_subj_patch_info_plus[""time""].iloc[0])
         ).round(3)
 
         # Plot it
-        box_colors = [""#0A0A0A""] + list(
-            subject_colors_dict.values()
-        )  # subject colors + mean color
+        box_colors = [""#0A0A0A""] + list(subject_colors_dict.values())  # subject colors + mean color
         patch_stats_fig = px.box(
             min_subj_patch_info_plus.sort_values(""patch""),
             x=""patch"",
@@ -841,9 +785,7 @@

             .dropna()
             .reset_index(drop=True)
         )
-        weights_block.drop(
-            columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore""
-        )
+        weights_block.drop(columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore"")
         weights_block.rename(columns={""weight_timestamps"": ""time""}, inplace=True)
         weights_block.set_index(""time"", inplace=True)
         weights_block.sort_index(inplace=True)
@@ -867,17 +809,13 @@

         # Figure 3 - Cumulative pellet count: over time, per subject, markered by patch
         # ---
         # Create dataframe with cumulative pellet count per subject
-        cum_pel_ct = (
-            min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
-        )
+        cum_pel_ct = min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
         patch_means = cum_pel_ct.loc[0:3][[""patch"", ""threshold""]].rename(
             columns={""threshold"": ""mean_thresh""}
         )
         patch_means[""mean_thresh""] = patch_means[""mean_thresh""].astype(float).round(1)
         cum_pel_ct = cum_pel_ct.merge(patch_means, on=""patch"", how=""left"")
-        cum_pel_ct = cum_pel_ct[
-            ~cum_pel_ct[""subject""].str.contains(""mean"")
-        ].reset_index(drop=True)
+        cum_pel_ct = cum_pel_ct[~cum_pel_ct[""subject""].str.contains(""mean"")].reset_index(drop=True)
         cum_pel_ct = (
             cum_pel_ct.groupby(""subject"", group_keys=False)
             .apply(lambda group: group.assign(counter=np.arange(len(group)) + 1))
@@ -887,9 +825,7 @@

         make_float_cols = [""threshold"", ""mean_thresh"", ""norm_time""]
         cum_pel_ct[make_float_cols] = cum_pel_ct[make_float_cols].astype(float)
         cum_pel_ct[""patch_label""] = (
-            cum_pel_ct[""patch""]
-            + "" μ: ""
-            + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
+            cum_pel_ct[""patch""] + "" μ: "" + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
         )
         cum_pel_ct[""norm_thresh_val""] = (
             (cum_pel_ct[""threshold""] - cum_pel_ct[""threshold""].min())
@@ -919,9 +855,7 @@

                     mode=""markers"",
                     marker={
                         ""symbol"": patch_markers_dict[patch_grp[""patch""].iloc[0]],
-                        ""color"": gen_hex_grad(
-                            pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                        ),
+                        ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                         ""size"": 8,
                     },
                     name=patch_val,
@@ -941,9 +875,7 @@

         cum_pel_per_subject_fig = go.Figure()
         for id_val, id_grp in cum_pel_ct.groupby(""subject""):
             for patch_val, patch_grp in id_grp.groupby(""patch""):
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][""mean_thresh""].values[0]
                 cur_p = patch_val.replace(""Patch"", ""P"")
                 cum_pel_per_subject_fig.add_trace(
                     go.Scatter(
@@ -958,9 +890,7 @@

                         # line=dict(width=2, color=subject_colors_dict[id_val]),
                         marker={
                             ""symbol"": patch_markers_dict[patch_val],
-                            ""color"": gen_hex_grad(
-                                pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                            ),
+                            ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                             ""size"": 8,
                         },
                         name=f""{id_val} - {cur_p} - μ: {cur_p_mean}"",
@@ -977,9 +907,7 @@

         # Figure 5 - Cumulative wheel distance: over time, per subject-patch
         # ---
         # Get wheel timestamps for each patch
-        wheel_ts = (BlockAnalysis.Patch & key).fetch(
-            ""patch_name"", ""wheel_timestamps"", as_dict=True
-        )
+        wheel_ts = (BlockAnalysis.Patch & key).fetch(""patch_name"", ""wheel_timestamps"", as_dict=True)
         wheel_ts = {d[""patch_name""]: d[""wheel_timestamps""] for d in wheel_ts}
         # Get subject patch data
         subj_wheel_cumsum_dist = (BlockSubjectAnalysis.Patch & key).fetch(
@@ -999,9 +927,7 @@

         for subj in subject_names:
             for patch_name in patch_names:
                 cur_cum_wheel_dist = subj_wheel_cumsum_dist[(subj, patch_name)]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 cum_wheel_dist_fig.add_trace(
                     go.Scatter(
@@ -1018,10 +944,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1040,15 +963,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1062,14 +981,10 @@

         # ---
         # Get and format a dataframe with preference data
         patch_pref = (BlockSubjectAnalysis.Preference & key).fetch(format=""frame"")
-        patch_pref.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        patch_pref.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         # Replace small vals with 0
         small_pref_thresh = 1e-3
-        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[
-            ""cumulative_preference_by_wheel""
-        ].apply(
+        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[""cumulative_preference_by_wheel""].apply(
             lambda arr: np.where(np.array(arr) < small_pref_thresh, 0, np.array(arr))
         )
 
@@ -1077,18 +992,14 @@

             # Sum pref at each ts
             total_pref = np.sum(np.vstack(group[pref_col].values), axis=0)
             # Calculate running pref
-            group[out_col] = group[pref_col].apply(
-                lambda x: np.nan_to_num(x / total_pref, 0.0)
-            )
+            group[out_col] = group[pref_col].apply(lambda x: np.nan_to_num(x / total_pref, 0.0))
             return group
 
         patch_pref = (
             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1108,12 +1019,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_wheel""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_wheel""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_wheel_plot.add_trace(
                     go.Scatter(
@@ -1130,10 +1037,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1152,15 +1056,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1176,12 +1076,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_time""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_time""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_patch_fig.add_trace(
                     go.Scatter(
@@ -1198,10 +1094,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1220,15 +1113,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1242,9 +1131,7 @@

         # Figure 8 - Weighted patch preference: weighted by 'wheel_dist_spun : pel_ct' ratio
         # ---
         # Create multi-indexed dataframe with weighted distance for each subject-patch pair
-        pel_patches = [
-            p for p in patch_names if ""dummy"" not in p.lower()
-        ]  # exclude dummy patches
+        pel_patches = [p for p in patch_names if ""dummy"" not in p.lower()]  # exclude dummy patches
         data = []
         for patch in pel_patches:
             for subject in subject_names:
@@ -1257,16 +1144,12 @@

                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
         subj_wheel_pel_weighted_dist[""weighted_dist""] = np.nan
 
         # Calculate weighted distance
         subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
-        subject_patch_data.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        subject_patch_data.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         subj_wheel_pel_weighted_dist = defaultdict(lambda: defaultdict(dict))
         for s in subject_names:
             for p in pel_patches:
@@ -1274,14 +1157,11 @@

                 cur_wheel_cum_dist_df = pd.DataFrame(columns=[""time"", ""cum_wheel_dist""])
                 cur_wheel_cum_dist_df[""time""] = wheel_ts[p]
                 cur_wheel_cum_dist_df[""cum_wheel_dist""] = (
-                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""]
-                    + 1
+                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""] + 1
                 )
                 # Get cumulative pellet count
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)],
                     cur_wheel_cum_dist_df.sort_values(""time""),
                     on=""time"",
                     direction=""forward"",
@@ -1300,9 +1180,7 @@

                         on=""time"",
                         direction=""forward"",
                     )
-                    max_weight = (
-                        cur_cum_pel_ct.iloc[-1][""counter""] + 1
-                    )  # for values after last pellet
+                    max_weight = cur_cum_pel_ct.iloc[-1][""counter""] + 1  # for values after last pellet
                     merged_df[""counter""] = merged_df[""counter""].fillna(max_weight)
                     merged_df[""weighted_cum_wheel_dist""] = (
                         merged_df.groupby(""counter"")
@@ -1313,9 +1191,7 @@

                 else:
                     weighted_dist = cur_wheel_cum_dist_df[""cum_wheel_dist""].values
                 # Assign to dict
-                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[
-                    ""time""
-                ].values
+                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[""time""].values
                 subj_wheel_pel_weighted_dist[p][s][""weighted_dist""] = weighted_dist
         # Convert back to dataframe
         data = []
@@ -1326,15 +1202,11 @@

                         ""patch_name"": p,
                         ""subject_name"": s,
                         ""time"": subj_wheel_pel_weighted_dist[p][s][""time""],
-                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][
-                            ""weighted_dist""
-                        ],
+                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][""weighted_dist""],
                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
 
         # Calculate normalized weighted value
         def norm_inv_norm(group):
@@ -1343,28 +1215,20 @@

             inv_norm_dist = 1 / norm_dist
             inv_norm_dist = inv_norm_dist / (np.sum(inv_norm_dist, axis=0))
             # Map each inv_norm_dist back to patch name.
-            return pd.Series(
-                inv_norm_dist.tolist(), index=group.index, name=""norm_value""
-            )
+            return pd.Series(inv_norm_dist.tolist(), index=group.index, name=""norm_value"")
 
         subj_wheel_pel_weighted_dist[""norm_value""] = (
             subj_wheel_pel_weighted_dist.groupby(""subject_name"")
             .apply(norm_inv_norm)
             .reset_index(level=0, drop=True)
         )
-        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[
-            ""running_preference_by_wheel""
-        ]
+        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[""running_preference_by_wheel""]
 
         # Plot it
         weighted_patch_pref_fig = make_subplots(
             rows=len(pel_patches),
             cols=len(subject_names),
-            subplot_titles=[
-                f""{patch} - {subject}""
-                for patch in pel_patches
-                for subject in subject_names
-            ],
+            subplot_titles=[f""{patch} - {subject}"" for patch in pel_patches for subject in subject_names],
             specs=[[{""secondary_y"": True}] * len(subject_names)] * len(pel_patches),
             shared_xaxes=True,
             vertical_spacing=0.1,
@@ -1546,9 +1410,7 @@

         for id_val, id_grp in centroid_df.groupby(""identity_name""):
             # Add counts of x,y points to a grid that will be used for heatmap
             img_grid = np.zeros((max_x + 1, max_y + 1))
-            points, counts = np.unique(
-                id_grp[[""x"", ""y""]].values, return_counts=True, axis=0
-            )
+            points, counts = np.unique(id_grp[[""x"", ""y""]].values, return_counts=True, axis=0)
             for point, count in zip(points, counts, strict=True):
                 img_grid[point[0], point[1]] = count
             img_grid /= img_grid.max()  # normalize
@@ -1557,9 +1419,7 @@

             # so 45 cm/frame ~= 9 px/frame
             win_sz = 9  # in pixels  (ensure odd for centering)
             kernel = np.ones((win_sz, win_sz)) / win_sz**2  # moving avg kernel
-            img_grid_p = np.pad(
-                img_grid, win_sz // 2, mode=""edge""
-            )  # pad for full output from convolution
+            img_grid_p = np.pad(img_grid, win_sz // 2, mode=""edge"")  # pad for full output from convolution
             img_grid_smooth = conv2d(img_grid_p, kernel)
             heatmaps.append((id_val, img_grid_smooth))
 
@@ -1588,17 +1448,11 @@

         # Figure 3 - Position ethogram
         # ---
         # Get Active Region (ROI) locations
-        epoch_query = acquisition.Epoch & (
-            acquisition.Chunk & key & chunk_restriction
-        ).proj(""epoch_start"")
+        epoch_query = acquisition.Epoch & (acquisition.Chunk & key & chunk_restriction).proj(""epoch_start"")
         active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query
-        roi_locs = dict(
-            zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True)
-        )
+        roi_locs = dict(zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True))
         # get RFID reader locations
-        recent_rfid_query = (
-            acquisition.Experiment.proj() * streams.Device.proj() & key
-        ).aggr(
+        recent_rfid_query = (acquisition.Experiment.proj() * streams.Device.proj() & key).aggr(
             streams.RfidReader & f""rfid_reader_install_time <= '{block_start}'"",
             rfid_reader_install_time=""max(rfid_reader_install_time)"",
         )
@@ -1608,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1636,30 +1487,18 @@

 
         # For each ROI, compute if within ROI
         for roi in rois:
-            if (
-                roi == ""Corridor""
-            ):  # special case for corridor, based on between inner and outer radius
+            if roi == ""Corridor"":  # special case for corridor, based on between inner and outer radius
                 dist = np.linalg.norm(
                     (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - arena_center,
                     axis=1,
                 )
-                pos_eth_df[roi] = (dist >= arena_inner_radius) & (
-                    dist <= arena_outer_radius
-                )
+                pos_eth_df[roi] = (dist >= arena_inner_radius) & (dist <= arena_outer_radius)
             elif roi == ""Nest"":  # special case for nest, based on 4 corners
                 nest_corners = roi_locs[""NestRegion""][""ArrayOfPoint""]
-                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(
-                    nest_corners[0][""Y""]
-                )
-                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(
-                    nest_corners[1][""Y""]
-                )
-                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(
-                    nest_corners[2][""Y""]
-                )
-                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(
-                    nest_corners[3][""Y""]
-                )
+                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(nest_corners[0][""Y""])
+                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(nest_corners[1][""Y""])
+                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(nest_corners[2][""Y""])
+                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(nest_corners[3][""Y""])
                 pos_eth_df[roi] = (
                     (centroid_df[""x""] <= nest_br_x)
                     & (centroid_df[""y""] >= nest_br_y)
@@ -1673,13 +1512,10 @@

             else:
                 roi_radius = gate_radius if roi == ""Gate"" else patch_radius
                 # Get ROI coords
-                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(
-                    rfid_locs[roi + ""Rfid""][""Y""]
-                )
+                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(rfid_locs[roi + ""Rfid""][""Y""])
                 # Check if in ROI
                 dist = np.linalg.norm(
-                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T)
-                    - (roi_x, roi_y),
+                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - (roi_x, roi_y),
                     axis=1,
                 )
                 pos_eth_df[roi] = dist < roi_radius
@@ -1755,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1774,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1785,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1806,15 +1644,14 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data
     # pellet delivery trigger
@@ -1822,9 +1659,9 @@

         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
     # beambreak
-    beambreak_df = fetch_stream(
-        streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-    )[start:end]
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
     # patch threshold
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
@@ -1841,29 +1678,22 @@

         )
 
     # Step 2 - Remove invalid rows (back-to-back events)
-    BTB_TIME_DIFF = (
-        1.2  # pellet delivery trigger - time difference is less than 1.2 seconds
-    )
-    invalid_rows = (
-        delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_TIME_DIFF
-    )
+    BTB_MIN_TIME_DIFF = 1.2  # pellet delivery trigger - time diff is less than 1.2 seconds
+    BB_MIN_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
+    PT_MIN_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
+
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_MIN_TIME_DIFF
     delivered_pellet_df = delivered_pellet_df[~invalid_rows]
     # exclude manual deliveries
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    BB_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
-    invalid_rows = (
-        beambreak_df.index.to_series().diff().dt.total_seconds() < BB_TIME_DIFF
-    )
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < BB_MIN_TIME_DIFF
     beambreak_df = beambreak_df[~invalid_rows]
 
-    PT_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    invalid_rows = (
-        depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_TIME_DIFF
-    )
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_MIN_TIME_DIFF
     depletion_state_df = depletion_state_df[~invalid_rows]
 
     # Return empty if no data
@@ -1880,24 +1710,20 @@

             beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
-            tolerance=pd.Timedelta(""{BTB_TIME_DIFF}s""),
+            tolerance=pd.Timedelta(""{BTB_MIN_TIME_DIFF}s""),
             direction=""forward"",
         )
         .set_index(""time"")
         .dropna(subset=[""beam_break_timestamp""])
     )
-    pellet_beam_break_df.drop_duplicates(
-        subset=""beam_break_timestamp"", keep=""last"", inplace=True
-    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
     # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
             depletion_state_df.reset_index(),
-            pellet_beam_break_df.reset_index().rename(
-                columns={""time"": ""pellet_timestamp""}
-            ),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
             left_on=""time"",
             right_on=""pellet_timestamp"",
             tolerance=pd.Timedelta(""100ms""),
@@ -1910,12 +1736,8 @@

     # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
     # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
-    pellet_ts_threshold_df.pellet_timestamp = (
-        pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    )
-    pellet_ts_threshold_df.beam_break_timestamp = (
-        pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    )
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
     pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
         subset=[""pellet_timestamp"", ""beam_break_timestamp""]
     )
@@ -1942,12 +1764,8 @@

     Returns:
         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
-    max_inactive_time = (
-        pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
-    )
-    bout_data = pd.DataFrame(
-        columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""]
-    )
+    max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
     if subject_patch_data.empty:
         return bout_data
@@ -1991,52 +1809,34 @@

         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
         max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
-        )
-        foraging_mask = max_windowed_wheel_vals > (
-            patch_spun_df[""cum_wheel_dist""] + min_wheel_movement
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (
-            max_inactive_win_len - 1
-        )
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
         n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
         bout_end_indxs = (
             np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
             + (max_inactive_win_len - 1)
             + n_samples_in_1s
         )
-        bout_end_indxs[-1] = min(
-            bout_end_indxs[-1], len(wheel_ts) - 1
-        )  # ensure last bout ends in block
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
         # Remove bout that starts at block end
         if bout_start_indxs[-1] >= len(wheel_ts):
             bout_start_indxs = bout_start_indxs[:-1]
             bout_end_indxs = bout_end_indxs[:-1]
         if len(bout_start_indxs) != len(bout_end_indxs):
-            raise ValueError(
-                ""Mismatch between the lengths of bout_start_indxs and bout_end_indxs.""
-            )
-        bout_durations = (
-            wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]
-        ).astype(  # in seconds
+            raise ValueError(""Mismatch between the lengths of bout_start_indxs and bout_end_indxs."")
+        bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]""
-        ).astype(
-            float
-        ) / 1e9
+        ).astype(float) / 1e9
         bout_starts_ends = np.array(
             [
                 (wheel_ts[start_idx], wheel_ts[end_idx])
-                for start_idx, end_idx in zip(
-                    bout_start_indxs, bout_end_indxs, strict=True
-                )
+                for start_idx, end_idx in zip(bout_start_indxs, bout_end_indxs, strict=True)
             ]
         )
         all_pel_ts = np.sort(
-            np.concatenate(
-                [arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0]
-            )
+            np.concatenate([arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0])
         )
         bout_pellets = np.array(
             [
@@ -2050,8 +1850,7 @@

         bout_pellets = bout_pellets[bout_pellets >= min_pellets]
         bout_cum_wheel_dist = np.array(
             [
-                patch_spun_df.loc[end, ""cum_wheel_dist""]
-                - patch_spun_df.loc[start, ""cum_wheel_dist""]
+                patch_spun_df.loc[end, ""cum_wheel_dist""] - patch_spun_df.loc[start, ""cum_wheel_dist""]
                 for start, end in bout_starts_ends
             ]
         )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820608209,,1844,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,," PLR2004: Replaced magic values with constant variables.
","     # Step 2 - Remove invalid rows (back-to-back events)
-    # pellet delivery trigger - time difference is less than 1.2 seconds
-    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < 1.2
+    BTB_TIME_DIFF = (","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -67,18 +60,14 @@

         # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
 
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
 
         chunk_restriction = acquisition.create_chunk_restriction(
             key[""experiment_name""], chunk_start, chunk_end
         )
 
-        block_state_query = (
-            acquisition.Environment.BlockState & exp_key & chunk_restriction
-        )
+        block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
         if block_state_df.empty:
             self.insert1(key)
@@ -94,19 +83,14 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
         if not blocks_df.empty:
             # calculate block end_times (use due_time) and durations
-            blocks_df[""end_time""] = blocks_df[""due_time""].apply(
-                lambda x: io_api.aeon(x)
-            )
-            blocks_df[""duration""] = (
-                blocks_df[""end_time""] - blocks_df.index
-            ).dt.total_seconds() / 3600
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
 
             for _, row in blocks_df.iterrows():
                 block_entries.append(
@@ -137,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time)."""""" # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -153,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -172,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -192,30 +182,36 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
-            if len(streams_table & chunk_keys) < len(
-                streams_table.key_source & chunk_keys
-            ):
+            if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
                 raise ValueError(
                     f""BlockAnalysis Not Ready - {streams_table.__name__}""
                     f""not yet fully ingested for block: {key}.""
                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
-
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], block_start, block_end
-        )
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
+
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
         patch_query = (
-            streams.UndergroundFeeder.join(
-                streams.UndergroundFeeder.RemovalTime, left=True
-            )
+            streams.UndergroundFeeder.join(streams.UndergroundFeeder.RemovalTime, left=True)
             & key
             & f'""{block_start}"" >= underground_feeder_install_time'
             & f'""{block_end}"" < IFNULL(underground_feeder_removal_time, ""2200-01-01"")'
@@ -229,14 +225,12 @@

                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
 
-            pellet_ts_threshold_df = get_threshold_associated_pellets(
-                patch_key, block_start, block_end
-            )
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
-            encoder_df = fetch_stream(
-                streams.UndergroundFeederEncoder & patch_key & chunk_restriction
-            )[block_start:block_end]
+            encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
+                block_start:block_end
+            ]
             # filter out maintenance period based on logs
             pellet_ts_threshold_df = filter_out_maintenance_periods(
                 pellet_ts_threshold_df,
@@ -254,39 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(
-                    f""No depletion state data found for block {key} - patch: {patch_name}""
-                )
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(
-                encoder_df.angle
-            )
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -294,21 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[
-                        ::wheel_downsampling_factor
-                    ],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,46 +309,65 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(
-                    ""identity_name"", part_name=""anchor_part""
-                )
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
-            pos_df = filter_out_maintenance_periods(
-                pos_df, maintenance_period, block_end
-            )
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
+            pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
                 continue
 
             position_diff = np.sqrt(
-                np.square(np.diff(pos_df.x.astype(float)))
-                + np.square(np.diff(pos_df.y.astype(float)))
+                np.square(np.diff(pos_df.x.astype(float))) + np.square(np.diff(pos_df.y.astype(float)))
             )
             cumsum_distance_travelled = np.concatenate([[0], np.cumsum(position_diff)])
 
             # weights
-            weight_query = (
-                acquisition.Environment.SubjectWeight & key & chunk_restriction
-            )
+            weight_query = acquisition.Environment.SubjectWeight & key & chunk_restriction
             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
@@ -384,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -412,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -447,10 +455,7 @@

         subjects_positions_df = pd.concat(
             [
                 pd.DataFrame(
-                    {
-                        ""subject_name"": [s[""subject_name""]]
-                        * len(s[""position_timestamps""])
-                    }
+                    {""subject_name"": [s[""subject_name""]] * len(s[""position_timestamps""])}
                     | {
                         k: s[k]
                         for k in (
@@ -466,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -478,8 +498,7 @@

             ""cum_pref_time"",
         ]
         all_subj_patch_pref_dict = {
-            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names}
-            for p in patch_names
+            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names} for p in patch_names
         }
 
         for patch in block_patches:
@@ -502,15 +521,11 @@

             ).fetch1(""attribute_value"")
             patch_center = (int(patch_center[""X""]), int(patch_center[""Y""]))
             subjects_xy = subjects_positions_df[[""position_x"", ""position_y""]].values
-            dist_to_patch = np.sqrt(
-                np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float)
-            )
+            dist_to_patch = np.sqrt(np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float))
             dist_to_patch_df = subjects_positions_df[[""subject_name""]].copy()
             dist_to_patch_df[""dist_to_patch""] = dist_to_patch
 
-            dist_to_patch_wheel_ts_id_df = pd.DataFrame(
-                index=cum_wheel_dist.index, columns=subject_names
-            )
+            dist_to_patch_wheel_ts_id_df = pd.DataFrame(index=cum_wheel_dist.index, columns=subject_names)
             dist_to_patch_pel_ts_id_df = pd.DataFrame(
                 index=patch[""pellet_timestamps""], columns=subject_names
             )
@@ -518,12 +533,10 @@

                 # Find closest match between pose_df indices and wheel indices
                 if not dist_to_patch_wheel_ts_id_df.empty:
                     dist_to_patch_wheel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_wheel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_wheel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -532,18 +545,16 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""100ms""),
                     )
-                    dist_to_patch_wheel_ts_id_df[subject_name] = (
-                        dist_to_patch_wheel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_wheel_ts_id_df[subject_name] = dist_to_patch_wheel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
                 # Find closest match between pose_df indices and pel indices
                 if not dist_to_patch_pel_ts_id_df.empty:
                     dist_to_patch_pel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_pel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_pel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -552,9 +563,9 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""200ms""),
                     )
-                    dist_to_patch_pel_ts_id_df[subject_name] = (
-                        dist_to_patch_pel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_pel_ts_id_df[subject_name] = dist_to_patch_pel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
 
             # Get closest subject to patch at each pellet timestep
             closest_subjects_pellet_ts = dist_to_patch_pel_ts_id_df.idxmin(axis=1)
@@ -566,12 +577,8 @@

             wheel_dist = cum_wheel_dist.diff().fillna(cum_wheel_dist.iloc[0])
             # Assign wheel dist to closest subject for each wheel timestep
             for subject_name in subject_names:
-                subj_idxs = cum_wheel_dist_subj_df[
-                    closest_subjects_wheel_ts == subject_name
-                ].index
-                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[
-                    subj_idxs
-                ]
+                subj_idxs = cum_wheel_dist_subj_df[closest_subjects_wheel_ts == subject_name].index
+                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[subj_idxs]
             cum_wheel_dist_subj_df = cum_wheel_dist_subj_df.cumsum(axis=0)
 
             # In patch time
@@ -579,14 +586,14 @@

             dt = np.median(np.diff(cum_wheel_dist.index)).astype(int) / 1e9  # s
             # Fill in `all_subj_patch_pref`
             for subject_name in subject_names:
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_dist""
-                ] = cum_wheel_dist_subj_df[subject_name].values
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_dist""] = (
+                    cum_wheel_dist_subj_df[subject_name].values
+                )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -597,14 +604,12 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
                         ""patch_threshold"": subj_patch_thresh,
-                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[
-                            subject_name
-                        ].values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
                     }
                 )
 
@@ -613,75 +618,49 @@

         for subject_name in subject_names:
             # Get sum of subj cum wheel dists and cum in patch time
             all_cum_dist = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1]
-                    for p in patch_names
-                ]
+                [all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1] for p in patch_names]
             )
             all_cum_time = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1]
-                    for p in patch_names
-                ]
-            )
+                [all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1] for p in patch_names]
+            )
+
+            CUM_PREF_DIST_MIN = 1e-3
+
             for patch_name in patch_names:
                 cum_pref_dist = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""]
-                    / all_cum_dist
-                )
-                CUM_PREF_DIST_MIN = 1e-3
-                cum_pref_dist = np.where(
-                    cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ] = cum_pref_dist
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""] / all_cum_dist
+                )
+                cum_pref_dist = np.where(cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist)
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""] = cum_pref_dist
 
                 cum_pref_time = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""]
-                    / all_cum_time
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ] = cum_pref_time
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""] / all_cum_time
+                )
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""] = cum_pref_time
 
             # sum pref at each ts across patches for each subject
             total_dist_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""] for p in patch_names]
                 ),
                 axis=0,
             )
             total_time_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""] for p in patch_names]
                 ),
                 axis=0,
             )
             for patch_name in patch_names:
-                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_dist_pref""
-                ] = np.divide(
+                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_dist_pref""] = np.divide(
                     cum_pref_dist,
                     total_dist_pref,
                     out=np.zeros_like(cum_pref_dist),
                     where=total_dist_pref != 0,
                 )
-                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_time_pref""
-                ] = np.divide(
+                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_time_pref""] = np.divide(
                     cum_pref_time,
                     total_time_pref,
                     out=np.zeros_like(cum_pref_time),
@@ -693,24 +672,12 @@

             | {
                 ""patch_name"": p,
                 ""subject_name"": s,
-                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ],
-                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ],
-                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""running_time_pref""
-                ],
-                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""running_dist_pref""
-                ],
-                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ][-1],
-                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ][-1],
+                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""],
+                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""],
+                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][""running_time_pref""],
+                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""running_dist_pref""],
+                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""][-1],
+                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""][-1],
             }
             for p, s in itertools.product(patch_names, subject_names)
         )
@@ -734,9 +701,7 @@

     def make(self, key):
         """"""Compute and plot various block-level statistics and visualizations.""""""
         # Define subject colors and patch styling for plotting
-        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(
-            ""subject"", order_by=""subject""
-        )
+        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(""subject"", order_by=""subject"")
         if not len(exp_subject_names):
             raise ValueError(
                 ""No subjects found in the `acquisition.Experiment.Subject`, missing a manual insert step?.""
@@ -755,10 +720,7 @@

         # Figure 1 - Patch stats: patch means and pellet threshold boxplots
         # ---
         subj_patch_info = (
-            (
-                BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"")
-                & key
-            )
+            (BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"") & key)
             .fetch(format=""frame"")
             .reset_index()
         )
@@ -772,46 +734,28 @@

             [""patch_name"", ""subject_name"", ""pellet_timestamps"", ""patch_threshold""]
         ]
         min_subj_patch_info = (
-            min_subj_patch_info.explode(
-                [""pellet_timestamps"", ""patch_threshold""], ignore_index=True
-            )
+            min_subj_patch_info.explode([""pellet_timestamps"", ""patch_threshold""], ignore_index=True)
             .dropna()
             .reset_index(drop=True)
         )
         # Rename and reindex columns
         min_subj_patch_info.columns = [""patch"", ""subject"", ""time"", ""threshold""]
-        min_subj_patch_info = min_subj_patch_info.reindex(
-            columns=[""time"", ""patch"", ""threshold"", ""subject""]
-        )
+        min_subj_patch_info = min_subj_patch_info.reindex(columns=[""time"", ""patch"", ""threshold"", ""subject""])
         # Add patch mean values and block-normalized delivery times to pellet info
         n_patches = len(patch_info)
-        patch_mean_info = pd.DataFrame(
-            index=np.arange(n_patches), columns=min_subj_patch_info.columns
-        )
+        patch_mean_info = pd.DataFrame(index=np.arange(n_patches), columns=min_subj_patch_info.columns)
         patch_mean_info[""subject""] = ""mean""
         patch_mean_info[""patch""] = [d[""patch_name""] for d in patch_info]
-        patch_mean_info[""threshold""] = [
-            ((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info
-        ]
+        patch_mean_info[""threshold""] = [((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info]
         patch_mean_info[""time""] = subj_patch_info[""block_start""][0]
-        min_subj_patch_info_plus = pd.concat(
-            (patch_mean_info, min_subj_patch_info)
-        ).reset_index(drop=True)
+        min_subj_patch_info_plus = pd.concat((patch_mean_info, min_subj_patch_info)).reset_index(drop=True)
         min_subj_patch_info_plus[""norm_time""] = (
-            (
-                min_subj_patch_info_plus[""time""]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
-            / (
-                min_subj_patch_info_plus[""time""].iloc[-1]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
+            (min_subj_patch_info_plus[""time""] - min_subj_patch_info_plus[""time""].iloc[0])
+            / (min_subj_patch_info_plus[""time""].iloc[-1] - min_subj_patch_info_plus[""time""].iloc[0])
         ).round(3)
 
         # Plot it
-        box_colors = [""#0A0A0A""] + list(
-            subject_colors_dict.values()
-        )  # subject colors + mean color
+        box_colors = [""#0A0A0A""] + list(subject_colors_dict.values())  # subject colors + mean color
         patch_stats_fig = px.box(
             min_subj_patch_info_plus.sort_values(""patch""),
             x=""patch"",
@@ -841,9 +785,7 @@

             .dropna()
             .reset_index(drop=True)
         )
-        weights_block.drop(
-            columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore""
-        )
+        weights_block.drop(columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore"")
         weights_block.rename(columns={""weight_timestamps"": ""time""}, inplace=True)
         weights_block.set_index(""time"", inplace=True)
         weights_block.sort_index(inplace=True)
@@ -867,17 +809,13 @@

         # Figure 3 - Cumulative pellet count: over time, per subject, markered by patch
         # ---
         # Create dataframe with cumulative pellet count per subject
-        cum_pel_ct = (
-            min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
-        )
+        cum_pel_ct = min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
         patch_means = cum_pel_ct.loc[0:3][[""patch"", ""threshold""]].rename(
             columns={""threshold"": ""mean_thresh""}
         )
         patch_means[""mean_thresh""] = patch_means[""mean_thresh""].astype(float).round(1)
         cum_pel_ct = cum_pel_ct.merge(patch_means, on=""patch"", how=""left"")
-        cum_pel_ct = cum_pel_ct[
-            ~cum_pel_ct[""subject""].str.contains(""mean"")
-        ].reset_index(drop=True)
+        cum_pel_ct = cum_pel_ct[~cum_pel_ct[""subject""].str.contains(""mean"")].reset_index(drop=True)
         cum_pel_ct = (
             cum_pel_ct.groupby(""subject"", group_keys=False)
             .apply(lambda group: group.assign(counter=np.arange(len(group)) + 1))
@@ -887,9 +825,7 @@

         make_float_cols = [""threshold"", ""mean_thresh"", ""norm_time""]
         cum_pel_ct[make_float_cols] = cum_pel_ct[make_float_cols].astype(float)
         cum_pel_ct[""patch_label""] = (
-            cum_pel_ct[""patch""]
-            + "" μ: ""
-            + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
+            cum_pel_ct[""patch""] + "" μ: "" + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
         )
         cum_pel_ct[""norm_thresh_val""] = (
             (cum_pel_ct[""threshold""] - cum_pel_ct[""threshold""].min())
@@ -919,9 +855,7 @@

                     mode=""markers"",
                     marker={
                         ""symbol"": patch_markers_dict[patch_grp[""patch""].iloc[0]],
-                        ""color"": gen_hex_grad(
-                            pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                        ),
+                        ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                         ""size"": 8,
                     },
                     name=patch_val,
@@ -941,9 +875,7 @@

         cum_pel_per_subject_fig = go.Figure()
         for id_val, id_grp in cum_pel_ct.groupby(""subject""):
             for patch_val, patch_grp in id_grp.groupby(""patch""):
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][""mean_thresh""].values[0]
                 cur_p = patch_val.replace(""Patch"", ""P"")
                 cum_pel_per_subject_fig.add_trace(
                     go.Scatter(
@@ -958,9 +890,7 @@

                         # line=dict(width=2, color=subject_colors_dict[id_val]),
                         marker={
                             ""symbol"": patch_markers_dict[patch_val],
-                            ""color"": gen_hex_grad(
-                                pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                            ),
+                            ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                             ""size"": 8,
                         },
                         name=f""{id_val} - {cur_p} - μ: {cur_p_mean}"",
@@ -977,9 +907,7 @@

         # Figure 5 - Cumulative wheel distance: over time, per subject-patch
         # ---
         # Get wheel timestamps for each patch
-        wheel_ts = (BlockAnalysis.Patch & key).fetch(
-            ""patch_name"", ""wheel_timestamps"", as_dict=True
-        )
+        wheel_ts = (BlockAnalysis.Patch & key).fetch(""patch_name"", ""wheel_timestamps"", as_dict=True)
         wheel_ts = {d[""patch_name""]: d[""wheel_timestamps""] for d in wheel_ts}
         # Get subject patch data
         subj_wheel_cumsum_dist = (BlockSubjectAnalysis.Patch & key).fetch(
@@ -999,9 +927,7 @@

         for subj in subject_names:
             for patch_name in patch_names:
                 cur_cum_wheel_dist = subj_wheel_cumsum_dist[(subj, patch_name)]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 cum_wheel_dist_fig.add_trace(
                     go.Scatter(
@@ -1018,10 +944,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1040,15 +963,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1062,14 +981,10 @@

         # ---
         # Get and format a dataframe with preference data
         patch_pref = (BlockSubjectAnalysis.Preference & key).fetch(format=""frame"")
-        patch_pref.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        patch_pref.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         # Replace small vals with 0
         small_pref_thresh = 1e-3
-        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[
-            ""cumulative_preference_by_wheel""
-        ].apply(
+        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[""cumulative_preference_by_wheel""].apply(
             lambda arr: np.where(np.array(arr) < small_pref_thresh, 0, np.array(arr))
         )
 
@@ -1077,18 +992,14 @@

             # Sum pref at each ts
             total_pref = np.sum(np.vstack(group[pref_col].values), axis=0)
             # Calculate running pref
-            group[out_col] = group[pref_col].apply(
-                lambda x: np.nan_to_num(x / total_pref, 0.0)
-            )
+            group[out_col] = group[pref_col].apply(lambda x: np.nan_to_num(x / total_pref, 0.0))
             return group
 
         patch_pref = (
             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1108,12 +1019,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_wheel""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_wheel""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_wheel_plot.add_trace(
                     go.Scatter(
@@ -1130,10 +1037,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1152,15 +1056,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1176,12 +1076,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_time""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_time""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_patch_fig.add_trace(
                     go.Scatter(
@@ -1198,10 +1094,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1220,15 +1113,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1242,9 +1131,7 @@

         # Figure 8 - Weighted patch preference: weighted by 'wheel_dist_spun : pel_ct' ratio
         # ---
         # Create multi-indexed dataframe with weighted distance for each subject-patch pair
-        pel_patches = [
-            p for p in patch_names if ""dummy"" not in p.lower()
-        ]  # exclude dummy patches
+        pel_patches = [p for p in patch_names if ""dummy"" not in p.lower()]  # exclude dummy patches
         data = []
         for patch in pel_patches:
             for subject in subject_names:
@@ -1257,16 +1144,12 @@

                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
         subj_wheel_pel_weighted_dist[""weighted_dist""] = np.nan
 
         # Calculate weighted distance
         subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
-        subject_patch_data.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        subject_patch_data.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         subj_wheel_pel_weighted_dist = defaultdict(lambda: defaultdict(dict))
         for s in subject_names:
             for p in pel_patches:
@@ -1274,14 +1157,11 @@

                 cur_wheel_cum_dist_df = pd.DataFrame(columns=[""time"", ""cum_wheel_dist""])
                 cur_wheel_cum_dist_df[""time""] = wheel_ts[p]
                 cur_wheel_cum_dist_df[""cum_wheel_dist""] = (
-                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""]
-                    + 1
+                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""] + 1
                 )
                 # Get cumulative pellet count
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)],
                     cur_wheel_cum_dist_df.sort_values(""time""),
                     on=""time"",
                     direction=""forward"",
@@ -1300,9 +1180,7 @@

                         on=""time"",
                         direction=""forward"",
                     )
-                    max_weight = (
-                        cur_cum_pel_ct.iloc[-1][""counter""] + 1
-                    )  # for values after last pellet
+                    max_weight = cur_cum_pel_ct.iloc[-1][""counter""] + 1  # for values after last pellet
                     merged_df[""counter""] = merged_df[""counter""].fillna(max_weight)
                     merged_df[""weighted_cum_wheel_dist""] = (
                         merged_df.groupby(""counter"")
@@ -1313,9 +1191,7 @@

                 else:
                     weighted_dist = cur_wheel_cum_dist_df[""cum_wheel_dist""].values
                 # Assign to dict
-                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[
-                    ""time""
-                ].values
+                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[""time""].values
                 subj_wheel_pel_weighted_dist[p][s][""weighted_dist""] = weighted_dist
         # Convert back to dataframe
         data = []
@@ -1326,15 +1202,11 @@

                         ""patch_name"": p,
                         ""subject_name"": s,
                         ""time"": subj_wheel_pel_weighted_dist[p][s][""time""],
-                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][
-                            ""weighted_dist""
-                        ],
+                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][""weighted_dist""],
                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
 
         # Calculate normalized weighted value
         def norm_inv_norm(group):
@@ -1343,28 +1215,20 @@

             inv_norm_dist = 1 / norm_dist
             inv_norm_dist = inv_norm_dist / (np.sum(inv_norm_dist, axis=0))
             # Map each inv_norm_dist back to patch name.
-            return pd.Series(
-                inv_norm_dist.tolist(), index=group.index, name=""norm_value""
-            )
+            return pd.Series(inv_norm_dist.tolist(), index=group.index, name=""norm_value"")
 
         subj_wheel_pel_weighted_dist[""norm_value""] = (
             subj_wheel_pel_weighted_dist.groupby(""subject_name"")
             .apply(norm_inv_norm)
             .reset_index(level=0, drop=True)
         )
-        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[
-            ""running_preference_by_wheel""
-        ]
+        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[""running_preference_by_wheel""]
 
         # Plot it
         weighted_patch_pref_fig = make_subplots(
             rows=len(pel_patches),
             cols=len(subject_names),
-            subplot_titles=[
-                f""{patch} - {subject}""
-                for patch in pel_patches
-                for subject in subject_names
-            ],
+            subplot_titles=[f""{patch} - {subject}"" for patch in pel_patches for subject in subject_names],
             specs=[[{""secondary_y"": True}] * len(subject_names)] * len(pel_patches),
             shared_xaxes=True,
             vertical_spacing=0.1,
@@ -1546,9 +1410,7 @@

         for id_val, id_grp in centroid_df.groupby(""identity_name""):
             # Add counts of x,y points to a grid that will be used for heatmap
             img_grid = np.zeros((max_x + 1, max_y + 1))
-            points, counts = np.unique(
-                id_grp[[""x"", ""y""]].values, return_counts=True, axis=0
-            )
+            points, counts = np.unique(id_grp[[""x"", ""y""]].values, return_counts=True, axis=0)
             for point, count in zip(points, counts, strict=True):
                 img_grid[point[0], point[1]] = count
             img_grid /= img_grid.max()  # normalize
@@ -1557,9 +1419,7 @@

             # so 45 cm/frame ~= 9 px/frame
             win_sz = 9  # in pixels  (ensure odd for centering)
             kernel = np.ones((win_sz, win_sz)) / win_sz**2  # moving avg kernel
-            img_grid_p = np.pad(
-                img_grid, win_sz // 2, mode=""edge""
-            )  # pad for full output from convolution
+            img_grid_p = np.pad(img_grid, win_sz // 2, mode=""edge"")  # pad for full output from convolution
             img_grid_smooth = conv2d(img_grid_p, kernel)
             heatmaps.append((id_val, img_grid_smooth))
 
@@ -1588,17 +1448,11 @@

         # Figure 3 - Position ethogram
         # ---
         # Get Active Region (ROI) locations
-        epoch_query = acquisition.Epoch & (
-            acquisition.Chunk & key & chunk_restriction
-        ).proj(""epoch_start"")
+        epoch_query = acquisition.Epoch & (acquisition.Chunk & key & chunk_restriction).proj(""epoch_start"")
         active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query
-        roi_locs = dict(
-            zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True)
-        )
+        roi_locs = dict(zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True))
         # get RFID reader locations
-        recent_rfid_query = (
-            acquisition.Experiment.proj() * streams.Device.proj() & key
-        ).aggr(
+        recent_rfid_query = (acquisition.Experiment.proj() * streams.Device.proj() & key).aggr(
             streams.RfidReader & f""rfid_reader_install_time <= '{block_start}'"",
             rfid_reader_install_time=""max(rfid_reader_install_time)"",
         )
@@ -1608,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1636,30 +1487,18 @@

 
         # For each ROI, compute if within ROI
         for roi in rois:
-            if (
-                roi == ""Corridor""
-            ):  # special case for corridor, based on between inner and outer radius
+            if roi == ""Corridor"":  # special case for corridor, based on between inner and outer radius
                 dist = np.linalg.norm(
                     (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - arena_center,
                     axis=1,
                 )
-                pos_eth_df[roi] = (dist >= arena_inner_radius) & (
-                    dist <= arena_outer_radius
-                )
+                pos_eth_df[roi] = (dist >= arena_inner_radius) & (dist <= arena_outer_radius)
             elif roi == ""Nest"":  # special case for nest, based on 4 corners
                 nest_corners = roi_locs[""NestRegion""][""ArrayOfPoint""]
-                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(
-                    nest_corners[0][""Y""]
-                )
-                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(
-                    nest_corners[1][""Y""]
-                )
-                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(
-                    nest_corners[2][""Y""]
-                )
-                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(
-                    nest_corners[3][""Y""]
-                )
+                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(nest_corners[0][""Y""])
+                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(nest_corners[1][""Y""])
+                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(nest_corners[2][""Y""])
+                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(nest_corners[3][""Y""])
                 pos_eth_df[roi] = (
                     (centroid_df[""x""] <= nest_br_x)
                     & (centroid_df[""y""] >= nest_br_y)
@@ -1673,13 +1512,10 @@

             else:
                 roi_radius = gate_radius if roi == ""Gate"" else patch_radius
                 # Get ROI coords
-                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(
-                    rfid_locs[roi + ""Rfid""][""Y""]
-                )
+                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(rfid_locs[roi + ""Rfid""][""Y""])
                 # Check if in ROI
                 dist = np.linalg.norm(
-                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T)
-                    - (roi_x, roi_y),
+                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - (roi_x, roi_y),
                     axis=1,
                 )
                 pos_eth_df[roi] = dist < roi_radius
@@ -1755,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1774,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1785,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1806,15 +1644,14 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data
     # pellet delivery trigger
@@ -1822,9 +1659,9 @@

         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
     # beambreak
-    beambreak_df = fetch_stream(
-        streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-    )[start:end]
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
     # patch threshold
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
@@ -1841,29 +1678,22 @@

         )
 
     # Step 2 - Remove invalid rows (back-to-back events)
-    BTB_TIME_DIFF = (
-        1.2  # pellet delivery trigger - time difference is less than 1.2 seconds
-    )
-    invalid_rows = (
-        delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_TIME_DIFF
-    )
+    BTB_MIN_TIME_DIFF = 1.2  # pellet delivery trigger - time diff is less than 1.2 seconds
+    BB_MIN_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
+    PT_MIN_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
+
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_MIN_TIME_DIFF
     delivered_pellet_df = delivered_pellet_df[~invalid_rows]
     # exclude manual deliveries
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    BB_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
-    invalid_rows = (
-        beambreak_df.index.to_series().diff().dt.total_seconds() < BB_TIME_DIFF
-    )
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < BB_MIN_TIME_DIFF
     beambreak_df = beambreak_df[~invalid_rows]
 
-    PT_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    invalid_rows = (
-        depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_TIME_DIFF
-    )
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_MIN_TIME_DIFF
     depletion_state_df = depletion_state_df[~invalid_rows]
 
     # Return empty if no data
@@ -1880,24 +1710,20 @@

             beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
-            tolerance=pd.Timedelta(""{BTB_TIME_DIFF}s""),
+            tolerance=pd.Timedelta(""{BTB_MIN_TIME_DIFF}s""),
             direction=""forward"",
         )
         .set_index(""time"")
         .dropna(subset=[""beam_break_timestamp""])
     )
-    pellet_beam_break_df.drop_duplicates(
-        subset=""beam_break_timestamp"", keep=""last"", inplace=True
-    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
     # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
             depletion_state_df.reset_index(),
-            pellet_beam_break_df.reset_index().rename(
-                columns={""time"": ""pellet_timestamp""}
-            ),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
             left_on=""time"",
             right_on=""pellet_timestamp"",
             tolerance=pd.Timedelta(""100ms""),
@@ -1910,12 +1736,8 @@

     # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
     # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
-    pellet_ts_threshold_df.pellet_timestamp = (
-        pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    )
-    pellet_ts_threshold_df.beam_break_timestamp = (
-        pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    )
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
     pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
         subset=[""pellet_timestamp"", ""beam_break_timestamp""]
     )
@@ -1942,12 +1764,8 @@

     Returns:
         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
-    max_inactive_time = (
-        pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
-    )
-    bout_data = pd.DataFrame(
-        columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""]
-    )
+    max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
     if subject_patch_data.empty:
         return bout_data
@@ -1991,52 +1809,34 @@

         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
         max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
-        )
-        foraging_mask = max_windowed_wheel_vals > (
-            patch_spun_df[""cum_wheel_dist""] + min_wheel_movement
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (
-            max_inactive_win_len - 1
-        )
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
         n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
         bout_end_indxs = (
             np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
             + (max_inactive_win_len - 1)
             + n_samples_in_1s
         )
-        bout_end_indxs[-1] = min(
-            bout_end_indxs[-1], len(wheel_ts) - 1
-        )  # ensure last bout ends in block
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
         # Remove bout that starts at block end
         if bout_start_indxs[-1] >= len(wheel_ts):
             bout_start_indxs = bout_start_indxs[:-1]
             bout_end_indxs = bout_end_indxs[:-1]
         if len(bout_start_indxs) != len(bout_end_indxs):
-            raise ValueError(
-                ""Mismatch between the lengths of bout_start_indxs and bout_end_indxs.""
-            )
-        bout_durations = (
-            wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]
-        ).astype(  # in seconds
+            raise ValueError(""Mismatch between the lengths of bout_start_indxs and bout_end_indxs."")
+        bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]""
-        ).astype(
-            float
-        ) / 1e9
+        ).astype(float) / 1e9
         bout_starts_ends = np.array(
             [
                 (wheel_ts[start_idx], wheel_ts[end_idx])
-                for start_idx, end_idx in zip(
-                    bout_start_indxs, bout_end_indxs, strict=True
-                )
+                for start_idx, end_idx in zip(bout_start_indxs, bout_end_indxs, strict=True)
             ]
         )
         all_pel_ts = np.sort(
-            np.concatenate(
-                [arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0]
-            )
+            np.concatenate([arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0])
         )
         bout_pellets = np.array(
             [
@@ -2050,8 +1850,7 @@

         bout_pellets = bout_pellets[bout_pellets >= min_pellets]
         bout_cum_wheel_dist = np.array(
             [
-                patch_spun_df.loc[end, ""cum_wheel_dist""]
-                - patch_spun_df.loc[start, ""cum_wheel_dist""]
+                patch_spun_df.loc[end, ""cum_wheel_dist""] - patch_spun_df.loc[start, ""cum_wheel_dist""]
                 for start, end in bout_starts_ends
             ]
         )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820609472,,2017,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,S101: Replaced assertions with exceptions to ensure expected behavior in all scenarios,"             bout_end_indxs = bout_end_indxs[:-1]
-        assert len(bout_start_indxs) == len(bout_end_indxs)
-        bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
+        if len(bout_start_indxs) != len(bout_end_indxs):","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -67,18 +60,14 @@

         # find the 0s in `pellet_ct` (these are times when the pellet count reset - i.e. new block)
         # that would mark the start of a new block
 
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
         exp_key = {""experiment_name"": key[""experiment_name""]}
 
         chunk_restriction = acquisition.create_chunk_restriction(
             key[""experiment_name""], chunk_start, chunk_end
         )
 
-        block_state_query = (
-            acquisition.Environment.BlockState & exp_key & chunk_restriction
-        )
+        block_state_query = acquisition.Environment.BlockState & exp_key & chunk_restriction
         block_state_df = fetch_stream(block_state_query)
         if block_state_df.empty:
             self.insert1(key)
@@ -94,19 +83,14 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
         if not blocks_df.empty:
             # calculate block end_times (use due_time) and durations
-            blocks_df[""end_time""] = blocks_df[""due_time""].apply(
-                lambda x: io_api.aeon(x)
-            )
-            blocks_df[""duration""] = (
-                blocks_df[""end_time""] - blocks_df.index
-            ).dt.total_seconds() / 3600
+            blocks_df[""end_time""] = blocks_df[""due_time""].apply(lambda x: io_api.aeon(x))
+            blocks_df[""duration""] = (blocks_df[""end_time""] - blocks_df.index).dt.total_seconds() / 3600
 
             for _, row in blocks_df.iterrows():
                 block_entries.append(
@@ -137,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time)."""""" # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -153,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -172,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -192,30 +182,36 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
-            if len(streams_table & chunk_keys) < len(
-                streams_table.key_source & chunk_keys
-            ):
+            if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
                 raise ValueError(
                     f""BlockAnalysis Not Ready - {streams_table.__name__}""
                     f""not yet fully ingested for block: {key}.""
                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
-
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], block_start, block_end
-        )
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
+
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
         patch_query = (
-            streams.UndergroundFeeder.join(
-                streams.UndergroundFeeder.RemovalTime, left=True
-            )
+            streams.UndergroundFeeder.join(streams.UndergroundFeeder.RemovalTime, left=True)
             & key
             & f'""{block_start}"" >= underground_feeder_install_time'
             & f'""{block_end}"" < IFNULL(underground_feeder_removal_time, ""2200-01-01"")'
@@ -229,14 +225,12 @@

                 streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
             )[block_start:block_end]
 
-            pellet_ts_threshold_df = get_threshold_associated_pellets(
-                patch_key, block_start, block_end
-            )
+            pellet_ts_threshold_df = get_threshold_associated_pellets(patch_key, block_start, block_end)
 
             # wheel encoder data
-            encoder_df = fetch_stream(
-                streams.UndergroundFeederEncoder & patch_key & chunk_restriction
-            )[block_start:block_end]
+            encoder_df = fetch_stream(streams.UndergroundFeederEncoder & patch_key & chunk_restriction)[
+                block_start:block_end
+            ]
             # filter out maintenance period based on logs
             pellet_ts_threshold_df = filter_out_maintenance_periods(
                 pellet_ts_threshold_df,
@@ -254,39 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(
-                    f""No depletion state data found for block {key} - patch: {patch_name}""
-                )
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(
-                encoder_df.angle
-            )
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -294,21 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[
-                        ::wheel_downsampling_factor
-                    ],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -320,46 +309,65 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(
-                    ""identity_name"", part_name=""anchor_part""
-                )
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
-            pos_df = filter_out_maintenance_periods(
-                pos_df, maintenance_period, block_end
-            )
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
+            pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
                 continue
 
             position_diff = np.sqrt(
-                np.square(np.diff(pos_df.x.astype(float)))
-                + np.square(np.diff(pos_df.y.astype(float)))
+                np.square(np.diff(pos_df.x.astype(float))) + np.square(np.diff(pos_df.y.astype(float)))
             )
             cumsum_distance_travelled = np.concatenate([[0], np.cumsum(position_diff)])
 
             # weights
-            weight_query = (
-                acquisition.Environment.SubjectWeight & key & chunk_restriction
-            )
+            weight_query = acquisition.Environment.SubjectWeight & key & chunk_restriction
             weight_df = fetch_stream(weight_query)[block_start:block_end]
             weight_df.query(f""subject_id == '{subject_name}'"", inplace=True)
 
@@ -384,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -412,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -447,10 +455,7 @@

         subjects_positions_df = pd.concat(
             [
                 pd.DataFrame(
-                    {
-                        ""subject_name"": [s[""subject_name""]]
-                        * len(s[""position_timestamps""])
-                    }
+                    {""subject_name"": [s[""subject_name""]] * len(s[""position_timestamps""])}
                     | {
                         k: s[k]
                         for k in (
@@ -466,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -478,8 +498,7 @@

             ""cum_pref_time"",
         ]
         all_subj_patch_pref_dict = {
-            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names}
-            for p in patch_names
+            p: {s: {a: pd.Series() for a in pref_attrs} for s in subject_names} for p in patch_names
         }
 
         for patch in block_patches:
@@ -502,15 +521,11 @@

             ).fetch1(""attribute_value"")
             patch_center = (int(patch_center[""X""]), int(patch_center[""Y""]))
             subjects_xy = subjects_positions_df[[""position_x"", ""position_y""]].values
-            dist_to_patch = np.sqrt(
-                np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float)
-            )
+            dist_to_patch = np.sqrt(np.sum((subjects_xy - patch_center) ** 2, axis=1).astype(float))
             dist_to_patch_df = subjects_positions_df[[""subject_name""]].copy()
             dist_to_patch_df[""dist_to_patch""] = dist_to_patch
 
-            dist_to_patch_wheel_ts_id_df = pd.DataFrame(
-                index=cum_wheel_dist.index, columns=subject_names
-            )
+            dist_to_patch_wheel_ts_id_df = pd.DataFrame(index=cum_wheel_dist.index, columns=subject_names)
             dist_to_patch_pel_ts_id_df = pd.DataFrame(
                 index=patch[""pellet_timestamps""], columns=subject_names
             )
@@ -518,12 +533,10 @@

                 # Find closest match between pose_df indices and wheel indices
                 if not dist_to_patch_wheel_ts_id_df.empty:
                     dist_to_patch_wheel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_wheel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_wheel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -532,18 +545,16 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""100ms""),
                     )
-                    dist_to_patch_wheel_ts_id_df[subject_name] = (
-                        dist_to_patch_wheel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_wheel_ts_id_df[subject_name] = dist_to_patch_wheel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
                 # Find closest match between pose_df indices and pel indices
                 if not dist_to_patch_pel_ts_id_df.empty:
                     dist_to_patch_pel_ts_subj = pd.merge_asof(
-                        left=pd.DataFrame(
-                            dist_to_patch_pel_ts_id_df[subject_name].copy()
-                        ).reset_index(names=""time""),
-                        right=dist_to_patch_df[
-                            dist_to_patch_df[""subject_name""] == subject_name
-                        ]
+                        left=pd.DataFrame(dist_to_patch_pel_ts_id_df[subject_name].copy()).reset_index(
+                            names=""time""
+                        ),
+                        right=dist_to_patch_df[dist_to_patch_df[""subject_name""] == subject_name]
                         .copy()
                         .reset_index(names=""time""),
                         on=""time"",
@@ -552,9 +563,9 @@

                         direction=""nearest"",
                         tolerance=pd.Timedelta(""200ms""),
                     )
-                    dist_to_patch_pel_ts_id_df[subject_name] = (
-                        dist_to_patch_pel_ts_subj[""dist_to_patch""].values
-                    )
+                    dist_to_patch_pel_ts_id_df[subject_name] = dist_to_patch_pel_ts_subj[
+                        ""dist_to_patch""
+                    ].values
 
             # Get closest subject to patch at each pellet timestep
             closest_subjects_pellet_ts = dist_to_patch_pel_ts_id_df.idxmin(axis=1)
@@ -566,12 +577,8 @@

             wheel_dist = cum_wheel_dist.diff().fillna(cum_wheel_dist.iloc[0])
             # Assign wheel dist to closest subject for each wheel timestep
             for subject_name in subject_names:
-                subj_idxs = cum_wheel_dist_subj_df[
-                    closest_subjects_wheel_ts == subject_name
-                ].index
-                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[
-                    subj_idxs
-                ]
+                subj_idxs = cum_wheel_dist_subj_df[closest_subjects_wheel_ts == subject_name].index
+                cum_wheel_dist_subj_df.loc[subj_idxs, subject_name] = wheel_dist[subj_idxs]
             cum_wheel_dist_subj_df = cum_wheel_dist_subj_df.cumsum(axis=0)
 
             # In patch time
@@ -579,14 +586,14 @@

             dt = np.median(np.diff(cum_wheel_dist.index)).astype(int) / 1e9  # s
             # Fill in `all_subj_patch_pref`
             for subject_name in subject_names:
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_dist""
-                ] = cum_wheel_dist_subj_df[subject_name].values
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_dist""] = (
+                    cum_wheel_dist_subj_df[subject_name].values
+                )
                 subject_in_patch = in_patch[subject_name]
                 subject_in_patch_cum_time = subject_in_patch.cumsum().values * dt
-                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][
-                    ""cum_time""
-                ] = subject_in_patch_cum_time
+                all_subj_patch_pref_dict[patch[""patch_name""]][subject_name][""cum_time""] = (
+                    subject_in_patch_cum_time
+                )
 
                 closest_subj_mask = closest_subjects_pellet_ts == subject_name
                 subj_pellets = closest_subjects_pellet_ts[closest_subj_mask]
@@ -597,14 +604,12 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
                         ""patch_threshold"": subj_patch_thresh,
-                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[
-                            subject_name
-                        ].values,
+                        ""wheel_cumsum_distance_travelled"": cum_wheel_dist_subj_df[subject_name].values,
                     }
                 )
 
@@ -613,75 +618,49 @@

         for subject_name in subject_names:
             # Get sum of subj cum wheel dists and cum in patch time
             all_cum_dist = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1]
-                    for p in patch_names
-                ]
+                [all_subj_patch_pref_dict[p][subject_name][""cum_dist""][-1] for p in patch_names]
             )
             all_cum_time = np.sum(
-                [
-                    all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1]
-                    for p in patch_names
-                ]
-            )
+                [all_subj_patch_pref_dict[p][subject_name][""cum_time""][-1] for p in patch_names]
+            )
+
+            CUM_PREF_DIST_MIN = 1e-3
+
             for patch_name in patch_names:
                 cum_pref_dist = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""]
-                    / all_cum_dist
-                )
-                CUM_PREF_DIST_MIN = 1e-3
-                cum_pref_dist = np.where(
-                    cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ] = cum_pref_dist
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_dist""] / all_cum_dist
+                )
+                cum_pref_dist = np.where(cum_pref_dist < CUM_PREF_DIST_MIN, 0, cum_pref_dist)
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""] = cum_pref_dist
 
                 cum_pref_time = (
-                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""]
-                    / all_cum_time
-                )
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ] = cum_pref_time
+                    all_subj_patch_pref_dict[patch_name][subject_name][""cum_time""] / all_cum_time
+                )
+                all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""] = cum_pref_time
 
             # sum pref at each ts across patches for each subject
             total_dist_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_dist""] for p in patch_names]
                 ),
                 axis=0,
             )
             total_time_pref = np.sum(
                 np.vstack(
-                    [
-                        all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""]
-                        for p in patch_names
-                    ]
+                    [all_subj_patch_pref_dict[p][subject_name][""cum_pref_time""] for p in patch_names]
                 ),
                 axis=0,
             )
             for patch_name in patch_names:
-                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_dist""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_dist_pref""
-                ] = np.divide(
+                cum_pref_dist = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_dist""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_dist_pref""] = np.divide(
                     cum_pref_dist,
                     total_dist_pref,
                     out=np.zeros_like(cum_pref_dist),
                     where=total_dist_pref != 0,
                 )
-                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""cum_pref_time""
-                ]
-                all_subj_patch_pref_dict[patch_name][subject_name][
-                    ""running_time_pref""
-                ] = np.divide(
+                cum_pref_time = all_subj_patch_pref_dict[patch_name][subject_name][""cum_pref_time""]
+                all_subj_patch_pref_dict[patch_name][subject_name][""running_time_pref""] = np.divide(
                     cum_pref_time,
                     total_time_pref,
                     out=np.zeros_like(cum_pref_time),
@@ -693,24 +672,12 @@

             | {
                 ""patch_name"": p,
                 ""subject_name"": s,
-                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ],
-                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ],
-                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""running_time_pref""
-                ],
-                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""running_dist_pref""
-                ],
-                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_time""
-                ][-1],
-                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][
-                    ""cum_pref_dist""
-                ][-1],
+                ""cumulative_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""],
+                ""cumulative_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""],
+                ""running_preference_by_time"": all_subj_patch_pref_dict[p][s][""running_time_pref""],
+                ""running_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""running_dist_pref""],
+                ""final_preference_by_time"": all_subj_patch_pref_dict[p][s][""cum_pref_time""][-1],
+                ""final_preference_by_wheel"": all_subj_patch_pref_dict[p][s][""cum_pref_dist""][-1],
             }
             for p, s in itertools.product(patch_names, subject_names)
         )
@@ -734,9 +701,7 @@

     def make(self, key):
         """"""Compute and plot various block-level statistics and visualizations.""""""
         # Define subject colors and patch styling for plotting
-        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(
-            ""subject"", order_by=""subject""
-        )
+        exp_subject_names = (acquisition.Experiment.Subject & key).fetch(""subject"", order_by=""subject"")
         if not len(exp_subject_names):
             raise ValueError(
                 ""No subjects found in the `acquisition.Experiment.Subject`, missing a manual insert step?.""
@@ -755,10 +720,7 @@

         # Figure 1 - Patch stats: patch means and pellet threshold boxplots
         # ---
         subj_patch_info = (
-            (
-                BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"")
-                & key
-            )
+            (BlockSubjectAnalysis.Patch.proj(""pellet_timestamps"", ""patch_threshold"") & key)
             .fetch(format=""frame"")
             .reset_index()
         )
@@ -772,46 +734,28 @@

             [""patch_name"", ""subject_name"", ""pellet_timestamps"", ""patch_threshold""]
         ]
         min_subj_patch_info = (
-            min_subj_patch_info.explode(
-                [""pellet_timestamps"", ""patch_threshold""], ignore_index=True
-            )
+            min_subj_patch_info.explode([""pellet_timestamps"", ""patch_threshold""], ignore_index=True)
             .dropna()
             .reset_index(drop=True)
         )
         # Rename and reindex columns
         min_subj_patch_info.columns = [""patch"", ""subject"", ""time"", ""threshold""]
-        min_subj_patch_info = min_subj_patch_info.reindex(
-            columns=[""time"", ""patch"", ""threshold"", ""subject""]
-        )
+        min_subj_patch_info = min_subj_patch_info.reindex(columns=[""time"", ""patch"", ""threshold"", ""subject""])
         # Add patch mean values and block-normalized delivery times to pellet info
         n_patches = len(patch_info)
-        patch_mean_info = pd.DataFrame(
-            index=np.arange(n_patches), columns=min_subj_patch_info.columns
-        )
+        patch_mean_info = pd.DataFrame(index=np.arange(n_patches), columns=min_subj_patch_info.columns)
         patch_mean_info[""subject""] = ""mean""
         patch_mean_info[""patch""] = [d[""patch_name""] for d in patch_info]
-        patch_mean_info[""threshold""] = [
-            ((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info
-        ]
+        patch_mean_info[""threshold""] = [((1 / d[""patch_rate""]) + d[""patch_offset""]) for d in patch_info]
         patch_mean_info[""time""] = subj_patch_info[""block_start""][0]
-        min_subj_patch_info_plus = pd.concat(
-            (patch_mean_info, min_subj_patch_info)
-        ).reset_index(drop=True)
+        min_subj_patch_info_plus = pd.concat((patch_mean_info, min_subj_patch_info)).reset_index(drop=True)
         min_subj_patch_info_plus[""norm_time""] = (
-            (
-                min_subj_patch_info_plus[""time""]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
-            / (
-                min_subj_patch_info_plus[""time""].iloc[-1]
-                - min_subj_patch_info_plus[""time""].iloc[0]
-            )
+            (min_subj_patch_info_plus[""time""] - min_subj_patch_info_plus[""time""].iloc[0])
+            / (min_subj_patch_info_plus[""time""].iloc[-1] - min_subj_patch_info_plus[""time""].iloc[0])
         ).round(3)
 
         # Plot it
-        box_colors = [""#0A0A0A""] + list(
-            subject_colors_dict.values()
-        )  # subject colors + mean color
+        box_colors = [""#0A0A0A""] + list(subject_colors_dict.values())  # subject colors + mean color
         patch_stats_fig = px.box(
             min_subj_patch_info_plus.sort_values(""patch""),
             x=""patch"",
@@ -841,9 +785,7 @@

             .dropna()
             .reset_index(drop=True)
         )
-        weights_block.drop(
-            columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore""
-        )
+        weights_block.drop(columns=[""experiment_name"", ""block_start""], inplace=True, errors=""ignore"")
         weights_block.rename(columns={""weight_timestamps"": ""time""}, inplace=True)
         weights_block.set_index(""time"", inplace=True)
         weights_block.sort_index(inplace=True)
@@ -867,17 +809,13 @@

         # Figure 3 - Cumulative pellet count: over time, per subject, markered by patch
         # ---
         # Create dataframe with cumulative pellet count per subject
-        cum_pel_ct = (
-            min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
-        )
+        cum_pel_ct = min_subj_patch_info_plus.sort_values(""time"").copy().reset_index(drop=True)
         patch_means = cum_pel_ct.loc[0:3][[""patch"", ""threshold""]].rename(
             columns={""threshold"": ""mean_thresh""}
         )
         patch_means[""mean_thresh""] = patch_means[""mean_thresh""].astype(float).round(1)
         cum_pel_ct = cum_pel_ct.merge(patch_means, on=""patch"", how=""left"")
-        cum_pel_ct = cum_pel_ct[
-            ~cum_pel_ct[""subject""].str.contains(""mean"")
-        ].reset_index(drop=True)
+        cum_pel_ct = cum_pel_ct[~cum_pel_ct[""subject""].str.contains(""mean"")].reset_index(drop=True)
         cum_pel_ct = (
             cum_pel_ct.groupby(""subject"", group_keys=False)
             .apply(lambda group: group.assign(counter=np.arange(len(group)) + 1))
@@ -887,9 +825,7 @@

         make_float_cols = [""threshold"", ""mean_thresh"", ""norm_time""]
         cum_pel_ct[make_float_cols] = cum_pel_ct[make_float_cols].astype(float)
         cum_pel_ct[""patch_label""] = (
-            cum_pel_ct[""patch""]
-            + "" μ: ""
-            + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
+            cum_pel_ct[""patch""] + "" μ: "" + cum_pel_ct[""mean_thresh""].astype(float).round(1).astype(str)
         )
         cum_pel_ct[""norm_thresh_val""] = (
             (cum_pel_ct[""threshold""] - cum_pel_ct[""threshold""].min())
@@ -919,9 +855,7 @@

                     mode=""markers"",
                     marker={
                         ""symbol"": patch_markers_dict[patch_grp[""patch""].iloc[0]],
-                        ""color"": gen_hex_grad(
-                            pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                        ),
+                        ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                         ""size"": 8,
                     },
                     name=patch_val,
@@ -941,9 +875,7 @@

         cum_pel_per_subject_fig = go.Figure()
         for id_val, id_grp in cum_pel_ct.groupby(""subject""):
             for patch_val, patch_grp in id_grp.groupby(""patch""):
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_val][""mean_thresh""].values[0]
                 cur_p = patch_val.replace(""Patch"", ""P"")
                 cum_pel_per_subject_fig.add_trace(
                     go.Scatter(
@@ -958,9 +890,7 @@

                         # line=dict(width=2, color=subject_colors_dict[id_val]),
                         marker={
                             ""symbol"": patch_markers_dict[patch_val],
-                            ""color"": gen_hex_grad(
-                                pel_mrkr_col, patch_grp[""norm_thresh_val""]
-                            ),
+                            ""color"": gen_hex_grad(pel_mrkr_col, patch_grp[""norm_thresh_val""]),
                             ""size"": 8,
                         },
                         name=f""{id_val} - {cur_p} - μ: {cur_p_mean}"",
@@ -977,9 +907,7 @@

         # Figure 5 - Cumulative wheel distance: over time, per subject-patch
         # ---
         # Get wheel timestamps for each patch
-        wheel_ts = (BlockAnalysis.Patch & key).fetch(
-            ""patch_name"", ""wheel_timestamps"", as_dict=True
-        )
+        wheel_ts = (BlockAnalysis.Patch & key).fetch(""patch_name"", ""wheel_timestamps"", as_dict=True)
         wheel_ts = {d[""patch_name""]: d[""wheel_timestamps""] for d in wheel_ts}
         # Get subject patch data
         subj_wheel_cumsum_dist = (BlockSubjectAnalysis.Patch & key).fetch(
@@ -999,9 +927,7 @@

         for subj in subject_names:
             for patch_name in patch_names:
                 cur_cum_wheel_dist = subj_wheel_cumsum_dist[(subj, patch_name)]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 cum_wheel_dist_fig.add_trace(
                     go.Scatter(
@@ -1018,10 +944,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1040,15 +963,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1062,14 +981,10 @@

         # ---
         # Get and format a dataframe with preference data
         patch_pref = (BlockSubjectAnalysis.Preference & key).fetch(format=""frame"")
-        patch_pref.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        patch_pref.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         # Replace small vals with 0
         small_pref_thresh = 1e-3
-        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[
-            ""cumulative_preference_by_wheel""
-        ].apply(
+        patch_pref[""cumulative_preference_by_wheel""] = patch_pref[""cumulative_preference_by_wheel""].apply(
             lambda arr: np.where(np.array(arr) < small_pref_thresh, 0, np.array(arr))
         )
 
@@ -1077,18 +992,14 @@

             # Sum pref at each ts
             total_pref = np.sum(np.vstack(group[pref_col].values), axis=0)
             # Calculate running pref
-            group[out_col] = group[pref_col].apply(
-                lambda x: np.nan_to_num(x / total_pref, 0.0)
-            )
+            group[out_col] = group[pref_col].apply(lambda x: np.nan_to_num(x / total_pref, 0.0))
             return group
 
         patch_pref = (
             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1108,12 +1019,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_wheel""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_wheel_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_wheel""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_wheel_plot.add_trace(
                     go.Scatter(
@@ -1130,10 +1037,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1152,15 +1056,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1176,12 +1076,8 @@

         # Add trace for each subject-patch combo
         for subj in subject_names:
             for patch_name in patch_names:
-                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][
-                    ""running_preference_by_time""
-                ]
-                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][
-                    ""mean_thresh""
-                ].values[0]
+                cur_run_time_pref = patch_pref.loc[patch_name].loc[subj][""running_preference_by_time""]
+                cur_p_mean = patch_means[patch_means[""patch""] == patch_name][""mean_thresh""].values[0]
                 cur_p = patch_name.replace(""Patch"", ""P"")
                 running_pref_by_patch_fig.add_trace(
                     go.Scatter(
@@ -1198,10 +1094,7 @@

                 )
                 # Add markers for each pellet
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == subj)
-                        & (cum_pel_ct[""patch""] == patch_name)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == subj) & (cum_pel_ct[""patch""] == patch_name)],
                     pd.DataFrame(
                         {
                             ""time"": wheel_ts[patch_name],
@@ -1220,15 +1113,11 @@

                             mode=""markers"",
                             marker={
                                 ""symbol"": patch_markers_dict[patch_name],
-                                ""color"": gen_hex_grad(
-                                    pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]
-                                ),
+                                ""color"": gen_hex_grad(pel_mrkr_col, cur_cum_pel_ct[""norm_thresh_val""]),
                                 ""size"": 8,
                             },
                             name=f""{subj} - {cur_p} pellets"",
-                            customdata=np.stack(
-                                (cur_cum_pel_ct[""threshold""],), axis=-1
-                            ),
+                            customdata=np.stack((cur_cum_pel_ct[""threshold""],), axis=-1),
                             hovertemplate=""Threshold: %{customdata[0]:.2f} cm"",
                         )
                     )
@@ -1242,9 +1131,7 @@

         # Figure 8 - Weighted patch preference: weighted by 'wheel_dist_spun : pel_ct' ratio
         # ---
         # Create multi-indexed dataframe with weighted distance for each subject-patch pair
-        pel_patches = [
-            p for p in patch_names if ""dummy"" not in p.lower()
-        ]  # exclude dummy patches
+        pel_patches = [p for p in patch_names if ""dummy"" not in p.lower()]  # exclude dummy patches
         data = []
         for patch in pel_patches:
             for subject in subject_names:
@@ -1257,16 +1144,12 @@

                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
         subj_wheel_pel_weighted_dist[""weighted_dist""] = np.nan
 
         # Calculate weighted distance
         subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
-        subject_patch_data.reset_index(
-            level=[""experiment_name"", ""block_start""], drop=True, inplace=True
-        )
+        subject_patch_data.reset_index(level=[""experiment_name"", ""block_start""], drop=True, inplace=True)
         subj_wheel_pel_weighted_dist = defaultdict(lambda: defaultdict(dict))
         for s in subject_names:
             for p in pel_patches:
@@ -1274,14 +1157,11 @@

                 cur_wheel_cum_dist_df = pd.DataFrame(columns=[""time"", ""cum_wheel_dist""])
                 cur_wheel_cum_dist_df[""time""] = wheel_ts[p]
                 cur_wheel_cum_dist_df[""cum_wheel_dist""] = (
-                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""]
-                    + 1
+                    subject_patch_data.loc[p].loc[s][""wheel_cumsum_distance_travelled""] + 1
                 )
                 # Get cumulative pellet count
                 cur_cum_pel_ct = pd.merge_asof(
-                    cum_pel_ct[
-                        (cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)
-                    ],
+                    cum_pel_ct[(cum_pel_ct[""subject""] == s) & (cum_pel_ct[""patch""] == p)],
                     cur_wheel_cum_dist_df.sort_values(""time""),
                     on=""time"",
                     direction=""forward"",
@@ -1300,9 +1180,7 @@

                         on=""time"",
                         direction=""forward"",
                     )
-                    max_weight = (
-                        cur_cum_pel_ct.iloc[-1][""counter""] + 1
-                    )  # for values after last pellet
+                    max_weight = cur_cum_pel_ct.iloc[-1][""counter""] + 1  # for values after last pellet
                     merged_df[""counter""] = merged_df[""counter""].fillna(max_weight)
                     merged_df[""weighted_cum_wheel_dist""] = (
                         merged_df.groupby(""counter"")
@@ -1313,9 +1191,7 @@

                 else:
                     weighted_dist = cur_wheel_cum_dist_df[""cum_wheel_dist""].values
                 # Assign to dict
-                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[
-                    ""time""
-                ].values
+                subj_wheel_pel_weighted_dist[p][s][""time""] = cur_wheel_cum_dist_df[""time""].values
                 subj_wheel_pel_weighted_dist[p][s][""weighted_dist""] = weighted_dist
         # Convert back to dataframe
         data = []
@@ -1326,15 +1202,11 @@

                         ""patch_name"": p,
                         ""subject_name"": s,
                         ""time"": subj_wheel_pel_weighted_dist[p][s][""time""],
-                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][
-                            ""weighted_dist""
-                        ],
+                        ""weighted_dist"": subj_wheel_pel_weighted_dist[p][s][""weighted_dist""],
                     }
                 )
         subj_wheel_pel_weighted_dist = pd.DataFrame(data)
-        subj_wheel_pel_weighted_dist.set_index(
-            [""patch_name"", ""subject_name""], inplace=True
-        )
+        subj_wheel_pel_weighted_dist.set_index([""patch_name"", ""subject_name""], inplace=True)
 
         # Calculate normalized weighted value
         def norm_inv_norm(group):
@@ -1343,28 +1215,20 @@

             inv_norm_dist = 1 / norm_dist
             inv_norm_dist = inv_norm_dist / (np.sum(inv_norm_dist, axis=0))
             # Map each inv_norm_dist back to patch name.
-            return pd.Series(
-                inv_norm_dist.tolist(), index=group.index, name=""norm_value""
-            )
+            return pd.Series(inv_norm_dist.tolist(), index=group.index, name=""norm_value"")
 
         subj_wheel_pel_weighted_dist[""norm_value""] = (
             subj_wheel_pel_weighted_dist.groupby(""subject_name"")
             .apply(norm_inv_norm)
             .reset_index(level=0, drop=True)
         )
-        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[
-            ""running_preference_by_wheel""
-        ]
+        subj_wheel_pel_weighted_dist[""wheel_pref""] = patch_pref[""running_preference_by_wheel""]
 
         # Plot it
         weighted_patch_pref_fig = make_subplots(
             rows=len(pel_patches),
             cols=len(subject_names),
-            subplot_titles=[
-                f""{patch} - {subject}""
-                for patch in pel_patches
-                for subject in subject_names
-            ],
+            subplot_titles=[f""{patch} - {subject}"" for patch in pel_patches for subject in subject_names],
             specs=[[{""secondary_y"": True}] * len(subject_names)] * len(pel_patches),
             shared_xaxes=True,
             vertical_spacing=0.1,
@@ -1546,9 +1410,7 @@

         for id_val, id_grp in centroid_df.groupby(""identity_name""):
             # Add counts of x,y points to a grid that will be used for heatmap
             img_grid = np.zeros((max_x + 1, max_y + 1))
-            points, counts = np.unique(
-                id_grp[[""x"", ""y""]].values, return_counts=True, axis=0
-            )
+            points, counts = np.unique(id_grp[[""x"", ""y""]].values, return_counts=True, axis=0)
             for point, count in zip(points, counts, strict=True):
                 img_grid[point[0], point[1]] = count
             img_grid /= img_grid.max()  # normalize
@@ -1557,9 +1419,7 @@

             # so 45 cm/frame ~= 9 px/frame
             win_sz = 9  # in pixels  (ensure odd for centering)
             kernel = np.ones((win_sz, win_sz)) / win_sz**2  # moving avg kernel
-            img_grid_p = np.pad(
-                img_grid, win_sz // 2, mode=""edge""
-            )  # pad for full output from convolution
+            img_grid_p = np.pad(img_grid, win_sz // 2, mode=""edge"")  # pad for full output from convolution
             img_grid_smooth = conv2d(img_grid_p, kernel)
             heatmaps.append((id_val, img_grid_smooth))
 
@@ -1588,17 +1448,11 @@

         # Figure 3 - Position ethogram
         # ---
         # Get Active Region (ROI) locations
-        epoch_query = acquisition.Epoch & (
-            acquisition.Chunk & key & chunk_restriction
-        ).proj(""epoch_start"")
+        epoch_query = acquisition.Epoch & (acquisition.Chunk & key & chunk_restriction).proj(""epoch_start"")
         active_region_query = acquisition.EpochConfig.ActiveRegion & epoch_query
-        roi_locs = dict(
-            zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True)
-        )
+        roi_locs = dict(zip(*active_region_query.fetch(""region_name"", ""region_data""), strict=True))
         # get RFID reader locations
-        recent_rfid_query = (
-            acquisition.Experiment.proj() * streams.Device.proj() & key
-        ).aggr(
+        recent_rfid_query = (acquisition.Experiment.proj() * streams.Device.proj() & key).aggr(
             streams.RfidReader & f""rfid_reader_install_time <= '{block_start}'"",
             rfid_reader_install_time=""max(rfid_reader_install_time)"",
         )
@@ -1608,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1636,30 +1487,18 @@

 
         # For each ROI, compute if within ROI
         for roi in rois:
-            if (
-                roi == ""Corridor""
-            ):  # special case for corridor, based on between inner and outer radius
+            if roi == ""Corridor"":  # special case for corridor, based on between inner and outer radius
                 dist = np.linalg.norm(
                     (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - arena_center,
                     axis=1,
                 )
-                pos_eth_df[roi] = (dist >= arena_inner_radius) & (
-                    dist <= arena_outer_radius
-                )
+                pos_eth_df[roi] = (dist >= arena_inner_radius) & (dist <= arena_outer_radius)
             elif roi == ""Nest"":  # special case for nest, based on 4 corners
                 nest_corners = roi_locs[""NestRegion""][""ArrayOfPoint""]
-                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(
-                    nest_corners[0][""Y""]
-                )
-                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(
-                    nest_corners[1][""Y""]
-                )
-                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(
-                    nest_corners[2][""Y""]
-                )
-                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(
-                    nest_corners[3][""Y""]
-                )
+                nest_br_x, nest_br_y = int(nest_corners[0][""X""]), int(nest_corners[0][""Y""])
+                nest_bl_x, nest_bl_y = int(nest_corners[1][""X""]), int(nest_corners[1][""Y""])
+                nest_tl_x, nest_tl_y = int(nest_corners[2][""X""]), int(nest_corners[2][""Y""])
+                nest_tr_x, nest_tr_y = int(nest_corners[3][""X""]), int(nest_corners[3][""Y""])
                 pos_eth_df[roi] = (
                     (centroid_df[""x""] <= nest_br_x)
                     & (centroid_df[""y""] >= nest_br_y)
@@ -1673,13 +1512,10 @@

             else:
                 roi_radius = gate_radius if roi == ""Gate"" else patch_radius
                 # Get ROI coords
-                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(
-                    rfid_locs[roi + ""Rfid""][""Y""]
-                )
+                roi_x, roi_y = int(rfid_locs[roi + ""Rfid""][""X""]), int(rfid_locs[roi + ""Rfid""][""Y""])
                 # Check if in ROI
                 dist = np.linalg.norm(
-                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T)
-                    - (roi_x, roi_y),
+                    (np.vstack((centroid_df[""x""], centroid_df[""y""])).T) - (roi_x, roi_y),
                     axis=1,
                 )
                 pos_eth_df[roi] = dist < roi_radius
@@ -1755,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1774,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1785,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1806,15 +1644,14 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
-    chunk_restriction = acquisition.create_chunk_restriction(
-        patch_key[""experiment_name""], start, end
-    )
+    """"""
+    chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data
     # pellet delivery trigger
@@ -1822,9 +1659,9 @@

         streams.UndergroundFeederDeliverPellet & patch_key & chunk_restriction
     )[start:end]
     # beambreak
-    beambreak_df = fetch_stream(
-        streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction
-    )[start:end]
+    beambreak_df = fetch_stream(streams.UndergroundFeederBeamBreak & patch_key & chunk_restriction)[
+        start:end
+    ]
     # patch threshold
     depletion_state_df = fetch_stream(
         streams.UndergroundFeederDepletionState & patch_key & chunk_restriction
@@ -1841,29 +1678,22 @@

         )
 
     # Step 2 - Remove invalid rows (back-to-back events)
-    BTB_TIME_DIFF = (
-        1.2  # pellet delivery trigger - time difference is less than 1.2 seconds
-    )
-    invalid_rows = (
-        delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_TIME_DIFF
-    )
+    BTB_MIN_TIME_DIFF = 1.2  # pellet delivery trigger - time diff is less than 1.2 seconds
+    BB_MIN_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
+    PT_MIN_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
+
+    invalid_rows = delivered_pellet_df.index.to_series().diff().dt.total_seconds() < BTB_MIN_TIME_DIFF
     delivered_pellet_df = delivered_pellet_df[~invalid_rows]
     # exclude manual deliveries
     delivered_pellet_df = delivered_pellet_df.loc[
         delivered_pellet_df.index.difference(manual_delivery_df.index)
     ]
 
-    BB_TIME_DIFF = 1.0  # beambreak - time difference is less than 1 seconds
-    invalid_rows = (
-        beambreak_df.index.to_series().diff().dt.total_seconds() < BB_TIME_DIFF
-    )
+    invalid_rows = beambreak_df.index.to_series().diff().dt.total_seconds() < BB_MIN_TIME_DIFF
     beambreak_df = beambreak_df[~invalid_rows]
 
-    PT_TIME_DIFF = 1.0  # patch threshold - time difference is less than 1 seconds
     depletion_state_df = depletion_state_df.dropna(subset=[""threshold""])
-    invalid_rows = (
-        depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_TIME_DIFF
-    )
+    invalid_rows = depletion_state_df.index.to_series().diff().dt.total_seconds() < PT_MIN_TIME_DIFF
     depletion_state_df = depletion_state_df[~invalid_rows]
 
     # Return empty if no data
@@ -1880,24 +1710,20 @@

             beambreak_df.reset_index().rename(columns={""time"": ""beam_break_timestamp""}),
             left_on=""time"",
             right_on=""beam_break_timestamp"",
-            tolerance=pd.Timedelta(""{BTB_TIME_DIFF}s""),
+            tolerance=pd.Timedelta(""{BTB_MIN_TIME_DIFF}s""),
             direction=""forward"",
         )
         .set_index(""time"")
         .dropna(subset=[""beam_break_timestamp""])
     )
-    pellet_beam_break_df.drop_duplicates(
-        subset=""beam_break_timestamp"", keep=""last"", inplace=True
-    )
+    pellet_beam_break_df.drop_duplicates(subset=""beam_break_timestamp"", keep=""last"", inplace=True)
 
     # Find pellet delivery triggers that approximately coincide with each threshold update
     # i.e. nearest pellet delivery within 100ms before or after threshold update
     pellet_ts_threshold_df = (
         pd.merge_asof(
             depletion_state_df.reset_index(),
-            pellet_beam_break_df.reset_index().rename(
-                columns={""time"": ""pellet_timestamp""}
-            ),
+            pellet_beam_break_df.reset_index().rename(columns={""time"": ""pellet_timestamp""}),
             left_on=""time"",
             right_on=""pellet_timestamp"",
             tolerance=pd.Timedelta(""100ms""),
@@ -1910,12 +1736,8 @@

     # Clean up the df
     pellet_ts_threshold_df = pellet_ts_threshold_df.drop(columns=[""event_x"", ""event_y""])
     # Shift back the pellet_timestamp values by 1 to match with the previous threshold update
-    pellet_ts_threshold_df.pellet_timestamp = (
-        pellet_ts_threshold_df.pellet_timestamp.shift(-1)
-    )
-    pellet_ts_threshold_df.beam_break_timestamp = (
-        pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
-    )
+    pellet_ts_threshold_df.pellet_timestamp = pellet_ts_threshold_df.pellet_timestamp.shift(-1)
+    pellet_ts_threshold_df.beam_break_timestamp = pellet_ts_threshold_df.beam_break_timestamp.shift(-1)
     pellet_ts_threshold_df = pellet_ts_threshold_df.dropna(
         subset=[""pellet_timestamp"", ""beam_break_timestamp""]
     )
@@ -1942,12 +1764,8 @@

     Returns:
         DataFrame containing foraging bouts. Columns: duration, n_pellets, cum_wheel_dist, subject.
     """"""
-    max_inactive_time = (
-        pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
-    )
-    bout_data = pd.DataFrame(
-        columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""]
-    )
+    max_inactive_time = pd.Timedelta(seconds=60) if max_inactive_time is None else max_inactive_time
+    bout_data = pd.DataFrame(columns=[""start"", ""end"", ""n_pellets"", ""cum_wheel_dist"", ""subject""])
     subject_patch_data = (BlockSubjectAnalysis.Patch() & key).fetch(format=""frame"")
     if subject_patch_data.empty:
         return bout_data
@@ -1991,52 +1809,34 @@

         wheel_s_r = pd.Timedelta(wheel_ts[1] - wheel_ts[0], unit=""ns"")
         max_inactive_win_len = int(max_inactive_time / wheel_s_r)
         # Find times when foraging
-        max_windowed_wheel_vals = (
-            patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
-        )
-        foraging_mask = max_windowed_wheel_vals > (
-            patch_spun_df[""cum_wheel_dist""] + min_wheel_movement
-        )
+        max_windowed_wheel_vals = patch_spun_df[""cum_wheel_dist""].shift(-(max_inactive_win_len - 1)).ffill()
+        foraging_mask = max_windowed_wheel_vals > (patch_spun_df[""cum_wheel_dist""] + min_wheel_movement)
         # Discretize into foraging bouts
-        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (
-            max_inactive_win_len - 1
-        )
+        bout_start_indxs = np.where(np.diff(foraging_mask, prepend=0) == 1)[0] + (max_inactive_win_len - 1)
         n_samples_in_1s = int(1 / wheel_s_r.total_seconds())
         bout_end_indxs = (
             np.where(np.diff(foraging_mask, prepend=0) == -1)[0]
             + (max_inactive_win_len - 1)
             + n_samples_in_1s
         )
-        bout_end_indxs[-1] = min(
-            bout_end_indxs[-1], len(wheel_ts) - 1
-        )  # ensure last bout ends in block
+        bout_end_indxs[-1] = min(bout_end_indxs[-1], len(wheel_ts) - 1)  # ensure last bout ends in block
         # Remove bout that starts at block end
         if bout_start_indxs[-1] >= len(wheel_ts):
             bout_start_indxs = bout_start_indxs[:-1]
             bout_end_indxs = bout_end_indxs[:-1]
         if len(bout_start_indxs) != len(bout_end_indxs):
-            raise ValueError(
-                ""Mismatch between the lengths of bout_start_indxs and bout_end_indxs.""
-            )
-        bout_durations = (
-            wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]
-        ).astype(  # in seconds
+            raise ValueError(""Mismatch between the lengths of bout_start_indxs and bout_end_indxs."")
+        bout_durations = (wheel_ts[bout_end_indxs] - wheel_ts[bout_start_indxs]).astype(  # in seconds
             ""timedelta64[ns]""
-        ).astype(
-            float
-        ) / 1e9
+        ).astype(float) / 1e9
         bout_starts_ends = np.array(
             [
                 (wheel_ts[start_idx], wheel_ts[end_idx])
-                for start_idx, end_idx in zip(
-                    bout_start_indxs, bout_end_indxs, strict=True
-                )
+                for start_idx, end_idx in zip(bout_start_indxs, bout_end_indxs, strict=True)
             ]
         )
         all_pel_ts = np.sort(
-            np.concatenate(
-                [arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0]
-            )
+            np.concatenate([arr for arr in cur_subject_data[""pellet_timestamps""] if len(arr) > 0])
         )
         bout_pellets = np.array(
             [
@@ -2050,8 +1850,7 @@

         bout_pellets = bout_pellets[bout_pellets >= min_pellets]
         bout_cum_wheel_dist = np.array(
             [
-                patch_spun_df.loc[end, ""cum_wheel_dist""]
-                - patch_spun_df.loc[start, ""cum_wheel_dist""]
+                patch_spun_df.loc[end, ""cum_wheel_dist""] - patch_spun_df.loc[start, ""cum_wheel_dist""]
                 for start, end in bout_starts_ends
             ]
         )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820610908,,146,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit.py,,Updated `datetime.utcnow()` due to deprecation issue,"         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.datetime.now() if start else ""2200-01-01""
+        end = datetime.now(timezone.utc) if start else ""2200-01-01""","--- 

+++ 

@@ -1,7 +1,7 @@

 """"""Module for visit-related tables in the analysis schema.""""""
 
-from datetime import datetime, timezone
 from collections import deque
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -69,15 +69,15 @@

     @property
     def key_source(self):
         """"""Key source for OverlapVisit.""""""
-        return dj.U(""experiment_name"", ""place"", ""overlap_start"") & (
-            Visit & VisitEnd
-        ).proj(overlap_start=""visit_start"")
+        return dj.U(""experiment_name"", ""place"", ""overlap_start"") & (Visit & VisitEnd).proj(
+            overlap_start=""visit_start""
+        )
 
     def make(self, key):
         """"""Populate OverlapVisit table with overlapping visits.""""""
-        visit_starts, visit_ends = (
-            Visit * VisitEnd & key & {""visit_start"": key[""overlap_start""]}
-        ).fetch(""visit_start"", ""visit_end"")
+        visit_starts, visit_ends = (Visit * VisitEnd & key & {""visit_start"": key[""overlap_start""]}).fetch(
+            ""visit_start"", ""visit_end""
+        )
         visit_start = min(visit_starts)
         visit_end = max(visit_ends)
 
@@ -91,9 +91,7 @@

             if len(overlap_query) <= 1:
                 break
             overlap_visits.extend(
-                overlap_query.proj(overlap_start=f'""{key[""overlap_start""]}""').fetch(
-                    as_dict=True
-                )
+                overlap_query.proj(overlap_start=f'""{key[""overlap_start""]}""').fetch(as_dict=True)
             )
             visit_starts, visit_ends = overlap_query.fetch(""visit_start"", ""visit_end"")
             if visit_start == max(visit_starts) and visit_end == max(visit_ends):
@@ -107,10 +105,7 @@

                 {
                     **key,
                     ""overlap_end"": visit_end,
-                    ""overlap_duration"": (
-                        visit_end - key[""overlap_start""]
-                    ).total_seconds()
-                    / 3600,
+                    ""overlap_duration"": (visit_end - key[""overlap_start""]).total_seconds() / 3600,
                     ""subject_count"": len({v[""subject""] for v in overlap_visits}),
                 }
             )
@@ -121,15 +116,16 @@

 
 
 def ingest_environment_visits(experiment_names: list | None = None):
-    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
-
-    This ingestion routine handles only those ""complete"" visits,
-    not ingesting any ""on-going"" visits using ""analyze"" method:
-    `aeon.analyze.utils.visits()`.
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
 
     Args:
         experiment_names (list, optional): list of names of the experiment
-        to populate into the Visit table. Defaults to None.
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
     """"""
     if experiment_names is None:
         experiment_names = [""exp0.2-r0""]
@@ -143,7 +139,7 @@

             .fetch(""last_visit"")
         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.now(timezone.utc) if start else ""2200-01-01""
+        end = datetime.now(UTC) if start else ""2200-01-01""
 
         enter_exit_query = (
             acquisition.SubjectEnterExit.Time * acquisition.EventType
@@ -158,10 +154,7 @@

         enter_exit_df = pd.DataFrame(
             zip(
                 *enter_exit_query.fetch(
-                    ""subject"",
-                    ""enter_exit_time"",
-                    ""event_type"",
-                    order_by=""enter_exit_time"",
+                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                 ),
                 strict=False,
             )
@@ -202,22 +195,16 @@

 def get_maintenance_periods(experiment_name, start, end):
     """"""Get maintenance periods for the specified experiment and time range.""""""
     # get states from acquisition.Environment.EnvironmentState
-    chunk_restriction = acquisition.create_chunk_restriction(
-        experiment_name, start, end
-    )
+    chunk_restriction = acquisition.create_chunk_restriction(experiment_name, start, end)
     state_query = (
-        acquisition.Environment.EnvironmentState
-        & {""experiment_name"": experiment_name}
-        & chunk_restriction
+        acquisition.Environment.EnvironmentState & {""experiment_name"": experiment_name} & chunk_restriction
     )
     env_state_df = fetch_stream(state_query)[start:end]
     if env_state_df.empty:
         return deque([])
 
     env_state_df.reset_index(inplace=True)
-    env_state_df = env_state_df[
-        env_state_df[""state""].shift() != env_state_df[""state""]
-    ].reset_index(
+    env_state_df = env_state_df[env_state_df[""state""].shift() != env_state_df[""state""]].reset_index(
         drop=True
     )  # remove duplicates and keep the first one
     # An experiment starts with visit start (anything before the first maintenance is experiment)
@@ -233,12 +220,8 @@

         env_state_df = pd.concat([env_state_df, log_df_end])
         env_state_df.reset_index(drop=True, inplace=True)
 
-    maintenance_starts = env_state_df.loc[
-        env_state_df[""state""] == ""Maintenance"", ""time""
-    ].values
-    maintenance_ends = env_state_df.loc[
-        env_state_df[""state""] != ""Maintenance"", ""time""
-    ].values
+    maintenance_starts = env_state_df.loc[env_state_df[""state""] == ""Maintenance"", ""time""].values
+    maintenance_ends = env_state_df.loc[env_state_df[""state""] != ""Maintenance"", ""time""].values
 
     return deque(
         [
@@ -255,9 +238,7 @@

         (maintenance_start, maintenance_end) = maint_period[0]
         if end_time < maintenance_start:  # no more maintenance for this date
             break
-        maintenance_filter = (data_df.index >= maintenance_start) & (
-            data_df.index <= maintenance_end
-        )
+        maintenance_filter = (data_df.index >= maintenance_start) & (data_df.index <= maintenance_end)
         data_df[maintenance_filter] = np.nan
         if end_time >= maintenance_end:  # remove this range
             maint_period.popleft()"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820612055,,22,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,, PLR2004: Replaced magic values with constant variables.," # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
+# Constants values","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820613090,,115,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,, SIM108: Refactored `visit_analysis` to use the ternary operator.,"-            # For chunks after the first chunk of this visit
-            start_time = chunk_start
-
+        start_time = (","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820613962,,211,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,S101: Replaced assertions with exceptions to ensure expected behavior in all scenarios,"+        """"""
         if visit_key is not None:
-            assert len(Visit & visit_key) == 1
+            if len(Visit & visit_key) != 1:","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820617034,,223,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,PLW0127: Variables are assigned to themselves. Could this code block be improved?,"-            start = start
-            end = end
-            subject = subject
+            start = start  # noqa PLW0127","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820617555,,317,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,PLR2004: Replaced magic values with constant variables.," 
             # filter for objects of the correct size
-            valid_position = (position.area > 0) & (position.area < 1000)
+            valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820618145,,472,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,PLR2004: Replaced magic values with constant variables.,"+            position = filter_out_maintenance_periods(
+                position, maintenance_period, day_end
+            )
             # filter for objects of the correct size","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -93,8 +94,7 @@

         + chunk starts after visit_start and ends before visit_end (or NOW() - i.e. ongoing visits).
         """"""
         return (
-            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"")
-            * acquisition.Chunk
+            Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") * acquisition.Chunk
             & acquisition.SubjectEnterExit
             & [
                 ""visit_start BETWEEN chunk_start AND chunk_end"",
@@ -107,9 +107,7 @@

 
     def make(self, key):
         """"""Populate VisitSubjectPosition for each visit.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         # -- Determine the time to start time_slicing in this chunk
         start_time = (
@@ -177,12 +175,8 @@

         end_time = np.array(end_time, dtype=""datetime64[ns]"")
 
         while time_slice_start < end_time:
-            time_slice_end = time_slice_start + min(
-                self._time_slice_duration, end_time - time_slice_start
-            )
-            in_time_slice = np.logical_and(
-                timestamps >= time_slice_start, timestamps < time_slice_end
-            )
+            time_slice_end = time_slice_start + min(self._time_slice_duration, end_time - time_slice_start)
+            in_time_slice = np.logical_and(timestamps >= time_slice_start, timestamps < time_slice_end)
             chunk_time_slices.append(
                 {
                     **key,
@@ -202,32 +196,33 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
-                raise ValueError(
-                    ""The `visit_key` must correspond to exactly one Visit.""
-                )
+                raise ValueError(""The `visit_key` must correspond to exactly one Visit."")
             start, end = (
-                Visit.join(VisitEnd, left=True).proj(
-                    visit_end=""IFNULL(visit_end, NOW())""
-                )
-                & visit_key
+                Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -277,9 +272,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitTimeDistribution for each visit.""""""
@@ -287,9 +280,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -309,16 +300,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
 
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
             # in corridor
             distance_from_center = tracking.compute_distance(
                 position[[""x"", ""y""]],
@@ -362,9 +349,9 @@

             in_food_patch_times = []
             for food_patch_key in food_patch_keys:
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -373,12 +360,10 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
-                patch_position = (
-                    acquisition.ExperimentFoodPatch.Position & food_patch_key
-                ).fetch1(""food_patch_position_x"", ""food_patch_position_y"")
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
+                patch_position = (acquisition.ExperimentFoodPatch.Position & food_patch_key).fetch1(
+                    ""food_patch_position_x"", ""food_patch_position_y""
+                )
                 in_patch = tracking.is_position_in_patch(
                     position,
                     patch_position,
@@ -433,9 +418,7 @@

         """"""
 
     # Work on finished visits
-    key_source = Visit & (
-        VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end""
-    )
+    key_source = Visit & (VisitEnd * VisitSubjectPosition.TimeSlice & ""time_slice_end = visit_end"")
 
     def make(self, key):
         """"""Populate VisitSummary for each visit.""""""
@@ -443,9 +426,7 @@

         visit_dates = pd.date_range(
             start=pd.Timestamp(visit_start.date()), end=pd.Timestamp(visit_end.date())
         )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
 
         for visit_date in visit_dates:
             day_start = datetime.datetime.combine(visit_date.date(), time.min)
@@ -466,18 +447,12 @@

                 subject=key[""subject""], start=day_start, end=day_end
             )
             # filter out maintenance period based on logs
-            position = filter_out_maintenance_periods(
-                position, maintenance_period, day_end
-            )
+            position = filter_out_maintenance_periods(position, maintenance_period, day_end)
             # filter for objects of the correct size
             valid_position = (position.area > MIN_AREA) & (position.area < MAX_AREA)
             position[~valid_position] = np.nan
-            position.rename(
-                columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True
-            )
-            position_diff = np.sqrt(
-                np.square(np.diff(position.x)) + np.square(np.diff(position.y))
-            )
+            position.rename(columns={""position_x"": ""x"", ""position_y"": ""y""}, inplace=True)
+            position_diff = np.sqrt(np.square(np.diff(position.x)) + np.square(np.diff(position.y)))
             total_distance_travelled = np.nansum(position_diff)
 
             # in food patches - loop through all in-use patches during this visit
@@ -513,9 +488,9 @@

                     dropna=True,
                 ).index.values
                 # wheel data
-                food_patch_description = (
-                    acquisition.ExperimentFoodPatch & food_patch_key
-                ).fetch1(""food_patch_description"")
+                food_patch_description = (acquisition.ExperimentFoodPatch & food_patch_key).fetch1(
+                    ""food_patch_description""
+                )
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
                     experiment_name=key[""experiment_name""],
                     start=pd.Timestamp(day_start),
@@ -524,9 +499,7 @@

                     using_aeon_io=True,
                 )
                 # filter out maintenance period based on logs
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, day_end
-                )
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, day_end)
 
                 food_patch_statistics.append(
                     {
@@ -534,15 +507,11 @@

                         **food_patch_key,
                         ""visit_date"": visit_date.date(),
                         ""pellet_count"": len(pellet_events),
-                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[
-                            -1
-                        ],
+                        ""wheel_distance_travelled"": wheel_data.distance_travelled.values[-1],
                     }
                 )
 
-            total_pellet_count = np.sum(
-                [p[""pellet_count""] for p in food_patch_statistics]
-            )
+            total_pellet_count = np.sum([p[""pellet_count""] for p in food_patch_statistics])
             total_wheel_distance_travelled = np.sum(
                 [p[""wheel_distance_travelled""] for p in food_patch_statistics]
             )
@@ -563,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """"""
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout
@@ -578,10 +547,7 @@

 
     # Work on 24/7 experiments
     key_source = (
-        Visit
-        & VisitSummary
-        & (VisitEnd & ""visit_duration > 24"")
-        & ""experiment_name= 'exp0.2-r0'""
+        Visit & VisitSummary & (VisitEnd & ""visit_duration > 24"") & ""experiment_name= 'exp0.2-r0'""
     ) * acquisition.ExperimentFoodPatch
 
     def make(self, key):
@@ -589,17 +555,13 @@

         visit_start, visit_end = (VisitEnd & key).fetch1(""visit_start"", ""visit_end"")
 
         # get in_patch timestamps
-        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(
-            ""food_patch_description""
-        )
+        food_patch_description = (acquisition.ExperimentFoodPatch & key).fetch1(""food_patch_description"")
         in_patch_times = np.concatenate(
-            (
-                VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key
-            ).fetch(""in_patch"", order_by=""visit_date"")
-        )
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
+            (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & key).fetch(
+                ""in_patch"", order_by=""visit_date""
+            )
+        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
         in_patch_times = filter_out_maintenance_periods(
             pd.DataFrame(
                 [[food_patch_description]] * len(in_patch_times),
@@ -627,12 +589,8 @@

             .set_index(""event_time"")
         )
         # TODO: handle multiple retries of pellet delivery
-        maintenance_period = get_maintenance_periods(
-            key[""experiment_name""], visit_start, visit_end
-        )
-        patch = filter_out_maintenance_periods(
-            patch, maintenance_period, visit_end, True
-        )
+        maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+        patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, True)
 
         if len(in_patch_times):
             change_ind = (
@@ -648,9 +606,7 @@

                     ts_array = in_patch_times[change_ind[i - 1] : change_ind[i]]
 
                 wheel_start, wheel_end = ts_array[0], ts_array[-1]
-                if (
-                    wheel_start >= wheel_end
-                ):  # skip if timestamps were misaligned or a single timestamp
+                if wheel_start >= wheel_end:  # skip if timestamps were misaligned or a single timestamp
                     continue
 
                 wheel_data = acquisition.FoodPatchWheel.get_wheel_data(
@@ -660,19 +616,14 @@

                     patch_name=food_patch_description,
                     using_aeon_io=True,
                 )
-                maintenance_period = get_maintenance_periods(
-                    key[""experiment_name""], visit_start, visit_end
-                )
-                wheel_data = filter_out_maintenance_periods(
-                    wheel_data, maintenance_period, visit_end, True
-                )
+                maintenance_period = get_maintenance_periods(key[""experiment_name""], visit_start, visit_end)
+                wheel_data = filter_out_maintenance_periods(wheel_data, maintenance_period, visit_end, True)
                 self.insert1(
                     {
                         **key,
                         ""bout_start"": ts_array[0],
                         ""bout_end"": ts_array[-1],
-                        ""bout_duration"": (ts_array[-1] - ts_array[0])
-                        / np.timedelta64(1, ""s""),
+                        ""bout_duration"": (ts_array[-1] - ts_array[0]) / np.timedelta64(1, ""s""),
                         ""wheel_distance_travelled"": wheel_data.distance_travelled[-1],
                         ""pellet_count"": len(patch.loc[wheel_start:wheel_end]),
                     }"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820625134,,179,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/create_experiments/create_socialexperiment_0.py,,Fixed bare except (E722) ,"         ld = [jl.levenshtein_distance(subjid, x[-len(subjid) :]) for x in valid_ids]
         return valid_ids[np.argmin(ld)]
-    except:
+    except ValueError:","--- 

+++ 

@@ -1,11 +1,9 @@

-""""""Function to create new experiments for social0-r1.""""""
+""""""Functions to create new experiments for social0-r1.""""""
 
 import pathlib
 
 from aeon.dj_pipeline import acquisition, lab, subject
-from aeon.dj_pipeline.create_experiments.create_experiment_01 import (
-    ingest_exp01_metadata,
-)
+from aeon.dj_pipeline.create_experiments.create_experiment_01 import ingest_exp01_metadata
 
 # ============ Manual and automatic steps to for experiment 0.1 populate ============
 experiment_name = ""social0-r1""
@@ -38,10 +36,7 @@

         skip_duplicates=True,
     )
     acquisition.Experiment.Subject.insert(
-        [
-            {""experiment_name"": experiment_name, ""subject"": s[""subject""]}
-            for s in subject_list
-        ],
+        [{""experiment_name"": experiment_name, ""subject"": s[""subject""]} for s in subject_list],
         skip_duplicates=True,
     )
 
@@ -97,12 +92,8 @@

     # manually update coordinates of foodpatch and nest
     patch_coordinates = {""Patch1"": (1.13, 1.59, 0), ""Patch2"": (1.19, 0.50, 0)}
 
-    for patch_key in (
-        acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}
-    ).fetch(""KEY""):
-        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(
-            ""food_patch_description""
-        )
+    for patch_key in (acquisition.ExperimentFoodPatch & {""experiment_name"": experiment_name}).fetch(""KEY""):
+        patch = (acquisition.ExperimentFoodPatch & patch_key).fetch1(""food_patch_description"")
         x, y, z = patch_coordinates[patch]
         acquisition.ExperimentFoodPatch.Position.update1(
             {"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820627416,,8,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/populate/worker.py,,Removed `RegisteredWorker` since it is unused,"+    DataJointWorker,
+    ErrorLog,
+    WorkerLog,
+)","--- 

+++ 

@@ -1,11 +1,7 @@

 """"""This module defines the workers for the AEON pipeline.""""""
 
 import datajoint as dj
-from datajoint_utilities.dj_worker import (
-    DataJointWorker,
-    ErrorLog,
-    WorkerLog,
-)
+from datajoint_utilities.dj_worker import DataJointWorker, ErrorLog, WorkerLog
 from datajoint_utilities.dj_worker.worker_schema import is_djtable
 
 from aeon.dj_pipeline import acquisition, db_prefix, qc, subject, tracking
@@ -104,15 +100,33 @@

 )
 
 analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
-analysis_worker(block_analysis.BlockPlots, max_calls=6)
 analysis_worker(block_analysis.BlockSubjectAnalysis, max_calls=6)
-analysis_worker(block_analysis.BlockSubjectPlots, max_calls=6)
+analysis_worker(block_analysis.BlockForaging, max_calls=6)
+analysis_worker(block_analysis.BlockPatchPlots, max_calls=6)
+analysis_worker(block_analysis.BlockSubjectPositionPlots, max_calls=6)
 
 
 def get_workflow_operation_overview():
     """"""Get the workflow operation overview for the worker schema.""""""
     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
 
-    return get_workflow_operation_overview(
-        worker_schema_name=worker_schema_name, db_prefixes=[db_prefix]
-    )
+    return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
+
+
+def retrieve_schemas_sizes(schema_only=False, all_schemas=False):
+    schema_names = [n for n in dj.list_schemas() if n != ""mysql""]
+    if not all_schemas:
+        schema_names = [n for n in schema_names
+                        if n.startswith(db_prefix) and not n.startswith(f""{db_prefix}archived"")]
+
+    if schema_only:
+        return {n: dj.Schema(n).size_on_disk / 1e9 for n in schema_names}
+
+    schema_sizes = {n: {} for n in schema_names}
+    for n in schema_names:
+        vm = dj.VirtualModule(n, n)
+        schema_sizes[n][""schema_gb""] = vm.schema.size_on_disk / 1e9
+        schema_sizes[n][""tables_gb""] = {n: t().size_on_disk / 1e9
+                                        for n, t in vm.__dict__.items()
+                                        if isinstance(t, dj.user_tables.TableMeta)}
+    return schema_sizes"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820630817,,12,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/scripts/update_timestamps_longblob.py,,S101: Replaced assertions with exceptions,"+logger = dj.logger
+
+
+if dj.__version__ < ""0.13.7"":","--- 

+++ 

@@ -10,9 +10,7 @@

 
 
 if dj.__version__ < ""0.13.7"":
-    raise ImportError(
-        f""DataJoint version must be at least 0.13.7, but found {dj.__version__}.""
-    )
+    raise ImportError(f""DataJoint version must be at least 0.13.7, but found {dj.__version__}."")
 
 
 schema = dj.schema(""u_thinh_aeonfix"")
@@ -40,13 +38,7 @@

     for schema_name in schema_names:
         vm = dj.create_virtual_module(schema_name, schema_name)
         table_names = [
-            ""."".join(
-                [
-                    dj.utils.to_camel_case(s)
-                    for s in tbl_name.strip(""`"").split(""__"")
-                    if s
-                ]
-            )
+            ""."".join([dj.utils.to_camel_case(s) for s in tbl_name.strip(""`"").split(""__"") if s])
             for tbl_name in vm.schema.list_tables()
         ]
         for table_name in table_names:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820631210,,68,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/scripts/update_timestamps_longblob.py,,S101: Replaced assertions with exceptions,"                             TimestampFix.insert1(fix_key)
                             continue
-                        assert isinstance(ts[0], datetime)
+                        if not isinstance(ts[0], datetime):","--- 

+++ 

@@ -10,9 +10,7 @@

 
 
 if dj.__version__ < ""0.13.7"":
-    raise ImportError(
-        f""DataJoint version must be at least 0.13.7, but found {dj.__version__}.""
-    )
+    raise ImportError(f""DataJoint version must be at least 0.13.7, but found {dj.__version__}."")
 
 
 schema = dj.schema(""u_thinh_aeonfix"")
@@ -40,13 +38,7 @@

     for schema_name in schema_names:
         vm = dj.create_virtual_module(schema_name, schema_name)
         table_names = [
-            ""."".join(
-                [
-                    dj.utils.to_camel_case(s)
-                    for s in tbl_name.strip(""`"").split(""__"")
-                    if s
-                ]
-            )
+            ""."".join([dj.utils.to_camel_case(s) for s in tbl_name.strip(""`"").split(""__"") if s])
             for tbl_name in vm.schema.list_tables()
         ]
         for table_name in table_names:"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820637111,,489,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/subject.py,,"UP038: Used the new X | Y syntax for isinstance calls, as introduced in Python 3.10.","         params_str_list = []
         for k, v in params.items():
-            if isinstance(v, (list, tuple)):
+            if isinstance(v, (list | tuple)):","--- 

+++ 

@@ -3,7 +3,7 @@

 import json
 import os
 import time
-from datetime import datetime, timedelta, timezone
+from datetime import UTC, datetime, timedelta
 
 import datajoint as dj
 import requests
@@ -67,6 +67,7 @@

             ""o"": 0,
             ""l"": 10,
             ""eartag"": eartag_or_id,
+            ""state"": [""live"", ""sacrificed"", ""exported""],
         }
         animal_resp = get_pyrat_data(endpoint=""animals"", params=params)
         if len(animal_resp) == 0:
@@ -86,9 +87,7 @@

                 )
             return
         elif len(animal_resp) > 1:
-            raise ValueError(
-                f""Found {len(animal_resp)} with eartag {eartag_or_id}, expect one""
-            )
+            raise ValueError(f""Found {len(animal_resp)} with eartag {eartag_or_id}, expect one"")
         else:
             animal_resp = animal_resp[0]
 
@@ -101,10 +100,7 @@

             }
         )
         Strain.insert1(
-            {
-                ""strain_id"": animal_resp[""strain_id""],
-                ""strain_name"": animal_resp[""strain_id""],
-            },
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
             skip_duplicates=True,
         )
         entry = {
@@ -113,13 +109,11 @@

             ""strain_id"": animal_resp[""strain_id""],
             ""cage_number"": animal_resp[""cagenumber""],
             ""lab_id"": animal_resp[""labid""],
+            ""available"": animal_resp.get(""state"", """") == ""live"",
         }
         if animal_resp[""gen_bg_id""] is not None:
             GeneticBackground.insert1(
-                {
-                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
-                    ""gen_bg"": animal_resp[""gen_bg""],
-                },
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
                 skip_duplicates=True,
             )
             entry[""gen_bg_id""] = animal_resp[""gen_bg_id""]
@@ -187,27 +181,23 @@

         """"""Get the reference weight for the subject.""""""
         subj_key = {""subject"": subject_name}
 
-        food_restrict_query = (
-            SubjectProcedure & subj_key & ""procedure_name = 'R02 - food restriction'""
-        )
+        food_restrict_query = SubjectProcedure & subj_key & ""procedure_name = 'R02 - food restriction'""
         if food_restrict_query:
-            ref_date = food_restrict_query.fetch(
-                ""procedure_date"", order_by=""procedure_date DESC"", limit=1
-            )[0]
+            ref_date = food_restrict_query.fetch(""procedure_date"", order_by=""procedure_date DESC"", limit=1)[
+                0
+            ]
         else:
-            ref_date = datetime.now(timezone.utc).date()
+            ref_date = datetime.now(UTC).date()
 
         weight_query = SubjectWeight & subj_key & f""weight_time < '{ref_date}'""
         ref_weight = (
-            weight_query.fetch(""weight"", order_by=""weight_time DESC"", limit=1)[0]
-            if weight_query
-            else -1
+            weight_query.fetch(""weight"", order_by=""weight_time DESC"", limit=1)[0] if weight_query else -1
         )
 
         entry = {
             ""subject"": subject_name,
             ""reference_weight"": ref_weight,
-            ""last_updated_time"": datetime.now(timezone.utc),
+            ""last_updated_time"": datetime.now(UTC),
         }
         cls.update1(entry) if cls & {""subject"": subject_name} else cls.insert1(entry)
 
@@ -250,7 +240,7 @@

 
     def _auto_schedule(self):
         """"""Automatically schedule the next task.""""""
-        utc_now = datetime.now(timezone.utc)
+        utc_now = datetime.now(UTC)
 
         next_task_schedule_time = utc_now + timedelta(hours=self.schedule_interval)
         if (
@@ -259,25 +249,22 @@

         ):
             return
 
-        PyratIngestionTask.insert1(
-            {""pyrat_task_scheduled_time"": next_task_schedule_time}
-        )
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": next_task_schedule_time})
 
     def make(self, key):
         """"""Automatically import or update entries in the Subject table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         new_eartags = []
         for responsible_id in lab.User.fetch(""responsible_id""):
             # 1 - retrieve all animals from this user
             animal_resp = get_pyrat_data(
-                endpoint=""animals"", params={""responsible_id"": responsible_id}
+                endpoint=""animals"",
+                params={""responsible_id"": responsible_id, ""state"": [""live"", ""sacrificed"", ""exported""]}
             )
             for animal_entry in animal_resp:
                 # 2 - find animal with comment - Project Aeon
                 eartag_or_id = animal_entry[""eartag_or_id""]
-                comment_resp = get_pyrat_data(
-                    endpoint=f""animals/{eartag_or_id}/comments""
-                )
+                comment_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/comments"")
                 for comment in comment_resp:
                     if comment[""attributes""]:
                         first_attr = comment[""attributes""][0]
@@ -301,14 +288,12 @@

             new_entry_count += 1
 
         logger.info(f""Inserting {new_entry_count} new subject(s) from Pyrat"")
-        completion_time = datetime.now(timezone.utc)
+        completion_time = datetime.now(UTC)
         self.insert1(
             {
                 **key,
                 ""execution_time"": execution_time,
-                ""execution_duration"": (
-                    completion_time - execution_time
-                ).total_seconds(),
+                ""execution_duration"": (completion_time - execution_time).total_seconds(),
                 ""new_pyrat_entry_count"": new_entry_count,
             }
         )
@@ -334,7 +319,7 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the PyratCommentWeightProcedure table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         logger.info(""Extracting weights/comments/procedures"")
 
         eartag_or_id = key[""subject""]
@@ -344,7 +329,7 @@

             if e.args[0].endswith(""response code: 404""):
                 SubjectDetail.update1(
                     {
-                        **key,
+                        ""subject"": key[""subject""],
                         ""available"": False,
                     }
                 )
@@ -354,9 +339,7 @@

             for cmt in comment_resp:
                 cmt[""subject""] = eartag_or_id
                 cmt[""attributes""] = json.dumps(cmt[""attributes""], default=str)
-            SubjectComment.insert(
-                comment_resp, skip_duplicates=True, allow_direct_insert=True
-            )
+            SubjectComment.insert(comment_resp, skip_duplicates=True, allow_direct_insert=True)
 
             weight_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/weights"")
             SubjectWeight.insert(
@@ -365,9 +348,7 @@

                 allow_direct_insert=True,
             )
 
-            procedure_resp = get_pyrat_data(
-                endpoint=f""animals/{eartag_or_id}/procedures""
-            )
+            procedure_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/procedures"")
             SubjectProcedure.insert(
                 [{**v, ""subject"": eartag_or_id} for v in procedure_resp],
                 skip_duplicates=True,
@@ -377,14 +358,26 @@

             # compute/update reference weight
             SubjectReferenceWeight.get_reference_weight(eartag_or_id)
         finally:
-            completion_time = datetime.now(timezone.utc)
+            # recheck for ""state"" to see if the animal is still available
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""k"": [""labid"", ""state""],
+                        ""eartag"": eartag_or_id,
+                        ""state"": [""live"", ""sacrificed"", ""exported""]})
+            animal_resp = animal_resp[0]
+            SubjectDetail.update1(
+                {
+                    ""subject"": key[""subject""],
+                    ""available"": animal_resp.get(""state"", """") == ""live"",
+                    ""lab_id"": animal_resp[""labid""],
+                }
+            )
+            completion_time = datetime.now(UTC)
             self.insert1(
                 {
                     **key,
                     ""execution_time"": execution_time,
-                    ""execution_duration"": (
-                        completion_time - execution_time
-                    ).total_seconds(),
+                    ""execution_duration"": (completion_time - execution_time).total_seconds(),
                 }
             )
 
@@ -397,9 +390,7 @@

 
     def make(self, key):
         """"""Create one new PyratIngestionTask for every newly added users.""""""
-        PyratIngestionTask.insert1(
-            {""pyrat_task_scheduled_time"": datetime.now(timezone.utc)}
-        )
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(UTC)})
         time.sleep(1)
         self.insert1(key)
 
@@ -469,7 +460,10 @@

 
 
 def get_pyrat_data(endpoint: str, params: dict = None, **kwargs):
-    """"""Get data from PyRat API.""""""
+    """"""Get data from PyRat API.
+
+    See docs at: https://swc.pyrat.cloud/api/v3/docs (production)
+    """"""
     base_url = ""https://swc.pyrat.cloud/api/v3/""
     pyrat_system_token = os.getenv(""PYRAT_SYSTEM_TOKEN"")
     pyrat_user_token = os.getenv(""PYRAT_USER_TOKEN"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820637748,,500,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/subject.py,,PLR2004: Replaced magic values with constant variables.,"     response = session.get(base_url + endpoint + params_str, **kwargs)
 
-    if response.status_code != 200:
+    RESPONSE_STATUS_CODE_OK = 200","--- 

+++ 

@@ -3,7 +3,7 @@

 import json
 import os
 import time
-from datetime import datetime, timedelta, timezone
+from datetime import UTC, datetime, timedelta
 
 import datajoint as dj
 import requests
@@ -67,6 +67,7 @@

             ""o"": 0,
             ""l"": 10,
             ""eartag"": eartag_or_id,
+            ""state"": [""live"", ""sacrificed"", ""exported""],
         }
         animal_resp = get_pyrat_data(endpoint=""animals"", params=params)
         if len(animal_resp) == 0:
@@ -86,9 +87,7 @@

                 )
             return
         elif len(animal_resp) > 1:
-            raise ValueError(
-                f""Found {len(animal_resp)} with eartag {eartag_or_id}, expect one""
-            )
+            raise ValueError(f""Found {len(animal_resp)} with eartag {eartag_or_id}, expect one"")
         else:
             animal_resp = animal_resp[0]
 
@@ -101,10 +100,7 @@

             }
         )
         Strain.insert1(
-            {
-                ""strain_id"": animal_resp[""strain_id""],
-                ""strain_name"": animal_resp[""strain_id""],
-            },
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
             skip_duplicates=True,
         )
         entry = {
@@ -113,13 +109,11 @@

             ""strain_id"": animal_resp[""strain_id""],
             ""cage_number"": animal_resp[""cagenumber""],
             ""lab_id"": animal_resp[""labid""],
+            ""available"": animal_resp.get(""state"", """") == ""live"",
         }
         if animal_resp[""gen_bg_id""] is not None:
             GeneticBackground.insert1(
-                {
-                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
-                    ""gen_bg"": animal_resp[""gen_bg""],
-                },
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
                 skip_duplicates=True,
             )
             entry[""gen_bg_id""] = animal_resp[""gen_bg_id""]
@@ -187,27 +181,23 @@

         """"""Get the reference weight for the subject.""""""
         subj_key = {""subject"": subject_name}
 
-        food_restrict_query = (
-            SubjectProcedure & subj_key & ""procedure_name = 'R02 - food restriction'""
-        )
+        food_restrict_query = SubjectProcedure & subj_key & ""procedure_name = 'R02 - food restriction'""
         if food_restrict_query:
-            ref_date = food_restrict_query.fetch(
-                ""procedure_date"", order_by=""procedure_date DESC"", limit=1
-            )[0]
+            ref_date = food_restrict_query.fetch(""procedure_date"", order_by=""procedure_date DESC"", limit=1)[
+                0
+            ]
         else:
-            ref_date = datetime.now(timezone.utc).date()
+            ref_date = datetime.now(UTC).date()
 
         weight_query = SubjectWeight & subj_key & f""weight_time < '{ref_date}'""
         ref_weight = (
-            weight_query.fetch(""weight"", order_by=""weight_time DESC"", limit=1)[0]
-            if weight_query
-            else -1
+            weight_query.fetch(""weight"", order_by=""weight_time DESC"", limit=1)[0] if weight_query else -1
         )
 
         entry = {
             ""subject"": subject_name,
             ""reference_weight"": ref_weight,
-            ""last_updated_time"": datetime.now(timezone.utc),
+            ""last_updated_time"": datetime.now(UTC),
         }
         cls.update1(entry) if cls & {""subject"": subject_name} else cls.insert1(entry)
 
@@ -250,7 +240,7 @@

 
     def _auto_schedule(self):
         """"""Automatically schedule the next task.""""""
-        utc_now = datetime.now(timezone.utc)
+        utc_now = datetime.now(UTC)
 
         next_task_schedule_time = utc_now + timedelta(hours=self.schedule_interval)
         if (
@@ -259,25 +249,22 @@

         ):
             return
 
-        PyratIngestionTask.insert1(
-            {""pyrat_task_scheduled_time"": next_task_schedule_time}
-        )
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": next_task_schedule_time})
 
     def make(self, key):
         """"""Automatically import or update entries in the Subject table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         new_eartags = []
         for responsible_id in lab.User.fetch(""responsible_id""):
             # 1 - retrieve all animals from this user
             animal_resp = get_pyrat_data(
-                endpoint=""animals"", params={""responsible_id"": responsible_id}
+                endpoint=""animals"",
+                params={""responsible_id"": responsible_id, ""state"": [""live"", ""sacrificed"", ""exported""]}
             )
             for animal_entry in animal_resp:
                 # 2 - find animal with comment - Project Aeon
                 eartag_or_id = animal_entry[""eartag_or_id""]
-                comment_resp = get_pyrat_data(
-                    endpoint=f""animals/{eartag_or_id}/comments""
-                )
+                comment_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/comments"")
                 for comment in comment_resp:
                     if comment[""attributes""]:
                         first_attr = comment[""attributes""][0]
@@ -301,14 +288,12 @@

             new_entry_count += 1
 
         logger.info(f""Inserting {new_entry_count} new subject(s) from Pyrat"")
-        completion_time = datetime.now(timezone.utc)
+        completion_time = datetime.now(UTC)
         self.insert1(
             {
                 **key,
                 ""execution_time"": execution_time,
-                ""execution_duration"": (
-                    completion_time - execution_time
-                ).total_seconds(),
+                ""execution_duration"": (completion_time - execution_time).total_seconds(),
                 ""new_pyrat_entry_count"": new_entry_count,
             }
         )
@@ -334,7 +319,7 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the PyratCommentWeightProcedure table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         logger.info(""Extracting weights/comments/procedures"")
 
         eartag_or_id = key[""subject""]
@@ -344,7 +329,7 @@

             if e.args[0].endswith(""response code: 404""):
                 SubjectDetail.update1(
                     {
-                        **key,
+                        ""subject"": key[""subject""],
                         ""available"": False,
                     }
                 )
@@ -354,9 +339,7 @@

             for cmt in comment_resp:
                 cmt[""subject""] = eartag_or_id
                 cmt[""attributes""] = json.dumps(cmt[""attributes""], default=str)
-            SubjectComment.insert(
-                comment_resp, skip_duplicates=True, allow_direct_insert=True
-            )
+            SubjectComment.insert(comment_resp, skip_duplicates=True, allow_direct_insert=True)
 
             weight_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/weights"")
             SubjectWeight.insert(
@@ -365,9 +348,7 @@

                 allow_direct_insert=True,
             )
 
-            procedure_resp = get_pyrat_data(
-                endpoint=f""animals/{eartag_or_id}/procedures""
-            )
+            procedure_resp = get_pyrat_data(endpoint=f""animals/{eartag_or_id}/procedures"")
             SubjectProcedure.insert(
                 [{**v, ""subject"": eartag_or_id} for v in procedure_resp],
                 skip_duplicates=True,
@@ -377,14 +358,26 @@

             # compute/update reference weight
             SubjectReferenceWeight.get_reference_weight(eartag_or_id)
         finally:
-            completion_time = datetime.now(timezone.utc)
+            # recheck for ""state"" to see if the animal is still available
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""k"": [""labid"", ""state""],
+                        ""eartag"": eartag_or_id,
+                        ""state"": [""live"", ""sacrificed"", ""exported""]})
+            animal_resp = animal_resp[0]
+            SubjectDetail.update1(
+                {
+                    ""subject"": key[""subject""],
+                    ""available"": animal_resp.get(""state"", """") == ""live"",
+                    ""lab_id"": animal_resp[""labid""],
+                }
+            )
+            completion_time = datetime.now(UTC)
             self.insert1(
                 {
                     **key,
                     ""execution_time"": execution_time,
-                    ""execution_duration"": (
-                        completion_time - execution_time
-                    ).total_seconds(),
+                    ""execution_duration"": (completion_time - execution_time).total_seconds(),
                 }
             )
 
@@ -397,9 +390,7 @@

 
     def make(self, key):
         """"""Create one new PyratIngestionTask for every newly added users.""""""
-        PyratIngestionTask.insert1(
-            {""pyrat_task_scheduled_time"": datetime.now(timezone.utc)}
-        )
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(UTC)})
         time.sleep(1)
         self.insert1(key)
 
@@ -469,7 +460,10 @@

 
 
 def get_pyrat_data(endpoint: str, params: dict = None, **kwargs):
-    """"""Get data from PyRat API.""""""
+    """"""Get data from PyRat API.
+
+    See docs at: https://swc.pyrat.cloud/api/v3/docs (production)
+    """"""
     base_url = ""https://swc.pyrat.cloud/api/v3/""
     pyrat_system_token = os.getenv(""PYRAT_SYSTEM_TOKEN"")
     pyrat_user_token = os.getenv(""PYRAT_USER_TOKEN"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820638624,,259,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,PLR2004: Replaced magic values with constant variables.," 
 # ---------- HELPER ------------------
 
+TARGET_LENGTH = 2","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -80,18 +75,14 @@

     ):
         """"""Insert a new set of parameters for a given tracking method.""""""
         if tracking_paramset_id is None:
-            tracking_paramset_id = (
-                dj.U().aggr(cls, n=""max(tracking_paramset_id)"").fetch1(""n"") or 0
-            ) + 1
+            tracking_paramset_id = (dj.U().aggr(cls, n=""max(tracking_paramset_id)"").fetch1(""n"") or 0) + 1
 
         param_dict = {
             ""tracking_method"": tracking_method,
             ""tracking_paramset_id"": tracking_paramset_id,
             ""paramset_description"": paramset_description,
             ""params"": params,
-            ""param_set_hash"": dict_to_uuid(
-                {**params, ""tracking_method"": tracking_method}
-            ),
+            ""param_set_hash"": dict_to_uuid({**params, ""tracking_method"": tracking_method}),
         }
         param_query = cls & {""param_set_hash"": param_dict[""param_set_hash""]}
 
@@ -120,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -161,9 +147,7 @@

         return (
             acquisition.Chunk
             * (
-                streams.SpinnakerVideoSource.join(
-                    streams.SpinnakerVideoSource.RemovalTime, left=True
-                )
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
                 & ""spinnaker_video_source_name='CameraTop'""
             )
             * (TrackingParamSet & ""tracking_paramset_id = 1"")
@@ -173,24 +157,29 @@

 
     def make(self, key):
         """"""Ingest SLEAP tracking data for a given chunk.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         data_dirs = acquisition.Experiment.get_data_directories(key)
 
-        device_name = (streams.SpinnakerVideoSource & key).fetch1(
-            ""spinnaker_video_source_name""
-        )
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
 
         devices_schema = getattr(
             aeon_schemas,
-            (
-                acquisition.Experiment.DevicesSchema
-                & {""experiment_name"": key[""experiment_name""]}
-            ).fetch1(""devices_schema_name""),
-        )
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -200,13 +189,16 @@

         )
 
         if not len(pose_data):
-            raise ValueError(
-                f""No SLEAP data found for {key['experiment_name']} - {device_name}""
-            )
+            raise ValueError(f""No SLEAP data found for {key['experiment_name']} - {device_name}"")
 
         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -214,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -235,9 +224,7 @@

                 if part == anchor_part:
                     identity_likelihood = part_position.identity_likelihood.values
                     if isinstance(identity_likelihood[0], dict):
-                        identity_likelihood = np.array(
-                            [v[identity] for v in identity_likelihood]
-                        )
+                        identity_likelihood = np.array([v[identity] for v in identity_likelihood])
 
             pose_identity_entries.append(
                 {
@@ -254,22 +241,152 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
-TARGET_LENGTH = 2
-
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target point.""""""
-    if len(target) != TARGET_LENGTH:
-        raise ValueError(f""Target must be a list of tuple of length {TARGET_LENGTH}."")
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
+        raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
 
 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0
@@ -318,9 +435,7 @@

     start_query = table & obj_restriction & start_restriction
     end_query = table & obj_restriction & end_restriction
     if not (start_query and end_query):
-        raise ValueError(
-            f""No position data found for {object_name} between {start} and {end}""
-        )
+        raise ValueError(f""No position data found for {object_name} between {start} and {end}"")
 
     time_restriction = (
         f'{start_attr} >= ""{min(start_query.fetch(start_attr))}""'
@@ -328,14 +443,10 @@

     )
 
     # subject's position data in the time slice
-    fetched_data = (table & obj_restriction & time_restriction).fetch(
-        *fetch_attrs, order_by=start_attr
-    )
+    fetched_data = (table & obj_restriction & time_restriction).fetch(*fetch_attrs, order_by=start_attr)
 
     if not len(fetched_data[0]):
-        raise ValueError(
-            f""No position data found for {object_name} between {start} and {end}""
-        )
+        raise ValueError(f""No position data found for {object_name} between {start} and {end}"")
 
     timestamp_attr = next(attr for attr in fetch_attrs if ""timestamps"" in attr)
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820638993,,264,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,S101: Replaced assertions with exceptions ," def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    assert len(target) == 2
+    """"""Compute the distance of the position data from a target point.""""""
+    if len(target) != TARGET_LENGTH:","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -80,18 +75,14 @@

     ):
         """"""Insert a new set of parameters for a given tracking method.""""""
         if tracking_paramset_id is None:
-            tracking_paramset_id = (
-                dj.U().aggr(cls, n=""max(tracking_paramset_id)"").fetch1(""n"") or 0
-            ) + 1
+            tracking_paramset_id = (dj.U().aggr(cls, n=""max(tracking_paramset_id)"").fetch1(""n"") or 0) + 1
 
         param_dict = {
             ""tracking_method"": tracking_method,
             ""tracking_paramset_id"": tracking_paramset_id,
             ""paramset_description"": paramset_description,
             ""params"": params,
-            ""param_set_hash"": dict_to_uuid(
-                {**params, ""tracking_method"": tracking_method}
-            ),
+            ""param_set_hash"": dict_to_uuid({**params, ""tracking_method"": tracking_method}),
         }
         param_query = cls & {""param_set_hash"": param_dict[""param_set_hash""]}
 
@@ -120,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -161,9 +147,7 @@

         return (
             acquisition.Chunk
             * (
-                streams.SpinnakerVideoSource.join(
-                    streams.SpinnakerVideoSource.RemovalTime, left=True
-                )
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
                 & ""spinnaker_video_source_name='CameraTop'""
             )
             * (TrackingParamSet & ""tracking_paramset_id = 1"")
@@ -173,24 +157,29 @@

 
     def make(self, key):
         """"""Ingest SLEAP tracking data for a given chunk.""""""
-        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-            ""chunk_start"", ""chunk_end""
-        )
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
 
         data_dirs = acquisition.Experiment.get_data_directories(key)
 
-        device_name = (streams.SpinnakerVideoSource & key).fetch1(
-            ""spinnaker_video_source_name""
-        )
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
 
         devices_schema = getattr(
             aeon_schemas,
-            (
-                acquisition.Experiment.DevicesSchema
-                & {""experiment_name"": key[""experiment_name""]}
-            ).fetch1(""devices_schema_name""),
-        )
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -200,13 +189,16 @@

         )
 
         if not len(pose_data):
-            raise ValueError(
-                f""No SLEAP data found for {key['experiment_name']} - {device_name}""
-            )
+            raise ValueError(f""No SLEAP data found for {key['experiment_name']} - {device_name}"")
 
         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -214,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -235,9 +224,7 @@

                 if part == anchor_part:
                     identity_likelihood = part_position.identity_likelihood.values
                     if isinstance(identity_likelihood[0], dict):
-                        identity_likelihood = np.array(
-                            [v[identity] for v in identity_likelihood]
-                        )
+                        identity_likelihood = np.array([v[identity] for v in identity_likelihood])
 
             pose_identity_entries.append(
                 {
@@ -254,22 +241,152 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
-TARGET_LENGTH = 2
-
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target point.""""""
-    if len(target) != TARGET_LENGTH:
-        raise ValueError(f""Target must be a list of tuple of length {TARGET_LENGTH}."")
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
+        raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
 
 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0
@@ -318,9 +435,7 @@

     start_query = table & obj_restriction & start_restriction
     end_query = table & obj_restriction & end_restriction
     if not (start_query and end_query):
-        raise ValueError(
-            f""No position data found for {object_name} between {start} and {end}""
-        )
+        raise ValueError(f""No position data found for {object_name} between {start} and {end}"")
 
     time_restriction = (
         f'{start_attr} >= ""{min(start_query.fetch(start_attr))}""'
@@ -328,14 +443,10 @@

     )
 
     # subject's position data in the time slice
-    fetched_data = (table & obj_restriction & time_restriction).fetch(
-        *fetch_attrs, order_by=start_attr
-    )
+    fetched_data = (table & obj_restriction & time_restriction).fetch(*fetch_attrs, order_by=start_attr)
 
     if not len(fetched_data[0]):
-        raise ValueError(
-            f""No position data found for {object_name} between {start} and {end}""
-        )
+        raise ValueError(f""No position data found for {object_name} between {start} and {end}"")
 
     timestamp_attr = next(attr for attr in fetch_attrs if ""timestamps"" in attr)
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820640077,,158,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/load_metadata.py,,S101: Replaced assertions with exceptions ,"         commit = epoch_config[""metadata""][""Revision""]
 
-    assert commit, f'Neither ""Commit"" nor ""Revision"" found in {metadata_yml_filepath}'
+    if not commit:","--- 

+++ 

@@ -18,33 +18,30 @@

 logger = dj.logger
 _weight_scale_rate = 100
 _weight_scale_nest = 1
-_aeon_schemas = [""social01"", ""social02""]
 
 
 def insert_stream_types():
     """"""Insert into streams.streamType table all streams in the aeon schemas.""""""
-    from aeon.schema import schemas as aeon_schemas
+    from aeon.schema import ingestion_schemas as aeon_schemas
 
     streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
 
-    schemas = [getattr(aeon_schemas, aeon_schema) for aeon_schema in _aeon_schemas]
-    for schema in schemas:
-        stream_entries = get_stream_entries(schema)
+    for devices_schema_name in aeon_schemas.__all__:
+        devices_schema = getattr(aeon_schemas, devices_schema_name)
+        stream_entries = get_stream_entries(devices_schema)
 
         for entry in stream_entries:
-            q_param = streams.StreamType & {""stream_hash"": entry[""stream_hash""]}
-            if q_param:  # If the specified stream type already exists
-                pname = q_param.fetch1(""stream_type"")
-                if pname == entry[""stream_type""]:
-                    continue
-                else:
-                    # If the existed stream type does not have the same name:
-                    # human error, trying to add the same content with different name
-                    raise dj.DataJointError(
-                        f""The specified stream type already exists - name: {pname}""
-                    )
-            else:
+            try:
                 streams.StreamType.insert1(entry)
+                logger.info(f""New stream type created: {entry['stream_type']}"")
+            except dj.errors.DuplicateError:
+                existing_stream = (streams.StreamType.proj(
+                    ""stream_reader"", ""stream_reader_kwargs"")
+                                   & {""stream_type"": entry[""stream_type""]}).fetch1()
+                existing_columns = existing_stream[""stream_reader_kwargs""].get(""columns"")
+                entry_columns = entry[""stream_reader_kwargs""].get(""columns"")
+                if existing_columns != entry_columns:
+                    logger.warning(f""Stream type already exists:\n\t{entry}\n\t{existing_stream}"")
 
 
 def insert_device_types(devices_schema: DotMap, metadata_yml_filepath: Path):
@@ -57,9 +54,7 @@

     streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
 
     device_info: dict[dict] = get_device_info(devices_schema)
-    device_type_mapper, device_sn = get_device_mapper(
-        devices_schema, metadata_yml_filepath
-    )
+    device_type_mapper, device_sn = get_device_mapper(devices_schema, metadata_yml_filepath)
 
     # Add device type to device_info. Only add if device types that are defined in Metadata.yml
     device_info = {
@@ -96,8 +91,7 @@

         {""device_type"": device_type, ""stream_type"": stream_type}
         for device_type, stream_list in device_stream_map.items()
         for stream_type in stream_list
-        if not streams.DeviceType.Stream
-        & {""device_type"": device_type, ""stream_type"": stream_type}
+        if not streams.DeviceType.Stream & {""device_type"": device_type, ""stream_type"": stream_type}
     ]
 
     new_devices = [
@@ -106,8 +100,7 @@

             ""device_type"": device_config[""device_type""],
         }
         for device_name, device_config in device_info.items()
-        if device_sn[device_name]
-        and not streams.Device & {""device_serial_number"": device_sn[device_name]}
+        if device_sn[device_name] and not streams.Device & {""device_serial_number"": device_sn[device_name]}
     ]
 
     # Insert new entries.
@@ -125,9 +118,7 @@

         streams.Device.insert(new_devices)
 
 
-def extract_epoch_config(
-    experiment_name: str, devices_schema: DotMap, metadata_yml_filepath: str
-) -> dict:
+def extract_epoch_config(experiment_name: str, devices_schema: DotMap, metadata_yml_filepath: str) -> dict:
     """"""Parse experiment metadata YAML file and extract epoch configuration.
 
     Args:
@@ -139,9 +130,7 @@

         dict: epoch_config [dict]
     """"""
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_start = datetime.datetime.strptime(
-        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
-    )
+    epoch_start = datetime.datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
     epoch_config: dict = (
         io_api.load(
             metadata_yml_filepath.parent.as_posix(),
@@ -156,22 +145,16 @@

         commit = epoch_config[""metadata""][""Revision""]
 
     if not commit:
-        raise ValueError(
-            f'Neither ""Commit"" nor ""Revision"" found in {metadata_yml_filepath}'
-        )
+        raise ValueError(f'Neither ""Commit"" nor ""Revision"" found in {metadata_yml_filepath}')
 
     devices: list[dict] = json.loads(
-        json.dumps(
-            epoch_config[""metadata""][""Devices""], default=lambda x: x.__dict__, indent=4
-        )
+        json.dumps(epoch_config[""metadata""][""Devices""], default=lambda x: x.__dict__, indent=4)
     )
 
     # Maintain backward compatibility - In exp02, it is a list of dict.
     # From presocial onward, it's a dict of dict.
     if isinstance(devices, list):
-        devices: dict = {
-            d.pop(""Name""): d for d in devices
-        }  # {deivce_name: device_config}
+        devices: dict = {d.pop(""Name""): d for d in devices}  # {deivce_name: device_config}
 
     return {
         ""experiment_name"": experiment_name,
@@ -195,17 +178,15 @@

 
     experiment_key = {""experiment_name"": experiment_name}
     metadata_yml_filepath = pathlib.Path(metadata_yml_filepath)
-    epoch_config = extract_epoch_config(
-        experiment_name, devices_schema, metadata_yml_filepath
-    )
+    epoch_config = extract_epoch_config(experiment_name, devices_schema, metadata_yml_filepath)
 
     previous_epoch = (acquisition.Experiment & experiment_key).aggr(
         acquisition.Epoch & f'epoch_start < ""{epoch_config[""epoch_start""]}""',
         epoch_start=""MAX(epoch_start)"",
     )
-    if len(acquisition.EpochConfig.Meta & previous_epoch) and epoch_config[
-        ""commit""
-    ] == (acquisition.EpochConfig.Meta & previous_epoch).fetch1(""commit""):
+    if len(acquisition.EpochConfig.Meta & previous_epoch) and epoch_config[""commit""] == (
+        acquisition.EpochConfig.Meta & previous_epoch
+    ).fetch1(""commit""):
         # if identical commit -> no changes
         return
 
@@ -239,9 +220,7 @@

             table_entry = {
                 ""experiment_name"": experiment_name,
                 **device_key,
-                f""{dj.utils.from_camel_case(table.__name__)}_install_time"": epoch_config[
-                    ""epoch_start""
-                ],
+                f""{dj.utils.from_camel_case(table.__name__)}_install_time"": epoch_config[""epoch_start""],
                 f""{dj.utils.from_camel_case(table.__name__)}_name"": device_name,
             }
 
@@ -258,25 +237,17 @@

                     {
                         **table_entry,
                         ""attribute_name"": ""SamplingFrequency"",
-                        ""attribute_value"": video_controller[
-                            device_config[""TriggerFrequency""]
-                        ],
+                        ""attribute_value"": video_controller[device_config[""TriggerFrequency""]],
                     }
                 )
 
-            """"""
-            Check if this device is currently installed.
-            If the same device serial number is currently installed check for changes in configuration.
-            If not, skip this.
-            """"""
-            current_device_query = (
-                table - table.RemovalTime & experiment_key & device_key
-            )
+            # Check if this device is currently installed.
+            # If the same device serial number is currently installed check for changes in configuration.
+            # If not, skip this.
+            current_device_query = table - table.RemovalTime & experiment_key & device_key
 
             if current_device_query:
-                current_device_config: list[dict] = (
-                    table.Attribute & current_device_query
-                ).fetch(
+                current_device_config: list[dict] = (table.Attribute & current_device_query).fetch(
                     ""experiment_name"",
                     ""device_serial_number"",
                     ""attribute_name"",
@@ -284,11 +255,7 @@

                     as_dict=True,
                 )
                 new_device_config: list[dict] = [
-                    {
-                        k: v
-                        for k, v in entry.items()
-                        if dj.utils.from_camel_case(table.__name__) not in k
-                    }
+                    {k: v for k, v in entry.items() if dj.utils.from_camel_case(table.__name__) not in k}
                     for entry in table_attribute_entry
                 ]
 
@@ -298,10 +265,7 @@

                         for config in current_device_config
                     }
                 ) == dict_to_uuid(
-                    {
-                        config[""attribute_name""]: config[""attribute_value""]
-                        for config in new_device_config
-                    }
+                    {config[""attribute_name""]: config[""attribute_value""] for config in new_device_config}
                 ):  # Skip if none of the configuration has changed.
                     continue
 
@@ -339,7 +303,7 @@

     return set(epoch_device_types)
 
 
-# region Get stream & device information
+# Get stream & device information
 def get_stream_entries(devices_schema: DotMap) -> list[dict]:
     """"""Returns a list of dictionaries containing the stream entries for a given device.
 
@@ -412,57 +376,35 @@

 
         if isinstance(device, DotMap):
             for stream_type, stream_obj in device.items():
-                if stream_obj.__class__.__module__ in [
-                    ""aeon.io.reader"",
-                    ""aeon.schema.foraging"",
-                    ""aeon.schema.octagon"",
-                    ""aeon.schema.social"",
-                ]:
-                    device_info[device_name][""stream_type""].append(stream_type)
-                    device_info[device_name][""stream_reader""].append(
-                        _get_class_path(stream_obj)
-                    )
-
-                    required_args = [
-                        k
-                        for k in inspect.signature(stream_obj.__init__).parameters
-                        if k != ""self""
-                    ]
-                    pattern = schema_dict[device_name][stream_type].get(""pattern"")
-                    schema_dict[device_name][stream_type][""pattern""] = pattern.replace(
-                        device_name, ""{pattern}""
-                    )
-
-                    kwargs = {
-                        k: v
-                        for k, v in schema_dict[device_name][stream_type].items()
-                        if k in required_args
-                    }
-                    device_info[device_name][""stream_reader_kwargs""].append(kwargs)
-                    # Add hash
-                    device_info[device_name][""stream_hash""].append(
-                        dict_to_uuid(
-                            {**kwargs, ""stream_reader"": _get_class_path(stream_obj)}
-                        )
-                    )
+                device_info[device_name][""stream_type""].append(stream_type)
+                device_info[device_name][""stream_reader""].append(_get_class_path(stream_obj))
+
+                required_args = [
+                    k for k in inspect.signature(stream_obj.__init__).parameters if k != ""self""
+                ]
+                pattern = schema_dict[device_name][stream_type].get(""pattern"")
+                schema_dict[device_name][stream_type][""pattern""] = pattern.replace(
+                    device_name, ""{pattern}""
+                )
+
+                kwargs = {
+                    k: v for k, v in schema_dict[device_name][stream_type].items() if k in required_args
+                }
+                device_info[device_name][""stream_reader_kwargs""].append(kwargs)
+                # Add hash
+                device_info[device_name][""stream_hash""].append(
+                    dict_to_uuid({**kwargs, ""stream_reader"": _get_class_path(stream_obj)})
+                )
         else:
             stream_type = device.__class__.__name__
             device_info[device_name][""stream_type""].append(stream_type)
             device_info[device_name][""stream_reader""].append(_get_class_path(device))
 
-            required_args = {
-                k: None
-                for k in inspect.signature(device.__init__).parameters
-                if k != ""self""
-            }
+            required_args = {k: None for k in inspect.signature(device.__init__).parameters if k != ""self""}
             pattern = schema_dict[device_name].get(""pattern"")
-            schema_dict[device_name][""pattern""] = pattern.replace(
-                device_name, ""{pattern}""
-            )
-
-            kwargs = {
-                k: v for k, v in schema_dict[device_name].items() if k in required_args
-            }
+            schema_dict[device_name][""pattern""] = pattern.replace(device_name, ""{pattern}"")
+
+            kwargs = {k: v for k, v in schema_dict[device_name].items() if k in required_args}
             device_info[device_name][""stream_reader_kwargs""].append(kwargs)
             # Add hash
             device_info[device_name][""stream_hash""].append(
@@ -558,9 +500,7 @@

         (""Wall8"", ""Wall""),
     ]
 
-    epoch_start = datetime.datetime.strptime(
-        metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S""
-    )
+    epoch_start = datetime.datetime.strptime(metadata_yml_filepath.parent.name, ""%Y-%m-%dT%H-%M-%S"")
 
     for device_idx, (device_name, device_type) in enumerate(oct01_devices):
         device_sn = f""oct01_{device_idx}""
@@ -569,13 +509,5 @@

             skip_duplicates=True,
         )
         experiment_table = getattr(streams, f""Experiment{device_type}"")
-        if not (
-            experiment_table
-            & {""experiment_name"": experiment_name, ""device_serial_number"": device_sn}
-        ):
-            experiment_table.insert1(
-                (experiment_name, device_sn, epoch_start, device_name)
-            )
-
-
-# endregion
+        if not (experiment_table & {""experiment_name"": experiment_name, ""device_serial_number"": device_sn}):
+            experiment_table.insert1((experiment_name, device_sn, epoch_start, device_name))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820641373,,56,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/paths.py,,"UP038: Used the new X | Y syntax for isinstance calls, as introduced in Python 3.10."," 
     # turn to list if only a single root directory is provided
-    if isinstance(root_directories, (str, pathlib.Path)):
+    if isinstance(root_directories, (str | pathlib.Path)):","--- 

+++ 

@@ -34,7 +34,7 @@

 def find_root_directory(
     root_directories: str | pathlib.Path, full_path: str | pathlib.Path
 ) -> pathlib.Path:
-    """"""Given multiple potential root directories and a full-path, search and return one directory that is the parent of the given path.
+    """"""Finds the parent directory of a given full path among multiple potential root directories.
 
     Args:
         root_directories (str | pathlib.Path): A list of potential root directories.
@@ -46,7 +46,7 @@

 
     Returns:
         pathlib.Path: The full path to the discovered root directory.
-    """"""  # noqa E501
+    """"""
     full_path = pathlib.Path(full_path)
 
     if not full_path.exists():
@@ -65,6 +65,5 @@

 
     except StopIteration as err:
         raise FileNotFoundError(
-            f""No valid root directory found (from {root_directories})""
-            f"" for {full_path}""
+            f""No valid root directory found (from {root_directories})"" f"" for {full_path}""
         ) from err"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820641889,,66,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/paths.py,,B904: Enhanced exception handling by using as err and raising exceptions with raise ... from err to differentiate errors.,"         )
 
-    except StopIteration:
+    except StopIteration as err:","--- 

+++ 

@@ -34,7 +34,7 @@

 def find_root_directory(
     root_directories: str | pathlib.Path, full_path: str | pathlib.Path
 ) -> pathlib.Path:
-    """"""Given multiple potential root directories and a full-path, search and return one directory that is the parent of the given path.
+    """"""Finds the parent directory of a given full path among multiple potential root directories.
 
     Args:
         root_directories (str | pathlib.Path): A list of potential root directories.
@@ -46,7 +46,7 @@

 
     Returns:
         pathlib.Path: The full path to the discovered root directory.
-    """"""  # noqa E501
+    """"""
     full_path = pathlib.Path(full_path)
 
     if not full_path.exists():
@@ -65,6 +65,5 @@

 
     except StopIteration as err:
         raise FileNotFoundError(
-            f""No valid root directory found (from {root_directories})""
-            f"" for {full_path}""
+            f""No valid root directory found (from {root_directories})"" f"" for {full_path}""
         ) from err"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820643418,,580,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,B006: Fixed mutable default argument in plotting.py.,"     Returns:
         region (pd.DataFrame): Timestamped region info
     """"""
+    if attrs is None:","--- 

+++ 

@@ -25,29 +25,26 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
-    ).fetch(
-        ""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff""
-    )
+    ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
 
     nSessions = len(sess_starts)
     longest_rateDiff = np.max([len(t) for t in rate_timestamps])
 
     max_session_idx = np.argmax([len(t) for t in rate_timestamps])
-    max_session_elapsed_times = (
-        rate_timestamps[max_session_idx] - rate_timestamps[max_session_idx][0]
-    )
+    max_session_elapsed_times = rate_timestamps[max_session_idx] - rate_timestamps[max_session_idx][0]
     x_labels = [t.total_seconds() / 60 for t in max_session_elapsed_times]
 
     y_labels = [
@@ -92,15 +89,12 @@

     ```
     """"""
     distance_travelled_query = (
-        analysis.InArenaSummary.FoodPatch
-        * acquisition.ExperimentFoodPatch.proj(""food_patch_description"")
+        analysis.InArenaSummary.FoodPatch * acquisition.ExperimentFoodPatch.proj(""food_patch_description"")
         & session_keys
     )
 
     distance_travelled_df = (
-        distance_travelled_query.proj(
-            ""food_patch_description"", ""wheel_distance_travelled""
-        )
+        distance_travelled_query.proj(""food_patch_description"", ""wheel_distance_travelled"")
         .fetch(format=""frame"")
         .reset_index()
     )
@@ -108,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -136,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -164,8 +156,7 @@

             & session_keys
         )
         .aggr(
-            analysis.InArenaTimeDistribution.FoodPatch
-            * acquisition.ExperimentFoodPatch,
+            analysis.InArenaTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch,
             avg_in_patch=""AVG(time_fraction_in_patch)"",
         )
         .fetch(""subject"", ""food_patch_description"", ""avg_in_patch"")
@@ -228,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -248,15 +240,11 @@

             .reset_index()
         )
     else:
-        visit_per_day_df = (
-            (VisitSummary & visit_key).fetch(format=""frame"").reset_index()
-        )
+        visit_per_day_df = (VisitSummary & visit_key).fetch(format=""frame"").reset_index()
         if not attr.startswith(""total""):
             attr = ""total_"" + attr
 
-    visit_per_day_df[""day""] = (
-        visit_per_day_df[""visit_date""] - visit_per_day_df[""visit_date""].min()
-    )
+    visit_per_day_df[""day""] = visit_per_day_df[""visit_date""] - visit_per_day_df[""visit_date""].min()
     visit_per_day_df[""day""] = visit_per_day_df[""day""].dt.days
 
     fig = px.bar(
@@ -317,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -350,14 +343,10 @@

         else [foraging_bouts[""bout_start""].dt.floor(""D"")]
     )
 
-    foraging_bouts_count = (
-        foraging_bouts.groupby(group_by_attrs).size().reset_index(name=""count"")
-    )
+    foraging_bouts_count = foraging_bouts.groupby(group_by_attrs).size().reset_index(name=""count"")
 
     visit_start = (VisitEnd & visit_key).fetch1(""visit_start"")
-    foraging_bouts_count[""day""] = (
-        foraging_bouts_count[""bout_start""].dt.date - visit_start.date()
-    ).dt.days
+    foraging_bouts_count[""day""] = (foraging_bouts_count[""bout_start""].dt.date - visit_start.date()).dt.days
 
     fig = px.bar(
         foraging_bouts_count,
@@ -371,10 +360,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        title=visit_key[""subject""]
-        + ""<br><i>Foraging bouts: count (freq='""
-        + freq
-        + ""')"",
+        title=visit_key[""subject""] + ""<br><i>Foraging bouts: count (freq='"" + freq + ""')"",
     )
 
     fig.update_layout(
@@ -448,9 +434,7 @@

 
     fig = go.Figure()
     if per_food_patch:
-        patch_names = (acquisition.ExperimentFoodPatch & visit_key).fetch(
-            ""food_patch_description""
-        )
+        patch_names = (acquisition.ExperimentFoodPatch & visit_key).fetch(""food_patch_description"")
         for patch in patch_names:
             bouts = foraging_bouts[foraging_bouts[""food_patch_description""] == patch]
             fig.add_trace(
@@ -477,9 +461,7 @@

     )
 
     fig.update_layout(
-        title_text=visit_key[""subject""]
-        + ""<br><i>Foraging bouts: ""
-        + attr.replace(""_"", "" ""),
+        title_text=visit_key[""subject""] + ""<br><i>Foraging bouts: "" + attr.replace(""_"", "" ""),
         xaxis_title=""date"",
         yaxis_title=attr.replace(""_"", "" ""),
         violingap=0,
@@ -488,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -518,17 +494,11 @@

     region = _get_region_data(visit_key)
 
     # Compute time spent per region
-    time_spent = (
-        region.groupby([region.index.floor(freq), ""region""])
-        .size()
-        .reset_index(name=""count"")
-    )
-    time_spent[""time_fraction""] = time_spent[""count""] / time_spent.groupby(
-        ""timestamps""
-    )[""count""].transform(""sum"")
-    time_spent[""day""] = (
-        time_spent[""timestamps""] - time_spent[""timestamps""].min()
-    ).dt.days
+    time_spent = region.groupby([region.index.floor(freq), ""region""]).size().reset_index(name=""count"")
+    time_spent[""time_fraction""] = time_spent[""count""] / time_spent.groupby(""timestamps"")[""count""].transform(
+        ""sum""
+    )
+    time_spent[""day""] = (time_spent[""timestamps""] - time_spent[""timestamps""].min()).dt.days
 
     fig = px.bar(
         time_spent,
@@ -540,10 +510,7 @@

             ""time_fraction"": ""time fraction"",
             ""timestamps"": ""date"" if freq == ""D"" else ""time"",
         },
-        title=visit_key[""subject""]
-        + ""<br><i>Fraction of time spent in each region (freq='""
-        + freq
-        + ""')"",
+        title=visit_key[""subject""] + ""<br><i>Fraction of time spent in each region (freq='"" + freq + ""')"",
         width=700,
         height=400,
         template=""simple_white"",
@@ -572,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info
@@ -587,9 +555,7 @@

     for attr in attrs:
         if attr == ""in_nest"":  # Nest
             in_nest = np.concatenate(
-                (VisitTimeDistribution.Nest & visit_key).fetch(
-                    attr, order_by=""visit_date""
-                )
+                (VisitTimeDistribution.Nest & visit_key).fetch(attr, order_by=""visit_date"")
             )
             region = pd.concat(
                 [
@@ -604,16 +570,14 @@

         elif attr == ""in_patch"":  # Food patch
             # Find all patches
             patches = np.unique(
-                (
-                    VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
-                    & visit_key
-                ).fetch(""food_patch_description"")
+                (VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch & visit_key).fetch(
+                    ""food_patch_description""
+                )
             )
             for patch in patches:
                 in_patch = np.concatenate(
                     (
-                        VisitTimeDistribution.FoodPatch
-                        * acquisition.ExperimentFoodPatch
+                        VisitTimeDistribution.FoodPatch * acquisition.ExperimentFoodPatch
                         & visit_key
                         & f""food_patch_description = '{patch}'""
                     ).fetch(""in_patch"", order_by=""visit_date"")
@@ -645,19 +609,13 @@

     region = region.sort_index().rename_axis(""timestamps"")
 
     # Exclude data during maintenance
-    maintenance_period = get_maintenance_periods(
-        visit_key[""experiment_name""], visit_start, visit_end
-    )
-    region = filter_out_maintenance_periods(
-        region, maintenance_period, visit_end, dropna=True
-    )
+    maintenance_period = get_maintenance_periods(visit_key[""experiment_name""], visit_start, visit_end)
+    region = filter_out_maintenance_periods(region, maintenance_period, visit_end, dropna=True)
 
     return region
 
 
-def plot_weight_patch_data(
-    visit_key, freq=""H"", smooth_weight=True, min_weight=0, max_weight=35
-):
+def plot_weight_patch_data(visit_key, freq=""H"", smooth_weight=True, min_weight=0, max_weight=35):
     """"""Plot subject weight and patch data (pellet trigger count) per visit.
 
     Args:
@@ -674,9 +632,7 @@

         >>> fig = plot_weight_patch_data(visit_key, freq=""H"", smooth_weight=True)
         >>> fig = plot_weight_patch_data(visit_key, freq=""D"")
     """"""
-    subject_weight = _get_filtered_subject_weight(
-        visit_key, smooth_weight, min_weight, max_weight
-    )
+    subject_weight = _get_filtered_subject_weight(visit_key, smooth_weight, min_weight, max_weight)
 
     # Count pellet trigger per patch per day/hour/...
     patch = _get_patch_data(visit_key)
@@ -704,12 +660,8 @@

     for p in patch_names:
         fig.add_trace(
             go.Bar(
-                x=patch_summary[patch_summary[""food_patch_description""] == p][
-                    ""event_time""
-                ],
-                y=patch_summary[patch_summary[""food_patch_description""] == p][
-                    ""event_type""
-                ],
+                x=patch_summary[patch_summary[""food_patch_description""] == p][""event_time""],
+                y=patch_summary[patch_summary[""food_patch_description""] == p][""event_type""],
                 name=p,
             ),
             secondary_y=False,
@@ -734,10 +686,7 @@

     fig.update_layout(
         barmode=""stack"",
         hovermode=""x"",
-        title_text=visit_key[""subject""]
-        + ""<br><i>Weight and pellet count (freq='""
-        + freq
-        + ""')"",
+        title_text=visit_key[""subject""] + ""<br><i>Weight and pellet count (freq='"" + freq + ""')"",
         xaxis_title=""date"" if freq == ""D"" else ""time"",
         yaxis={""title"": ""pellet count""},
         yaxis2={""title"": ""weight""},
@@ -758,9 +707,7 @@

     return fig
 
 
-def _get_filtered_subject_weight(
-    visit_key, smooth_weight=True, min_weight=0, max_weight=35
-):
+def _get_filtered_subject_weight(visit_key, smooth_weight=True, min_weight=0, max_weight=35):
     """"""Retrieve subject weight from WeightMeasurementFiltered table.
 
     Args:
@@ -799,9 +746,7 @@

     subject_weight = subject_weight.loc[visit_start:visit_end]
 
     # Exclude data during maintenance
-    maintenance_period = get_maintenance_periods(
-        visit_key[""experiment_name""], visit_start, visit_end
-    )
+    maintenance_period = get_maintenance_periods(visit_key[""experiment_name""], visit_start, visit_end)
     subject_weight = filter_out_maintenance_periods(
         subject_weight, maintenance_period, visit_end, dropna=True
     )
@@ -818,9 +763,7 @@

     subject_weight = subject_weight.resample(""1T"").mean().dropna()
 
     if smooth_weight:
-        subject_weight[""weight_subject""] = savgol_filter(
-            subject_weight[""weight_subject""], 10, 3
-        )
+        subject_weight[""weight_subject""] = savgol_filter(subject_weight[""weight_subject""], 10, 3)
 
     return subject_weight
 
@@ -841,9 +784,7 @@

         (
             dj.U(""event_time"", ""event_type"", ""food_patch_description"")
             & (
-                acquisition.FoodPatchEvent
-                * acquisition.EventType
-                * acquisition.ExperimentFoodPatch
+                acquisition.FoodPatchEvent * acquisition.EventType * acquisition.ExperimentFoodPatch
                 & f'event_time BETWEEN ""{visit_start}"" AND ""{visit_end}""'
                 & 'event_type = ""TriggerPellet""'
             )
@@ -856,11 +797,7 @@

     # TODO: handle repeat attempts (pellet delivery trigger and beam break)
 
     # Exclude data during maintenance
-    maintenance_period = get_maintenance_periods(
-        visit_key[""experiment_name""], visit_start, visit_end
-    )
-    patch = filter_out_maintenance_periods(
-        patch, maintenance_period, visit_end, dropna=True
-    )
+    maintenance_period = get_maintenance_periods(visit_key[""experiment_name""], visit_start, visit_end)
+    patch = filter_out_maintenance_periods(patch, maintenance_period, visit_end, dropna=True)
 
     return patch"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820644944,,150,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"B021: Removed dynamic references and made the docstring static in DeviceDataStream.key_source, ensuring it remains informative.","-            Only the combination of Chunk and {device_type} with overlapping time
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
+            """"""Only the combination of Chunk and device_type with overlapping time.","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,22 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed
-                                                    # and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -111,30 +110,27 @@

     # DeviceDataStream table(s)
     stream_detail = (
         streams_module.StreamType
-        & (
-            streams_module.DeviceType.Stream
-            & {""device_type"": device_type, ""stream_type"": stream_type}
-        )
+        & (streams_module.DeviceType.Stream & {""device_type"": device_type, ""stream_type"": stream_type})
     ).fetch1()
 
-    for i, n in enumerate(stream_detail[""stream_reader""].split(""."")):
-        reader = aeon if i == 0 else getattr(reader, n)  # noqa: F821
+    reader = aeon
+    for n in stream_detail[""stream_reader""].split(""."")[1:]:
+        reader = getattr(reader, n)
 
     if reader is aeon.io.reader.Pose:
-        logger.warning(
-            ""Automatic generation of stream table for Pose reader is not supported. Skipping...""
-        )
+        logger.warning(""Automatic generation of stream table for Pose reader is not supported. Skipping..."")
         return None, None
 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -147,42 +143,33 @@

 
         @property
         def key_source(self):
-            """"""Only the combination of Chunk and device_type with overlapping time.
-
-            +  Chunk(s) that started after device_type install time and ended before device_type remove time
-            +  Chunk(s) that started after device_type install time for device_type that are not yet removed
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
             """"""
-            key_source_query = (
-                acquisition.Chunk
-                * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
-                & f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""
-                & f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,\
-                ""2200-01-01"")'
-            )
-
-            return key_source_query
+            self.__doc__ = docstring
+            device_type_name = dj.utils.from_camel_case(device_type)
+            return (
+                acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
+                & f""chunk_start >= {device_type_name}_install_time""
+                & f'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'
+            )
 
         def make(self, key):
             """"""Load and insert the data for the DeviceDataStream table.""""""
-            chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-                ""chunk_start"", ""chunk_end""
-            )
+            chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
-            device_name = (ExperimentDevice & key).fetch1(
-                f""{dj.utils.from_camel_case(device_type)}_name""
-            )
+            device_name = (ExperimentDevice & key).fetch1(f""{dj.utils.from_camel_case(device_type)}_name"")
 
             devices_schema = getattr(
                 aeon_schemas,
-                (
-                    acquisition.Experiment.DevicesSchema
-                    & {""experiment_name"": key[""experiment_name""]}
-                ).fetch1(""devices_schema_name""),
-            )
-            stream_reader = getattr(
-                getattr(devices_schema, device_name), ""{stream_type}""
-            )
+                (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                    ""devices_schema_name""
+                ),
+            )
+            stream_reader = getattr(getattr(devices_schema, device_name), ""{stream_type}"")
 
             stream_data = io_api.load(
                 root=data_dirs,
@@ -226,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -235,10 +222,6 @@

                 device_table_def = inspect.getsource(table_class).lstrip()
                 full_def = ""@schema \n"" + device_table_def + ""\n\n""
                 f.write(full_def)
-    else:
-        raise FileExistsError(
-            f""File {_STREAMS_MODULE_FILE} already exists. Please remove it and try again.""
-        )
 
     streams = importlib.import_module(""aeon.dj_pipeline.streams"")
 
@@ -289,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa: E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa: W291, E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820646910,,238,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,Enhanced the condition here by adding an `else`,"                 device_table_def = inspect.getsource(table_class).lstrip()
                 full_def = ""@schema \n"" + device_table_def + ""\n\n""
                 f.write(full_def)
+    else:","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,22 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed
-                                                    # and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -111,30 +110,27 @@

     # DeviceDataStream table(s)
     stream_detail = (
         streams_module.StreamType
-        & (
-            streams_module.DeviceType.Stream
-            & {""device_type"": device_type, ""stream_type"": stream_type}
-        )
+        & (streams_module.DeviceType.Stream & {""device_type"": device_type, ""stream_type"": stream_type})
     ).fetch1()
 
-    for i, n in enumerate(stream_detail[""stream_reader""].split(""."")):
-        reader = aeon if i == 0 else getattr(reader, n)  # noqa: F821
+    reader = aeon
+    for n in stream_detail[""stream_reader""].split(""."")[1:]:
+        reader = getattr(reader, n)
 
     if reader is aeon.io.reader.Pose:
-        logger.warning(
-            ""Automatic generation of stream table for Pose reader is not supported. Skipping...""
-        )
+        logger.warning(""Automatic generation of stream table for Pose reader is not supported. Skipping..."")
         return None, None
 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -147,42 +143,33 @@

 
         @property
         def key_source(self):
-            """"""Only the combination of Chunk and device_type with overlapping time.
-
-            +  Chunk(s) that started after device_type install time and ended before device_type remove time
-            +  Chunk(s) that started after device_type install time for device_type that are not yet removed
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
             """"""
-            key_source_query = (
-                acquisition.Chunk
-                * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
-                & f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""
-                & f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,\
-                ""2200-01-01"")'
-            )
-
-            return key_source_query
+            self.__doc__ = docstring
+            device_type_name = dj.utils.from_camel_case(device_type)
+            return (
+                acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
+                & f""chunk_start >= {device_type_name}_install_time""
+                & f'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'
+            )
 
         def make(self, key):
             """"""Load and insert the data for the DeviceDataStream table.""""""
-            chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(
-                ""chunk_start"", ""chunk_end""
-            )
+            chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
-            device_name = (ExperimentDevice & key).fetch1(
-                f""{dj.utils.from_camel_case(device_type)}_name""
-            )
+            device_name = (ExperimentDevice & key).fetch1(f""{dj.utils.from_camel_case(device_type)}_name"")
 
             devices_schema = getattr(
                 aeon_schemas,
-                (
-                    acquisition.Experiment.DevicesSchema
-                    & {""experiment_name"": key[""experiment_name""]}
-                ).fetch1(""devices_schema_name""),
-            )
-            stream_reader = getattr(
-                getattr(devices_schema, device_name), ""{stream_type}""
-            )
+                (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                    ""devices_schema_name""
+                ),
+            )
+            stream_reader = getattr(getattr(devices_schema, device_name), ""{stream_type}"")
 
             stream_data = io_api.load(
                 root=data_dirs,
@@ -226,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -235,10 +222,6 @@

                 device_table_def = inspect.getsource(table_class).lstrip()
                 full_def = ""@schema \n"" + device_table_def + ""\n\n""
                 f.write(full_def)
-    else:
-        raise FileExistsError(
-            f""File {_STREAMS_MODULE_FILE} already exists. Please remove it and try again.""
-        )
 
     streams = importlib.import_module(""aeon.dj_pipeline.streams"")
 
@@ -289,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa: E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa: W291, E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820647368,,27,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/video.py,,S101: Replaced assertions with exceptions ,"     raw_data_dir = Path(raw_data_dir)
-    assert raw_data_dir.exists()
-
+    if not raw_data_dir.exists():",
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820650333,,172,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,UP015 Unnecessary open mode parameters,"     def read(self, file):
         """"""Reads data from the specified jsonl file.""""""
-        with open(file, ""r"") as f:
+        with open(file) as f:","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -70,29 +69,22 @@

         payloadtype = _payloadtypes[data[4] & ~0x10]
         elementsize = payloadtype.itemsize
         payloadshape = (length, payloadsize // elementsize)
-        seconds = np.ndarray(
-            length, dtype=np.uint32, buffer=data, offset=5, strides=stride
-        )
-        ticks = np.ndarray(
-            length, dtype=np.uint16, buffer=data, offset=9, strides=stride
-        )
+        seconds = np.ndarray(length, dtype=np.uint32, buffer=data, offset=5, strides=stride)
+        ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
-            data = pd.DataFrame(
-                payload, index=seconds, columns=self.columns[: payloadshape[1]]
-            )
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -117,17 +109,13 @@

 
     def __init__(self, pattern=""Metadata""):
         """"""Initialize the object with the specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml""
-        )
+        super().__init__(pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml"")
 
     def read(self, file):
         """"""Returns metadata for the specified epoch.""""""
         epoch_str = file.parts[-2]
         date_str, time_str = epoch_str.split(""T"")
-        time = datetime.datetime.fromisoformat(
-            date_str + ""T"" + time_str.replace(""-"", "":"")
-        )
+        time = datetime.datetime.fromisoformat(date_str + ""T"" + time_str.replace(""-"", "":""))
         with open(file) as fp:
             metadata = json.load(fp)
         workflow = metadata.pop(""Workflow"")
@@ -173,7 +161,7 @@

             df = pd.read_json(f, lines=True)
         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])
+            df[column] = df[self.root_key].apply(lambda x: x[column])  # noqa B023
         return df
 
 
@@ -181,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -196,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -211,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -223,60 +214,43 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
-        super().__init__(
-            pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""]
-        )
+        super().__init__(pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""])
 
 
 class BitmaskEvent(Harp):
     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -300,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -322,15 +297,14 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""]
-        )
+        super().__init__(pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""])
         self._rawcolumns = [""time""] + self.columns[0:2]
 
     def read(self, file):
@@ -347,29 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
-    """"""
-
-    def __init__(
-        self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""
-    ):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
+    """"""
+
+    def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -396,53 +388,34 @@

         # Drop any repeat parts.
         unique_parts, unique_idxs = np.unique(parts, return_index=True)
         repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
-        if (
-            repeat_idxs
-        ):  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
             init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
-            rep_part_col_idxs = np.concatenate(
-                [np.arange(i, i + 3) for i in init_rep_part_col_idx]
-            )
-            keep_part_col_idxs = np.setdiff1d(
-                np.arange(len(data.columns)), rep_part_col_idxs
-            )
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
             data = data.iloc[:, keep_part_col_idxs]
             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = pd.Series(
-            [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
-        )
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
-                columns[0 : (len(identities) + 1)]
-                if bonsai_sleap_v == BONSAI_SLEAP_V3
-                else columns[0:2]
+                columns[0 : (len(identities) + 1)] if bonsai_sleap_v == BONSAI_SLEAP_V3 else columns[0:2]
             )
             part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             part_data = pd.DataFrame(data[part_columns])
             if bonsai_sleap_v == BONSAI_SLEAP_V3:
                 # combine all identity_likelihood cols into a single col as dict
                 part_data[""identity_likelihood""] = part_data.apply(
-                    lambda row: {
-                        identity: row[f""{identity}_likelihood""]
-                        for identity in identities
-                    },
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
                     axis=1,
                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -481,29 +454,19 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod
-    def get_config_file(
-        cls, config_file_dir: Path, config_file_names: None | list[str] = None
-    ) -> Path:
+    def get_config_file(cls, config_file_dir: Path, config_file_names: None | list[str] = None) -> Path:
         """"""Returns the config file from a model's config directory.""""""
         if config_file_names is None:
-            config_file_names = [
-                ""confmap_config.json""
-            ]  # SLEAP (add for other trackers to this list)
+            config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
         config_file = None
         for f in config_file_names:
             if (config_file_dir / f).exists():
@@ -522,21 +485,14 @@

         return globals()[reader_type](pattern=pattern, **kwargs)
 
     return DotMap(
-        {
-            k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k)
-            for k, v in data.items()
-        }
+        {k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k) for k, v in data.items()}
     )
 
 
 def to_dict(dotmap):
     """"""Converts a DotMap object to a dictionary.""""""
     if isinstance(dotmap, Reader):
-        kwargs = {
-            k: v
-            for k, v in vars(dotmap).items()
-            if k not in [""pattern""] and not k.startswith(""_"")
-        }
+        kwargs = {k: v for k, v in vars(dotmap).items() if k not in [""pattern""] and not k.startswith(""_"")}
         kwargs[""type""] = type(dotmap).__name__
         return kwargs
     return {k: to_dict(v) for k, v in dotmap.items()}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820652338,,176,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,Refactor: Updated the `read` method in the `JsonList` class within `aeon/io/reader.py.`,"         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x: x[column])
+            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -70,29 +69,22 @@

         payloadtype = _payloadtypes[data[4] & ~0x10]
         elementsize = payloadtype.itemsize
         payloadshape = (length, payloadsize // elementsize)
-        seconds = np.ndarray(
-            length, dtype=np.uint32, buffer=data, offset=5, strides=stride
-        )
-        ticks = np.ndarray(
-            length, dtype=np.uint16, buffer=data, offset=9, strides=stride
-        )
+        seconds = np.ndarray(length, dtype=np.uint32, buffer=data, offset=5, strides=stride)
+        ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
-            data = pd.DataFrame(
-                payload, index=seconds, columns=self.columns[: payloadshape[1]]
-            )
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -117,17 +109,13 @@

 
     def __init__(self, pattern=""Metadata""):
         """"""Initialize the object with the specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml""
-        )
+        super().__init__(pattern, columns=[""workflow"", ""commit"", ""metadata""], extension=""yml"")
 
     def read(self, file):
         """"""Returns metadata for the specified epoch.""""""
         epoch_str = file.parts[-2]
         date_str, time_str = epoch_str.split(""T"")
-        time = datetime.datetime.fromisoformat(
-            date_str + ""T"" + time_str.replace(""-"", "":"")
-        )
+        time = datetime.datetime.fromisoformat(date_str + ""T"" + time_str.replace(""-"", "":""))
         with open(file) as fp:
             metadata = json.load(fp)
         workflow = metadata.pop(""Workflow"")
@@ -173,7 +161,7 @@

             df = pd.read_json(f, lines=True)
         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])
+            df[column] = df[self.root_key].apply(lambda x: x[column])  # noqa B023
         return df
 
 
@@ -181,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -196,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -211,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -223,60 +214,43 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
-        super().__init__(
-            pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""]
-        )
+        super().__init__(pattern, columns=[""x"", ""y"", ""angle"", ""major"", ""minor"", ""area"", ""id""])
 
 
 class BitmaskEvent(Harp):
     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -300,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -322,15 +297,14 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern.""""""
-        super().__init__(
-            pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""]
-        )
+        super().__init__(pattern, columns=[""hw_counter"", ""hw_timestamp"", ""_frame"", ""_path"", ""_epoch""])
         self._rawcolumns = [""time""] + self.columns[0:2]
 
     def read(self, file):
@@ -347,29 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
-    """"""
-
-    def __init__(
-        self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""
-    ):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
+    """"""
+
+    def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -396,53 +388,34 @@

         # Drop any repeat parts.
         unique_parts, unique_idxs = np.unique(parts, return_index=True)
         repeat_idxs = np.setdiff1d(np.arange(len(parts)), unique_idxs)
-        if (
-            repeat_idxs
-        ):  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
+        if repeat_idxs:  # drop x, y, and likelihood cols for repeat parts (skip first 5 cols)
             init_rep_part_col_idx = (repeat_idxs - 1) * 3 + 5
-            rep_part_col_idxs = np.concatenate(
-                [np.arange(i, i + 3) for i in init_rep_part_col_idx]
-            )
-            keep_part_col_idxs = np.setdiff1d(
-                np.arange(len(data.columns)), rep_part_col_idxs
-            )
+            rep_part_col_idxs = np.concatenate([np.arange(i, i + 3) for i in init_rep_part_col_idx])
+            keep_part_col_idxs = np.setdiff1d(np.arange(len(data.columns)), rep_part_col_idxs)
             data = data.iloc[:, keep_part_col_idxs]
             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
-        new_columns = pd.Series(
-            [""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""]
-        )
+        new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
         new_data = pd.DataFrame(columns=new_columns)
         for i, part in enumerate(parts):
             part_columns = (
-                columns[0 : (len(identities) + 1)]
-                if bonsai_sleap_v == BONSAI_SLEAP_V3
-                else columns[0:2]
+                columns[0 : (len(identities) + 1)] if bonsai_sleap_v == BONSAI_SLEAP_V3 else columns[0:2]
             )
             part_columns.extend([f""{part}_x"", f""{part}_y"", f""{part}_likelihood""])
             part_data = pd.DataFrame(data[part_columns])
             if bonsai_sleap_v == BONSAI_SLEAP_V3:
                 # combine all identity_likelihood cols into a single col as dict
                 part_data[""identity_likelihood""] = part_data.apply(
-                    lambda row: {
-                        identity: row[f""{identity}_likelihood""]
-                        for identity in identities
-                    },
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
                     axis=1,
                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -481,29 +454,19 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod
-    def get_config_file(
-        cls, config_file_dir: Path, config_file_names: None | list[str] = None
-    ) -> Path:
+    def get_config_file(cls, config_file_dir: Path, config_file_names: None | list[str] = None) -> Path:
         """"""Returns the config file from a model's config directory.""""""
         if config_file_names is None:
-            config_file_names = [
-                ""confmap_config.json""
-            ]  # SLEAP (add for other trackers to this list)
+            config_file_names = [""confmap_config.json""]  # SLEAP (add for other trackers to this list)
         config_file = None
         for f in config_file_names:
             if (config_file_dir / f).exists():
@@ -522,21 +485,14 @@

         return globals()[reader_type](pattern=pattern, **kwargs)
 
     return DotMap(
-        {
-            k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k)
-            for k, v in data.items()
-        }
+        {k: from_dict(v, f""{pattern}_{k}"" if pattern is not None else k) for k, v in data.items()}
     )
 
 
 def to_dict(dotmap):
     """"""Converts a DotMap object to a dictionary.""""""
     if isinstance(dotmap, Reader):
-        kwargs = {
-            k: v
-            for k, v in vars(dotmap).items()
-            if k not in [""pattern""] and not k.startswith(""_"")
-        }
+        kwargs = {k: v for k, v in vars(dotmap).items() if k not in [""pattern""] and not k.startswith(""_"")}
         kwargs[""type""] = type(dotmap).__name__
         return kwargs
     return {k: to_dict(v) for k, v in dotmap.items()}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820655585,,2,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/social_03.py,,deleted unused dependencies,"-import json
-import pandas as pd
+""""""This module contains the schema for the social_03 dataset.""""""
+","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""This module contains the schema for the social_03 dataset.""""""
+""""""Schema definition for social_03 experiments-specific data streams.""""""
 
 import aeon.io.reader as _reader
 from aeon.schema.streams import Stream
@@ -11,7 +11,6 @@

 
 
 class EnvironmentActiveConfiguration(Stream):
-
     def __init__(self, path):
         """"""Initializes the EnvironmentActiveConfiguration stream.""""""
         super().__init__(_reader.JsonList(f""{path}_ActiveConfiguration_*"", columns=[""name""]))"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820656508,,59,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/conftest.py,,S101: Replaced assertions with exceptions,"+    """"""
     dj_config_fp = pathlib.Path(""dj_local_conf.json"")
-    assert dj_config_fp.exists()
+    if not dj_config_fp.exists():","--- 

+++ 

@@ -18,7 +18,6 @@

 
 _tear_down = True  # always set to True since most fixtures are session-scoped
 _populate_settings = {""suppress_errors"": True}
-logger = dj.logger
 
 
 def data_dir():
@@ -56,14 +55,10 @@

     DataJoint configuration.
     """"""
     dj_config_fp = pathlib.Path(""dj_local_conf.json"")
-    if not dj_config_fp.exists():
-        raise FileNotFoundError(
-            f""DataJoint configuration file not found: {dj_config_fp}""
-        )
+    assert dj_config_fp.exists()
     dj.config.load(dj_config_fp)
     dj.config[""safemode""] = False
-    if ""custom"" not in dj.config:
-        raise KeyError(""'custom' not found in DataJoint configuration."")
+    assert ""custom"" in dj.config
     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820656629,,65,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/conftest.py,,S101: Replaced assertions with exceptions,"     dj.config[""safemode""] = False
-    assert ""custom"" in dj.config
-    dj.config[""custom""][""database.prefix""] = f""u_{dj.config['database.user']}_testsuite_""
+    if ""custom"" not in dj.config:","--- 

+++ 

@@ -18,7 +18,6 @@

 
 _tear_down = True  # always set to True since most fixtures are session-scoped
 _populate_settings = {""suppress_errors"": True}
-logger = dj.logger
 
 
 def data_dir():
@@ -56,14 +55,10 @@

     DataJoint configuration.
     """"""
     dj_config_fp = pathlib.Path(""dj_local_conf.json"")
-    if not dj_config_fp.exists():
-        raise FileNotFoundError(
-            f""DataJoint configuration file not found: {dj_config_fp}""
-        )
+    assert dj_config_fp.exists()
     dj.config.load(dj_config_fp)
     dj.config[""safemode""] = False
-    if ""custom"" not in dj.config:
-        raise KeyError(""'custom' not found in DataJoint configuration."")
+    assert ""custom"" in dj.config
     dj.config[""custom""][
         ""database.prefix""
     ] = f""u_{dj.config['database.user']}_testsuite_"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820657240,,4,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_acquisition.py,,fixed import of pytest,"+""""""Tests for the acquisition pipeline.""""""
 
+import datajoint as dj
+import pytest","--- 

+++ 

@@ -1,29 +1,20 @@

 """"""Tests for the acquisition pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.ingestion
 def test_epoch_chunk_ingestion(test_params, pipeline, epoch_chunk_ingestion):
     acquisition = pipeline[""acquisition""]
-    epoch_count = len(
-        acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]}
+
+    assert (
+        len(acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""epoch_count""]
     )
-    chunk_count = len(
-        acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""chunk_count""]
     )
-    if epoch_count != test_params[""epoch_count""]:
-        raise AssertionError(
-            f""Expected {test_params['epoch_count']} epochs, but got {epoch_count}.""
-        )
-
-    if chunk_count != test_params[""chunk_count""]:
-        raise AssertionError(
-            f""Expected {test_params['chunk_count']} chunks, but got {chunk_count}.""
-        )
 
 
 @pytest.mark.ingestion
@@ -32,32 +23,24 @@

 ):
     acquisition = pipeline[""acquisition""]
 
-    exp_log_message_count = len(
-        acquisition.ExperimentLog.Message
-        & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(
+            acquisition.ExperimentLog.Message
+            & {""experiment_name"": test_params[""experiment_name""]}
+        )
+        == test_params[""experiment_log_message_count""]
     )
-    if exp_log_message_count != test_params[""experiment_log_message_count""]:
-        raise AssertionError(
-            f""Expected {test_params['experiment_log_message_count']} log messages,""
-            f""but got {exp_log_message_count}.""
+    assert (
+        len(
+            acquisition.SubjectEnterExit.Time
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_enter_exit_count = len(
-        acquisition.SubjectEnterExit.Time
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_enter_exit_count""]
     )
-    if subject_enter_exit_count != test_params[""subject_enter_exit_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_enter_exit_count']} subject enter/exit events,""
-            f""but got {subject_enter_exit_count}.""
+    assert (
+        len(
+            acquisition.SubjectWeight.WeightTime
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_weight_time_count = len(
-        acquisition.SubjectWeight.WeightTime
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_weight_time_count""]
     )
-    if subject_weight_time_count != test_params[""subject_weight_time_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_weight_time_count']} subject weight events,""
-            f""but got {subject_weight_time_count}.""
-        )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820657620,,12,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_acquisition.py,,S101: Replaced assertions with exceptions,"-    assert (
-        len(acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]})
-        == test_params[""epoch_count""]
+    epoch_count = len(","--- 

+++ 

@@ -1,29 +1,20 @@

 """"""Tests for the acquisition pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.ingestion
 def test_epoch_chunk_ingestion(test_params, pipeline, epoch_chunk_ingestion):
     acquisition = pipeline[""acquisition""]
-    epoch_count = len(
-        acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]}
+
+    assert (
+        len(acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""epoch_count""]
     )
-    chunk_count = len(
-        acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""chunk_count""]
     )
-    if epoch_count != test_params[""epoch_count""]:
-        raise AssertionError(
-            f""Expected {test_params['epoch_count']} epochs, but got {epoch_count}.""
-        )
-
-    if chunk_count != test_params[""chunk_count""]:
-        raise AssertionError(
-            f""Expected {test_params['chunk_count']} chunks, but got {chunk_count}.""
-        )
 
 
 @pytest.mark.ingestion
@@ -32,32 +23,24 @@

 ):
     acquisition = pipeline[""acquisition""]
 
-    exp_log_message_count = len(
-        acquisition.ExperimentLog.Message
-        & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(
+            acquisition.ExperimentLog.Message
+            & {""experiment_name"": test_params[""experiment_name""]}
+        )
+        == test_params[""experiment_log_message_count""]
     )
-    if exp_log_message_count != test_params[""experiment_log_message_count""]:
-        raise AssertionError(
-            f""Expected {test_params['experiment_log_message_count']} log messages,""
-            f""but got {exp_log_message_count}.""
+    assert (
+        len(
+            acquisition.SubjectEnterExit.Time
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_enter_exit_count = len(
-        acquisition.SubjectEnterExit.Time
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_enter_exit_count""]
     )
-    if subject_enter_exit_count != test_params[""subject_enter_exit_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_enter_exit_count']} subject enter/exit events,""
-            f""but got {subject_enter_exit_count}.""
+    assert (
+        len(
+            acquisition.SubjectWeight.WeightTime
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_weight_time_count = len(
-        acquisition.SubjectWeight.WeightTime
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_weight_time_count""]
     )
-    if subject_weight_time_count != test_params[""subject_weight_time_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_weight_time_count']} subject weight events,""
-            f""but got {subject_weight_time_count}.""
-        )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820657843,,35,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_acquisition.py,,S101: Replaced assertions with exceptions,"-    assert (
-        len(acquisition.ExperimentLog.Message & {""experiment_name"": test_params[""experiment_name""]})
-        == test_params[""experiment_log_message_count""]
+    exp_log_message_count = len(","--- 

+++ 

@@ -1,29 +1,20 @@

 """"""Tests for the acquisition pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.ingestion
 def test_epoch_chunk_ingestion(test_params, pipeline, epoch_chunk_ingestion):
     acquisition = pipeline[""acquisition""]
-    epoch_count = len(
-        acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]}
+
+    assert (
+        len(acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""epoch_count""]
     )
-    chunk_count = len(
-        acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""chunk_count""]
     )
-    if epoch_count != test_params[""epoch_count""]:
-        raise AssertionError(
-            f""Expected {test_params['epoch_count']} epochs, but got {epoch_count}.""
-        )
-
-    if chunk_count != test_params[""chunk_count""]:
-        raise AssertionError(
-            f""Expected {test_params['chunk_count']} chunks, but got {chunk_count}.""
-        )
 
 
 @pytest.mark.ingestion
@@ -32,32 +23,24 @@

 ):
     acquisition = pipeline[""acquisition""]
 
-    exp_log_message_count = len(
-        acquisition.ExperimentLog.Message
-        & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(
+            acquisition.ExperimentLog.Message
+            & {""experiment_name"": test_params[""experiment_name""]}
+        )
+        == test_params[""experiment_log_message_count""]
     )
-    if exp_log_message_count != test_params[""experiment_log_message_count""]:
-        raise AssertionError(
-            f""Expected {test_params['experiment_log_message_count']} log messages,""
-            f""but got {exp_log_message_count}.""
+    assert (
+        len(
+            acquisition.SubjectEnterExit.Time
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_enter_exit_count = len(
-        acquisition.SubjectEnterExit.Time
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_enter_exit_count""]
     )
-    if subject_enter_exit_count != test_params[""subject_enter_exit_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_enter_exit_count']} subject enter/exit events,""
-            f""but got {subject_enter_exit_count}.""
+    assert (
+        len(
+            acquisition.SubjectWeight.WeightTime
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_weight_time_count = len(
-        acquisition.SubjectWeight.WeightTime
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_weight_time_count""]
     )
-    if subject_weight_time_count != test_params[""subject_weight_time_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_weight_time_count']} subject weight events,""
-            f""but got {subject_weight_time_count}.""
-        )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658039,,49,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_acquisition.py,,S101: Replaced assertions with exceptions,"-    assert (
-        len(acquisition.SubjectWeight.WeightTime & {""experiment_name"": test_params[""experiment_name""]})
-        == test_params[""subject_weight_time_count""]
+    if subject_enter_exit_count != test_params[""subject_enter_exit_count""]:","--- 

+++ 

@@ -1,29 +1,20 @@

 """"""Tests for the acquisition pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.ingestion
 def test_epoch_chunk_ingestion(test_params, pipeline, epoch_chunk_ingestion):
     acquisition = pipeline[""acquisition""]
-    epoch_count = len(
-        acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]}
+
+    assert (
+        len(acquisition.Epoch & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""epoch_count""]
     )
-    chunk_count = len(
-        acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(acquisition.Chunk & {""experiment_name"": test_params[""experiment_name""]})
+        == test_params[""chunk_count""]
     )
-    if epoch_count != test_params[""epoch_count""]:
-        raise AssertionError(
-            f""Expected {test_params['epoch_count']} epochs, but got {epoch_count}.""
-        )
-
-    if chunk_count != test_params[""chunk_count""]:
-        raise AssertionError(
-            f""Expected {test_params['chunk_count']} chunks, but got {chunk_count}.""
-        )
 
 
 @pytest.mark.ingestion
@@ -32,32 +23,24 @@

 ):
     acquisition = pipeline[""acquisition""]
 
-    exp_log_message_count = len(
-        acquisition.ExperimentLog.Message
-        & {""experiment_name"": test_params[""experiment_name""]}
+    assert (
+        len(
+            acquisition.ExperimentLog.Message
+            & {""experiment_name"": test_params[""experiment_name""]}
+        )
+        == test_params[""experiment_log_message_count""]
     )
-    if exp_log_message_count != test_params[""experiment_log_message_count""]:
-        raise AssertionError(
-            f""Expected {test_params['experiment_log_message_count']} log messages,""
-            f""but got {exp_log_message_count}.""
+    assert (
+        len(
+            acquisition.SubjectEnterExit.Time
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_enter_exit_count = len(
-        acquisition.SubjectEnterExit.Time
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_enter_exit_count""]
     )
-    if subject_enter_exit_count != test_params[""subject_enter_exit_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_enter_exit_count']} subject enter/exit events,""
-            f""but got {subject_enter_exit_count}.""
+    assert (
+        len(
+            acquisition.SubjectWeight.WeightTime
+            & {""experiment_name"": test_params[""experiment_name""]}
         )
-
-    subject_weight_time_count = len(
-        acquisition.SubjectWeight.WeightTime
-        & {""experiment_name"": test_params[""experiment_name""]}
+        == test_params[""subject_weight_time_count""]
     )
-    if subject_weight_time_count != test_params[""subject_weight_time_count""]:
-        raise AssertionError(
-            f""Expected {test_params['subject_weight_time_count']} subject weight events,""
-            f""but got {subject_weight_time_count}.""
-        )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658303,,4,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_pipeline_instantiation.py,,fixed import pytest,"+""""""Tests for pipeline instantiation and experiment creation.""""""
 
+import datajoint as dj
+import pytest","--- 

+++ 

@@ -1,36 +1,16 @@

 """"""Tests for pipeline instantiation and experiment creation.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.instantiation
 def test_pipeline_instantiation(pipeline):
-    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):
-        raise AssertionError(
-            ""Pipeline acquisition does not have 'FoodPatchEvent' attribute.""
-        )
-
-    if not hasattr(pipeline[""lab""], ""Arena""):
-        raise AssertionError(""Pipeline lab does not have 'Arena' attribute."")
-
-    if not hasattr(pipeline[""qc""], ""CameraQC""):
-        raise AssertionError(""Pipeline qc does not have 'CameraQC' attribute."")
-
-    if not hasattr(pipeline[""report""], ""InArenaSummaryPlot""):
-        raise AssertionError(
-            ""Pipeline report does not have 'InArenaSummaryPlot' attribute.""
-        )
-
-    if not hasattr(pipeline[""subject""], ""Subject""):
-        raise AssertionError(""Pipeline subject does not have 'Subject' attribute."")
-
-    if not hasattr(pipeline[""tracking""], ""CameraTracking""):
-        raise AssertionError(
-            ""Pipeline tracking does not have 'CameraTracking' attribute.""
-        )
+    assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
+    assert hasattr(pipeline[""lab""], ""Arena"")
+    assert hasattr(pipeline[""qc""], ""CameraQC"")
+    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
+    assert hasattr(pipeline[""subject""], ""Subject"")
+    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
 
 
 @pytest.mark.instantiation
@@ -38,30 +18,14 @@

     acquisition = pipeline[""acquisition""]
 
     experiment_name = test_params[""experiment_name""]
-    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")
-    if fetched_experiment_name != experiment_name:
-        raise AssertionError(
-            f""Expected experiment name '{experiment_name}', but got '{fetched_experiment_name}'.""
-        )
-
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
         acquisition.Experiment.Directory
         & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
-    if raw_dir != test_params[""raw_dir""]:
-        raise AssertionError(
-            f""Expected raw directory '{test_params['raw_dir']}', but got '{raw_dir}'.""
-        )
-
+    assert raw_dir == test_params[""raw_dir""]
     exp_subjects = (
         acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
     ).fetch(""subject"")
-    if len(exp_subjects) != test_params[""subject_count""]:
-        raise AssertionError(
-            f""Expected subject count {test_params['subject_count']}, but got {len(exp_subjects)}.""
-        )
-
-    if ""BAA-1100701"" not in exp_subjects:
-        raise AssertionError(
-            ""Expected subject 'BAA-1100701' not found in experiment subjects.""
-        )
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658460,,11,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_pipeline_instantiation.py,,S101: Replaced assertions with exceptions,"-    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
-    assert hasattr(pipeline[""subject""], ""Subject"")
-    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
+    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):","--- 

+++ 

@@ -1,36 +1,16 @@

 """"""Tests for pipeline instantiation and experiment creation.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.instantiation
 def test_pipeline_instantiation(pipeline):
-    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):
-        raise AssertionError(
-            ""Pipeline acquisition does not have 'FoodPatchEvent' attribute.""
-        )
-
-    if not hasattr(pipeline[""lab""], ""Arena""):
-        raise AssertionError(""Pipeline lab does not have 'Arena' attribute."")
-
-    if not hasattr(pipeline[""qc""], ""CameraQC""):
-        raise AssertionError(""Pipeline qc does not have 'CameraQC' attribute."")
-
-    if not hasattr(pipeline[""report""], ""InArenaSummaryPlot""):
-        raise AssertionError(
-            ""Pipeline report does not have 'InArenaSummaryPlot' attribute.""
-        )
-
-    if not hasattr(pipeline[""subject""], ""Subject""):
-        raise AssertionError(""Pipeline subject does not have 'Subject' attribute."")
-
-    if not hasattr(pipeline[""tracking""], ""CameraTracking""):
-        raise AssertionError(
-            ""Pipeline tracking does not have 'CameraTracking' attribute.""
-        )
+    assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
+    assert hasattr(pipeline[""lab""], ""Arena"")
+    assert hasattr(pipeline[""qc""], ""CameraQC"")
+    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
+    assert hasattr(pipeline[""subject""], ""Subject"")
+    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
 
 
 @pytest.mark.instantiation
@@ -38,30 +18,14 @@

     acquisition = pipeline[""acquisition""]
 
     experiment_name = test_params[""experiment_name""]
-    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")
-    if fetched_experiment_name != experiment_name:
-        raise AssertionError(
-            f""Expected experiment name '{experiment_name}', but got '{fetched_experiment_name}'.""
-        )
-
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
         acquisition.Experiment.Directory
         & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
-    if raw_dir != test_params[""raw_dir""]:
-        raise AssertionError(
-            f""Expected raw directory '{test_params['raw_dir']}', but got '{raw_dir}'.""
-        )
-
+    assert raw_dir == test_params[""raw_dir""]
     exp_subjects = (
         acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
     ).fetch(""subject"")
-    if len(exp_subjects) != test_params[""subject_count""]:
-        raise AssertionError(
-            f""Expected subject count {test_params['subject_count']}, but got {len(exp_subjects)}.""
-        )
-
-    if ""BAA-1100701"" not in exp_subjects:
-        raise AssertionError(
-            ""Expected subject 'BAA-1100701' not found in experiment subjects.""
-        )
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658716,,41,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_pipeline_instantiation.py,,S101: Replaced assertions with exceptions," 
     experiment_name = test_params[""experiment_name""]
-    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
+    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")","--- 

+++ 

@@ -1,36 +1,16 @@

 """"""Tests for pipeline instantiation and experiment creation.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.instantiation
 def test_pipeline_instantiation(pipeline):
-    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):
-        raise AssertionError(
-            ""Pipeline acquisition does not have 'FoodPatchEvent' attribute.""
-        )
-
-    if not hasattr(pipeline[""lab""], ""Arena""):
-        raise AssertionError(""Pipeline lab does not have 'Arena' attribute."")
-
-    if not hasattr(pipeline[""qc""], ""CameraQC""):
-        raise AssertionError(""Pipeline qc does not have 'CameraQC' attribute."")
-
-    if not hasattr(pipeline[""report""], ""InArenaSummaryPlot""):
-        raise AssertionError(
-            ""Pipeline report does not have 'InArenaSummaryPlot' attribute.""
-        )
-
-    if not hasattr(pipeline[""subject""], ""Subject""):
-        raise AssertionError(""Pipeline subject does not have 'Subject' attribute."")
-
-    if not hasattr(pipeline[""tracking""], ""CameraTracking""):
-        raise AssertionError(
-            ""Pipeline tracking does not have 'CameraTracking' attribute.""
-        )
+    assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
+    assert hasattr(pipeline[""lab""], ""Arena"")
+    assert hasattr(pipeline[""qc""], ""CameraQC"")
+    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
+    assert hasattr(pipeline[""subject""], ""Subject"")
+    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
 
 
 @pytest.mark.instantiation
@@ -38,30 +18,14 @@

     acquisition = pipeline[""acquisition""]
 
     experiment_name = test_params[""experiment_name""]
-    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")
-    if fetched_experiment_name != experiment_name:
-        raise AssertionError(
-            f""Expected experiment name '{experiment_name}', but got '{fetched_experiment_name}'.""
-        )
-
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
         acquisition.Experiment.Directory
         & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
-    if raw_dir != test_params[""raw_dir""]:
-        raise AssertionError(
-            f""Expected raw directory '{test_params['raw_dir']}', but got '{raw_dir}'.""
-        )
-
+    assert raw_dir == test_params[""raw_dir""]
     exp_subjects = (
         acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
     ).fetch(""subject"")
-    if len(exp_subjects) != test_params[""subject_count""]:
-        raise AssertionError(
-            f""Expected subject count {test_params['subject_count']}, but got {len(exp_subjects)}.""
-        )
-
-    if ""BAA-1100701"" not in exp_subjects:
-        raise AssertionError(
-            ""Expected subject 'BAA-1100701' not found in experiment subjects.""
-        )
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820658854,,51,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_pipeline_instantiation.py,,S101: Replaced assertions with exceptions,"-    exp_subjects = (acquisition.Experiment.Subject & {""experiment_name"": experiment_name}).fetch(""subject"")
-    assert len(exp_subjects) == test_params[""subject_count""]
-    assert ""BAA-1100701"" in exp_subjects
+    if raw_dir != test_params[""raw_dir""]:","--- 

+++ 

@@ -1,36 +1,16 @@

 """"""Tests for pipeline instantiation and experiment creation.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.instantiation
 def test_pipeline_instantiation(pipeline):
-    if not hasattr(pipeline[""acquisition""], ""FoodPatchEvent""):
-        raise AssertionError(
-            ""Pipeline acquisition does not have 'FoodPatchEvent' attribute.""
-        )
-
-    if not hasattr(pipeline[""lab""], ""Arena""):
-        raise AssertionError(""Pipeline lab does not have 'Arena' attribute."")
-
-    if not hasattr(pipeline[""qc""], ""CameraQC""):
-        raise AssertionError(""Pipeline qc does not have 'CameraQC' attribute."")
-
-    if not hasattr(pipeline[""report""], ""InArenaSummaryPlot""):
-        raise AssertionError(
-            ""Pipeline report does not have 'InArenaSummaryPlot' attribute.""
-        )
-
-    if not hasattr(pipeline[""subject""], ""Subject""):
-        raise AssertionError(""Pipeline subject does not have 'Subject' attribute."")
-
-    if not hasattr(pipeline[""tracking""], ""CameraTracking""):
-        raise AssertionError(
-            ""Pipeline tracking does not have 'CameraTracking' attribute.""
-        )
+    assert hasattr(pipeline[""acquisition""], ""FoodPatchEvent"")
+    assert hasattr(pipeline[""lab""], ""Arena"")
+    assert hasattr(pipeline[""qc""], ""CameraQC"")
+    assert hasattr(pipeline[""report""], ""InArenaSummaryPlot"")
+    assert hasattr(pipeline[""subject""], ""Subject"")
+    assert hasattr(pipeline[""tracking""], ""CameraTracking"")
 
 
 @pytest.mark.instantiation
@@ -38,30 +18,14 @@

     acquisition = pipeline[""acquisition""]
 
     experiment_name = test_params[""experiment_name""]
-    fetched_experiment_name = acquisition.Experiment.fetch1(""experiment_name"")
-    if fetched_experiment_name != experiment_name:
-        raise AssertionError(
-            f""Expected experiment name '{experiment_name}', but got '{fetched_experiment_name}'.""
-        )
-
+    assert acquisition.Experiment.fetch1(""experiment_name"") == experiment_name
     raw_dir = (
         acquisition.Experiment.Directory
         & {""experiment_name"": experiment_name, ""directory_type"": ""raw""}
     ).fetch1(""directory_path"")
-    if raw_dir != test_params[""raw_dir""]:
-        raise AssertionError(
-            f""Expected raw directory '{test_params['raw_dir']}', but got '{raw_dir}'.""
-        )
-
+    assert raw_dir == test_params[""raw_dir""]
     exp_subjects = (
         acquisition.Experiment.Subject & {""experiment_name"": experiment_name}
     ).fetch(""subject"")
-    if len(exp_subjects) != test_params[""subject_count""]:
-        raise AssertionError(
-            f""Expected subject count {test_params['subject_count']}, but got {len(exp_subjects)}.""
-        )
-
-    if ""BAA-1100701"" not in exp_subjects:
-        raise AssertionError(
-            ""Expected subject 'BAA-1100701' not found in experiment subjects.""
-        )
+    assert len(exp_subjects) == test_params[""subject_count""]
+    assert ""BAA-1100701"" in exp_subjects"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820659070,,4,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_qc.py,,fixed import pytest,"+""""""Tests for the QC pipeline.""""""
 
+import datajoint as dj
+import pytest","--- 

+++ 

@@ -1,19 +1,10 @@

 """"""Tests for the QC pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.qc
 def test_camera_qc_ingestion(test_params, pipeline, camera_qc_ingestion):
     qc = pipeline[""qc""]
 
-    camera_qc_count = len(qc.CameraQC())
-    expected_camera_qc_count = test_params[""camera_qc_count""]
-
-    if camera_qc_count != expected_camera_qc_count:
-        raise AssertionError(
-            f""Expected camera QC count {expected_camera_qc_count}, but got {camera_qc_count}.""
-        )
+    assert len(qc.CameraQC()) == test_params[""camera_qc_count""]"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820659201,,13,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_qc.py,,S101: Replaced assertions with exceptions,"     qc = pipeline[""qc""]
 
-    assert len(qc.CameraQC()) == test_params[""camera_qc_count""]
+    camera_qc_count = len(qc.CameraQC())","--- 

+++ 

@@ -1,19 +1,10 @@

 """"""Tests for the QC pipeline.""""""
 
-import datajoint as dj
 import pytest
-
-logger = dj.logger
 
 
 @pytest.mark.qc
 def test_camera_qc_ingestion(test_params, pipeline, camera_qc_ingestion):
     qc = pipeline[""qc""]
 
-    camera_qc_count = len(qc.CameraQC())
-    expected_camera_qc_count = test_params[""camera_qc_count""]
-
-    if camera_qc_count != expected_camera_qc_count:
-        raise AssertionError(
-            f""Expected camera QC count {expected_camera_qc_count}, but got {camera_qc_count}.""
-        )
+    assert len(qc.CameraQC()) == test_params[""camera_qc_count""]"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820659832,,49,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_tracking.py,,S101: Replaced assertions with exceptions,"+@pytest.mark.tracking
 def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
     tracking = pipeline[""tracking""]
 ","--- 

+++ 

@@ -47,12 +47,10 @@

 def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
     tracking = pipeline[""tracking""]
 
-    camera_tracking_object_count = len(tracking.CameraTracking.Object())
-    if camera_tracking_object_count != test_params[""camera_tracking_object_count""]:
-        raise AssertionError(
-            f""Expected camera tracking object count {test_params['camera_tracking_object_count']},""
-            f""but got {camera_tracking_object_count}.""
-        )
+    assert (
+        len(tracking.CameraTracking.Object())
+        == test_params[""camera_tracking_object_count""]
+    )
 
     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
     file_name = (
@@ -70,15 +68,13 @@

     )
 
     test_file = pathlib.Path(test_params[""test_dir""] + ""/"" + file_name)
-    if not test_file.exists():
-        raise AssertionError(f""Test file '{test_file}' does not exist."")
+    assert test_file.exists()
 
     print(f""\nTesting {file_name}"")
 
     data = np.load(test_file)
-    expected_data = (tracking.CameraTracking.Object() & key).fetch(column_name)[0]
-
-    if not np.allclose(data, expected_data, equal_nan=True):
-        raise AssertionError(
-            f""Loaded data does not match the expected data.nExpected: {expected_data}, but got: {data}.""
-        )
+    assert np.allclose(
+        data,
+        (tracking.CameraTracking.Object() & key).fetch(column_name)[0],
+        equal_nan=True,
+    )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820660235,,73,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_tracking.py,,S101: Replaced assertions with exceptions," 
     test_file = pathlib.Path(test_params[""test_dir""] + ""/"" + file_name)
-    assert test_file.exists()
+    if not test_file.exists():","--- 

+++ 

@@ -47,12 +47,10 @@

 def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
     tracking = pipeline[""tracking""]
 
-    camera_tracking_object_count = len(tracking.CameraTracking.Object())
-    if camera_tracking_object_count != test_params[""camera_tracking_object_count""]:
-        raise AssertionError(
-            f""Expected camera tracking object count {test_params['camera_tracking_object_count']},""
-            f""but got {camera_tracking_object_count}.""
-        )
+    assert (
+        len(tracking.CameraTracking.Object())
+        == test_params[""camera_tracking_object_count""]
+    )
 
     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
     file_name = (
@@ -70,15 +68,13 @@

     )
 
     test_file = pathlib.Path(test_params[""test_dir""] + ""/"" + file_name)
-    if not test_file.exists():
-        raise AssertionError(f""Test file '{test_file}' does not exist."")
+    assert test_file.exists()
 
     print(f""\nTesting {file_name}"")
 
     data = np.load(test_file)
-    expected_data = (tracking.CameraTracking.Object() & key).fetch(column_name)[0]
-
-    if not np.allclose(data, expected_data, equal_nan=True):
-        raise AssertionError(
-            f""Loaded data does not match the expected data.nExpected: {expected_data}, but got: {data}.""
-        )
+    assert np.allclose(
+        data,
+        (tracking.CameraTracking.Object() & key).fetch(column_name)[0],
+        equal_nan=True,
+    )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820660381,,79,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/dj_pipeline/test_tracking.py,,S101: Replaced assertions with exceptions,"-        (tracking.CameraTracking.Object() & key).fetch(column_name)[0],
-        equal_nan=True,
-    )
+    expected_data = (tracking.CameraTracking.Object() & key).fetch(column_name)[0]","--- 

+++ 

@@ -47,12 +47,10 @@

 def test_camera_tracking_ingestion(test_params, pipeline, camera_tracking_ingestion):
     tracking = pipeline[""tracking""]
 
-    camera_tracking_object_count = len(tracking.CameraTracking.Object())
-    if camera_tracking_object_count != test_params[""camera_tracking_object_count""]:
-        raise AssertionError(
-            f""Expected camera tracking object count {test_params['camera_tracking_object_count']},""
-            f""but got {camera_tracking_object_count}.""
-        )
+    assert (
+        len(tracking.CameraTracking.Object())
+        == test_params[""camera_tracking_object_count""]
+    )
 
     key = tracking.CameraTracking.Object().fetch(""KEY"")[index]
     file_name = (
@@ -70,15 +68,13 @@

     )
 
     test_file = pathlib.Path(test_params[""test_dir""] + ""/"" + file_name)
-    if not test_file.exists():
-        raise AssertionError(f""Test file '{test_file}' does not exist."")
+    assert test_file.exists()
 
     print(f""\nTesting {file_name}"")
 
     data = np.load(test_file)
-    expected_data = (tracking.CameraTracking.Object() & key).fetch(column_name)[0]
-
-    if not np.allclose(data, expected_data, equal_nan=True):
-        raise AssertionError(
-            f""Loaded data does not match the expected data.nExpected: {expected_data}, but got: {data}.""
-        )
+    assert np.allclose(
+        data,
+        (tracking.CameraTracking.Object() & key).fetch(column_name)[0],
+        equal_nan=True,
+    )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820660553,,15,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/io/test_api.py,,fixed import pytest," 
 
-@mark.api
+@pytest.mark.api","--- 

+++ 

@@ -6,34 +6,27 @@

 import pytest
 
 import aeon
+from aeon.schema.ingestion_schemas import social03
 from aeon.schema.schemas import exp02
 
+monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 nonmonotonic_path = Path(__file__).parent.parent / ""data"" / ""nonmonotonic""
-monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 
 
 @pytest.mark.api
 def test_load_start_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        start=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_end_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        end=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
@@ -41,66 +34,43 @@

     data = aeon.load(
         nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_monotonic():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
-
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(""Data index is not monotonic increasing."")
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
+    assert len(data) > 0
+    assert data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Data index is monotonic increasing, but it should not be.""
-        )
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    assert not data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_encoder_with_downsampling():
     DOWNSAMPLE_PERIOD = 0.02
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
-    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    data = aeon.load(monotonic_path, social03.Patch2.Encoder)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
 
     # Check that the length of the downsampled data is less than the raw data
-    if len(data) >= len(raw_data):
-        raise AssertionError(
-            ""Downsampled data length should be less than raw data length.""
-        )
+    assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[0] - raw_data.index[0]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            ""The first timestamp of downsampled data is not within 20ms of raw data.""
-        )
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[-1] - raw_data.index[-1]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""The last timestamp of downsampled data is not within {DOWNSAMPLE_PERIOD*1000} ms of raw data.""
-        )
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    min_diff = data.index.to_series().diff().dt.total_seconds().min()
-    if min_diff < DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""Minimum difference between consecutive timestamps is less than {DOWNSAMPLE_PERIOD} seconds.""
-        )
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
 
     # Check that the timestamps in the downsampled data are strictly increasing
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Timestamps in downsampled data are not strictly increasing.""
-        )
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820660747,,23,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/io/test_api.py,,S101: Replaced assertions with exceptions,"+        downsample=None,
     )
-    assert len(data) > 0
+    if len(data) <= 0:","--- 

+++ 

@@ -6,34 +6,27 @@

 import pytest
 
 import aeon
+from aeon.schema.ingestion_schemas import social03
 from aeon.schema.schemas import exp02
 
+monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 nonmonotonic_path = Path(__file__).parent.parent / ""data"" / ""nonmonotonic""
-monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 
 
 @pytest.mark.api
 def test_load_start_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        start=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_end_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        end=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
@@ -41,66 +34,43 @@

     data = aeon.load(
         nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_monotonic():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
-
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(""Data index is not monotonic increasing."")
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
+    assert len(data) > 0
+    assert data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Data index is monotonic increasing, but it should not be.""
-        )
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    assert not data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_encoder_with_downsampling():
     DOWNSAMPLE_PERIOD = 0.02
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
-    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    data = aeon.load(monotonic_path, social03.Patch2.Encoder)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
 
     # Check that the length of the downsampled data is less than the raw data
-    if len(data) >= len(raw_data):
-        raise AssertionError(
-            ""Downsampled data length should be less than raw data length.""
-        )
+    assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[0] - raw_data.index[0]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            ""The first timestamp of downsampled data is not within 20ms of raw data.""
-        )
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[-1] - raw_data.index[-1]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""The last timestamp of downsampled data is not within {DOWNSAMPLE_PERIOD*1000} ms of raw data.""
-        )
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    min_diff = data.index.to_series().diff().dt.total_seconds().min()
-    if min_diff < DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""Minimum difference between consecutive timestamps is less than {DOWNSAMPLE_PERIOD} seconds.""
-        )
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
 
     # Check that the timestamps in the downsampled data are strictly increasing
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Timestamps in downsampled data are not strictly increasing.""
-        )
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820660979,,35,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/io/test_api.py,,S101: Replaced assertions with exceptions,"+        downsample=None,
     )
-    assert len(data) > 0
+    if len(data) <= 0:","--- 

+++ 

@@ -6,34 +6,27 @@

 import pytest
 
 import aeon
+from aeon.schema.ingestion_schemas import social03
 from aeon.schema.schemas import exp02
 
+monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 nonmonotonic_path = Path(__file__).parent.parent / ""data"" / ""nonmonotonic""
-monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 
 
 @pytest.mark.api
 def test_load_start_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        start=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_end_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        end=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
@@ -41,66 +34,43 @@

     data = aeon.load(
         nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_monotonic():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
-
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(""Data index is not monotonic increasing."")
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
+    assert len(data) > 0
+    assert data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Data index is monotonic increasing, but it should not be.""
-        )
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    assert not data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_encoder_with_downsampling():
     DOWNSAMPLE_PERIOD = 0.02
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
-    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    data = aeon.load(monotonic_path, social03.Patch2.Encoder)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
 
     # Check that the length of the downsampled data is less than the raw data
-    if len(data) >= len(raw_data):
-        raise AssertionError(
-            ""Downsampled data length should be less than raw data length.""
-        )
+    assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[0] - raw_data.index[0]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            ""The first timestamp of downsampled data is not within 20ms of raw data.""
-        )
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[-1] - raw_data.index[-1]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""The last timestamp of downsampled data is not within {DOWNSAMPLE_PERIOD*1000} ms of raw data.""
-        )
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    min_diff = data.index.to_series().diff().dt.total_seconds().min()
-    if min_diff < DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""Minimum difference between consecutive timestamps is less than {DOWNSAMPLE_PERIOD} seconds.""
-        )
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
 
     # Check that the timestamps in the downsampled data are strictly increasing
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Timestamps in downsampled data are not strictly increasing.""
-        )
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820661199,,51,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/io/test_api.py,,S101: Replaced assertions with exceptions,"     data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    assert len(data) > 0
-    assert data.index.is_monotonic_increasing
+    if len(data) <= 0:","--- 

+++ 

@@ -6,34 +6,27 @@

 import pytest
 
 import aeon
+from aeon.schema.ingestion_schemas import social03
 from aeon.schema.schemas import exp02
 
+monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 nonmonotonic_path = Path(__file__).parent.parent / ""data"" / ""nonmonotonic""
-monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 
 
 @pytest.mark.api
 def test_load_start_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        start=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_end_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        end=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
@@ -41,66 +34,43 @@

     data = aeon.load(
         nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_monotonic():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
-
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(""Data index is not monotonic increasing."")
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
+    assert len(data) > 0
+    assert data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Data index is monotonic increasing, but it should not be.""
-        )
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    assert not data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_encoder_with_downsampling():
     DOWNSAMPLE_PERIOD = 0.02
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
-    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    data = aeon.load(monotonic_path, social03.Patch2.Encoder)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
 
     # Check that the length of the downsampled data is less than the raw data
-    if len(data) >= len(raw_data):
-        raise AssertionError(
-            ""Downsampled data length should be less than raw data length.""
-        )
+    assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[0] - raw_data.index[0]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            ""The first timestamp of downsampled data is not within 20ms of raw data.""
-        )
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[-1] - raw_data.index[-1]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""The last timestamp of downsampled data is not within {DOWNSAMPLE_PERIOD*1000} ms of raw data.""
-        )
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    min_diff = data.index.to_series().diff().dt.total_seconds().min()
-    if min_diff < DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""Minimum difference between consecutive timestamps is less than {DOWNSAMPLE_PERIOD} seconds.""
-        )
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
 
     # Check that the timestamps in the downsampled data are strictly increasing
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Timestamps in downsampled data are not strictly increasing.""
-        )
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1820661356,,61,cc7e759625e0b1851032d4f686f6ace397ea66b2,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,tests/io/test_api.py,,S101: Replaced assertions with exceptions," def test_load_nonmonotonic():
     data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    assert not data.index.is_monotonic_increasing
+    if data.index.is_monotonic_increasing:","--- 

+++ 

@@ -6,34 +6,27 @@

 import pytest
 
 import aeon
+from aeon.schema.ingestion_schemas import social03
 from aeon.schema.schemas import exp02
 
+monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 nonmonotonic_path = Path(__file__).parent.parent / ""data"" / ""nonmonotonic""
-monotonic_path = Path(__file__).parent.parent / ""data"" / ""monotonic""
 
 
 @pytest.mark.api
 def test_load_start_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        start=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, start=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_end_only():
     data = aeon.load(
-        nonmonotonic_path,
-        exp02.Patch2.Encoder,
-        end=pd.Timestamp(""2022-06-06T13:00:49""),
-        downsample=None,
+        nonmonotonic_path, exp02.Patch2.Encoder, end=pd.Timestamp(""2022-06-06T13:00:49"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
@@ -41,66 +34,43 @@

     data = aeon.load(
         nonmonotonic_path, exp02.Metadata, start=pd.Timestamp(""2022-06-06T09:00:00"")
     )
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
+    assert len(data) > 0
 
 
 @pytest.mark.api
 def test_load_monotonic():
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if len(data) <= 0:
-        raise AssertionError(""Loaded data is empty. Expected non-empty data."")
-
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(""Data index is not monotonic increasing."")
+    data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
+    assert len(data) > 0
+    assert data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_nonmonotonic():
-    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder, downsample=None)
-    if data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Data index is monotonic increasing, but it should not be.""
-        )
+    data = aeon.load(nonmonotonic_path, exp02.Patch2.Encoder)
+    assert not data.index.is_monotonic_increasing
 
 
 @pytest.mark.api
 def test_load_encoder_with_downsampling():
     DOWNSAMPLE_PERIOD = 0.02
-    data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=True)
-    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder, downsample=None)
+    data = aeon.load(monotonic_path, social03.Patch2.Encoder)
+    raw_data = aeon.load(monotonic_path, exp02.Patch2.Encoder)
 
     # Check that the length of the downsampled data is less than the raw data
-    if len(data) >= len(raw_data):
-        raise AssertionError(
-            ""Downsampled data length should be less than raw data length.""
-        )
+    assert len(data) < len(raw_data)
 
     # Check that the first timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[0] - raw_data.index[0]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            ""The first timestamp of downsampled data is not within 20ms of raw data.""
-        )
+    assert abs(data.index[0] - raw_data.index[0]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the last timestamp of the downsampled data is within 20ms of the raw data
-    if abs(data.index[-1] - raw_data.index[-1]).total_seconds() > DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""The last timestamp of downsampled data is not within {DOWNSAMPLE_PERIOD*1000} ms of raw data.""
-        )
+    assert abs(data.index[-1] - raw_data.index[-1]).total_seconds() <= DOWNSAMPLE_PERIOD
 
     # Check that the minimum difference between consecutive timestamps in the downsampled data
     # is at least 20ms (50Hz)
-    min_diff = data.index.to_series().diff().dt.total_seconds().min()
-    if min_diff < DOWNSAMPLE_PERIOD:
-        raise AssertionError(
-            f""Minimum difference between consecutive timestamps is less than {DOWNSAMPLE_PERIOD} seconds.""
-        )
+    assert data.index.to_series().diff().dt.total_seconds().min() >= DOWNSAMPLE_PERIOD
 
     # Check that the timestamps in the downsampled data are strictly increasing
-    if not data.index.is_monotonic_increasing:
-        raise AssertionError(
-            ""Timestamps in downsampled data are not strictly increasing.""
-        )
+    assert data.index.is_monotonic_increasing
 
 
 if __name__ == ""__main__"":"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1824905241,8.0,9,fc49b511a4cd1c4445e65c0aafe5c61901499f83,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/streams.py,,@MilagrosMarin would you revert this change as well?,"+import datajoint as dj
+import pandas as pd","--- 

+++ 

@@ -2,34 +2,34 @@

 #---- THIS FILE IS AUTO-GENERATED BY `streams_maker.py` ----
 
 import re
-from uuid import UUID
-
-import aeon
 import datajoint as dj
 import pandas as pd
+from uuid import UUID
+
+import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.Schema(get_schema_name(""streams""))
 
 
-@schema
+@schema 
 class StreamType(dj.Lookup):
     """"""Catalog of all steam types for the different device types used across Project Aeon. One StreamType corresponds to one reader class in `aeon.io.reader`. The combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data loading routine for a particular device, using the `aeon.io.utils`.""""""
 
     definition = """"""  # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""
-
-
-@schema
+    """"""
+
+
+@schema 
 class DeviceType(dj.Lookup):
     """"""Catalog of all device types used across Project Aeon.""""""
 
@@ -46,7 +46,7 @@

         """"""
 
 
-@schema
+@schema 
 class Device(dj.Lookup):
     definition = """"""  # Physical devices, of a particular type, identified by unique serial number
     device_serial_number: varchar(12)
@@ -55,7 +55,7 @@

     """"""
 
 
-@schema
+@schema 
 class RfidReader(dj.Manual):
         definition = f""""""
         # rfid_reader placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -82,7 +82,7 @@

             """"""
 
 
-@schema
+@schema 
 class SpinnakerVideoSource(dj.Manual):
         definition = f""""""
         # spinnaker_video_source placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -109,7 +109,7 @@

             """"""
 
 
-@schema
+@schema 
 class UndergroundFeeder(dj.Manual):
         definition = f""""""
         # underground_feeder placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -136,7 +136,7 @@

             """"""
 
 
-@schema
+@schema 
 class WeightScale(dj.Manual):
         definition = f""""""
         # weight_scale placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-unknown)
@@ -163,7 +163,7 @@

             """"""
 
 
-@schema
+@schema 
 class RfidReaderRfidEvents(dj.Imported):
         definition = """"""  # Raw per-chunk RfidEvents data stream from RfidReader (auto-generated with aeon_mecha-unknown)
     -> RfidReader
@@ -189,7 +189,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (RfidReader & key).fetch1('rfid_reader_name')
@@ -224,7 +223,7 @@

             )
 
 
-@schema
+@schema 
 class SpinnakerVideoSourceVideo(dj.Imported):
         definition = """"""  # Raw per-chunk Video data stream from SpinnakerVideoSource (auto-generated with aeon_mecha-unknown)
     -> SpinnakerVideoSource
@@ -232,7 +231,6 @@

     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of Video data
-    hw_counter: longblob
     hw_timestamp: longblob
     """"""
 
@@ -251,7 +249,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (SpinnakerVideoSource & key).fetch1('spinnaker_video_source_name')
@@ -286,7 +283,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederBeamBreak(dj.Imported):
         definition = """"""  # Raw per-chunk BeamBreak data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -312,7 +309,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -347,7 +343,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederDeliverPellet(dj.Imported):
         definition = """"""  # Raw per-chunk DeliverPellet data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -373,7 +369,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -408,7 +403,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederDepletionState(dj.Imported):
         definition = """"""  # Raw per-chunk DepletionState data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -436,7 +431,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -471,7 +465,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederEncoder(dj.Imported):
         definition = """"""  # Raw per-chunk Encoder data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -498,7 +492,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -533,7 +526,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederManualDelivery(dj.Imported):
         definition = """"""  # Raw per-chunk ManualDelivery data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -559,7 +552,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -594,7 +586,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederMissedPellet(dj.Imported):
         definition = """"""  # Raw per-chunk MissedPellet data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -620,7 +612,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -655,7 +646,7 @@

             )
 
 
-@schema
+@schema 
 class UndergroundFeederRetriedDelivery(dj.Imported):
         definition = """"""  # Raw per-chunk RetriedDelivery data stream from UndergroundFeeder (auto-generated with aeon_mecha-unknown)
     -> UndergroundFeeder
@@ -681,7 +672,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (UndergroundFeeder & key).fetch1('underground_feeder_name')
@@ -716,7 +706,7 @@

             )
 
 
-@schema
+@schema 
 class WeightScaleWeightFiltered(dj.Imported):
         definition = """"""  # Raw per-chunk WeightFiltered data stream from WeightScale (auto-generated with aeon_mecha-unknown)
     -> WeightScale
@@ -743,7 +733,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (WeightScale & key).fetch1('weight_scale_name')
@@ -778,7 +767,7 @@

             )
 
 
-@schema
+@schema 
 class WeightScaleWeightRaw(dj.Imported):
         definition = """"""  # Raw per-chunk WeightRaw data stream from WeightScale (auto-generated with aeon_mecha-unknown)
     -> WeightScale
@@ -805,7 +794,6 @@

 
         def make(self, key):
             chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
-
             data_dirs = acquisition.Experiment.get_data_directories(key)
 
             device_name = (WeightScale & key).fetch1('weight_scale_name')
@@ -838,3 +826,5 @@

                 },
                 ignore_extra_fields=True,
             )
+
+"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826078933,,1,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/README.md,,"```suggestion
```
Ruff should only check python files. Did `ruff check` fail? ","@@ -1 +1 @@
-#
+# README # noqa D100","--- 

+++ 

@@ -1 +0,0 @@

-# README # noqa D100"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826121570,66.0,73,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/analysis/block_plotting.py,,"I suggest changing
 ```
+    """"""Based on a list of patches, generates a dictionary of the following items.
+
     - patch_colors_dict: patch name to color
     - patch_markers_dict: patch name to marker
     - patch_symbols_dict: patch name to symbol
     - patch_linestyles_dict: patch name to linestyle
+
     """"""
```
 to
```
+    """"""
+    Generates a dictionary of patch styles given a list of patch_names.
+
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
+
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
+    """"""
```","+    """"""Based on a list of patches, generates a dictionary of the following items.
+
     - patch_colors_dict: patch name to color
     - patch_markers_dict: patch name to marker
     - patch_symbols_dict: patch name to symbol
     - patch_linestyles_dict: patch name to linestyle
+
     """"""","--- 

+++ 

@@ -34,11 +34,11 @@

     )
     grad = np.empty(shape=(len(vals),), dtype=""<U10"")  # init grad
     for i, val in enumerate(vals):
-        curl_lightness = (lightness * val) + (
+        cur_lightness = (lightness * val) + (
             min_lightness * (1 - val)
         )  # get cur lightness relative to `hex_col`
-        curl_lightness = max(min(curl_lightness, lightness), min_lightness)  # set min, max bounds
-        cur_rgb_col = hls_to_rgb(hue, curl_lightness, saturation)  # convert to rgb
+        cur_lightness = max(min(cur_lightness, lightness), min_lightness)  # set min, max bounds
+        cur_rgb_col = hls_to_rgb(hue, cur_lightness, saturation)  # convert to rgb
         cur_hex_col = ""#{:02x}{:02x}{:02x}"".format(
             *tuple(int(c * 255) for c in cur_rgb_col)
         )  # convert to hex
@@ -63,13 +63,15 @@

 
 
 def gen_patch_style_dict(patch_names):
-    """"""Based on a list of patches, generates a dictionary of the following items.
+    """"""Generates a dictionary of patch styles given a list of patch_names.
 
-    - patch_colors_dict: patch name to color
-    - patch_markers_dict: patch name to marker
-    - patch_symbols_dict: patch name to symbol
-    - patch_linestyles_dict: patch name to linestyle
+    The dictionary contains dictionaries which map patch names to their respective styles.
+    Below are the keys for each nested dictionary and their contents:
 
+    - colors: patch name to color
+    - markers: patch name to marker
+    - symbols: patch name to symbol
+    - linestyles: patch name to linestyle
     """"""
     return {
         ""colors"": dict(zip(patch_names, patch_colors, strict=False)),"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826162294,53.0,58,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/__init__.py,,"I suggest changing
 ```
+    df = df.convert_dtypes(
+        convert_string=False,
+        convert_integer=False,
+        convert_boolean=False,
+        convert_floating=False,
+    )
```
 to
```
+    df = df.convert_dtypes(
+        convert_string=False, convert_integer=False, convert_boolean=False, convert_floating=False
+    )
```","+    df = df.convert_dtypes(
+        convert_string=False,
+        convert_integer=False,
+        convert_boolean=False,
+        convert_floating=False,
+    )","--- 

+++ 

@@ -1,6 +1,7 @@

 """"""DataJoint pipeline for Aeon.""""""
 
 import hashlib
+import logging
 import os
 import uuid
 
@@ -34,11 +35,17 @@

     return uuid.UUID(hex=hashed.hexdigest())
 
 
-def fetch_stream(query, drop_pk=True):
+def fetch_stream(query, drop_pk=True, round_microseconds=True):
     """"""Fetches data from a Stream table based on a query and returns it as a DataFrame.
 
     Provided a query containing data from a Stream table,
     fetch and aggregate the data into one DataFrame indexed by ""time""
+
+    Args:
+        query (datajoint.Query): A query object containing data from a Stream table
+        drop_pk (bool, optional): Drop primary key columns. Defaults to True.
+        round_microseconds (bool, optional): Round timestamps to microseconds. Defaults to True.
+            (this is important as timestamps in mysql is only accurate to microseconds)
     """"""
     df = (query & ""sample_count > 0"").fetch(format=""frame"").reset_index()
     cols2explode = [
@@ -54,8 +61,12 @@

         convert_string=False,
         convert_integer=False,
         convert_boolean=False,
-        convert_floating=False,
+        convert_floating=False
     )
+    if not df.empty and round_microseconds:
+        logging.warning(""Rounding timestamps to microseconds is now enabled by default.""
+                        "" To disable, set round_microseconds=False."")
+        df.index = df.index.round(""us"")
     return df
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826174458,,291,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/acquisition.py,,"I suggest changing
 ```
+        commit: varchar(64) # e.g., git commit hash of aeon_experiment used to generate this epoch
```
 to
```
+        commit: varchar(64) # e.g. git commit hash of aeon_experiment used to generate this epoch
```
Just to be consistent with the next line","         ---
         bonsai_workflow: varchar(36)
-        commit: varchar(64)   # e.g. git commit hash of aeon_experiment used to generated this particular epoch
+        commit: varchar(64) # e.g., git commit hash of aeon_experiment used to generate this epoch","--- 

+++ 

@@ -12,7 +12,7 @@

 from aeon.dj_pipeline.utils import paths
 from aeon.io import api as io_api
 from aeon.io import reader as io_reader
-from aeon.schema import schemas as aeon_schemas
+from aeon.schema import ingestion_schemas as aeon_schemas
 
 schema = dj.schema(get_schema_name(""acquisition""))
 
@@ -169,6 +169,24 @@

             for dir_type in directory_types
             if (d := cls.get_data_directory(experiment_key, dir_type, as_posix=as_posix)) is not None
         ]
+
+
+@schema
+class ExperimentTimeline(dj.Manual):
+    definition = """"""  # different parts of an experiment timeline
+    -> Experiment
+    name: varchar(32)  # e.g. presocial, social, postsocial
+    ---
+    start: datetime
+    end: datetime
+    note='': varchar(1000)
+    """"""
+
+    class Subject(dj.Part):
+        definition = """"""  # the subjects participating in this part of the experiment timeline
+        -> master
+        -> Experiment.Subject
+        """"""
 
 
 # ------------------- ACQUISITION EPOCH --------------------
@@ -288,7 +306,7 @@

         -> master
         ---
         bonsai_workflow: varchar(36)
-        commit: varchar(64) # e.g., git commit hash of aeon_experiment used to generate this epoch
+        commit: varchar(64) # e.g. git commit hash of aeon_experiment used to generate this epoch
         source='': varchar(16)  # e.g. aeon_experiment or aeon_acquisition (or others)
         metadata: longblob
         metadata_file_path: varchar(255)  # path of the file, relative to the experiment repository
@@ -625,9 +643,7 @@

     directory_types = [""quality-control"", ""raw""]
     raw_data_dirs = {
         dir_type: Experiment.get_data_directory(
-            experiment_key={""experiment_name"": experiment_name},
-            directory_type=dir_type,
-            as_posix=False,
+            experiment_key={""experiment_name"": experiment_name}, directory_type=dir_type, as_posix=False
         )
         for dir_type in directory_types
     }
@@ -663,10 +679,14 @@

 
 def create_chunk_restriction(experiment_name, start_time, end_time):
     """"""Create a time restriction string for the chunks between the specified ""start"" and ""end"" times.""""""
+    exp_key = {""experiment_name"": experiment_name}
     start_restriction = f'""{start_time}"" BETWEEN chunk_start AND chunk_end'
     end_restriction = f'""{end_time}"" BETWEEN chunk_start AND chunk_end'
-    start_query = Chunk & {""experiment_name"": experiment_name} & start_restriction
-    end_query = Chunk & {""experiment_name"": experiment_name} & end_restriction
+    start_query = Chunk & exp_key & start_restriction
+    end_query = Chunk & exp_key & end_restriction
+    if not end_query:
+        # No chunk contains the end time, so we need to find the last chunk that starts before the end time
+        end_query = Chunk & exp_key & f'chunk_end BETWEEN ""{start_time}"" AND ""{end_time}""'
     if not (start_query and end_query):
         raise ValueError(f""No Chunk found between {start_time} and {end_time}"")
     time_restriction = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826178649,628.0,630,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/acquisition.py,,"I suggest changing
 ```
+            experiment_key={""experiment_name"": experiment_name},
+            directory_type=dir_type,
+            as_posix=False,
```
 to
```
+            experiment_key={""experiment_name"": experiment_name}, directory_type=dir_type, as_posix=False
```
Un-apply black (as ruff doesn't remove the last comma) ","+            experiment_key={""experiment_name"": experiment_name},
+            directory_type=dir_type,
+            as_posix=False,","--- 

+++ 

@@ -12,7 +12,7 @@

 from aeon.dj_pipeline.utils import paths
 from aeon.io import api as io_api
 from aeon.io import reader as io_reader
-from aeon.schema import schemas as aeon_schemas
+from aeon.schema import ingestion_schemas as aeon_schemas
 
 schema = dj.schema(get_schema_name(""acquisition""))
 
@@ -169,6 +169,24 @@

             for dir_type in directory_types
             if (d := cls.get_data_directory(experiment_key, dir_type, as_posix=as_posix)) is not None
         ]
+
+
+@schema
+class ExperimentTimeline(dj.Manual):
+    definition = """"""  # different parts of an experiment timeline
+    -> Experiment
+    name: varchar(32)  # e.g. presocial, social, postsocial
+    ---
+    start: datetime
+    end: datetime
+    note='': varchar(1000)
+    """"""
+
+    class Subject(dj.Part):
+        definition = """"""  # the subjects participating in this part of the experiment timeline
+        -> master
+        -> Experiment.Subject
+        """"""
 
 
 # ------------------- ACQUISITION EPOCH --------------------
@@ -288,7 +306,7 @@

         -> master
         ---
         bonsai_workflow: varchar(36)
-        commit: varchar(64) # e.g., git commit hash of aeon_experiment used to generate this epoch
+        commit: varchar(64) # e.g. git commit hash of aeon_experiment used to generate this epoch
         source='': varchar(16)  # e.g. aeon_experiment or aeon_acquisition (or others)
         metadata: longblob
         metadata_file_path: varchar(255)  # path of the file, relative to the experiment repository
@@ -625,9 +643,7 @@

     directory_types = [""quality-control"", ""raw""]
     raw_data_dirs = {
         dir_type: Experiment.get_data_directory(
-            experiment_key={""experiment_name"": experiment_name},
-            directory_type=dir_type,
-            as_posix=False,
+            experiment_key={""experiment_name"": experiment_name}, directory_type=dir_type, as_posix=False
         )
         for dir_type in directory_types
     }
@@ -663,10 +679,14 @@

 
 def create_chunk_restriction(experiment_name, start_time, end_time):
     """"""Create a time restriction string for the chunks between the specified ""start"" and ""end"" times.""""""
+    exp_key = {""experiment_name"": experiment_name}
     start_restriction = f'""{start_time}"" BETWEEN chunk_start AND chunk_end'
     end_restriction = f'""{end_time}"" BETWEEN chunk_start AND chunk_end'
-    start_query = Chunk & {""experiment_name"": experiment_name} & start_restriction
-    end_query = Chunk & {""experiment_name"": experiment_name} & end_restriction
+    start_query = Chunk & exp_key & start_restriction
+    end_query = Chunk & exp_key & end_restriction
+    if not end_query:
+        # No chunk contains the end time, so we need to find the last chunk that starts before the end time
+        end_query = Chunk & exp_key & f'chunk_end BETWEEN ""{start_time}"" AND ""{end_time}""'
     if not (start_query and end_query):
         raise ValueError(f""No Chunk found between {start_time} and {end_time}"")
     time_restriction = ("
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826186555,,132,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
```
 to
```
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
```","     def key_source(self):
-        # Ensure that the chunk ingestion has caught up with this block before processing
-        # (there exists a chunk that ends after the block end time)
+        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826199429,167.0,169,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+        """"""
+        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+
```
 to
```
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
+
```","+        """"""
+        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826199873,,174,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+        """"""  # noqa 501
```
 to
```
+        """"""
```","         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""
+        """"""  # noqa 501","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826205598,,386,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
```
 to
```
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
```","         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob  # timestamps in which a particular subject is spending time at a particular patch
+        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826207148,950.0,952,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+                    group,
+                    ""cumulative_preference_by_wheel"",
+                    ""running_preference_by_wheel"",
```
 to
```
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
```
reverting black","+                    group,
+                    ""cumulative_preference_by_wheel"",
+                    ""running_preference_by_wheel"",","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826208441,1415.0,1418,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"I suggest changing
 ```
+            zip(
+                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
+                strict=True,
+            )
```
 to
```
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
```
reverting black","+            zip(
+                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
+                strict=True,
+            )","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1826216808,,1585,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/block_analysis.py,,"Full suggestion that gets rid of noqa 501. Note that bullet lists need to start after a linebreak. When breaking the bullet description into multiple lines, these need to align with the first line for the docs to render correctly.
```python
    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.

    1. Get all patch state update timestamps (DepletionState): let's call these events ""A""

       - Remove all events within 1 second of each other
       - Remove all events without threshold value (NaN)
    2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""

       - Find matching beam break timestamps within 1.2s after each pellet delivery
    3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""

       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
       previous threshold update
    5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""

    Args:
        patch_key (dict): primary key for the patch
        start (datetime): start timestamp
        end (datetime): end timestamp

    Returns:
        pd.DataFrame: DataFrame with the following columns:

        - threshold_update_timestamp (index)
        - pellet_timestamp
        - beam_break_timestamp
        - offset
        - rate
    """"""
```
","     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
-    4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the previous threshold update
+        - These are the pellet delivery events ""B"" associated with the previous threshold update","--- 

+++ 

@@ -3,7 +3,7 @@

 import itertools
 import json
 from collections import defaultdict
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
@@ -21,17 +21,8 @@

     gen_subject_colors_dict,
     subject_colors,
 )
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    streams,
-    tracking,
-)
-from aeon.dj_pipeline.analysis.visit import (
-    filter_out_maintenance_periods,
-    get_maintenance_periods,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name, streams, tracking
+from aeon.dj_pipeline.analysis.visit import filter_out_maintenance_periods, get_maintenance_periods
 from aeon.io import api as io_api
 
 schema = dj.schema(get_schema_name(""block_analysis""))
@@ -55,6 +46,8 @@

     -> acquisition.Environment
     """"""
 
+    key_source = acquisition.Environment - {""experiment_name"": ""social0.1-aeon3""}
+
     def make(self, key):
         """"""On a per-chunk basis, check for the presence of new block, insert into Block table.
 
@@ -90,8 +83,7 @@

         blocks_df = block_state_df[block_state_df.pellet_ct == 0]
         # account for the double 0s - find any 0s that are within 1 second of each other, remove the 2nd one
         double_0s = blocks_df.index.to_series().diff().dt.total_seconds() < 1
-        # find the indices of the 2nd 0s and remove
-        double_0s = double_0s.shift(-1).fillna(False)
+        # keep the first 0s
         blocks_df = blocks_df[~double_0s]
 
         block_entries = []
@@ -129,7 +121,10 @@

 
     @property
     def key_source(self):
-        """"""Ensure that the chunk ingestion has caught up with this block before processing (there exists a chunk that ends after the block end time).""""""  # noqa 501
+        """"""Ensures chunk ingestion is complete before processing the block.
+
+        This is done by checking that there exists a chunk that ends after the block end time.
+        """"""
         ks = Block.aggr(acquisition.Chunk, latest_chunk_end=""MAX(chunk_end)"")
         ks = ks * Block & ""latest_chunk_end >= block_end"" & ""block_end IS NOT NULL""
         return ks
@@ -145,8 +140,8 @@

         wheel_timestamps: longblob
         patch_threshold: longblob
         patch_threshold_timestamps: longblob
-        patch_rate: float
-        patch_offset: float
+        patch_rate=null: float
+        patch_offset=null: float
         """"""
 
     class Subject(dj.Part):
@@ -164,14 +159,17 @@

         """"""
 
     def make(self, key):
-        """"""
-        Restrict, fetch and aggregate data from different streams to produce intermediate data products at a per-block level (for different patches and different subjects).
+        """"""Collates data from various streams to produce per-block intermediate data products.
+
+        The intermediate data products consist of data for each ``Patch``
+        and each ``Subject`` within the  ``Block``.
+        The steps to restrict, fetch, and aggregate data from various streams are as follows:
 
         1. Query data for all chunks within the block.
         2. Fetch streams, filter by maintenance period.
         3. Fetch subject position data (SLEAP).
         4. Aggregate and insert into the table.
-        """"""  # noqa 501
+        """"""
         block_start, block_end = (Block & key).fetch1(""block_start"", ""block_end"")
 
         chunk_restriction = acquisition.create_chunk_restriction(
@@ -184,7 +182,6 @@

             streams.UndergroundFeederDepletionState,
             streams.UndergroundFeederDeliverPellet,
             streams.UndergroundFeederEncoder,
-            tracking.SLEAPTracking,
         )
         for streams_table in streams_tables:
             if len(streams_table & chunk_keys) < len(streams_table.key_source & chunk_keys):
@@ -194,9 +191,22 @@

                     f""Skipping (to retry later)...""
                 )
 
+        # Check if SLEAPTracking is ready, if not, see if BlobPosition can be used instead
+        use_blob_position = False
+        if len(tracking.SLEAPTracking & chunk_keys) < len(tracking.SLEAPTracking.key_source & chunk_keys):
+            if len(tracking.BlobPosition & chunk_keys) < len(tracking.BlobPosition.key_source & chunk_keys):
+                raise ValueError(
+                    ""BlockAnalysis Not Ready - ""
+                    f""SLEAPTracking (and BlobPosition) not yet fully ingested for block: {key}. ""
+                    ""Skipping (to retry later)...""
+                )
+            else:
+                use_blob_position = True
+
         # Patch data - TriggerPellet, DepletionState, Encoder (distancetravelled)
-        # For wheel data, downsample to 10Hz
-        final_encoder_fs = 10
+        # For wheel data, downsample to 50Hz
+        final_encoder_hz = 50
+        freq = 1 / final_encoder_hz * 1e3  # in ms
 
         maintenance_period = get_maintenance_periods(key[""experiment_name""], block_start, block_end)
 
@@ -238,35 +248,41 @@

                 encoder_df, maintenance_period, block_end, dropna=True
             )
 
-            if depletion_state_df.empty:
-                raise ValueError(f""No depletion state data found for block {key} - patch: {patch_name}"")
-
-            encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
-
-            if len(depletion_state_df.rate.unique()) > 1:
-                # multiple patch rates per block is unexpected
-                # log a note and pick the first rate to move forward
-                AnalysisNote.insert1(
-                    {
-                        ""note_timestamp"": datetime.now(timezone.utc),
-                        ""note_type"": ""Multiple patch rates"",
-                        ""note"": (
-                            f""Found multiple patch rates for block {key} ""
-                            f""- patch: {patch_name} ""
-                            f""- rates: {depletion_state_df.rate.unique()}""
-                        ),
-                    }
-                )
-
-            patch_rate = depletion_state_df.rate.iloc[0]
-            patch_offset = depletion_state_df.offset.iloc[0]
-            # handles patch rate value being INF
-            patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
-
-            encoder_fs = (
-                1 / encoder_df.index.to_series().diff().dt.total_seconds().median()
-            )  # mean or median?
-            wheel_downsampling_factor = int(encoder_fs / final_encoder_fs)
+            # if all dataframes are empty, skip
+            if pellet_ts_threshold_df.empty and depletion_state_df.empty and encoder_df.empty:
+                continue
+
+            if encoder_df.empty:
+                encoder_df[""distance_travelled""] = 0
+            else:
+                # -1 is for placement of magnetic encoder, where wheel movement actually decreases encoder
+                encoder_df[""distance_travelled""] = -1 * analysis_utils.distancetravelled(encoder_df.angle)
+                encoder_df = encoder_df.resample(f""{freq}ms"").first()
+
+            if not depletion_state_df.empty:
+                if len(depletion_state_df.rate.unique()) > 1:
+                    # multiple patch rates per block is unexpected
+                    # log a note and pick the first rate to move forward
+                    AnalysisNote.insert1(
+                        {
+                            ""note_timestamp"": datetime.now(UTC),
+                            ""note_type"": ""Multiple patch rates"",
+                            ""note"": (
+                                f""Found multiple patch rates for block {key} ""
+                                f""- patch: {patch_name} ""
+                                f""- rates: {depletion_state_df.rate.unique()}""
+                            ),
+                        }
+                    )
+
+                patch_rate = depletion_state_df.rate.iloc[0]
+                patch_offset = depletion_state_df.offset.iloc[0]
+                # handles patch rate value being INF
+                patch_rate = 999999999 if np.isinf(patch_rate) else patch_rate
+            else:
+                logger.warning(f""No depletion state data found for block {key} - patch: {patch_name}"")
+                patch_rate = None
+                patch_offset = None
 
             block_patch_entries.append(
                 {
@@ -274,19 +290,14 @@

                     ""patch_name"": patch_name,
                     ""pellet_count"": len(pellet_ts_threshold_df),
                     ""pellet_timestamps"": pellet_ts_threshold_df.pellet_timestamp.values,
-                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values[
-                        ::wheel_downsampling_factor
-                    ],
-                    ""wheel_timestamps"": encoder_df.index.values[::wheel_downsampling_factor],
+                    ""wheel_cumsum_distance_travelled"": encoder_df.distance_travelled.values,
+                    ""wheel_timestamps"": encoder_df.index.values,
                     ""patch_threshold"": pellet_ts_threshold_df.threshold.values,
                     ""patch_threshold_timestamps"": pellet_ts_threshold_df.index.values,
                     ""patch_rate"": patch_rate,
                     ""patch_offset"": patch_offset,
                 }
             )
-
-            # update block_end if last timestamp of encoder_df is before the current block_end
-            block_end = min(encoder_df.index[-1], block_end)
 
         # Subject data
         # Get all unique subjects that visited the environment over the entire exp;
@@ -298,27 +309,53 @@

             & f'chunk_start <= ""{chunk_keys[-1][""chunk_start""]}""'
         )[:block_start]
         subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
         subject_names = []
         for subject_name in set(subject_visits_df.id):
             _df = subject_visits_df[subject_visits_df.id == subject_name]
             if _df.type.iloc[-1] != ""Exit"":
                 subject_names.append(subject_name)
 
+        if use_blob_position and len(subject_names) > 1:
+            raise ValueError(
+                f""Without SLEAPTracking, BlobPosition can only handle a single-subject block. ""
+                f""Found {len(subject_names)} subjects.""
+            )
+
         block_subject_entries = []
         for subject_name in subject_names:
             # positions - query for CameraTop, identity_name matches subject_name,
-            pos_query = (
-                streams.SpinnakerVideoSource
-                * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
-                * tracking.SLEAPTracking.Part
-                & key
-                & {
-                    ""spinnaker_video_source_name"": ""CameraTop"",
-                    ""identity_name"": subject_name,
-                }
-                & chunk_restriction
-            )
-            pos_df = fetch_stream(pos_query)[block_start:block_end]
+            if use_blob_position:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.BlobPosition.Object
+                    & key
+                    & chunk_restriction
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name
+                    }
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+                pos_df[""likelihood""] = np.nan
+                # keep only rows with area between 0 and 1000 - likely artifacts otherwise
+                MIN_AREA = 0
+                MAX_AREA = 1000
+                pos_df = pos_df[(pos_df.area > MIN_AREA) & (pos_df.area < MAX_AREA)]
+            else:
+                pos_query = (
+                    streams.SpinnakerVideoSource
+                    * tracking.SLEAPTracking.PoseIdentity.proj(""identity_name"", part_name=""anchor_part"")
+                    * tracking.SLEAPTracking.Part
+                    & key
+                    & {
+                        ""spinnaker_video_source_name"": ""CameraTop"",
+                        ""identity_name"": subject_name,
+                    }
+                    & chunk_restriction
+                )
+                pos_df = fetch_stream(pos_query)[block_start:block_end]
+
             pos_df = filter_out_maintenance_periods(pos_df, maintenance_period, block_end)
 
             if pos_df.empty:
@@ -355,8 +392,8 @@

             {
                 **key,
                 ""block_duration"": (block_end - block_start).total_seconds() / 3600,
-                ""patch_count"": len(patch_keys),
-                ""subject_count"": len(subject_names),
+                ""patch_count"": len(block_patch_entries),
+                ""subject_count"": len(block_subject_entries),
             }
         )
         self.Patch.insert(block_patch_entries)
@@ -383,7 +420,7 @@

         -> BlockAnalysis.Patch
         -> BlockAnalysis.Subject
         ---
-        in_patch_timestamps: longblob # timestamps when a subject spends time at a specific patch
+        in_patch_timestamps: longblob # timestamps when a subject is at a specific patch
         in_patch_time: float  # total seconds spent in this patch for this block
         pellet_count: int
         pellet_timestamps: longblob
@@ -434,6 +471,21 @@

         )
         subjects_positions_df.set_index(""position_timestamps"", inplace=True)
 
+        # Ensure wheel_timestamps are of the same length across all patches
+        wheel_lens = [len(p[""wheel_timestamps""]) for p in block_patches]
+        MAX_WHEEL_DIFF = 10
+
+        if len(set(wheel_lens)) > 1:
+            max_diff = max(wheel_lens) - min(wheel_lens)
+            if max_diff > MAX_WHEEL_DIFF:
+                # if diff is more than 10 samples, raise error, this is unexpected, some patches crash?
+                raise ValueError(
+                    f""Inconsistent wheel data lengths across patches ({max_diff} samples diff)""
+                )
+            min_wheel_len = min(wheel_lens)
+            for p in block_patches:
+                p[""wheel_timestamps""] = p[""wheel_timestamps""][:min_wheel_len]
+                p[""wheel_cumsum_distance_travelled""] = p[""wheel_cumsum_distance_travelled""][:min_wheel_len]
         self.insert1(key)
 
         in_patch_radius = 130  # pixels
@@ -552,7 +604,7 @@

                     | {
                         ""patch_name"": patch[""patch_name""],
                         ""subject_name"": subject_name,
-                        ""in_patch_timestamps"": subject_in_patch.index.values,
+                        ""in_patch_timestamps"": subject_in_patch[in_patch[subject_name]].index.values,
                         ""in_patch_time"": subject_in_patch_cum_time[-1],
                         ""pellet_count"": len(subj_pellets),
                         ""pellet_timestamps"": subj_pellets.index.values,
@@ -947,9 +999,7 @@

             patch_pref.groupby(""subject_name"")
             .apply(
                 lambda group: calculate_running_preference(
-                    group,
-                    ""cumulative_preference_by_wheel"",
-                    ""running_preference_by_wheel"",
+                    group, ""cumulative_preference_by_wheel"", ""running_preference_by_wheel""
                 )
             )
             .droplevel(0)
@@ -1412,10 +1462,7 @@

             & ""attribute_name = 'Location'""
         )
         rfid_locs = dict(
-            zip(
-                *rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""),
-                strict=True,
-            )
+            zip(*rfid_location_query.fetch(""rfid_reader_name"", ""attribute_value""), strict=True)
         )
 
         ## Create position ethogram df
@@ -1544,10 +1591,10 @@

         foraging_bout_df = get_foraging_bouts(key)
         foraging_bout_df.rename(
             columns={
-                ""subject_name"": ""subject"",
-                ""bout_start"": ""start"",
-                ""bout_end"": ""end"",
-                ""pellet_count"": ""n_pellets"",
+                ""subject"": ""subject_name"",
+                ""start"": ""bout_start"",
+                ""end"": ""bout_end"",
+                ""n_pellets"": ""pellet_count"",
                 ""cum_wheel_dist"": ""cum_wheel_dist"",
             },
             inplace=True,
@@ -1563,7 +1610,7 @@

 @schema
 class AnalysisNote(dj.Manual):
     definition = """"""  # Generic table to catch all notes generated during analysis
-    note_timestamp: datetime
+    note_timestamp: datetime(6)
     ---
     note_type='': varchar(64)
     note: varchar(3000)
@@ -1574,18 +1621,20 @@

 
 
 def get_threshold_associated_pellets(patch_key, start, end):
-    """"""Retrieve the pellet delivery timestamps associated with each patch threshold update within the specified start-end time.
+    """"""Gets pellet delivery timestamps for each patch threshold update within the specified time range.
 
     1. Get all patch state update timestamps (DepletionState): let's call these events ""A""
-        - Remove all events within 1 second of each other
-        - Remove all events without threshold value (NaN)
+
+       - Remove all events within 1 second of each other
+       - Remove all events without threshold value (NaN)
     2. Get all pellet delivery timestamps (DeliverPellet): let's call these events ""B""
-        - Find matching beam break timestamps within 1.2s after each pellet delivery
+
+       - Find matching beam break timestamps within 1.2s after each pellet delivery
     3. For each event ""A"", find the nearest event ""B"" within 100ms before or after the event ""A""
-        - These are the pellet delivery events ""B"" associated with the previous threshold update
-        event ""A""
+
+       - These are the pellet delivery events ""B"" associated with the previous threshold update event ""A""
     4. Shift back the pellet delivery timestamps by 1 to match the pellet delivery with the
-    previous threshold update
+       previous threshold update
     5. Remove all threshold updates events ""A"" without a corresponding pellet delivery event ""B""
 
     Args:
@@ -1595,12 +1644,13 @@

 
     Returns:
         pd.DataFrame: DataFrame with the following columns:
+
         - threshold_update_timestamp (index)
         - pellet_timestamp
         - beam_break_timestamp
         - offset
         - rate
-    """"""  # noqa 501
+    """"""
     chunk_restriction = acquisition.create_chunk_restriction(patch_key[""experiment_name""], start, end)
 
     # Step 1 - fetch data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1827881697,15.0,17,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit.py,,Can we remove these? They don't seem to be used.,"+    lab,
+    qc,
+    tracking,","--- 

+++ 

@@ -1,21 +1,14 @@

 """"""Module for visit-related tables in the analysis schema.""""""
 
 from collections import deque
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
 import pandas as pd
 
 from aeon.analysis import utils as analysis_utils
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    lab,
-    qc,
-    tracking,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name
 
 schema = dj.schema(get_schema_name(""analysis""))
 
@@ -123,15 +116,16 @@

 
 
 def ingest_environment_visits(experiment_names: list | None = None):
-    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
-
-    This ingestion routine handles only those ""complete"" visits,
-    not ingesting any ""on-going"" visits using ""analyze"" method:
-    `aeon.analyze.utils.visits()`.
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
 
     Args:
         experiment_names (list, optional): list of names of the experiment
-        to populate into the Visit table. Defaults to None.
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
     """"""
     if experiment_names is None:
         experiment_names = [""exp0.2-r0""]
@@ -145,7 +139,7 @@

             .fetch(""last_visit"")
         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.now(timezone.utc) if start else ""2200-01-01""
+        end = datetime.now(UTC) if start else ""2200-01-01""
 
         enter_exit_query = (
             acquisition.SubjectEnterExit.Time * acquisition.EventType
@@ -160,10 +154,7 @@

         enter_exit_df = pd.DataFrame(
             zip(
                 *enter_exit_query.fetch(
-                    ""subject"",
-                    ""enter_exit_time"",
-                    ""event_type"",
-                    order_by=""enter_exit_time"",
+                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                 ),
                 strict=False,
             )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1827947068,126.0,130,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit.py,,"I suggest changing
 ```
+    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
+
+    This ingestion routine handles only those ""complete"" visits,
+    not ingesting any ""on-going"" visits using ""analyze"" method:
+    `aeon.analyze.utils.visits()`.
```
 to
```
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
```
The [`` :role:`target` `` syntax](https://www.sphinx-doc.org/en/master/usage/domains/python.html#cross-referencing-python-objects) will resolve into the correct URL in the API reference on the docs website.","+    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
+
+    This ingestion routine handles only those ""complete"" visits,
+    not ingesting any ""on-going"" visits using ""analyze"" method:
+    `aeon.analyze.utils.visits()`.","--- 

+++ 

@@ -1,21 +1,14 @@

 """"""Module for visit-related tables in the analysis schema.""""""
 
 from collections import deque
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
 import pandas as pd
 
 from aeon.analysis import utils as analysis_utils
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    lab,
-    qc,
-    tracking,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name
 
 schema = dj.schema(get_schema_name(""analysis""))
 
@@ -123,15 +116,16 @@

 
 
 def ingest_environment_visits(experiment_names: list | None = None):
-    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
-
-    This ingestion routine handles only those ""complete"" visits,
-    not ingesting any ""on-going"" visits using ""analyze"" method:
-    `aeon.analyze.utils.visits()`.
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
 
     Args:
         experiment_names (list, optional): list of names of the experiment
-        to populate into the Visit table. Defaults to None.
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
     """"""
     if experiment_names is None:
         experiment_names = [""exp0.2-r0""]
@@ -145,7 +139,7 @@

             .fetch(""last_visit"")
         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.now(timezone.utc) if start else ""2200-01-01""
+        end = datetime.now(UTC) if start else ""2200-01-01""
 
         enter_exit_query = (
             acquisition.SubjectEnterExit.Time * acquisition.EventType
@@ -160,10 +154,7 @@

         enter_exit_df = pd.DataFrame(
             zip(
                 *enter_exit_query.fetch(
-                    ""subject"",
-                    ""enter_exit_time"",
-                    ""event_type"",
-                    order_by=""enter_exit_time"",
+                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                 ),
                 strict=False,
             )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1827948943,133.0,134,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit.py,,"I suggest changing
 ```
+        experiment_names (list, optional): list of names of the experiment
+        to populate into the Visit table. Defaults to None.
```
 to
```
+        experiment_names (list, optional): list of names of the experiment
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
```
Need to indent subsequent lines for docs to render correctly.","+        experiment_names (list, optional): list of names of the experiment
+        to populate into the Visit table. Defaults to None.","--- 

+++ 

@@ -1,21 +1,14 @@

 """"""Module for visit-related tables in the analysis schema.""""""
 
 from collections import deque
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
 import pandas as pd
 
 from aeon.analysis import utils as analysis_utils
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    lab,
-    qc,
-    tracking,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name
 
 schema = dj.schema(get_schema_name(""analysis""))
 
@@ -123,15 +116,16 @@

 
 
 def ingest_environment_visits(experiment_names: list | None = None):
-    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
-
-    This ingestion routine handles only those ""complete"" visits,
-    not ingesting any ""on-going"" visits using ""analyze"" method:
-    `aeon.analyze.utils.visits()`.
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
 
     Args:
         experiment_names (list, optional): list of names of the experiment
-        to populate into the Visit table. Defaults to None.
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
     """"""
     if experiment_names is None:
         experiment_names = [""exp0.2-r0""]
@@ -145,7 +139,7 @@

             .fetch(""last_visit"")
         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.now(timezone.utc) if start else ""2200-01-01""
+        end = datetime.now(UTC) if start else ""2200-01-01""
 
         enter_exit_query = (
             acquisition.SubjectEnterExit.Time * acquisition.EventType
@@ -160,10 +154,7 @@

         enter_exit_df = pd.DataFrame(
             zip(
                 *enter_exit_query.fetch(
-                    ""subject"",
-                    ""enter_exit_time"",
-                    ""event_type"",
-                    order_by=""enter_exit_time"",
+                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                 ),
                 strict=False,
             )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1827952244,,168,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit.py,,"```python
        enter_exit_df = pd.DataFrame(
            zip(
                *enter_exit_query.fetch(
                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                ),
                strict=False,
            )
        )
```

Revert black","                     order_by=""enter_exit_time"",
-                )
+                ),
+                strict=False,","--- 

+++ 

@@ -1,21 +1,14 @@

 """"""Module for visit-related tables in the analysis schema.""""""
 
 from collections import deque
-from datetime import datetime, timezone
+from datetime import UTC, datetime
 
 import datajoint as dj
 import numpy as np
 import pandas as pd
 
 from aeon.analysis import utils as analysis_utils
-from aeon.dj_pipeline import (
-    acquisition,
-    fetch_stream,
-    get_schema_name,
-    lab,
-    qc,
-    tracking,
-)
+from aeon.dj_pipeline import acquisition, fetch_stream, get_schema_name
 
 schema = dj.schema(get_schema_name(""analysis""))
 
@@ -123,15 +116,16 @@

 
 
 def ingest_environment_visits(experiment_names: list | None = None):
-    """"""Function to populate into `Visit` and `VisitEnd` for specified experiments (default: 'exp0.2-r0').
-
-    This ingestion routine handles only those ""complete"" visits,
-    not ingesting any ""on-going"" visits using ""analyze"" method:
-    `aeon.analyze.utils.visits()`.
+    """"""Populates ``Visit`` and ``VisitEnd`` for the specified experiment names.
+
+    This ingestion routine includes only ""complete"" visits and
+    does not ingest any ""on-going"" visits.
+    Visits are retrieved using :func:`aeon.analysis.utils.visits`.
 
     Args:
         experiment_names (list, optional): list of names of the experiment
-        to populate into the Visit table. Defaults to None.
+            to populate into the ``Visit`` table.
+            If unspecified, defaults to ``None`` and ``['exp0.2-r0']`` is used.
     """"""
     if experiment_names is None:
         experiment_names = [""exp0.2-r0""]
@@ -145,7 +139,7 @@

             .fetch(""last_visit"")
         )
         start = min(subjects_last_visits) if len(subjects_last_visits) else ""1900-01-01""
-        end = datetime.now(timezone.utc) if start else ""2200-01-01""
+        end = datetime.now(UTC) if start else ""2200-01-01""
 
         enter_exit_query = (
             acquisition.SubjectEnterExit.Time * acquisition.EventType
@@ -160,10 +154,7 @@

         enter_exit_df = pd.DataFrame(
             zip(
                 *enter_exit_query.fetch(
-                    ""subject"",
-                    ""enter_exit_time"",
-                    ""event_type"",
-                    order_by=""enter_exit_time"",
+                    ""subject"", ""enter_exit_time"", ""event_type"", order_by=""enter_exit_time""
                 ),
                 strict=False,
             )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828019583,198.0,202,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,"I suggest changing
 ```
+        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
+
+        Given a key to a single Visit, return a Pandas DataFrame for
+        the position data of the subject for the specified Visit time period.
+        """"""
```
 to
```
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or 
+        a combination of ``subject``, ``start``, and ``end``. 
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name. 
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+        """"""
```","+        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
+
+        Given a key to a single Visit, return a Pandas DataFrame for
+        the position data of the subject for the specified Visit time period.
+        """"""","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -195,10 +196,21 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
@@ -207,15 +219,10 @@

                 Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -525,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """""" # Time from animal's entry to exit of a food patch while moving the wheel.
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828039232,,528,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,"I suggest changing
 ```
+    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
```
 to
```
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
```"," @schema
 class VisitForagingBout(dj.Computed):
-    definition = """""" # A time period spanning the time when the animal enters a food patch and moves the wheel to when it leaves the food patch
+    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -195,10 +196,21 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
@@ -207,15 +219,10 @@

                 Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -525,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """""" # Time from animal's entry to exit of a food patch while moving the wheel.
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828077906,,530,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/analysis/visit_analysis.py,,"I suggest changing
 ```
+    definition = """""" # Time from animal's entry to exit of a food patch while moving the wheel.
```
 to
```
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
```","-    definition = """""" # A time period spanning the time when the animal enters a food patch and moves the wheel to when it leaves the food patch
+    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
+
+    definition = """""" # Time from animal's entry to exit of a food patch while moving the wheel.","--- 

+++ 

@@ -16,6 +16,7 @@

 )
 
 logger = dj.logger
+
 # schema = dj.schema(get_schema_name(""analysis""))
 schema = dj.schema()
 
@@ -195,10 +196,21 @@

 
     @classmethod
     def get_position(cls, visit_key=None, subject=None, start=None, end=None):
-        """"""Return a Pandas df of the subject's position data for a specified Visit given its key.
-
-        Given a key to a single Visit, return a Pandas DataFrame for
-        the position data of the subject for the specified Visit time period.
+        """"""Retrieves a Pandas DataFrame of a subject's position data for a specified ``Visit``.
+
+        A ``Visit`` is specified by either a ``visit_key`` or
+        a combination of ``subject``, ``start``, and ``end``.
+        If all four arguments are provided, the ``visit_key`` is ignored.
+
+        Args:
+            visit_key (dict, optional): key to a single ``Visit``.
+                Only required if ``subject``, ``start``, and ``end`` are not provided.
+            subject (str, optional): subject name.
+                Only required if ``visit_key`` is not provided.
+            start (datetime): start time of the period of interest.
+                Only required if ``visit_key`` is not provided.
+            end (datetime, optional): end time of the period of interest.
+                Only required if ``visit_key`` is not provided.
         """"""
         if visit_key is not None:
             if len(Visit & visit_key) != 1:
@@ -207,15 +219,10 @@

                 Visit.join(VisitEnd, left=True).proj(visit_end=""IFNULL(visit_end, NOW())"") & visit_key
             ).fetch1(""visit_start"", ""visit_end"")
             subject = visit_key[""subject""]
-        elif all((subject, start, end)):
-            start = start  # noqa PLW0127
-            end = end  # noqa PLW0127
-            subject = subject  # noqa PLW0127
-        else:
+        elif not all((subject, start, end)):
             raise ValueError(
-                'Either ""visit_key"" or all three ""subject"", ""start"" and ""end"" has to be specified'
-            )
-
+                'Either ""visit_key"" or all three ""subject"", ""start"", and ""end"" must be specified.'
+        )
         return tracking._get_position(
             cls.TimeSlice,
             object_attr=""subject"",
@@ -525,9 +532,9 @@

 
 @schema
 class VisitForagingBout(dj.Computed):
-    """"""Time period from when the animal enters to when it leaves a food patch while moving the wheel.""""""
-
-    definition = """""" # Time from animal's entry to exit of a food patch while moving the wheel.
+    """"""Time period when a subject enters a food patch, moves the wheel, and then leaves the patch.""""""
+
+    definition = """""" # Time from subject's entry to exit of a food patch to interact with the wheel.
     -> Visit
     -> acquisition.ExperimentFoodPatch
     bout_start: datetime(6)                    # start time of bout"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828092409,,1,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/create_experiments/create_experiment_02.py,,"I suggest changing
 ```
+""""""Function to create new experiments for experiment0.2.""""""
```
 to
```
+""""""Functions to create new experiments for experiment0.2.""""""
```
Same suggestion applies to all other `create_experiment` scripts","@@ -1,3 +1,5 @@
+""""""Function to create new experiments for experiment0.2.""""""","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""Function to create new experiments for experiment0.2.""""""
+""""""Functions to create new experiments for experiment0.2.""""""
 
 from aeon.dj_pipeline import acquisition, lab, subject
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828095075,57.0,60,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/create_experiments/create_socialexperiment.py,,"I suggest changing
 ```
+            {
+                ""experiment_name"": experiment_name,
+                ""devices_schema_name"": exp_name.replace(""."", """"),
+            },
```
 to
```
+            {""experiment_name"": experiment_name, ""devices_schema_name"": exp_name.replace(""."", """")},
```
Revert black","+            {
+                ""experiment_name"": experiment_name,
+                ""devices_schema_name"": exp_name.replace(""."", """"),
+            },","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""Function to create new social experiments.""""""
+""""""Functions to create new social experiments.""""""
 
 from datetime import datetime
 
@@ -54,9 +54,6 @@

         )
         acquisition.Experiment.Directory.insert(experiment_directories, skip_duplicates=True)
         acquisition.Experiment.DevicesSchema.insert1(
-            {
-                ""experiment_name"": experiment_name,
-                ""devices_schema_name"": exp_name.replace(""."", """"),
-            },
+            {""experiment_name"": experiment_name, ""devices_schema_name"": exp_name.replace(""."", """")},
             skip_duplicates=True,
         )"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828096882,6.0,8,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/create_experiments/create_socialexperiment_0.py,,"I suggest changing
 ```
+from aeon.dj_pipeline.create_experiments.create_experiment_01 import (
+    ingest_exp01_metadata,
+)
```
 to
```
+from aeon.dj_pipeline.create_experiments.create_experiment_01 import ingest_exp01_metadata
```
Revert black","+from aeon.dj_pipeline.create_experiments.create_experiment_01 import (
+    ingest_exp01_metadata,
+)","--- 

+++ 

@@ -1,11 +1,9 @@

-""""""Function to create new experiments for social0-r1.""""""
+""""""Functions to create new experiments for social0-r1.""""""
 
 import pathlib
 
 from aeon.dj_pipeline import acquisition, lab, subject
-from aeon.dj_pipeline.create_experiments.create_experiment_01 import (
-    ingest_exp01_metadata,
-)
+from aeon.dj_pipeline.create_experiments.create_experiment_01 import ingest_exp01_metadata
 
 # ============ Manual and automatic steps to for experiment 0.1 populate ============
 experiment_name = ""social0-r1"""
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828112826,4.0,8,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/populate/worker.py,,"I suggest changing
 ```
+from datajoint_utilities.dj_worker import (
+    DataJointWorker,
+    ErrorLog,
+    WorkerLog,
+)
```
 to
```
+from datajoint_utilities.dj_worker import DataJointWorker, ErrorLog, WorkerLog
```
Revert black","+from datajoint_utilities.dj_worker import (
+    DataJointWorker,
+    ErrorLog,
+    WorkerLog,
+)","--- 

+++ 

@@ -1,11 +1,7 @@

 """"""This module defines the workers for the AEON pipeline.""""""
 
 import datajoint as dj
-from datajoint_utilities.dj_worker import (
-    DataJointWorker,
-    ErrorLog,
-    WorkerLog,
-)
+from datajoint_utilities.dj_worker import DataJointWorker, ErrorLog, WorkerLog
 from datajoint_utilities.dj_worker.worker_schema import is_djtable
 
 from aeon.dj_pipeline import acquisition, db_prefix, qc, subject, tracking
@@ -104,9 +100,10 @@

 )
 
 analysis_worker(block_analysis.BlockAnalysis, max_calls=6)
-analysis_worker(block_analysis.BlockPlots, max_calls=6)
 analysis_worker(block_analysis.BlockSubjectAnalysis, max_calls=6)
-analysis_worker(block_analysis.BlockSubjectPlots, max_calls=6)
+analysis_worker(block_analysis.BlockForaging, max_calls=6)
+analysis_worker(block_analysis.BlockPatchPlots, max_calls=6)
+analysis_worker(block_analysis.BlockSubjectPositionPlots, max_calls=6)
 
 
 def get_workflow_operation_overview():
@@ -114,3 +111,22 @@

     from datajoint_utilities.dj_worker.utils import get_workflow_operation_overview
 
     return get_workflow_operation_overview(worker_schema_name=worker_schema_name, db_prefixes=[db_prefix])
+
+
+def retrieve_schemas_sizes(schema_only=False, all_schemas=False):
+    schema_names = [n for n in dj.list_schemas() if n != ""mysql""]
+    if not all_schemas:
+        schema_names = [n for n in schema_names
+                        if n.startswith(db_prefix) and not n.startswith(f""{db_prefix}archived"")]
+
+    if schema_only:
+        return {n: dj.Schema(n).size_on_disk / 1e9 for n in schema_names}
+
+    schema_sizes = {n: {} for n in schema_names}
+    for n in schema_names:
+        vm = dj.VirtualModule(n, n)
+        schema_sizes[n][""schema_gb""] = vm.schema.size_on_disk / 1e9
+        schema_sizes[n][""tables_gb""] = {n: t().size_on_disk / 1e9
+                                        for n, t in vm.__dict__.items()
+                                        if isinstance(t, dj.user_tables.TableMeta)}
+    return schema_sizes"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828119342,16.0,24,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/scripts/clone_and_freeze_exp01.py,,"I suggest changing
 ```
+    for schema_name in (
+        ""lab"",
+        ""subject"",
+        ""acquisition"",
+        ""tracking"",
+        ""qc"",
+        ""report"",
+        ""analysis"",
+    )
```
 to
```
+    for schema_name in (""lab"", ""subject"", ""acquisition"", ""tracking"", ""qc"", ""report"", ""analysis"")
```
Revert black","+    for schema_name in (
+        ""lab"",
+        ""subject"",
+        ""acquisition"",
+        ""tracking"",
+        ""qc"",
+        ""report"",
+        ""analysis"",
+    )","--- 

+++ 

@@ -13,15 +13,7 @@

 
 schema_name_mapper = {
     source_db_prefix + schema_name: target_db_prefix + schema_name
-    for schema_name in (
-        ""lab"",
-        ""subject"",
-        ""acquisition"",
-        ""tracking"",
-        ""qc"",
-        ""report"",
-        ""analysis"",
-    )
+    for schema_name in (""lab"", ""subject"", ""acquisition"", ""tracking"", ""qc"", ""report"", ""analysis"")
 }
 
 restriction = {""experiment_name"": ""exp0.1-r0""}"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828120642,22.0,30,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/scripts/clone_and_freeze_exp02.py,,"I suggest changing
 ```
+    for schema_name in (
+        ""lab"",
+        ""subject"",
+        ""acquisition"",
+        ""tracking"",
+        ""qc"",
+        ""analysis"",
+        ""report"",
+    )
```
 to
```
+    for schema_name in (""lab"", ""subject"", ""acquisition"", ""tracking"", ""qc"", ""analysis"", ""report"")
```
Revert black","+    for schema_name in (
+        ""lab"",
+        ""subject"",
+        ""acquisition"",
+        ""tracking"",
+        ""qc"",
+        ""analysis"",
+        ""report"",
+    )","--- 

+++ 

@@ -19,15 +19,7 @@

 
 schema_name_mapper = {
     source_db_prefix + schema_name: target_db_prefix + schema_name
-    for schema_name in (
-        ""lab"",
-        ""subject"",
-        ""acquisition"",
-        ""tracking"",
-        ""qc"",
-        ""analysis"",
-        ""report"",
-    )
+    for schema_name in (""lab"", ""subject"", ""acquisition"", ""tracking"", ""qc"", ""analysis"", ""report"")
 }
 
 restriction = [{""experiment_name"": ""exp0.2-r0""}, {""experiment_name"": ""social0-r1""}]"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828128411,102.0,105,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/subject.py,,"I suggest changing
 ```
+            {
+                ""strain_id"": animal_resp[""strain_id""],
+                ""strain_name"": animal_resp[""strain_id""],
+            },
```
 to
```
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
```
Revert black

Likewise the following dicts can fit in a single line
https://github.com/SainsburyWellcomeCentre/aeon_mecha/blob/48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b/aeon/dj_pipeline/subject.py#L74-L85","+            {
+                ""strain_id"": animal_resp[""strain_id""],
+                ""strain_name"": animal_resp[""strain_id""],
+            },","--- 

+++ 

@@ -3,7 +3,7 @@

 import json
 import os
 import time
-from datetime import datetime, timedelta, timezone
+from datetime import UTC, datetime, timedelta
 
 import datajoint as dj
 import requests
@@ -67,6 +67,7 @@

             ""o"": 0,
             ""l"": 10,
             ""eartag"": eartag_or_id,
+            ""state"": [""live"", ""sacrificed"", ""exported""],
         }
         animal_resp = get_pyrat_data(endpoint=""animals"", params=params)
         if len(animal_resp) == 0:
@@ -99,10 +100,7 @@

             }
         )
         Strain.insert1(
-            {
-                ""strain_id"": animal_resp[""strain_id""],
-                ""strain_name"": animal_resp[""strain_id""],
-            },
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
             skip_duplicates=True,
         )
         entry = {
@@ -111,13 +109,11 @@

             ""strain_id"": animal_resp[""strain_id""],
             ""cage_number"": animal_resp[""cagenumber""],
             ""lab_id"": animal_resp[""labid""],
+            ""available"": animal_resp.get(""state"", """") == ""live"",
         }
         if animal_resp[""gen_bg_id""] is not None:
             GeneticBackground.insert1(
-                {
-                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
-                    ""gen_bg"": animal_resp[""gen_bg""],
-                },
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
                 skip_duplicates=True,
             )
             entry[""gen_bg_id""] = animal_resp[""gen_bg_id""]
@@ -191,7 +187,7 @@

                 0
             ]
         else:
-            ref_date = datetime.now(timezone.utc).date()
+            ref_date = datetime.now(UTC).date()
 
         weight_query = SubjectWeight & subj_key & f""weight_time < '{ref_date}'""
         ref_weight = (
@@ -201,7 +197,7 @@

         entry = {
             ""subject"": subject_name,
             ""reference_weight"": ref_weight,
-            ""last_updated_time"": datetime.now(timezone.utc),
+            ""last_updated_time"": datetime.now(UTC),
         }
         cls.update1(entry) if cls & {""subject"": subject_name} else cls.insert1(entry)
 
@@ -244,7 +240,7 @@

 
     def _auto_schedule(self):
         """"""Automatically schedule the next task.""""""
-        utc_now = datetime.now(timezone.utc)
+        utc_now = datetime.now(UTC)
 
         next_task_schedule_time = utc_now + timedelta(hours=self.schedule_interval)
         if (
@@ -257,11 +253,14 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the Subject table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         new_eartags = []
         for responsible_id in lab.User.fetch(""responsible_id""):
             # 1 - retrieve all animals from this user
-            animal_resp = get_pyrat_data(endpoint=""animals"", params={""responsible_id"": responsible_id})
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""responsible_id"": responsible_id, ""state"": [""live"", ""sacrificed"", ""exported""]}
+            )
             for animal_entry in animal_resp:
                 # 2 - find animal with comment - Project Aeon
                 eartag_or_id = animal_entry[""eartag_or_id""]
@@ -289,7 +288,7 @@

             new_entry_count += 1
 
         logger.info(f""Inserting {new_entry_count} new subject(s) from Pyrat"")
-        completion_time = datetime.now(timezone.utc)
+        completion_time = datetime.now(UTC)
         self.insert1(
             {
                 **key,
@@ -320,7 +319,7 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the PyratCommentWeightProcedure table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         logger.info(""Extracting weights/comments/procedures"")
 
         eartag_or_id = key[""subject""]
@@ -330,7 +329,7 @@

             if e.args[0].endswith(""response code: 404""):
                 SubjectDetail.update1(
                     {
-                        **key,
+                        ""subject"": key[""subject""],
                         ""available"": False,
                     }
                 )
@@ -359,7 +358,21 @@

             # compute/update reference weight
             SubjectReferenceWeight.get_reference_weight(eartag_or_id)
         finally:
-            completion_time = datetime.now(timezone.utc)
+            # recheck for ""state"" to see if the animal is still available
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""k"": [""labid"", ""state""],
+                        ""eartag"": eartag_or_id,
+                        ""state"": [""live"", ""sacrificed"", ""exported""]})
+            animal_resp = animal_resp[0]
+            SubjectDetail.update1(
+                {
+                    ""subject"": key[""subject""],
+                    ""available"": animal_resp.get(""state"", """") == ""live"",
+                    ""lab_id"": animal_resp[""labid""],
+                }
+            )
+            completion_time = datetime.now(UTC)
             self.insert1(
                 {
                     **key,
@@ -377,7 +390,7 @@

 
     def make(self, key):
         """"""Create one new PyratIngestionTask for every newly added users.""""""
-        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(timezone.utc)})
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(UTC)})
         time.sleep(1)
         self.insert1(key)
 
@@ -447,7 +460,10 @@

 
 
 def get_pyrat_data(endpoint: str, params: dict = None, **kwargs):
-    """"""Get data from PyRat API.""""""
+    """"""Get data from PyRat API.
+
+    See docs at: https://swc.pyrat.cloud/api/v3/docs (production)
+    """"""
     base_url = ""https://swc.pyrat.cloud/api/v3/""
     pyrat_system_token = os.getenv(""PYRAT_SYSTEM_TOKEN"")
     pyrat_user_token = os.getenv(""PYRAT_USER_TOKEN"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828129894,117.0,120,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/subject.py,,"I suggest changing
 ```
+                {
+                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
+                    ""gen_bg"": animal_resp[""gen_bg""],
+                },
```
 to
```
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
```
Revert black","+                {
+                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
+                    ""gen_bg"": animal_resp[""gen_bg""],
+                },","--- 

+++ 

@@ -3,7 +3,7 @@

 import json
 import os
 import time
-from datetime import datetime, timedelta, timezone
+from datetime import UTC, datetime, timedelta
 
 import datajoint as dj
 import requests
@@ -67,6 +67,7 @@

             ""o"": 0,
             ""l"": 10,
             ""eartag"": eartag_or_id,
+            ""state"": [""live"", ""sacrificed"", ""exported""],
         }
         animal_resp = get_pyrat_data(endpoint=""animals"", params=params)
         if len(animal_resp) == 0:
@@ -99,10 +100,7 @@

             }
         )
         Strain.insert1(
-            {
-                ""strain_id"": animal_resp[""strain_id""],
-                ""strain_name"": animal_resp[""strain_id""],
-            },
+            {""strain_id"": animal_resp[""strain_id""], ""strain_name"": animal_resp[""strain_id""]},
             skip_duplicates=True,
         )
         entry = {
@@ -111,13 +109,11 @@

             ""strain_id"": animal_resp[""strain_id""],
             ""cage_number"": animal_resp[""cagenumber""],
             ""lab_id"": animal_resp[""labid""],
+            ""available"": animal_resp.get(""state"", """") == ""live"",
         }
         if animal_resp[""gen_bg_id""] is not None:
             GeneticBackground.insert1(
-                {
-                    ""gen_bg_id"": animal_resp[""gen_bg_id""],
-                    ""gen_bg"": animal_resp[""gen_bg""],
-                },
+                {""gen_bg_id"": animal_resp[""gen_bg_id""], ""gen_bg"": animal_resp[""gen_bg""]},
                 skip_duplicates=True,
             )
             entry[""gen_bg_id""] = animal_resp[""gen_bg_id""]
@@ -191,7 +187,7 @@

                 0
             ]
         else:
-            ref_date = datetime.now(timezone.utc).date()
+            ref_date = datetime.now(UTC).date()
 
         weight_query = SubjectWeight & subj_key & f""weight_time < '{ref_date}'""
         ref_weight = (
@@ -201,7 +197,7 @@

         entry = {
             ""subject"": subject_name,
             ""reference_weight"": ref_weight,
-            ""last_updated_time"": datetime.now(timezone.utc),
+            ""last_updated_time"": datetime.now(UTC),
         }
         cls.update1(entry) if cls & {""subject"": subject_name} else cls.insert1(entry)
 
@@ -244,7 +240,7 @@

 
     def _auto_schedule(self):
         """"""Automatically schedule the next task.""""""
-        utc_now = datetime.now(timezone.utc)
+        utc_now = datetime.now(UTC)
 
         next_task_schedule_time = utc_now + timedelta(hours=self.schedule_interval)
         if (
@@ -257,11 +253,14 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the Subject table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         new_eartags = []
         for responsible_id in lab.User.fetch(""responsible_id""):
             # 1 - retrieve all animals from this user
-            animal_resp = get_pyrat_data(endpoint=""animals"", params={""responsible_id"": responsible_id})
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""responsible_id"": responsible_id, ""state"": [""live"", ""sacrificed"", ""exported""]}
+            )
             for animal_entry in animal_resp:
                 # 2 - find animal with comment - Project Aeon
                 eartag_or_id = animal_entry[""eartag_or_id""]
@@ -289,7 +288,7 @@

             new_entry_count += 1
 
         logger.info(f""Inserting {new_entry_count} new subject(s) from Pyrat"")
-        completion_time = datetime.now(timezone.utc)
+        completion_time = datetime.now(UTC)
         self.insert1(
             {
                 **key,
@@ -320,7 +319,7 @@

 
     def make(self, key):
         """"""Automatically import or update entries in the PyratCommentWeightProcedure table.""""""
-        execution_time = datetime.now(timezone.utc)
+        execution_time = datetime.now(UTC)
         logger.info(""Extracting weights/comments/procedures"")
 
         eartag_or_id = key[""subject""]
@@ -330,7 +329,7 @@

             if e.args[0].endswith(""response code: 404""):
                 SubjectDetail.update1(
                     {
-                        **key,
+                        ""subject"": key[""subject""],
                         ""available"": False,
                     }
                 )
@@ -359,7 +358,21 @@

             # compute/update reference weight
             SubjectReferenceWeight.get_reference_weight(eartag_or_id)
         finally:
-            completion_time = datetime.now(timezone.utc)
+            # recheck for ""state"" to see if the animal is still available
+            animal_resp = get_pyrat_data(
+                endpoint=""animals"",
+                params={""k"": [""labid"", ""state""],
+                        ""eartag"": eartag_or_id,
+                        ""state"": [""live"", ""sacrificed"", ""exported""]})
+            animal_resp = animal_resp[0]
+            SubjectDetail.update1(
+                {
+                    ""subject"": key[""subject""],
+                    ""available"": animal_resp.get(""state"", """") == ""live"",
+                    ""lab_id"": animal_resp[""labid""],
+                }
+            )
+            completion_time = datetime.now(UTC)
             self.insert1(
                 {
                     **key,
@@ -377,7 +390,7 @@

 
     def make(self, key):
         """"""Create one new PyratIngestionTask for every newly added users.""""""
-        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(timezone.utc)})
+        PyratIngestionTask.insert1({""pyrat_task_scheduled_time"": datetime.now(UTC)})
         time.sleep(1)
         self.insert1(key)
 
@@ -447,7 +460,10 @@

 
 
 def get_pyrat_data(endpoint: str, params: dict = None, **kwargs):
-    """"""Get data from PyRat API.""""""
+    """"""Get data from PyRat API.
+
+    See docs at: https://swc.pyrat.cloud/api/v3/docs (production)
+    """"""
     base_url = ""https://swc.pyrat.cloud/api/v3/""
     pyrat_system_token = os.getenv(""PYRAT_SYSTEM_TOKEN"")
     pyrat_user_token = os.getenv(""PYRAT_USER_TOKEN"")"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828135022,8.0,14,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,"I suggest changing
 ```
+from aeon.dj_pipeline import (
+    acquisition,
+    dict_to_uuid,
+    get_schema_name,
+    lab,
+    streams,
+)
```
 to
```
+from aeon.dj_pipeline import acquisition, dict_to_uuid, get_schema_name, lab, streams
```
Revert black","+from aeon.dj_pipeline import (
+    acquisition,
+    dict_to_uuid,
+    get_schema_name,
+    lab,
+    streams,
+)","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828143165,119.0,126,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,"I suggest changing
 ```
+    """"""Tracking data from SLEAP for multi-animal experiments.
+
+    Tracked objects position data from a particular
+    VideoSource for multi-animal experiment using the SLEAP tracking
+    method per chunk.
+    """"""
+
+    definition = """"""
```
 to
```
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Tracked objects position data from a particular
+VideoSource for multi-animal experiment using the SLEAP tracking method per chunk.
```
We can still keep the definition of the table right?","+    """"""Tracking data from SLEAP for multi-animal experiments.
+
+    Tracked objects position data from a particular
+    VideoSource for multi-animal experiment using the SLEAP tracking
+    method per chunk.
+    """"""
+
+    definition = """"""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828153409,,246,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,"I suggest changing
 ```
+    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
```
 to
```
+    """"""Compute the distance between the position and the target.
+    
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
```"," 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    assert len(target) == 2
+    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828170826,,255,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/tracking.py,,"I suggest changing
 ```
+    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
```
 to
```
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
```"," def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
+    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""","--- 

+++ 

@@ -5,15 +5,10 @@

 import numpy as np
 import pandas as pd
 
-from aeon.dj_pipeline import (
-    acquisition,
-    dict_to_uuid,
-    get_schema_name,
-    lab,
-    streams,
-)
+from aeon.dj_pipeline import acquisition, dict_to_uuid, fetch_stream, get_schema_name, lab, streams
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 schema = dj.schema(get_schema_name(""tracking""))
 logger = dj.logger
@@ -116,14 +111,9 @@

 
 @schema
 class SLEAPTracking(dj.Imported):
-    """"""Tracking data from SLEAP for multi-animal experiments.
-
-    Tracked objects position data from a particular
-    VideoSource for multi-animal experiment using the SLEAP tracking
-    method per chunk.
-    """"""
-
-    definition = """"""
+    """"""Tracking data from SLEAP for multi-animal experiments.""""""
+
+    definition = """""" # Position data from a VideoSource for multi-animal experiments using SLEAP per chunk
     -> acquisition.Chunk
     -> streams.SpinnakerVideoSource
     -> TrackingParamSet
@@ -179,7 +169,17 @@

                 ""devices_schema_name""
             ),
         )
+
         stream_reader = getattr(devices_schema, device_name).Pose
+
+        # special ingestion case for social0.2 full-pose data (using Pose reader from social03)
+        # fullpose for social0.2 has a different ""pattern"" for non-fullpose, hence the Pose03 reader
+        if key[""experiment_name""].startswith(""social0.2""):
+            from aeon.io import reader as io_reader
+            stream_reader = getattr(devices_schema, device_name).Pose03
+            if not isinstance(stream_reader, io_reader.Pose):
+                raise TypeError(""Pose03 is not a Pose reader"")
+            data_dirs = [acquisition.Experiment.get_data_directory(key, ""processed"")]
 
         pose_data = io_api.load(
             root=data_dirs,
@@ -194,6 +194,11 @@

         # get identity names
         class_names = np.unique(pose_data.identity)
         identity_mapping = {n: i for i, n in enumerate(class_names)}
+
+        # get anchor part
+        # this logic is valid only if the different animals have the same skeleton and anchor part
+        #   which should be the case within one chunk
+        anchor_part = next(v.replace(""_x"", """") for v in stream_reader.columns if v.endswith(""_x""))
 
         # ingest parts and classes
         pose_identity_entries, part_entries = [], []
@@ -201,9 +206,6 @@

             identity_position = pose_data[pose_data[""identity""] == identity]
             if identity_position.empty:
                 continue
-
-            # get anchor part - always the first one of all the body parts
-            anchor_part = np.unique(identity_position.part)[0]
 
             for part in set(identity_position.part.values):
                 part_position = identity_position[identity_position.part == part]
@@ -239,12 +241,137 @@

         self.Part.insert(part_entries)
 
 
+# ---------- Blob Position Tracking ------------------
+
+
+@schema
+class BlobPosition(dj.Imported):
+    definition = """"""  # Blob object position tracking from a particular camera, for a particular chunk
+    -> acquisition.Chunk
+    -> streams.SpinnakerVideoSource
+    ---
+    object_count: int  # number of objects tracked in this chunk
+    subject_count: int  # number of subjects present in the arena during this chunk
+    subject_names: varchar(256)  # names of subjects present in arena during this chunk
+    """"""
+
+    class Object(dj.Part):
+        definition = """"""  # Position data of object tracked by a particular camera tracking
+        -> master
+        object_id: int    # id=-1 means ""unknown""; could be the same object as those with other values
+        ---
+        identity_name='': varchar(16)
+        sample_count:  int       # number of data points acquired from this stream for a given chunk
+        x:             longblob  # (px) object's x-position, in the arena's coordinate frame
+        y:             longblob  # (px) object's y-position, in the arena's coordinate frame
+        timestamps:    longblob  # (datetime) timestamps of the position data
+        area=null:     longblob  # (px^2) object's size detected in the camera
+        """"""
+
+    @property
+    def key_source(self):
+        """"""Return the keys to be processed.""""""
+        ks = (
+            acquisition.Chunk
+            * (
+                streams.SpinnakerVideoSource.join(streams.SpinnakerVideoSource.RemovalTime, left=True)
+                & ""spinnaker_video_source_name='CameraTop'""
+            )
+            & ""chunk_start >= spinnaker_video_source_install_time""
+            & 'chunk_start < IFNULL(spinnaker_video_source_removal_time, ""2200-01-01"")'
+        )
+        return ks - SLEAPTracking  # do this only when SLEAPTracking is not available
+
+    def make(self, key):
+        """"""Ingest blob position data for a given chunk.""""""
+        chunk_start, chunk_end = (acquisition.Chunk & key).fetch1(""chunk_start"", ""chunk_end"")
+
+        data_dirs = acquisition.Experiment.get_data_directories(key)
+
+        device_name = (streams.SpinnakerVideoSource & key).fetch1(""spinnaker_video_source_name"")
+
+        devices_schema = getattr(
+            aeon_schemas,
+            (acquisition.Experiment.DevicesSchema & {""experiment_name"": key[""experiment_name""]}).fetch1(
+                ""devices_schema_name""
+            ),
+        )
+
+        stream_reader = devices_schema.CameraTop.Position
+
+        positiondata = io_api.load(
+            root=data_dirs,
+            reader=stream_reader,
+            start=pd.Timestamp(chunk_start),
+            end=pd.Timestamp(chunk_end),
+        )
+
+        if not len(positiondata):
+            raise ValueError(f""No Blob position data found for {key['experiment_name']} - {device_name}"")
+
+        # replace id=NaN with -1
+        positiondata.fillna({""id"": -1}, inplace=True)
+        positiondata[""identity_name""] = """"
+
+        # Find animal(s) in the arena during the chunk
+        # Get all unique subjects that visited the environment over the entire exp;
+        # For each subject, see 'type' of visit most recent to start of block
+        # If ""Exit"", this animal was not in the block.
+        subject_visits_df = fetch_stream(
+            acquisition.Environment.SubjectVisits
+            & {""experiment_name"": key[""experiment_name""]}
+            & f'chunk_start <= ""{chunk_start}""'
+        )[:chunk_end]
+        subject_visits_df = subject_visits_df[subject_visits_df.region == ""Environment""]
+        subject_visits_df = subject_visits_df[~subject_visits_df.id.str.contains(""Test"", case=False)]
+        subject_names = []
+        for subject_name in set(subject_visits_df.id):
+            _df = subject_visits_df[subject_visits_df.id == subject_name]
+            if _df.type.iloc[-1] != ""Exit"":
+                subject_names.append(subject_name)
+
+        if len(subject_names) == 1:
+            # if there is only one known subject, replace all object ids with the subject name
+            positiondata[""id""] = [0] * len(positiondata)
+            positiondata[""identity_name""] = subject_names[0]
+
+        object_positions = []
+        for obj_id in set(positiondata.id.values):
+            obj_position = positiondata[positiondata.id == obj_id]
+
+            object_positions.append(
+                {
+                    **key,
+                    ""object_id"": obj_id,
+                    ""identity_name"": obj_position.identity_name.values[0],
+                    ""sample_count"": len(obj_position.index.values),
+                    ""timestamps"": obj_position.index.values,
+                    ""x"": obj_position.x.values,
+                    ""y"": obj_position.y.values,
+                    ""area"": obj_position.area.values,
+                }
+            )
+
+        self.insert1({**key, ""object_count"": len(object_positions),
+                      ""subject_count"": len(subject_names),
+                      ""subject_names"": "","".join(subject_names)})
+        self.Object.insert(object_positions)
+
+
 # ---------- HELPER ------------------
 
 
 def compute_distance(position_df, target, xcol=""x"", ycol=""y""):
-    """"""Compute the distance of the position data from a target coordinate (X,Y).""""""
-    if len(target) != 2:  # noqa PLR2004
+    """"""Compute the distance between the position and the target.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        target (tuple): Tuple of length 2 indicating the target x and y position.
+        xcol (str): x column name in ``position_df``. Default is 'x'.
+        ycol (str): y column name in ``position_df``. Default is 'y'.
+    """"""
+    COORDS = 2 # x, y
+    if len(target) != COORDS:
         raise ValueError(""Target must be a list of tuple of length 2."")
     return np.sqrt(np.square(position_df[[xcol, ycol]] - target).sum(axis=1))
 
@@ -252,7 +379,14 @@

 def is_position_in_patch(
     position_df, patch_position, wheel_distance_travelled, patch_radius=0.2
 ) -> pd.Series:
-    """"""The function returns a boolean array indicating whether the position is inside the patch.""""""
+    """"""Returns a boolean array of whether a given position is inside the patch and the wheel is moving.
+
+    Args:
+        position_df (pd.DataFrame): DataFrame containing the position data.
+        patch_position (tuple): Tuple of length 2 indicating the patch x and y position.
+        wheel_distance_travelled (pd.Series): distance travelled by the wheel.
+        patch_radius (float): Radius of the patch. Default is 0.2.
+    """"""
     distance_from_patch = compute_distance(position_df, patch_position)
     in_patch = distance_from_patch < patch_radius
     exit_patch = in_patch.astype(np.int8).diff() < 0"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828182759,245.0,249,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/load_metadata.py,,"I suggest changing
 ```
+            """"""
+            Check if this device is currently installed.
+            If the same device serial number is currently installed check for changes in configuration.
+            If not, skip this.
+            """"""
```
 to
```
+            # Check if this device is currently installed.
+            # If the same device serial number is currently installed check for changes in configuration.
+            # If not, skip this.
```","+            """"""
+            Check if this device is currently installed.
+            If the same device serial number is currently installed check for changes in configuration.
+            If not, skip this.
+            """"""","--- 

+++ 

@@ -18,31 +18,30 @@

 logger = dj.logger
 _weight_scale_rate = 100
 _weight_scale_nest = 1
-_aeon_schemas = [""social01"", ""social02""]
 
 
 def insert_stream_types():
     """"""Insert into streams.streamType table all streams in the aeon schemas.""""""
-    from aeon.schema import schemas as aeon_schemas
+    from aeon.schema import ingestion_schemas as aeon_schemas
 
     streams = dj.VirtualModule(""streams"", streams_maker.schema_name)
 
-    schemas = [getattr(aeon_schemas, aeon_schema) for aeon_schema in _aeon_schemas]
-    for schema in schemas:
-        stream_entries = get_stream_entries(schema)
+    for devices_schema_name in aeon_schemas.__all__:
+        devices_schema = getattr(aeon_schemas, devices_schema_name)
+        stream_entries = get_stream_entries(devices_schema)
 
         for entry in stream_entries:
-            q_param = streams.StreamType & {""stream_hash"": entry[""stream_hash""]}
-            if q_param:  # If the specified stream type already exists
-                pname = q_param.fetch1(""stream_type"")
-                if pname == entry[""stream_type""]:
-                    continue
-                else:
-                    # If the existed stream type does not have the same name:
-                    # human error, trying to add the same content with different name
-                    raise dj.DataJointError(f""The specified stream type already exists - name: {pname}"")
-            else:
+            try:
                 streams.StreamType.insert1(entry)
+                logger.info(f""New stream type created: {entry['stream_type']}"")
+            except dj.errors.DuplicateError:
+                existing_stream = (streams.StreamType.proj(
+                    ""stream_reader"", ""stream_reader_kwargs"")
+                                   & {""stream_type"": entry[""stream_type""]}).fetch1()
+                existing_columns = existing_stream[""stream_reader_kwargs""].get(""columns"")
+                entry_columns = entry[""stream_reader_kwargs""].get(""columns"")
+                if existing_columns != entry_columns:
+                    logger.warning(f""Stream type already exists:\n\t{entry}\n\t{existing_stream}"")
 
 
 def insert_device_types(devices_schema: DotMap, metadata_yml_filepath: Path):
@@ -242,11 +241,9 @@

                     }
                 )
 
-            """"""
-            Check if this device is currently installed.
-            If the same device serial number is currently installed check for changes in configuration.
-            If not, skip this.
-            """"""
+            # Check if this device is currently installed.
+            # If the same device serial number is currently installed check for changes in configuration.
+            # If not, skip this.
             current_device_query = table - table.RemovalTime & experiment_key & device_key
 
             if current_device_query:
@@ -306,7 +303,7 @@

     return set(epoch_device_types)
 
 
-# region Get stream & device information
+# Get stream & device information
 def get_stream_entries(devices_schema: DotMap) -> list[dict]:
     """"""Returns a list of dictionaries containing the stream entries for a given device.
 
@@ -379,31 +376,25 @@

 
         if isinstance(device, DotMap):
             for stream_type, stream_obj in device.items():
-                if stream_obj.__class__.__module__ in [
-                    ""aeon.io.reader"",
-                    ""aeon.schema.foraging"",
-                    ""aeon.schema.octagon"",
-                    ""aeon.schema.social"",
-                ]:
-                    device_info[device_name][""stream_type""].append(stream_type)
-                    device_info[device_name][""stream_reader""].append(_get_class_path(stream_obj))
-
-                    required_args = [
-                        k for k in inspect.signature(stream_obj.__init__).parameters if k != ""self""
-                    ]
-                    pattern = schema_dict[device_name][stream_type].get(""pattern"")
-                    schema_dict[device_name][stream_type][""pattern""] = pattern.replace(
-                        device_name, ""{pattern}""
-                    )
-
-                    kwargs = {
-                        k: v for k, v in schema_dict[device_name][stream_type].items() if k in required_args
-                    }
-                    device_info[device_name][""stream_reader_kwargs""].append(kwargs)
-                    # Add hash
-                    device_info[device_name][""stream_hash""].append(
-                        dict_to_uuid({**kwargs, ""stream_reader"": _get_class_path(stream_obj)})
-                    )
+                device_info[device_name][""stream_type""].append(stream_type)
+                device_info[device_name][""stream_reader""].append(_get_class_path(stream_obj))
+
+                required_args = [
+                    k for k in inspect.signature(stream_obj.__init__).parameters if k != ""self""
+                ]
+                pattern = schema_dict[device_name][stream_type].get(""pattern"")
+                schema_dict[device_name][stream_type][""pattern""] = pattern.replace(
+                    device_name, ""{pattern}""
+                )
+
+                kwargs = {
+                    k: v for k, v in schema_dict[device_name][stream_type].items() if k in required_args
+                }
+                device_info[device_name][""stream_reader_kwargs""].append(kwargs)
+                # Add hash
+                device_info[device_name][""stream_hash""].append(
+                    dict_to_uuid({**kwargs, ""stream_reader"": _get_class_path(stream_obj)})
+                )
         else:
             stream_type = device.__class__.__name__
             device_info[device_name][""stream_type""].append(stream_type)
@@ -520,6 +511,3 @@

         experiment_table = getattr(streams, f""Experiment{device_type}"")
         if not (experiment_table & {""experiment_name"": experiment_name, ""device_serial_number"": device_sn}):
             experiment_table.insert1((experiment_name, device_sn, epoch_start, device_name))
-
-
-# endregion"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828194747,,49,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/paths.py,,"Suggested description that is within the allowed line length:
    """"""Finds the parent directory of a given full path among multiple potential root directories.
","     Returns:
         pathlib.Path: The full path to the discovered root directory.
-    """"""
+    """"""  # noqa E501","--- 

+++ 

@@ -34,7 +34,7 @@

 def find_root_directory(
     root_directories: str | pathlib.Path, full_path: str | pathlib.Path
 ) -> pathlib.Path:
-    """"""Given multiple potential root directories and a full-path, search and return one directory that is the parent of the given path.
+    """"""Finds the parent directory of a given full path among multiple potential root directories.
 
     Args:
         root_directories (str | pathlib.Path): A list of potential root directories.
@@ -46,7 +46,7 @@

 
     Returns:
         pathlib.Path: The full path to the discovered root directory.
-    """"""  # noqa E501
+    """"""
     full_path = pathlib.Path(full_path)
 
     if not full_path.exists():"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828208374,,37,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"Suggested docstring:
```python
    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).

    The reward rate differences between the two food patches are plotted
    for all sessions from all subjects in ``subject_keys``.

    Examples:
        >>> subject_keys = (
        ...     acquisition.Experiment.Subject 
        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
        >>> fig = plot_reward_rate_differences(subject_keys)
    """"""
```","     fig = plot_reward_rate_differences(subject_keys)
     ```
-    """"""
+    """"""  # noqa E501","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828210090,104.0,106,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+            distance_travelled_df.subject,
+            distance_travelled_df.in_arena_start,
+            strict=False,
```
 to
```
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
```
Revert black","+            distance_travelled_df.subject,
+            distance_travelled_df.in_arena_start,
+            strict=False,","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828210498,,132,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+    """"""Plotting the average time spent in different regions.""""""
```
 to
```
+    """"""Plots the average time spent in different regions.""""""
```"," 
 
 def plot_average_time_distribution(session_keys):
+    """"""Plotting the average time spent in different regions.""""""","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828226178,,80,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"Update examples:
```python
    """"""Plot wheel-travelled-distance for different patches for all sessions specified in session_keys.

    Examples:
        >>> session_keys = (
        ...     acquisition.Session
        ...     & acquisition.SessionEnd
        ...     & {""experiment_name"": ""exp0.1-r0"", ""subject"": ""BAA-1099794""}
        ... ).fetch(""KEY"")
        >>> fig = plot_wheel_travelled_distance(session_keys)
    """"""
```"," 
 def plot_wheel_travelled_distance(session_keys):
-    """"""Plotting the wheel travelled distance for different patches for all sessions specified in ""session_keys"".
+    """"""Plot wheel-travelled-distance for different patches for all sessions specified in session_keys.","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828231455,223.0,228,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
+        per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(visit_key,
+        attr='wheel_distance_travelled', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(visit_key,
+        attr='total_distance_travelled')
```
 to
```
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
```","+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
+        per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(visit_key,
+        attr='wheel_distance_travelled', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(visit_key,
+        attr='total_distance_travelled')","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828233187,308.0,309,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
+        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
```
 to
```
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
```","+        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
+        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828234991,468.0,474,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+        legend={
+            ""orientation"": ""h"",
+            ""yanchor"": ""bottom"",
+            ""y"": 1,
+            ""xanchor"": ""right"",
+            ""x"": 1,
+        },
```
 to
```
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
```
Revert black","+        legend={
+            ""orientation"": ""h"",
+            ""yanchor"": ""bottom"",
+            ""y"": 1,
+            ""xanchor"": ""right"",
+            ""x"": 1,
+        },","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1828244102,542.0,543,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/plotting.py,,"I suggest changing
 ```
+        attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
+        Defaults is None, which will create a new list with the desired default values inside the function.
```
 to
```
+        attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
```","+        attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
+        Defaults is None, which will create a new list with the desired default values inside the function.","--- 

+++ 

@@ -25,16 +25,17 @@

 
 
 def plot_reward_rate_differences(subject_keys):
-    """"""Plotting the reward rate differences between food patches (Patch 2 - Patch 1) for all sessions from all subjects specified in ""subject_keys"".
+    """"""Plots the reward rate differences between two food patches (Patch 2 - Patch 1).
+
+    The reward rate differences between the two food patches are plotted
+    for all sessions from all subjects in ``subject_keys``.
 
     Examples:
-    ```
-    subject_keys =
-    (acquisition.Experiment.Subject & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
-
-    fig = plot_reward_rate_differences(subject_keys)
-    ```
-    """"""  # noqa E501
+        >>> subject_keys = (
+        ...     acquisition.Experiment.Subject
+        ...     & 'experiment_name = ""exp0.1-r0""').fetch('KEY')
+        >>> fig = plot_reward_rate_differences(subject_keys)
+    """"""
     subj_names, sess_starts, rate_timestamps, rate_diffs = (
         analysis.InArenaRewardRate & subject_keys
     ).fetch(""subject"", ""in_arena_start"", ""pellet_rate_timestamps"", ""patch2_patch1_rate_diff"")
@@ -101,9 +102,7 @@

     distance_travelled_df[""in_arena""] = [
         f'{subj_name}_{sess_start.strftime(""%m/%d/%Y"")}'
         for subj_name, sess_start in zip(
-            distance_travelled_df.subject,
-            distance_travelled_df.in_arena_start,
-            strict=False,
+            distance_travelled_df.subject, distance_travelled_df.in_arena_start, strict=False
         )
     ]
 
@@ -129,7 +128,7 @@

 
 
 def plot_average_time_distribution(session_keys):
-    """"""Plotting the average time spent in different regions.""""""
+    """"""Plots the average time spent in different regions.""""""
     subject_list, arena_location_list, avg_time_spent_list = [], [], []
 
     # Time spent in arena and corridor
@@ -220,12 +219,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count',
-        per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='wheel_distance_travelled', per_food_patch=True)
-        >>> fig = plot_visit_daily_summary(visit_key,
-        attr='total_distance_travelled')
+        >>> fig = plot_visit_daily_summary(visit_key, attr='pellet_count', per_food_patch=True)
+        >>> fig = plot_visit_daily_summary(
+        ...    visit_key,
+        ...    attr=""wheel_distance_travelled""
+        ...    per_food_patch=True,
+        ... )
+        >>> fig = plot_visit_daily_summary(visit_key, attr='total_distance_travelled')
     """"""
     per_food_patch = not attr.startswith(""total"")
     color = ""food_patch_description"" if per_food_patch else None
@@ -305,8 +305,13 @@

         fig: Figure object
 
     Examples:
-        >>> fig = plot_foraging_bouts_count(visit_key, freq=""D"",
-        per_food_patch=True, min_bout_duration=1, min_wheel_dist=1)
+        >>> fig = plot_foraging_bouts_count(
+        ...     visit_key,
+        ...     freq=""D"",
+        ...     per_food_patch=True,
+        ...     min_bout_duration=1,
+        ...     min_wheel_dist=1
+        ... )
     """"""
     # Get all foraging bouts for the visit
     foraging_bouts = (
@@ -465,13 +470,7 @@

         width=700,
         height=400,
         template=""simple_white"",
-        legend={
-            ""orientation"": ""h"",
-            ""yanchor"": ""bottom"",
-            ""y"": 1,
-            ""xanchor"": ""right"",
-            ""x"": 1,
-        },
+        legend={""orientation"": ""h"", ""yanchor"": ""bottom"", ""y"": 1, ""xanchor"": ""right"", ""x"": 1},
     )
 
     return fig
@@ -540,7 +539,8 @@

     Args:
         visit_key (dict): Key from the Visit table
         attrs (list, optional): List of column names (in VisitTimeDistribution tables) to retrieve.
-        Defaults is None, which will create a new list with the desired default values inside the function.
+            If unspecified, defaults to `None` and ``[""in_nest"", ""in_arena"", ""in_corridor"", ""in_patch""]``
+            is used.
 
     Returns:
         region (pd.DataFrame): Timestamped region info"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829153616,,37,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"I suggest changing
 ```
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
```
 to
```
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
```
To get rid of noqa: E501","+    definition = """""" # Catalog of all stream types used across Project Aeon
     stream_type          : varchar(20)
     ---
     stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829154021,,42,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"I suggest changing
 ```
+    """"""  # noqa: E501
```
 to
```
+    """"""
```
If you adopt the above suggestion, we could remove this","     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
     unique index (stream_hash)
-    """"""
+    """"""  # noqa: E501","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829216684,,78,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"I suggest changing
 ```
+        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
```
 to
```
+        definition = f"""""" # {device_title} placement and operation for a particular time period, \
+        at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
```
Could we split the definition over multiple lines, so we could drop noqa: E501? Would something like this work?
","     class ExperimentDevice(dj.Manual):
-        definition = f""""""
-        # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829238593,,145,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"I suggest changing
 ```
+            f""""""Only the combination of Chunk and {device_type} with overlapping time.
```
 to
```
+            """"""Only the combination of Chunk and ``device_type`` with overlapping time.
```
Since this is just docstring we can just use monospace formatting and drop noqa: B021","         def key_source(self):
-            f""""""
-            Only the combination of Chunk and {device_type} with overlapping time
+            f""""""Only the combination of Chunk and {device_type} with overlapping time.","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829239853,,149,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,"I suggest changing
 ```
+            """"""  # noqa B021
```
 to
```
+            """"""
```
and change all `{device_type}` in docstring to use monospace formatting","             +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
             +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""
+            """"""  # noqa B021","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829244808,,283,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/dj_pipeline/utils/streams_maker.py,,We can refactor the repeated `dj.utils.from_camel_case(device_type)` as was done in L150,"+                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                ),
+                'f""{dj.utils.from_camel_case(device_type)}_name""': (
+                    f""'{dj.utils.from_camel_case(device_type)}_name'""","--- 

+++ 

@@ -11,7 +11,8 @@

 import aeon
 from aeon.dj_pipeline import acquisition, get_schema_name
 from aeon.io import api as io_api
-from aeon.schema import schemas as aeon_schemas
+
+aeon_schemas = acquisition.aeon_schemas
 
 logger = dj.logger
 
@@ -25,21 +26,20 @@

 class StreamType(dj.Lookup):
     """"""Catalog of all stream types used across Project Aeon.
 
-    Catalog of all steam types for the different device types used across Project Aeon.
-    One StreamType corresponds to one reader class in `aeon.io.reader`.The
-    combination of `stream_reader` and `stream_reader_kwargs` should fully specify the data
-    loading routine for a particular device, using the `aeon.io.utils`.
+    Catalog of all stream types for the different device types used across Project Aeon.
+    One StreamType corresponds to one Reader class in :mod:`aeon.io.reader`.
+    The combination of ``stream_reader`` and ``stream_reader_kwargs`` should fully specify the data
+    loading routine for a particular device, using :func:`aeon.io.api.load`.
     """"""
 
     definition = """""" # Catalog of all stream types used across Project Aeon
-    stream_type          : varchar(20)
+    stream_type          : varchar(36)
     ---
-    stream_reader        : varchar(256)     # name of the reader class found in `aeon_mecha` package (e.g. aeon.io.reader.Video)
+    stream_reader        : varchar(256) # reader class name in aeon.io.reader (e.g. aeon.io.reader.Video)
     stream_reader_kwargs : longblob  # keyword arguments to instantiate the reader class
     stream_description='': varchar(256)
     stream_hash          : uuid    # hash of dict(stream_reader_kwargs, stream_reader=stream_reader)
-    unique index (stream_hash)
-    """"""  # noqa: E501
+    """"""
 
 
 class DeviceType(dj.Lookup):
@@ -75,21 +75,21 @@

     device_type = dj.utils.from_camel_case(device_type)
 
     class ExperimentDevice(dj.Manual):
-        definition = f"""""" # {device_title} placement and operation for a particular time period, at a certain location, for a given experiment (auto-generated with aeon_mecha-{aeon.__version__})
+        definition = f""""""# {device_title} operation for time,location, experiment (v{aeon.__version__})
         -> acquisition.Experiment
         -> Device
-        {device_type}_install_time  : datetime(6)   # time of the {device_type} placed and started operation at this position
+        {device_type}_install_time : datetime(6)  # {device_type} time of placement and start operation
         ---
-        {device_type}_name          : varchar(36)
-        """"""  # noqa: E501
+        {device_type}_name         : varchar(36)
+        """"""
 
         class Attribute(dj.Part):
-            definition = """"""  # metadata/attributes (e.g. FPS, config, calibration, etc.) associated with this experimental device
+            definition = """"""  # Metadata (e.g. FPS, config, calibration) for this experimental device
             -> master
             attribute_name          : varchar(32)
             ---
             attribute_value=null    : longblob
-            """"""  # noqa: E501
+            """"""
 
         class RemovalTime(dj.Part):
             definition = f""""""
@@ -123,13 +123,14 @@

 
     stream = reader(**stream_detail[""stream_reader_kwargs""])
 
-    table_definition = f"""""" # Raw per-chunk {stream_type} data stream from {device_type} (auto-generated with aeon_mecha-{aeon.__version__})
+    ver = aeon.__version__
+    table_definition = f"""""" # Raw per-chunk {stream_type} from {device_type}(auto-generated with v{ver})
     -> {device_type}
     -> acquisition.Chunk
     ---
     sample_count: int      # number of data points acquired from this stream for a given chunk
     timestamps: longblob   # (datetime) timestamps of {stream_type} data
-    """"""  # noqa: E501
+    """"""
 
     for col in stream.columns:
         if col.startswith(""_""):
@@ -142,11 +143,12 @@

 
         @property
         def key_source(self):
-            f""""""Only the combination of Chunk and {device_type} with overlapping time.
-
-            +  Chunk(s) that started after {device_type} install time and ended before {device_type} remove time
-            +  Chunk(s) that started after {device_type} install time for {device_type} that are not yet removed
-            """"""  # noqa B021
+            docstring = f""""""Only the combination of Chunk and {device_type} with overlapping time.
+
+            + Chunk(s) started after {device_type} install time & ended before {device_type} remove time
+            + Chunk(s) started after {device_type} install time for {device_type} and not yet removed
+            """"""
+            self.__doc__ = docstring
             device_type_name = dj.utils.from_camel_case(device_type)
             return (
                 acquisition.Chunk * ExperimentDevice.join(ExperimentDevice.RemovalTime, left=True)
@@ -211,8 +213,8 @@

                 ""from uuid import UUID\n\n""
                 ""import aeon\n""
                 ""from aeon.dj_pipeline import acquisition, get_schema_name\n""
-                ""from aeon.io import api as io_api\n""
-                ""from aeon.schema import schemas as aeon_schemas\n\n""
+                ""from aeon.io import api as io_api\n\n""
+                ""aeon_schemas = acquisition.aeon_schemas\n\n""
                 'schema = dj.Schema(get_schema_name(""streams""))\n\n\n'
             )
             f.write(imports_str)
@@ -270,17 +272,18 @@

             device_stream_table_def = inspect.getsource(table_class).lstrip()
 
             # Replace the definition
+            device_type_name = dj.utils.from_camel_case(device_type)
             replacements = {
                 ""DeviceDataStream"": f""{device_type}{stream_type}"",
                 ""ExperimentDevice"": device_type,
-                'f""chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time""': (
-                    f""'chunk_start >= {dj.utils.from_camel_case(device_type)}_install_time'""
+                'f""chunk_start >= {device_type_name}_install_time""': (
+                    f""'chunk_start >= {device_type_name}_install_time'""
                 ),
-                """"""f'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time, ""2200-01-01"")'"""""": (  # noqa E501
-                    f""""""'chunk_start < IFNULL({dj.utils.from_camel_case(device_type)}_removal_time,""2200-01-01"")'""""""  # noqa E501
+                """"""f'chunk_start < IFNULL({device_type_name}_removal_time, ""2200-01-01"")'"""""": (
+                    f""""""'chunk_start < IFNULL({device_type_name}_removal_time,""2200-01-01"")'""""""
                 ),
-                'f""{dj.utils.from_camel_case(device_type)}_name""': (
-                    f""'{dj.utils.from_camel_case(device_type)}_name'""
+                'f""{device_type_name}_name""': (
+                    f""'{device_type_name}_name'""
                 ),
                 ""{device_type}"": device_type,
                 ""{stream_type}"": stream_type,"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829250184,152.0,153,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/api.py,,"I suggest changing
 ```
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
+                    stacklevel=2,
```
 to
```
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"", stacklevel=2
```
Revert black","+                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
+                    stacklevel=2,","--- 

+++ 

@@ -75,7 +75,7 @@

     :param datetime, optional end: The right bound of the time range to extract.
     :param datetime, optional time: An object or series specifying the timestamps to extract.
     :param datetime, optional tolerance:
-    The maximum distance between original and new timestamps for inexact matches.
+        The maximum distance between original and new timestamps for inexact matches.
     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
     :param optional kwargs: Optional keyword arguments to forward to the reader when reading chunk data.
     :return: A pandas data frame containing epoch event metadata, sorted by time.
@@ -149,15 +149,11 @@

 
             if not data.index.has_duplicates:
                 warnings.warn(
-                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
-                    stacklevel=2,
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"", stacklevel=2
                 )
                 data = data.sort_index()
             else:
-                warnings.warn(
-                    f""data index for {reader.pattern} contains duplicate keys!"",
-                    stacklevel=2,
-                )
+                warnings.warn(f""data index for {reader.pattern} contains duplicate keys!"", stacklevel=2)
                 data = data[~data.index.duplicated(keep=""first"")]
             return data.loc[start:end]
     return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829251188,157.0,160,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/api.py,,"I suggest changing
 ```
+                warnings.warn(
+                    f""data index for {reader.pattern} contains duplicate keys!"",
+                    stacklevel=2,
+                )
```
 to
```
+                warnings.warn(f""data index for {reader.pattern} contains duplicate keys!"", stacklevel=2)
```
Revert black","+                warnings.warn(
+                    f""data index for {reader.pattern} contains duplicate keys!"",
+                    stacklevel=2,
+                )","--- 

+++ 

@@ -75,7 +75,7 @@

     :param datetime, optional end: The right bound of the time range to extract.
     :param datetime, optional time: An object or series specifying the timestamps to extract.
     :param datetime, optional tolerance:
-    The maximum distance between original and new timestamps for inexact matches.
+        The maximum distance between original and new timestamps for inexact matches.
     :param str, optional epoch: A wildcard pattern to use when searching epoch data.
     :param optional kwargs: Optional keyword arguments to forward to the reader when reading chunk data.
     :return: A pandas data frame containing epoch event metadata, sorted by time.
@@ -149,15 +149,11 @@

 
             if not data.index.has_duplicates:
                 warnings.warn(
-                    f""data index for {reader.pattern} contains out-of-order timestamps!"",
-                    stacklevel=2,
+                    f""data index for {reader.pattern} contains out-of-order timestamps!"", stacklevel=2
                 )
                 data = data.sort_index()
             else:
-                warnings.warn(
-                    f""data index for {reader.pattern} contains duplicate keys!"",
-                    stacklevel=2,
-                )
+                warnings.warn(f""data index for {reader.pattern} contains duplicate keys!"", stacklevel=2)
                 data = data[~data.index.duplicated(keep=""first"")]
             return data.loc[start:end]
     return data"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829256566,77.0,81,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,"I suggest changing
 ```
+            payloadshape,
+            dtype=payloadtype,
+            buffer=data,
+            offset=11,
+            strides=(stride, elementsize),
```
 to
```
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
```
Revert black","+            payloadshape,
+            dtype=payloadtype,
+            buffer=data,
+            offset=11,
+            strides=(stride, elementsize),","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -74,19 +73,18 @@

         ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
             data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -171,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -186,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -201,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -213,46 +214,30 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
@@ -264,7 +249,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -288,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -310,8 +297,9 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
@@ -333,27 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -388,7 +396,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
@@ -407,13 +415,7 @@

                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -452,18 +454,12 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829277659,,57,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/foraging.py,,"I suggest changing
 ```
+        """"""Initializes  the Weight class.""""""
```
 to
```
+        """"""Initializes the Weight class.""""""
```","     """"""
 
     def __init__(self, pattern):
+        """"""Initializes  the Weight class.""""""","--- 

+++ 

@@ -54,7 +54,7 @@

     """"""
 
     def __init__(self, pattern):
-        """"""Initializes  the Weight class.""""""
+        """"""Initializes the Weight class.""""""
         super().__init__(pattern, columns=[""value"", ""stable""])
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829317530,,166,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,"Or
I suggest changing
 ```
+            df[column] = df[self.root_key].apply(lambda x: x[column])  # noqa B023
```
 to
```
+            df[column] = df[self.root_key].apply(lambda x, col=column: x[col])
```","         df.set_index(""seconds"", inplace=True)
         for column in self.columns:
-            df[column] = df[self.root_key].apply(lambda x: x[column])
+            df[column] = df[self.root_key].apply(lambda x: x[column])  # noqa B023","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -74,19 +73,18 @@

         ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
             data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -171,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -186,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -201,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -213,46 +214,30 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
@@ -264,7 +249,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -288,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -310,8 +297,9 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
@@ -333,27 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -388,7 +396,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
@@ -407,13 +415,7 @@

                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -452,18 +454,12 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829318550,405.0,406,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,"I suggest changing
 ```
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
+                    axis=1,
```
 to
```
+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities}, axis=1
```
Revert black","+                    lambda row: {identity: row[f""{identity}_likelihood""] for identity in identities},
+                    axis=1,","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -74,19 +73,18 @@

         ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
             data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -171,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -186,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -201,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -213,46 +214,30 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
@@ -264,7 +249,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -288,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -310,8 +297,9 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
@@ -333,27 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -388,7 +396,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
@@ -407,13 +415,7 @@

                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -452,18 +454,12 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829319469,410.0,416,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/io/reader.py,,"I suggest changing
 ```
+                    [
+                        ""identity"",
+                        ""identity_likelihood"",
+                        f""{part}_x"",
+                        f""{part}_y"",
+                        f""{part}_likelihood"",
+                    ]
```
 to
```
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
```
Revert black","+                    [
+                        ""identity"",
+                        ""identity_likelihood"",
+                        f""{part}_x"",
+                        f""{part}_y"",
+                        f""{part}_likelihood"",
+                    ]","--- 

+++ 

@@ -13,8 +13,7 @@

 from dotmap import DotMap
 
 from aeon import util
-from aeon.io.api import aeon as aeon_time
-from aeon.io.api import chunk, chunk_key
+from aeon.io.api import chunk_key
 
 _SECONDS_PER_TICK = 32e-6
 _payloadtypes = {
@@ -74,19 +73,18 @@

         ticks = np.ndarray(length, dtype=np.uint16, buffer=data, offset=9, strides=stride)
         seconds = ticks * _SECONDS_PER_TICK + seconds
         payload = np.ndarray(
-            payloadshape,
-            dtype=payloadtype,
-            buffer=data,
-            offset=11,
-            strides=(stride, elementsize),
+            payloadshape, dtype=payloadtype, buffer=data, offset=11, strides=(stride, elementsize)
         )
 
         if self.columns is not None and payloadshape[1] < len(self.columns):
             data = pd.DataFrame(payload, index=seconds, columns=self.columns[: payloadshape[1]])
             data[self.columns[payloadshape[1] :]] = math.nan
-            return data
         else:
-            return pd.DataFrame(payload, index=seconds, columns=self.columns)
+            data = pd.DataFrame(payload, index=seconds, columns=self.columns)
+
+        # remove rows where the index is zero (why? corrupted data in harp files?)
+        data = data[data.index != 0]
+        return data
 
 
 class Chunk(Reader):
@@ -171,10 +169,11 @@

     """"""Extracts metadata for subjects entering and exiting the environment.
 
     Columns:
-        id (str): Unique identifier of a subject in the environment.
-        weight (float): Weight measurement of the subject on entering
-            or exiting the environment.
-        event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
+
+    - id (str): Unique identifier of a subject in the environment.
+    - weight (float): Weight measurement of the subject on entering
+      or exiting the environment.
+    - event (str): Event type. Can be one of `Enter`, `Exit` or `Remain`.
     """"""
 
     def __init__(self, pattern):
@@ -186,10 +185,11 @@

     """"""Extracts message log data.
 
     Columns:
-        priority (str): Priority level of the message.
-        type (str): Type of the log message.
-        message (str): Log message data. Can be structured using tab
-            separated values.
+
+    - priority (str): Priority level of the message.
+    - type (str): Type of the log message.
+    - message (str): Log message data. Can be structured using tab
+      separated values.
     """"""
 
     def __init__(self, pattern):
@@ -201,7 +201,8 @@

     """"""Extract periodic heartbeat event data.
 
     Columns:
-        second (int): The whole second corresponding to the heartbeat, in seconds.
+
+    - second (int): The whole second corresponding to the heartbeat, in seconds.
     """"""
 
     def __init__(self, pattern):
@@ -213,46 +214,30 @@

     """"""Extract magnetic encoder data.
 
     Columns:
-        angle (float): Absolute angular position, in radians, of the magnetic encoder.
-        intensity (float): Intensity of the magnetic field.
+
+    - angle (float): Absolute angular position, in radians, of the magnetic encoder.
+    - intensity (float): Intensity of the magnetic field.
     """"""
 
     def __init__(self, pattern):
         """"""Initialize the object with a specified pattern and columns.""""""
         super().__init__(pattern, columns=[""angle"", ""intensity""])
 
-    def read(self, file, downsample=True):
-        """"""Reads encoder data from the specified Harp binary file.
-
-        By default the encoder data is downsampled to 50Hz. Setting downsample to
-        False or None can be used to force the raw data to be returned.
-        """"""
-        data = super().read(file)
-        if downsample is True:
-            # resample requires a DatetimeIndex so we convert early
-            data.index = aeon_time(data.index)
-
-            first_index = data.first_valid_index()
-            if first_index is not None:
-                # since data is absolute angular position we decimate by taking first of each bin
-                chunk_origin = chunk(first_index)
-                data = data.resample(""20ms"", origin=chunk_origin).first()
-        return data
-
 
 class Position(Harp):
     """"""Extract 2D position tracking data for a specific camera.
 
     Columns:
-        x (float): x-coordinate of the object center of mass.
-        y (float): y-coordinate of the object center of mass.
-        angle (float): angle, in radians, of the ellipse fit to the object.
-        major (float): length, in pixels, of the major axis of the ellipse
-            fit to the object.
-        minor (float): length, in pixels, of the minor axis of the ellipse
-            fit to the object.
-        area (float): number of pixels in the object mass.
-        id (float): unique tracking ID of the object in a frame.
+
+    - x (float): x-coordinate of the object center of mass.
+    - y (float): y-coordinate of the object center of mass.
+    - angle (float): angle, in radians, of the ellipse fit to the object.
+    - major (float): length, in pixels, of the major axis of the ellipse
+      fit to the object.
+    - minor (float): length, in pixels, of the minor axis of the ellipse
+      fit to the object.
+    - area (float): number of pixels in the object mass.
+    - id (float): unique tracking ID of the object in a frame.
     """"""
 
     def __init__(self, pattern):
@@ -264,7 +249,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, value, tag):
@@ -288,7 +274,8 @@

     """"""Extracts event data matching a specific digital I/O bitmask.
 
     Columns:
-        event (str): Unique identifier for the event code.
+
+    - event (str): Unique identifier for the event code.
     """"""
 
     def __init__(self, pattern, mask, columns):
@@ -310,8 +297,9 @@

     """"""Extracts video frame metadata.
 
     Columns:
-        hw_counter (int): Hardware frame counter value for the current frame.
-        hw_timestamp (int): Internal camera timestamp for the current frame.
+
+    - hw_counter (int): Hardware frame counter value for the current frame.
+    - hw_timestamp (int): Internal camera timestamp for the current frame.
     """"""
 
     def __init__(self, pattern):
@@ -333,27 +321,47 @@

     """"""Reader for Harp-binarized tracking data given a model that outputs id, parts, and likelihoods.
 
     Columns:
-        class (int): Int ID of a subject in the environment.
-        class_likelihood (float): Likelihood of the subject's identity.
-        part (str): Bodypart on the subject.
-        part_likelihood (float): Likelihood of the specified bodypart.
-        x (float): X-coordinate of the bodypart.
-        y (float): Y-coordinate of the bodypart.
+
+    - class (int): Int ID of a subject in the environment.
+    - class_likelihood (float): Likelihood of the subject's identity.
+    - part (str): Bodypart on the subject.
+    - part_likelihood (float): Likelihood of the specified bodypart.
+    - x (float): X-coordinate of the bodypart.
+    - y (float): Y-coordinate of the bodypart.
     """"""
 
     def __init__(self, pattern: str, model_root: str = ""/ceph/aeon/aeon/data/processed""):
-        """"""Pose reader constructor.""""""
-        # `pattern` for this reader should typically be '<hpcnode>_<jobid>*'
+        """"""Pose reader constructor.
+
+        The pattern for this reader should typically be `<device>_<hpcnode>_<jobid>*`.
+        If a register prefix is required, the pattern should end with a trailing
+        underscore, e.g. `Camera_202_*`. Otherwise, the pattern should include a
+        common prefix for the pose model folder excluding the trailing underscore,
+        e.g. `Camera_model-dir*`.
+        """"""
         super().__init__(pattern, columns=None)
         self._model_root = model_root
+        self._pattern_offset = pattern.rfind(""_"") + 1
 
     def read(self, file: Path) -> pd.DataFrame:
         """"""Reads data from the Harp-binarized tracking file.""""""
         # Get config file from `file`, then bodyparts from config file.
-        model_dir = Path(*Path(file.stem.replace(""_"", ""/"")).parent.parts[-4:])
-        config_file_dir = Path(self._model_root) / model_dir
-        if not config_file_dir.exists():
-            raise FileNotFoundError(f""Cannot find model dir {config_file_dir}"")
+        model_dir = Path(file.stem[self._pattern_offset :].replace(""_"", ""/"")).parent
+
+        # Check if model directory exists in local or shared directories.
+        # Local directory is prioritized over shared directory.
+        local_config_file_dir = file.parent / model_dir
+        shared_config_file_dir = Path(self._model_root) / model_dir
+        if local_config_file_dir.exists():
+            config_file_dir = local_config_file_dir
+        elif shared_config_file_dir.exists():
+            config_file_dir = shared_config_file_dir
+        else:
+            raise FileNotFoundError(
+                f""""""Cannot find model dir in either local ({local_config_file_dir}) \
+                    or shared ({shared_config_file_dir}) directories""""""
+            )
+
         config_file = self.get_config_file(config_file_dir)
         identities = self.get_class_names(config_file)
         parts = self.get_bodyparts(config_file)
@@ -388,7 +396,7 @@

             parts = unique_parts
 
         # Set new columns, and reformat `data`.
-        data = self.class_int2str(data, config_file)
+        data = self.class_int2str(data, identities)
         n_parts = len(parts)
         part_data_list = [pd.DataFrame()] * n_parts
         new_columns = pd.Series([""identity"", ""identity_likelihood"", ""part"", ""x"", ""y"", ""part_likelihood""])
@@ -407,13 +415,7 @@

                 )
                 part_data.drop(columns=columns[1 : (len(identities) + 1)], inplace=True)
                 part_data = part_data[  # reorder columns
-                    [
-                        ""identity"",
-                        ""identity_likelihood"",
-                        f""{part}_x"",
-                        f""{part}_y"",
-                        f""{part}_likelihood"",
-                    ]
+                    [""identity"", ""identity_likelihood"", f""{part}_x"", f""{part}_y"", f""{part}_likelihood""]
                 ]
             part_data.insert(2, ""part"", part)
             part_data.columns = new_columns
@@ -452,18 +454,12 @@

         return parts
 
     @staticmethod
-    def class_int2str(data: pd.DataFrame, config_file: Path) -> pd.DataFrame:
+    def class_int2str(data: pd.DataFrame, classes: list[str]) -> pd.DataFrame:
         """"""Converts a class integer in a tracking data dataframe to its associated string (subject id).""""""
-        if config_file.stem == ""confmap_config"":  # SLEAP
-            with open(config_file) as f:
-                config = json.load(f)
-            try:
-                heads = config[""model""][""heads""]
-                classes = util.find_nested_key(heads, ""classes"")
-            except KeyError as err:
-                raise KeyError(f""Cannot find classes in {config_file}."") from err
-            for i, subj in enumerate(classes):
-                data.loc[data[""identity""] == i, ""identity""] = subj
+        if not classes:
+            raise ValueError(""Classes list cannot be None or empty."")
+        identity_mapping = dict(enumerate(classes))
+        data[""identity""] = data[""identity""].replace(identity_mapping)
         return data
 
     @classmethod"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829467328,22.0,25,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/octagon.py,,"I suggest changing
 ```
+                _reader.Csv(
+                    f""{pattern}_backgroundcolor_*"",
+                    columns=[""typetag"", ""r"", ""g"", ""b"", ""a""],
+                )
```
 to
```
+                _reader.Csv(f""{pattern}_backgroundcolor_*"", columns=[""typetag"", ""r"", ""g"", ""b"", ""a""])
```
Revert black","+                _reader.Csv(
+                    f""{pattern}_backgroundcolor_*"",
+                    columns=[""typetag"", ""r"", ""g"", ""b"", ""a""],
+                )","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""Octagon schema definition.""""""
+""""""Schema definition for octagon experiments-specific data streams.""""""
 
 import aeon.io.reader as _reader
 from aeon.schema.streams import Stream, StreamGroup
@@ -19,10 +19,7 @@

         def __init__(self, pattern):
             """"""Initializes the BackgroundColor stream.""""""
             super().__init__(
-                _reader.Csv(
-                    f""{pattern}_backgroundcolor_*"",
-                    columns=[""typetag"", ""r"", ""g"", ""b"", ""a""],
-                )
+                _reader.Csv(f""{pattern}_backgroundcolor_*"", columns=[""typetag"", ""r"", ""g"", ""b"", ""a""])
             )
 
     class ChangeSubjectState(Stream):
@@ -92,8 +89,7 @@

             """"""Initialises the Response class.""""""
             super().__init__(
                 _reader.Csv(
-                    f""{pattern}_response_*"",
-                    columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""],
+                    f""{pattern}_response_*"", columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""]
                 )
             )
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829478079,95.0,96,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/octagon.py,,"I suggest changing
 ```
+                    f""{pattern}_response_*"",
+                    columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""],
```
 to
```
+                    f""{pattern}_response_*"", columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""]
```
Revert black","+                    f""{pattern}_response_*"",
+                    columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""],","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""Octagon schema definition.""""""
+""""""Schema definition for octagon experiments-specific data streams.""""""
 
 import aeon.io.reader as _reader
 from aeon.schema.streams import Stream, StreamGroup
@@ -19,10 +19,7 @@

         def __init__(self, pattern):
             """"""Initializes the BackgroundColor stream.""""""
             super().__init__(
-                _reader.Csv(
-                    f""{pattern}_backgroundcolor_*"",
-                    columns=[""typetag"", ""r"", ""g"", ""b"", ""a""],
-                )
+                _reader.Csv(f""{pattern}_backgroundcolor_*"", columns=[""typetag"", ""r"", ""g"", ""b"", ""a""])
             )
 
     class ChangeSubjectState(Stream):
@@ -92,8 +89,7 @@

             """"""Initialises the Response class.""""""
             super().__init__(
                 _reader.Csv(
-                    f""{pattern}_response_*"",
-                    columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""],
+                    f""{pattern}_response_*"", columns=[""typetag"", ""wall_id"", ""poke_id"", ""response_time""]
                 )
             )
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829487358,189.0,197,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/schemas.py,,"I suggest changing
 ```
+__all__ = [
+    ""exp01"",
+    ""exp02"",
+    ""octagon01"",
+    ""social01"",
+    ""social02"",
+    ""social03"",
+    ""social04"",
+]
```
 to
```
+__all__ = [""exp01"", ""exp02"", ""octagon01"", ""social01"", ""social02"", ""social03"", ""social04""]
```
Revert black","+__all__ = [
+    ""exp01"",
+    ""exp02"",
+    ""octagon01"",
+    ""social01"",
+    ""social02"",
+    ""social03"",
+    ""social04"",
+]","--- 

+++ 

@@ -186,12 +186,4 @@

 )
 
 
-__all__ = [
-    ""exp01"",
-    ""exp02"",
-    ""octagon01"",
-    ""social01"",
-    ""social02"",
-    ""social03"",
-    ""social04"",
-]
+__all__ = [""exp01"", ""exp02"", ""octagon01"", ""social01"", ""social02"", ""social03"", ""social04""]"
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829489211,19.0,22,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,aeon/schema/social_02.py,,"I suggest changing
 ```
+                _reader.Csv(
+                    f""{path}_BlockState_*"",
+                    columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""],
+                )
```
 to
```
+                _reader.Csv(f""{path}_BlockState_*"", columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""])
```
Revert black","+                _reader.Csv(
+                    f""{path}_BlockState_*"",
+                    columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""],
+                )","--- 

+++ 

@@ -1,4 +1,4 @@

-""""""This module defines the schema for the social_02 dataset.""""""
+""""""Schema definition for social_02 experiments-specific data streams.""""""
 
 import aeon.io.reader as _reader
 from aeon.schema import core, foraging
@@ -16,10 +16,7 @@

         def __init__(self, path):
             """"""Initializes the BlockState stream.""""""
             super().__init__(
-                _reader.Csv(
-                    f""{path}_BlockState_*"",
-                    columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""],
-                )
+                _reader.Csv(f""{path}_BlockState_*"", columns=[""pellet_ct"", ""pellet_ct_thresh"", ""due_time""])
             )
 
     class LightEvents(Stream):
@@ -62,9 +59,15 @@

         super().__init__(_reader.Pose(f""{path}_test-node1*""))
 
 
+class Pose03(Stream):
+    def __init__(self, path):
+        """"""Initializes the Pose stream.""""""
+        super().__init__(_reader.Pose(f""{path}_202_*""))
+
+
 class WeightRaw(Stream):
     def __init__(self, path):
-        """"""Initialize the WeightRaw stream.""""""
+        """"""Initializes the WeightRaw stream.""""""
         super().__init__(_reader.Harp(f""{path}_200_*"", [""weight(g)"", ""stability""]))
 
 "
https://api.github.com/repos/SainsburyWellcomeCentre/aeon_mecha/pulls/comments/1829515344,,105,48493cbc0de8b6e2a5e20bcebc0f30f3f7e6719b,9be1f8eafe59d9db06cd9d4b11d7cace799d8343,pyproject.toml,,Remove this as project requires python >= 3.11 we can use the `datetime.UTC` alias,"+  ""PLR0912"", 
   ""PLR0913"",
   ""PLR0915"",
+  ""UP017""  # skip `datetime.UTC` alias","--- 

+++ 

@@ -102,7 +102,6 @@

   ""PLR0912"", 
   ""PLR0913"",
   ""PLR0915"",
-  ""UP017""  # skip `datetime.UTC` alias
 ]
 extend-exclude = [
   "".git"","
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804413477,16.0,18,fb9cb401dac354221e97fa956328e7d7faf74e2d,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/backend/utils.py,,"I suggest changing
 ```
+            ""https://api.platerecognizer.com/v1/{}/{}/"".format(
+                path, license_key.strip()
+            )
```
 to
```
+            f""https://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
```","+            ""https://api.platerecognizer.com/v1/{}/{}/"".format(
+                path, license_key.strip()
+            )","--- 

+++ 

@@ -1,9 +1,12 @@

+import logging
+
 try:
     from urllib.error import URLError
     from urllib.request import Request, urlopen
 except ImportError:
     from urllib2 import Request, URLError, urlopen  # type: ignore
-from ssl import SSLError
+
+lgr = logging.getLogger(__name__)
 
 
 def verify_token(token, license_key, is_stream=True):
@@ -13,22 +16,11 @@

     path = ""stream/license"" if is_stream else ""sdk-webhooks""
     try:
         req = Request(
-            ""https://api.platerecognizer.com/v1/{}/{}/"".format(
-                path, license_key.strip()
-            )
+            f""https://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
         )
         req.add_header(""Authorization"", f""Token {token.strip()}"")
         urlopen(req).read()
         return True, None
-
-    except SSLError:
-        req = Request(
-            f""http://api.platerecognizer.com/v1/{path}/{license_key.strip()}/""
-        )
-        req.add_header(""Authorization"", f""Token {token.strip()}"")
-        urlopen(req).read()
-        return True, None
-
     except URLError as e:
         if ""404"" in str(e):
             return (
@@ -40,3 +32,6 @@

 
         else:
             return True, None
+    except Exception as e:
+        lgr.exception(e)
+        return False, str(e)"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804554056,,92,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Stream.tsx,,"I suggest changing
 ```
+            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
```
 to
```
+            href=""https://app.platerecognizer.com/service/stream/""
```","+        <Form.Label column sm={4}>
+          Please enter your Plate Recognizer{"" ""}
+          <a
+            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""","--- 

+++ 

@@ -1,12 +1,7 @@

 import {
-  Container,
   Row,
   Col,
-  Card,
   Button,
-  Alert,
-  Navbar,
-  Nav,
 } from ""react-bootstrap"";
 import React, { useState } from ""react"";
 
@@ -26,15 +21,16 @@

   const [license, setLicense] = useState<string>("""");
   const [tokenValidated, setTokenValidated] = useState(false);
   const [isLoading, setLoading] = useState(false);
+  const [restartPolicy, setRestartPolicy] = useState('no');
 
   const ddClient = useDockerDesktopClient();
 
-  const handleLinkClick = (e: any) => {
+  const handleLinkClick = (e: React.MouseEvent<HTMLAnchorElement>) => {
     e.preventDefault();
-    openBrowserUrl(ddClient, e.target.href);
-  };
-
-  const handleConfigureClick = (e: any) => {
+    openBrowserUrl(ddClient, e.currentTarget.href);
+  };
+
+  const handleConfigureClick = () => {
     if (license) {
       const url = ""https://app.platerecognizer.com/stream-config/"" + license;
       openBrowserUrl(ddClient, url);
@@ -42,17 +38,19 @@

       ddClient.desktopUI.toast.error(""License Key is required"");
     }
   };
-  const handleInputChange = (e: any) => {
+  const handleInputChange = (e: React.ChangeEvent<HTMLInputElement>) => {
     const { name, value } = e.target;
     if (name == ""license"") {
       setLicense(value);
+    } else if (name == ""restart-policy"") {
+      setRestartPolicy(value);
     }
     setTokenValidated(false);
   };
 
-  const handleSubmit = async (event: React.SyntheticEvent) => {
+  const handleSubmit = async (event: React.FormEvent<HTMLFormElement> ) => {
     event.preventDefault();
-    const form: any = event.target;
+    const form: HTMLFormElement = event.currentTarget;
     const formData = new FormData(form);
 
     const data: any = Object.fromEntries(formData.entries());
@@ -67,8 +65,9 @@

         if (valid) {
           // Pull image and update
           ddClient.docker.cli.exec(""pull"", [STREAM_IMAGE]).then((result) => {
-            const autoBoot = data.startOnBoot
-              ? "" --restart unless-stopped""
+            console.debug(result)
+            const autoBoot = restartPolicy != 'no'
+              ? "" --restart "" + restartPolicy
               : ""--rm"";
             const command = `docker run ${autoBoot} -t -v ${data.streamPath}:/user-data/ -e LICENSE_KEY=${data.license} -e TOKEN=${data.token} ${STREAM_IMAGE}`;
             setCommand(command);
@@ -89,7 +88,7 @@

         <Form.Label column sm={4}>
           Please enter your Plate Recognizer{"" ""}
           <a
-            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
+            href=""https://app.platerecognizer.com/service/stream/""
             onClick={handleLinkClick}
           >
             API Token
@@ -111,7 +110,7 @@

         <Form.Label column sm={4}>
           Please enter your{"" ""}
           <a
-            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
+            href=""https://app.platerecognizer.com/service/stream/""
             onClick={handleLinkClick}
           >
             Stream License Key
@@ -144,17 +143,46 @@

         </Col>
       </Form.Group>
 
-      <Form.Group as={Row} className=""mb-3"" controlId=""streamRestartPolicy"">
-        <Form.Label column sm={4}>
-          Do you want Stream to automatically start on system startup?
-        </Form.Label>
-        <Col sm={8} className=""mt-2"">
-          <Form.Check
-            type=""switch""
-            name=""startOnBoot""
-            onChange={handleInputChange}
-          />
-        </Col>
+      <Form.Group as={Row} className=""mb-3"">
+        <Form.Label column sm={4}>
+          Restart policy
+        </Form.Label>
+        <Col sm={8} className=""mt-2 d-flex justify-content-between"">
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='No (Docker Default)'
+            id='rp1'
+            value='no'
+            checked={restartPolicy == 'no'}
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Unless Stopped'
+            id='rp2'
+            value='unless-stopped'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Always'
+            id='rp3'
+            value='always'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='On Failure'
+            id='rp4'
+            value='on-failure'
+            onChange={handleInputChange}
+          />
+        </Col>
+
       </Form.Group>
 
       <ShowCommand curlPort={""""} command={command} validated={tokenValidated} />"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804555158,,114,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Stream.tsx,,"
I suggest changing
 ```
+            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
```
 to
```
+            href=""https://app.platerecognizer.com/service/stream/""
```","+        <Form.Label column sm={4}>
+          Please enter your{"" ""}
+          <a
+            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""","--- 

+++ 

@@ -1,12 +1,7 @@

 import {
-  Container,
   Row,
   Col,
-  Card,
   Button,
-  Alert,
-  Navbar,
-  Nav,
 } from ""react-bootstrap"";
 import React, { useState } from ""react"";
 
@@ -26,15 +21,16 @@

   const [license, setLicense] = useState<string>("""");
   const [tokenValidated, setTokenValidated] = useState(false);
   const [isLoading, setLoading] = useState(false);
+  const [restartPolicy, setRestartPolicy] = useState('no');
 
   const ddClient = useDockerDesktopClient();
 
-  const handleLinkClick = (e: any) => {
+  const handleLinkClick = (e: React.MouseEvent<HTMLAnchorElement>) => {
     e.preventDefault();
-    openBrowserUrl(ddClient, e.target.href);
-  };
-
-  const handleConfigureClick = (e: any) => {
+    openBrowserUrl(ddClient, e.currentTarget.href);
+  };
+
+  const handleConfigureClick = () => {
     if (license) {
       const url = ""https://app.platerecognizer.com/stream-config/"" + license;
       openBrowserUrl(ddClient, url);
@@ -42,17 +38,19 @@

       ddClient.desktopUI.toast.error(""License Key is required"");
     }
   };
-  const handleInputChange = (e: any) => {
+  const handleInputChange = (e: React.ChangeEvent<HTMLInputElement>) => {
     const { name, value } = e.target;
     if (name == ""license"") {
       setLicense(value);
+    } else if (name == ""restart-policy"") {
+      setRestartPolicy(value);
     }
     setTokenValidated(false);
   };
 
-  const handleSubmit = async (event: React.SyntheticEvent) => {
+  const handleSubmit = async (event: React.FormEvent<HTMLFormElement> ) => {
     event.preventDefault();
-    const form: any = event.target;
+    const form: HTMLFormElement = event.currentTarget;
     const formData = new FormData(form);
 
     const data: any = Object.fromEntries(formData.entries());
@@ -67,8 +65,9 @@

         if (valid) {
           // Pull image and update
           ddClient.docker.cli.exec(""pull"", [STREAM_IMAGE]).then((result) => {
-            const autoBoot = data.startOnBoot
-              ? "" --restart unless-stopped""
+            console.debug(result)
+            const autoBoot = restartPolicy != 'no'
+              ? "" --restart "" + restartPolicy
               : ""--rm"";
             const command = `docker run ${autoBoot} -t -v ${data.streamPath}:/user-data/ -e LICENSE_KEY=${data.license} -e TOKEN=${data.token} ${STREAM_IMAGE}`;
             setCommand(command);
@@ -89,7 +88,7 @@

         <Form.Label column sm={4}>
           Please enter your Plate Recognizer{"" ""}
           <a
-            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
+            href=""https://app.platerecognizer.com/service/stream/""
             onClick={handleLinkClick}
           >
             API Token
@@ -111,7 +110,7 @@

         <Form.Label column sm={4}>
           Please enter your{"" ""}
           <a
-            href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app""
+            href=""https://app.platerecognizer.com/service/stream/""
             onClick={handleLinkClick}
           >
             Stream License Key
@@ -144,17 +143,46 @@

         </Col>
       </Form.Group>
 
-      <Form.Group as={Row} className=""mb-3"" controlId=""streamRestartPolicy"">
-        <Form.Label column sm={4}>
-          Do you want Stream to automatically start on system startup?
-        </Form.Label>
-        <Col sm={8} className=""mt-2"">
-          <Form.Check
-            type=""switch""
-            name=""startOnBoot""
-            onChange={handleInputChange}
-          />
-        </Col>
+      <Form.Group as={Row} className=""mb-3"">
+        <Form.Label column sm={4}>
+          Restart policy
+        </Form.Label>
+        <Col sm={8} className=""mt-2 d-flex justify-content-between"">
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='No (Docker Default)'
+            id='rp1'
+            value='no'
+            checked={restartPolicy == 'no'}
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Unless Stopped'
+            id='rp2'
+            value='unless-stopped'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Always'
+            id='rp3'
+            value='always'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='On Failure'
+            id='rp4'
+            value='on-failure'
+            onChange={handleInputChange}
+          />
+        </Col>
+
       </Form.Group>
 
       <ShowCommand curlPort={""""} command={command} validated={tokenValidated} />"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804565898,,1,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/logo.svg:Zone.Identifier,,"You can remove this file, it's windows metadata dumped when copying from NTFS to an other filesystem.","@@ -0,0 +1,4 @@
+[ZoneTransfer]",File_Deleted
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804572792,,138,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Snapshot.tsx,,"I suggest changing
 ```
+          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
```
 to
```
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
```","+      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotToken"">
+        <Form.Label column sm={4}>
+          Please enter your Plate Recognizer{"" ""}
+          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>","--- 

+++ 

@@ -1,99 +1,140 @@

 import {
-  Container,
   Row,
   Col,
-  Card,
   Button,
-  Alert,
-  Navbar,
-  Nav,
 } from ""react-bootstrap"";
 
 import Form from ""react-bootstrap/Form"";
 
-// import { verifyToken } from ""./helpers"";
-import React, { useState } from ""react"";
+import React, { useState, useEffect } from ""react"";
 import { useDockerDesktopClient } from ""../hooks/useDockerDesktopClient"";
 import Loader from ""./Loader"";
 import Uninstall from ""./Uninstall"";
 import Update from ""./Update"";
 import ShowCommand from ""./ShowCommand"";
-import {openBrowserUrl} from '../helpers'
-
-
-const snapshotImageOptions = [
-  {
-    label: ""Intel CPU"",
-    value: ""platerecognizer/alpr:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""Raspberry"",
-    value: ""platerecognizer/alpr-raspberry-pi:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""GPU (Nvidia Only)"",
-    value: ""platerecognizer/alpr-gpu:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Jetson Nano"",
-    value: ""platerecognizer/alpr-jetson:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""ZCU104"",
-    value: ""platerecognizer/alpr-zcu104:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Thailand"",
-    value: ""platerecognizer/alpr:thailand"",
-    uninstall: false,
-  },
+import { openBrowserUrl } from '../helpers'
+
+const countryOptions = [
+  { value: '', label: 'Select country' },
+  { value: 'Global', label: 'Global' },
+  { value: 'egypt', label: 'Egypt' },
+  { value: 'germany', label: 'Germany' },
+  { value: 'japan', label: 'Japan' },
+  { value: 'korea', label: 'Korea' },
+  { value: 'thailand', label: 'Thailand' },
+  { value: 'uae', label: 'United Arab Emirates' },
 ];
 
-const DEFAULT_SNAPSHOT_IMAGE = snapshotImageOptions[0][""value""];
+const architectureOptionsSnapshot = [
+  { value: 'alpr', label: 'Intel x86 or amd64(x64)' },
+  { value: 'alpr-no-avx', label: 'Intel x86 or amd64(x64) no-avx' },
+  { value: 'alpr-gpu', label: 'Intel x86 or amd64(x64) with Nvidia GPU' },
+  { value: 'alpr-arm', label: 'ARM based CPUs, Raspberry Pi or Apple M1' },
+  { value: 'alpr-jetson', label: 'Nvidia Jetson (with GPU) for Jetpack 4.6 (r32)' },
+  { value: 'alpr-jetson:r35', label: 'Nvidia Jetson (with GPU) for Jetpack 5.x (r35)' },
+  { value: 'alpr-zcu104', label: 'ZCU' },
+];
+
 
 export default function Snapshot() {
+  const [licenseKey, setLicenseKey] = useState('');
+  const [token, setToken] = useState('');
   const [tokenValidated, setTokenValidated] = useState(false);
-  const [uninstall, setUninstall] = useState(true);
   const [isLoading, setLoading] = useState(false);
+
   const [command, setCommand] = useState<string>("""");
-  const [curlPort, setCurlPort] = useState("""");
-  const [image, setImage] = useState(DEFAULT_SNAPSHOT_IMAGE);
+  const [curlPort, setCurlPort] = useState(""8080"");
+  const [dockerimage, setDockerimage] = useState('');
+  const [country, setCountry] = useState('Global');
+  const [architecture, setArchitecture] = useState('alpr');
+  const [restartPolicy, setRestartPolicy] = useState('no');
+
   const ddClient = useDockerDesktopClient();
 
-  const handleInputChange = (e: any) => {
+  const handleInputChange = (e:  React.ChangeEvent<HTMLInputElement>) => {
     setTokenValidated(false);
-  };
-
-  const handleImageChange = (e: any) => {
+    const { name, value } = e.target;
+    if (name == ""license"") {
+      setLicenseKey(value);
+    } else if (name == ""token"") {
+      setToken(value)
+    } else if (name == ""port"") {
+      setCurlPort(value);
+    } else if (name == ""restart-policy"") {
+      setRestartPolicy(value);
+    }
+  };
+
+  const handleArchitectureChange = (e:React.ChangeEvent<HTMLSelectElement>) => {
     setTokenValidated(false);
-    let image: string = e.target.value;
-    setImage(image);
-    const snapshotImageOption: any = snapshotImageOptions.find((element) => {
-      return element.value === image;
-    });
-    setUninstall(snapshotImageOption.uninstall);
-  };
-
-  interface SnapshotData {
-    port: string;
-    license: string;
-    token: string;
-    image: string;
-  }
-
-  const handleSubmit = async (event: React.SyntheticEvent) => {
+    setArchitecture(e.target.value);
+  };
+
+
+  // type SnapshotData = {
+  //   port: string;
+  //   license: string;
+  //   token: string;
+  //   image: string;
+  // }
+  const generateDockerImage = () => {
+    let dockerImage = 'platerecognizer/';
+    if (country === 'Global' || architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx') {
+      dockerImage += `${architecture}`;
+    } else {
+      dockerImage += `${architecture}:${country}`;
+    }
+    setDockerimage(dockerImage)
+    return (dockerImage)
+  };
+  const generateDockerRunCommand = (dockerImage:string) => {
+    let restartOption;
+    switch (restartPolicy) {
+      case 'no':
+        restartOption = ''
+        break
+      default:
+        restartOption = `--restart=${restartPolicy} `
+        break
+    }
+    const baseCommand = `docker run ${restartOption}-t -p ${curlPort}:8080 -v license:/license`;
+    let platformSpecificCommand = '';
+
+    switch (architecture) {
+      case 'alpr-jetson':
+      case 'alpr-jetson:r35':
+        platformSpecificCommand = ` --runtime nvidia -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}   ${dockerImage}`;
+        break;
+      case 'alpr-gpu':
+        platformSpecificCommand = ` --gpus all -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      case 'alpr':
+      case 'alpr-no-avx':
+      case 'alpr-arm':
+      case 'alpr-zcu104':
+        platformSpecificCommand = `  -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      default:
+        break;
+    }
+
+    setCommand(`${baseCommand} ${platformSpecificCommand}`);
+
+  };
+
+  useEffect(() => {
+    const imagem = generateDockerImage()
+    generateDockerRunCommand(imagem)
+  }, [country, architecture, token, licenseKey, restartPolicy]);
+
+
+  const handleSubmit = async (event: React.FormEvent<HTMLFormElement>) => {
     event.preventDefault();
-    const form: any = event.target;
+    const form: HTMLFormElement = event.currentTarget;
     const formData = new FormData(form);
 
     const data: any = Object.fromEntries(formData.entries());
     // console.log(data);
-    setCurlPort(data.port);
     setLoading(true);
 
     ddClient.extension.vm?.service
@@ -104,16 +145,8 @@

         const message = res[""message""];
         if (valid) {
           // Pull image and update
-          ddClient.docker.cli.exec(""pull"", [data.image]).then((result) => {
-            const autoBoot = data.startOnBoot
-              ? "" --restart unless-stopped""
-              : ""--rm"";
-            const gpus = data.image.includes(""gpu"") ? "" --gpus all"" : """";
-            const nvidia = data.image.includes(""jetson"")
-              ? "" --runtime nvidia""
-              : """";
-            const command = `docker run${gpus}${nvidia}${autoBoot} -t -p ${data.port}:8080 -v license:/license -e LICENSE_KEY=${data.license} -e TOKEN=${data.token} ${data.image}`;
-            setCommand(command);
+          ddClient.docker.cli.exec(""pull"", [dockerimage]).then((result) => {
+            console.debug(result)
             setTokenValidated(valid);
             setLoading(false);
           });
@@ -123,10 +156,13 @@

         }
       });
   };
-
-  const handleLinkClick = (e: any) => {
+  const handleCountryChange = (e: any) => {
+    setTokenValidated(false);
+    setCountry(e.target.value);
+  };
+  const handleLinkClick = (e:  React.MouseEvent<HTMLAnchorElement>) => {
     e.preventDefault();
-    openBrowserUrl(ddClient, e.target.href);
+    openBrowserUrl(ddClient, e.currentTarget.href);
   };
   return (
     <Form onSubmit={handleSubmit}>
@@ -135,7 +171,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotToken"">
         <Form.Label column sm={4}>
           Please enter your Plate Recognizer{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             API Token
           </a>
           :
@@ -154,7 +190,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotLicense"">
         <Form.Label column sm={4}>
           Please enter your{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             Snapshot License Key
           </a>
           :
@@ -170,17 +206,46 @@

         </Col>
       </Form.Group>
 
-      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotRestartPolicy"">
-        <Form.Label column sm={4}>
-          Start Snapshot automatically on system startup?
-        </Form.Label>
-        <Col sm={8} className=""mt-2"">
+      <Form.Group as={Row} className=""mb-3"">
+        <Form.Label column sm={4}>
+          Restart policy
+        </Form.Label>
+        <Col sm={8} className=""mt-2 d-flex justify-content-between"">
           <Form.Check
-            type=""switch""
-            name=""startOnBoot""
-            onChange={handleInputChange}
-          />
-        </Col>
+            type=""radio""
+            name=""restart-policy""
+            label='No (Docker Default)'
+            id='rps1'
+            value='no'
+            checked={restartPolicy == 'no'}
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Unless Stopped'
+            id='rps2'
+            value='unless-stopped'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Always'
+            id='rps3'
+            value='always'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='On Failure'
+            id='rps4'
+            value='on-failure'
+            onChange={handleInputChange}
+          />
+        </Col>
+
       </Form.Group>
 
       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotPort"">
@@ -205,16 +270,31 @@

         <Form.Label column sm={4}>
           Docker image to use:
         </Form.Label>
-        <Col sm={8}>
+        <Col sm={3}>
+          <Form.Select
+            aria-label=""Snapshot Docker Image Country""
+            onChange={handleCountryChange}
+            name=""country""
+            defaultValue={country}
+            disabled={architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx'}
+          >
+            {countryOptions.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
+              </option>
+            ))}
+          </Form.Select>
+        </Col>
+        <Col sm={5}>
           <Form.Select
             aria-label=""Snapshot Docker Image""
-            onChange={handleImageChange}
+            onChange={handleArchitectureChange}
             name=""image""
-            defaultValue={image}
+            defaultValue={architecture}
           >
-            {snapshotImageOptions.map((snapshotImageOption, index) => (
-              <option value={snapshotImageOption.value} key={index}>
-                {snapshotImageOption.label}
+            {architectureOptionsSnapshot.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
               </option>
             ))}
           </Form.Select>
@@ -238,8 +318,8 @@

         </label>
       </Form.Group>
 
-      <Update isEnabled={uninstall} image={image} />
-      <Uninstall isEnabled={uninstall} image={image} />
+      <Update isEnabled={true} image={dockerimage} />
+      <Uninstall isEnabled={true} image={dockerimage} />
     </Form>
   );
 }"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804583111,,157,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Snapshot.tsx,,"I suggest changing
 ```
+          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
```
 to
```
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
```","+      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotLicense"">
+        <Form.Label column sm={4}>
+          Please enter your{"" ""}
+          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>","--- 

+++ 

@@ -1,99 +1,140 @@

 import {
-  Container,
   Row,
   Col,
-  Card,
   Button,
-  Alert,
-  Navbar,
-  Nav,
 } from ""react-bootstrap"";
 
 import Form from ""react-bootstrap/Form"";
 
-// import { verifyToken } from ""./helpers"";
-import React, { useState } from ""react"";
+import React, { useState, useEffect } from ""react"";
 import { useDockerDesktopClient } from ""../hooks/useDockerDesktopClient"";
 import Loader from ""./Loader"";
 import Uninstall from ""./Uninstall"";
 import Update from ""./Update"";
 import ShowCommand from ""./ShowCommand"";
-import {openBrowserUrl} from '../helpers'
-
-
-const snapshotImageOptions = [
-  {
-    label: ""Intel CPU"",
-    value: ""platerecognizer/alpr:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""Raspberry"",
-    value: ""platerecognizer/alpr-raspberry-pi:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""GPU (Nvidia Only)"",
-    value: ""platerecognizer/alpr-gpu:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Jetson Nano"",
-    value: ""platerecognizer/alpr-jetson:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""ZCU104"",
-    value: ""platerecognizer/alpr-zcu104:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Thailand"",
-    value: ""platerecognizer/alpr:thailand"",
-    uninstall: false,
-  },
+import { openBrowserUrl } from '../helpers'
+
+const countryOptions = [
+  { value: '', label: 'Select country' },
+  { value: 'Global', label: 'Global' },
+  { value: 'egypt', label: 'Egypt' },
+  { value: 'germany', label: 'Germany' },
+  { value: 'japan', label: 'Japan' },
+  { value: 'korea', label: 'Korea' },
+  { value: 'thailand', label: 'Thailand' },
+  { value: 'uae', label: 'United Arab Emirates' },
 ];
 
-const DEFAULT_SNAPSHOT_IMAGE = snapshotImageOptions[0][""value""];
+const architectureOptionsSnapshot = [
+  { value: 'alpr', label: 'Intel x86 or amd64(x64)' },
+  { value: 'alpr-no-avx', label: 'Intel x86 or amd64(x64) no-avx' },
+  { value: 'alpr-gpu', label: 'Intel x86 or amd64(x64) with Nvidia GPU' },
+  { value: 'alpr-arm', label: 'ARM based CPUs, Raspberry Pi or Apple M1' },
+  { value: 'alpr-jetson', label: 'Nvidia Jetson (with GPU) for Jetpack 4.6 (r32)' },
+  { value: 'alpr-jetson:r35', label: 'Nvidia Jetson (with GPU) for Jetpack 5.x (r35)' },
+  { value: 'alpr-zcu104', label: 'ZCU' },
+];
+
 
 export default function Snapshot() {
+  const [licenseKey, setLicenseKey] = useState('');
+  const [token, setToken] = useState('');
   const [tokenValidated, setTokenValidated] = useState(false);
-  const [uninstall, setUninstall] = useState(true);
   const [isLoading, setLoading] = useState(false);
+
   const [command, setCommand] = useState<string>("""");
-  const [curlPort, setCurlPort] = useState("""");
-  const [image, setImage] = useState(DEFAULT_SNAPSHOT_IMAGE);
+  const [curlPort, setCurlPort] = useState(""8080"");
+  const [dockerimage, setDockerimage] = useState('');
+  const [country, setCountry] = useState('Global');
+  const [architecture, setArchitecture] = useState('alpr');
+  const [restartPolicy, setRestartPolicy] = useState('no');
+
   const ddClient = useDockerDesktopClient();
 
-  const handleInputChange = (e: any) => {
+  const handleInputChange = (e:  React.ChangeEvent<HTMLInputElement>) => {
     setTokenValidated(false);
-  };
-
-  const handleImageChange = (e: any) => {
+    const { name, value } = e.target;
+    if (name == ""license"") {
+      setLicenseKey(value);
+    } else if (name == ""token"") {
+      setToken(value)
+    } else if (name == ""port"") {
+      setCurlPort(value);
+    } else if (name == ""restart-policy"") {
+      setRestartPolicy(value);
+    }
+  };
+
+  const handleArchitectureChange = (e:React.ChangeEvent<HTMLSelectElement>) => {
     setTokenValidated(false);
-    let image: string = e.target.value;
-    setImage(image);
-    const snapshotImageOption: any = snapshotImageOptions.find((element) => {
-      return element.value === image;
-    });
-    setUninstall(snapshotImageOption.uninstall);
-  };
-
-  interface SnapshotData {
-    port: string;
-    license: string;
-    token: string;
-    image: string;
-  }
-
-  const handleSubmit = async (event: React.SyntheticEvent) => {
+    setArchitecture(e.target.value);
+  };
+
+
+  // type SnapshotData = {
+  //   port: string;
+  //   license: string;
+  //   token: string;
+  //   image: string;
+  // }
+  const generateDockerImage = () => {
+    let dockerImage = 'platerecognizer/';
+    if (country === 'Global' || architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx') {
+      dockerImage += `${architecture}`;
+    } else {
+      dockerImage += `${architecture}:${country}`;
+    }
+    setDockerimage(dockerImage)
+    return (dockerImage)
+  };
+  const generateDockerRunCommand = (dockerImage:string) => {
+    let restartOption;
+    switch (restartPolicy) {
+      case 'no':
+        restartOption = ''
+        break
+      default:
+        restartOption = `--restart=${restartPolicy} `
+        break
+    }
+    const baseCommand = `docker run ${restartOption}-t -p ${curlPort}:8080 -v license:/license`;
+    let platformSpecificCommand = '';
+
+    switch (architecture) {
+      case 'alpr-jetson':
+      case 'alpr-jetson:r35':
+        platformSpecificCommand = ` --runtime nvidia -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}   ${dockerImage}`;
+        break;
+      case 'alpr-gpu':
+        platformSpecificCommand = ` --gpus all -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      case 'alpr':
+      case 'alpr-no-avx':
+      case 'alpr-arm':
+      case 'alpr-zcu104':
+        platformSpecificCommand = `  -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      default:
+        break;
+    }
+
+    setCommand(`${baseCommand} ${platformSpecificCommand}`);
+
+  };
+
+  useEffect(() => {
+    const imagem = generateDockerImage()
+    generateDockerRunCommand(imagem)
+  }, [country, architecture, token, licenseKey, restartPolicy]);
+
+
+  const handleSubmit = async (event: React.FormEvent<HTMLFormElement>) => {
     event.preventDefault();
-    const form: any = event.target;
+    const form: HTMLFormElement = event.currentTarget;
     const formData = new FormData(form);
 
     const data: any = Object.fromEntries(formData.entries());
     // console.log(data);
-    setCurlPort(data.port);
     setLoading(true);
 
     ddClient.extension.vm?.service
@@ -104,16 +145,8 @@

         const message = res[""message""];
         if (valid) {
           // Pull image and update
-          ddClient.docker.cli.exec(""pull"", [data.image]).then((result) => {
-            const autoBoot = data.startOnBoot
-              ? "" --restart unless-stopped""
-              : ""--rm"";
-            const gpus = data.image.includes(""gpu"") ? "" --gpus all"" : """";
-            const nvidia = data.image.includes(""jetson"")
-              ? "" --runtime nvidia""
-              : """";
-            const command = `docker run${gpus}${nvidia}${autoBoot} -t -p ${data.port}:8080 -v license:/license -e LICENSE_KEY=${data.license} -e TOKEN=${data.token} ${data.image}`;
-            setCommand(command);
+          ddClient.docker.cli.exec(""pull"", [dockerimage]).then((result) => {
+            console.debug(result)
             setTokenValidated(valid);
             setLoading(false);
           });
@@ -123,10 +156,13 @@

         }
       });
   };
-
-  const handleLinkClick = (e: any) => {
+  const handleCountryChange = (e: any) => {
+    setTokenValidated(false);
+    setCountry(e.target.value);
+  };
+  const handleLinkClick = (e:  React.MouseEvent<HTMLAnchorElement>) => {
     e.preventDefault();
-    openBrowserUrl(ddClient, e.target.href);
+    openBrowserUrl(ddClient, e.currentTarget.href);
   };
   return (
     <Form onSubmit={handleSubmit}>
@@ -135,7 +171,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotToken"">
         <Form.Label column sm={4}>
           Please enter your Plate Recognizer{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             API Token
           </a>
           :
@@ -154,7 +190,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotLicense"">
         <Form.Label column sm={4}>
           Please enter your{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             Snapshot License Key
           </a>
           :
@@ -170,17 +206,46 @@

         </Col>
       </Form.Group>
 
-      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotRestartPolicy"">
-        <Form.Label column sm={4}>
-          Start Snapshot automatically on system startup?
-        </Form.Label>
-        <Col sm={8} className=""mt-2"">
+      <Form.Group as={Row} className=""mb-3"">
+        <Form.Label column sm={4}>
+          Restart policy
+        </Form.Label>
+        <Col sm={8} className=""mt-2 d-flex justify-content-between"">
           <Form.Check
-            type=""switch""
-            name=""startOnBoot""
-            onChange={handleInputChange}
-          />
-        </Col>
+            type=""radio""
+            name=""restart-policy""
+            label='No (Docker Default)'
+            id='rps1'
+            value='no'
+            checked={restartPolicy == 'no'}
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Unless Stopped'
+            id='rps2'
+            value='unless-stopped'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Always'
+            id='rps3'
+            value='always'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='On Failure'
+            id='rps4'
+            value='on-failure'
+            onChange={handleInputChange}
+          />
+        </Col>
+
       </Form.Group>
 
       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotPort"">
@@ -205,16 +270,31 @@

         <Form.Label column sm={4}>
           Docker image to use:
         </Form.Label>
-        <Col sm={8}>
+        <Col sm={3}>
+          <Form.Select
+            aria-label=""Snapshot Docker Image Country""
+            onChange={handleCountryChange}
+            name=""country""
+            defaultValue={country}
+            disabled={architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx'}
+          >
+            {countryOptions.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
+              </option>
+            ))}
+          </Form.Select>
+        </Col>
+        <Col sm={5}>
           <Form.Select
             aria-label=""Snapshot Docker Image""
-            onChange={handleImageChange}
+            onChange={handleArchitectureChange}
             name=""image""
-            defaultValue={image}
+            defaultValue={architecture}
           >
-            {snapshotImageOptions.map((snapshotImageOption, index) => (
-              <option value={snapshotImageOption.value} key={index}>
-                {snapshotImageOption.label}
+            {architectureOptionsSnapshot.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
               </option>
             ))}
           </Form.Select>
@@ -238,8 +318,8 @@

         </label>
       </Form.Group>
 
-      <Update isEnabled={uninstall} image={image} />
-      <Uninstall isEnabled={uninstall} image={image} />
+      <Update isEnabled={true} image={dockerimage} />
+      <Uninstall isEnabled={true} image={dockerimage} />
     </Form>
   );
 }"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1804590059,,268,cdeab34e2877bcfaee8a115b76be392fb33058cb,39eb3e9a4e416dc5bd5d6a6e92ac6c285281cf58,docker/dd-extension/ui/src/components/Snapshot.tsx,,"Could you make two separate dropdowns?
- One for the country
- One for the architecture
Up to date reference: https://guides.platerecognizer.com/docs/snapshot/manual-install/","+        </Col>
+      </Form.Group>
+
+      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotDockerImage"">","--- 

+++ 

@@ -1,99 +1,140 @@

 import {
-  Container,
   Row,
   Col,
-  Card,
   Button,
-  Alert,
-  Navbar,
-  Nav,
 } from ""react-bootstrap"";
 
 import Form from ""react-bootstrap/Form"";
 
-// import { verifyToken } from ""./helpers"";
-import React, { useState } from ""react"";
+import React, { useState, useEffect } from ""react"";
 import { useDockerDesktopClient } from ""../hooks/useDockerDesktopClient"";
 import Loader from ""./Loader"";
 import Uninstall from ""./Uninstall"";
 import Update from ""./Update"";
 import ShowCommand from ""./ShowCommand"";
-import {openBrowserUrl} from '../helpers'
-
-
-const snapshotImageOptions = [
-  {
-    label: ""Intel CPU"",
-    value: ""platerecognizer/alpr:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""Raspberry"",
-    value: ""platerecognizer/alpr-raspberry-pi:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""GPU (Nvidia Only)"",
-    value: ""platerecognizer/alpr-gpu:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Jetson Nano"",
-    value: ""platerecognizer/alpr-jetson:latest"",
-    uninstall: true,
-  },
-  {
-    label: ""ZCU104"",
-    value: ""platerecognizer/alpr-zcu104:latest"",
-    uninstall: false,
-  },
-  {
-    label: ""Thailand"",
-    value: ""platerecognizer/alpr:thailand"",
-    uninstall: false,
-  },
+import { openBrowserUrl } from '../helpers'
+
+const countryOptions = [
+  { value: '', label: 'Select country' },
+  { value: 'Global', label: 'Global' },
+  { value: 'egypt', label: 'Egypt' },
+  { value: 'germany', label: 'Germany' },
+  { value: 'japan', label: 'Japan' },
+  { value: 'korea', label: 'Korea' },
+  { value: 'thailand', label: 'Thailand' },
+  { value: 'uae', label: 'United Arab Emirates' },
 ];
 
-const DEFAULT_SNAPSHOT_IMAGE = snapshotImageOptions[0][""value""];
+const architectureOptionsSnapshot = [
+  { value: 'alpr', label: 'Intel x86 or amd64(x64)' },
+  { value: 'alpr-no-avx', label: 'Intel x86 or amd64(x64) no-avx' },
+  { value: 'alpr-gpu', label: 'Intel x86 or amd64(x64) with Nvidia GPU' },
+  { value: 'alpr-arm', label: 'ARM based CPUs, Raspberry Pi or Apple M1' },
+  { value: 'alpr-jetson', label: 'Nvidia Jetson (with GPU) for Jetpack 4.6 (r32)' },
+  { value: 'alpr-jetson:r35', label: 'Nvidia Jetson (with GPU) for Jetpack 5.x (r35)' },
+  { value: 'alpr-zcu104', label: 'ZCU' },
+];
+
 
 export default function Snapshot() {
+  const [licenseKey, setLicenseKey] = useState('');
+  const [token, setToken] = useState('');
   const [tokenValidated, setTokenValidated] = useState(false);
-  const [uninstall, setUninstall] = useState(true);
   const [isLoading, setLoading] = useState(false);
+
   const [command, setCommand] = useState<string>("""");
-  const [curlPort, setCurlPort] = useState("""");
-  const [image, setImage] = useState(DEFAULT_SNAPSHOT_IMAGE);
+  const [curlPort, setCurlPort] = useState(""8080"");
+  const [dockerimage, setDockerimage] = useState('');
+  const [country, setCountry] = useState('Global');
+  const [architecture, setArchitecture] = useState('alpr');
+  const [restartPolicy, setRestartPolicy] = useState('no');
+
   const ddClient = useDockerDesktopClient();
 
-  const handleInputChange = (e: any) => {
+  const handleInputChange = (e:  React.ChangeEvent<HTMLInputElement>) => {
     setTokenValidated(false);
-  };
-
-  const handleImageChange = (e: any) => {
+    const { name, value } = e.target;
+    if (name == ""license"") {
+      setLicenseKey(value);
+    } else if (name == ""token"") {
+      setToken(value)
+    } else if (name == ""port"") {
+      setCurlPort(value);
+    } else if (name == ""restart-policy"") {
+      setRestartPolicy(value);
+    }
+  };
+
+  const handleArchitectureChange = (e:React.ChangeEvent<HTMLSelectElement>) => {
     setTokenValidated(false);
-    let image: string = e.target.value;
-    setImage(image);
-    const snapshotImageOption: any = snapshotImageOptions.find((element) => {
-      return element.value === image;
-    });
-    setUninstall(snapshotImageOption.uninstall);
-  };
-
-  interface SnapshotData {
-    port: string;
-    license: string;
-    token: string;
-    image: string;
-  }
-
-  const handleSubmit = async (event: React.SyntheticEvent) => {
+    setArchitecture(e.target.value);
+  };
+
+
+  // type SnapshotData = {
+  //   port: string;
+  //   license: string;
+  //   token: string;
+  //   image: string;
+  // }
+  const generateDockerImage = () => {
+    let dockerImage = 'platerecognizer/';
+    if (country === 'Global' || architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx') {
+      dockerImage += `${architecture}`;
+    } else {
+      dockerImage += `${architecture}:${country}`;
+    }
+    setDockerimage(dockerImage)
+    return (dockerImage)
+  };
+  const generateDockerRunCommand = (dockerImage:string) => {
+    let restartOption;
+    switch (restartPolicy) {
+      case 'no':
+        restartOption = ''
+        break
+      default:
+        restartOption = `--restart=${restartPolicy} `
+        break
+    }
+    const baseCommand = `docker run ${restartOption}-t -p ${curlPort}:8080 -v license:/license`;
+    let platformSpecificCommand = '';
+
+    switch (architecture) {
+      case 'alpr-jetson':
+      case 'alpr-jetson:r35':
+        platformSpecificCommand = ` --runtime nvidia -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}   ${dockerImage}`;
+        break;
+      case 'alpr-gpu':
+        platformSpecificCommand = ` --gpus all -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      case 'alpr':
+      case 'alpr-no-avx':
+      case 'alpr-arm':
+      case 'alpr-zcu104':
+        platformSpecificCommand = `  -e LICENSE_KEY=${licenseKey} -e TOKEN=${token}  ${dockerImage}`;
+        break;
+      default:
+        break;
+    }
+
+    setCommand(`${baseCommand} ${platformSpecificCommand}`);
+
+  };
+
+  useEffect(() => {
+    const imagem = generateDockerImage()
+    generateDockerRunCommand(imagem)
+  }, [country, architecture, token, licenseKey, restartPolicy]);
+
+
+  const handleSubmit = async (event: React.FormEvent<HTMLFormElement>) => {
     event.preventDefault();
-    const form: any = event.target;
+    const form: HTMLFormElement = event.currentTarget;
     const formData = new FormData(form);
 
     const data: any = Object.fromEntries(formData.entries());
     // console.log(data);
-    setCurlPort(data.port);
     setLoading(true);
 
     ddClient.extension.vm?.service
@@ -104,16 +145,8 @@

         const message = res[""message""];
         if (valid) {
           // Pull image and update
-          ddClient.docker.cli.exec(""pull"", [data.image]).then((result) => {
-            const autoBoot = data.startOnBoot
-              ? "" --restart unless-stopped""
-              : ""--rm"";
-            const gpus = data.image.includes(""gpu"") ? "" --gpus all"" : """";
-            const nvidia = data.image.includes(""jetson"")
-              ? "" --runtime nvidia""
-              : """";
-            const command = `docker run${gpus}${nvidia}${autoBoot} -t -p ${data.port}:8080 -v license:/license -e LICENSE_KEY=${data.license} -e TOKEN=${data.token} ${data.image}`;
-            setCommand(command);
+          ddClient.docker.cli.exec(""pull"", [dockerimage]).then((result) => {
+            console.debug(result)
             setTokenValidated(valid);
             setLoading(false);
           });
@@ -123,10 +156,13 @@

         }
       });
   };
-
-  const handleLinkClick = (e: any) => {
+  const handleCountryChange = (e: any) => {
+    setTokenValidated(false);
+    setCountry(e.target.value);
+  };
+  const handleLinkClick = (e:  React.MouseEvent<HTMLAnchorElement>) => {
     e.preventDefault();
-    openBrowserUrl(ddClient, e.target.href);
+    openBrowserUrl(ddClient, e.currentTarget.href);
   };
   return (
     <Form onSubmit={handleSubmit}>
@@ -135,7 +171,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotToken"">
         <Form.Label column sm={4}>
           Please enter your Plate Recognizer{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             API Token
           </a>
           :
@@ -154,7 +190,7 @@

       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotLicense"">
         <Form.Label column sm={4}>
           Please enter your{"" ""}
-          <a href=""https://app.platerecognizer.com/accounts/plan/#sdk/?utm_source=dd-extension&utm_medium=app"" onClick={handleLinkClick}>
+          <a href=""https://app.platerecognizer.com/service/snapshot-sdk/"" onClick={handleLinkClick}>
             Snapshot License Key
           </a>
           :
@@ -170,17 +206,46 @@

         </Col>
       </Form.Group>
 
-      <Form.Group as={Row} className=""mb-3"" controlId=""snapshotRestartPolicy"">
-        <Form.Label column sm={4}>
-          Start Snapshot automatically on system startup?
-        </Form.Label>
-        <Col sm={8} className=""mt-2"">
+      <Form.Group as={Row} className=""mb-3"">
+        <Form.Label column sm={4}>
+          Restart policy
+        </Form.Label>
+        <Col sm={8} className=""mt-2 d-flex justify-content-between"">
           <Form.Check
-            type=""switch""
-            name=""startOnBoot""
-            onChange={handleInputChange}
-          />
-        </Col>
+            type=""radio""
+            name=""restart-policy""
+            label='No (Docker Default)'
+            id='rps1'
+            value='no'
+            checked={restartPolicy == 'no'}
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Unless Stopped'
+            id='rps2'
+            value='unless-stopped'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='Always'
+            id='rps3'
+            value='always'
+            onChange={handleInputChange}
+          />
+          <Form.Check
+            type=""radio""
+            name=""restart-policy""
+            label='On Failure'
+            id='rps4'
+            value='on-failure'
+            onChange={handleInputChange}
+          />
+        </Col>
+
       </Form.Group>
 
       <Form.Group as={Row} className=""mb-3"" controlId=""snapshotPort"">
@@ -205,16 +270,31 @@

         <Form.Label column sm={4}>
           Docker image to use:
         </Form.Label>
-        <Col sm={8}>
+        <Col sm={3}>
+          <Form.Select
+            aria-label=""Snapshot Docker Image Country""
+            onChange={handleCountryChange}
+            name=""country""
+            defaultValue={country}
+            disabled={architecture === 'alpr-jetson:r35' || architecture === 'alpr-no-avx'}
+          >
+            {countryOptions.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
+              </option>
+            ))}
+          </Form.Select>
+        </Col>
+        <Col sm={5}>
           <Form.Select
             aria-label=""Snapshot Docker Image""
-            onChange={handleImageChange}
+            onChange={handleArchitectureChange}
             name=""image""
-            defaultValue={image}
+            defaultValue={architecture}
           >
-            {snapshotImageOptions.map((snapshotImageOption, index) => (
-              <option value={snapshotImageOption.value} key={index}>
-                {snapshotImageOption.label}
+            {architectureOptionsSnapshot.map((option, index) => (
+              <option key={index} value={option.value}>
+                {option.label}
               </option>
             ))}
           </Form.Select>
@@ -238,8 +318,8 @@

         </label>
       </Form.Group>
 
-      <Update isEnabled={uninstall} image={image} />
-      <Uninstall isEnabled={uninstall} image={image} />
+      <Update isEnabled={true} image={dockerimage} />
+      <Uninstall isEnabled={true} image={dockerimage} />
     </Form>
   );
 }"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1932756839,47.0,54,1ca4b4c9bb53a69bcb5b1214bc919ccb5ab8ae34,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/crop_plate.py,,"I suggest changing
 ```
     response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
 
     if response.status_code == 200:
+        logging.info(""Webhook request sent successfully."")
         return ""Webhook request sent successfully."", response.status_code
     else:
+        logging.error(f""Webhook request failed. Response code: {response.status_code}"")
         return ""Webhook request failed."", response.status_code
```
 to
```
+    try:
+        response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
+        response.raise_for_status()
+        logging.info(f""Vehicle: {plate}. Request was successful."")
+        return ""Request was successful"", response.status_code
+    except requests.exceptions.RequestException as err:
+        logging.error(f""Vehicle: {plate}. Error processing the request: {err}"")
+        return f""Failed to process the request: {err}"", response.status_code
```","     response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
 
     if response.status_code == 200:
+        logging.info(""Webhook request sent successfully."")
         return ""Webhook request sent successfully."", response.status_code
     else:
+        logging.error(f""Webhook request failed. Response code: {response.status_code}"")
         return ""Webhook request failed."", response.status_code","--- 

+++ 

@@ -28,6 +28,7 @@

         return ""No file uploaded."", 400
 
     data = json_data[""data""][""results""][0]
+    plate = data.get(""plate"")
 
     plate_bounding_box = data.get(""box"") or data[""vehicle""][""box""]
     crop_box = (
@@ -44,11 +45,11 @@

     }
     data = {""json"": json.dumps(json_data)}
 
-    response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
-
-    if response.status_code == 200:
-        logging.info(""Webhook request sent successfully."")
-        return ""Webhook request sent successfully."", response.status_code
-    else:
-        logging.error(f""Webhook request failed. Response code: {response.status_code}"")
-        return ""Webhook request failed."", response.status_code
+    try:
+        response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
+        response.raise_for_status()
+        logging.info(f""Vehicle: {plate}. Request was successful."")
+        return ""Request was successful"", response.status_code
+    except requests.exceptions.RequestException as err:
+        logging.error(f""Vehicle: {plate}. Error processing the request: {err}"")
+        return f""Failed to process the request: {err}"", response.status_code"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1932757941,,30,1ca4b4c9bb53a69bcb5b1214bc919ccb5ab8ae34,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/crop_plate.py,,"I suggest changing
 ```
     data = json_data[""data""][""results""][0]
```
 to
```
+    data = json_data[""data""][""results""][0]
+    plate = data.get(""plate"")
```","+        logging.error(""No file uploaded."")
         return ""No file uploaded."", 400
 
     data = json_data[""data""][""results""][0]","--- 

+++ 

@@ -28,6 +28,7 @@

         return ""No file uploaded."", 400
 
     data = json_data[""data""][""results""][0]
+    plate = data.get(""plate"")
 
     plate_bounding_box = data.get(""box"") or data[""vehicle""][""box""]
     crop_box = (
@@ -44,11 +45,11 @@

     }
     data = {""json"": json.dumps(json_data)}
 
-    response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
-
-    if response.status_code == 200:
-        logging.info(""Webhook request sent successfully."")
-        return ""Webhook request sent successfully."", response.status_code
-    else:
-        logging.error(f""Webhook request failed. Response code: {response.status_code}"")
-        return ""Webhook request failed."", response.status_code
+    try:
+        response = requests.post(os.getenv(""WEBHOOK_URL"", """"), data=data, files=files)
+        response.raise_for_status()
+        logging.info(f""Vehicle: {plate}. Request was successful."")
+        return ""Request was successful"", response.status_code
+    except requests.exceptions.RequestException as err:
+        logging.error(f""Vehicle: {plate}. Error processing the request: {err}"")
+        return f""Failed to process the request: {err}"", response.status_code"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1932762312,,70,1ca4b4c9bb53a69bcb5b1214bc919ccb5ab8ae34,f832a1a7d88de4a14d653c43c0ef513fa4d46b22,webhooks/middleware/protocols/salient.py,,"I suggest changing
 ```
             )
```
 to
```
+            )
+            logging.info(f""Vehicle: {plate}. Request was successful."")
```","@@ -69,6 +69,7 @@ def process_request(
                 username, password, vms_api_url, camera_uid, camera_id, plate, timestamp
             )","--- 

+++ 

@@ -68,6 +68,7 @@

             notify_salient(
                 username, password, vms_api_url, camera_uid, camera_id, plate, timestamp
             )
+            logging.info(f""Vehicle: {plate}. Notified Salient successfully."")
         except Exception as e:
             logging.error(f""Vehicle: {plate}. Failed to notify Salient: {e}"")
             return f""Failed to notify Salient: {e}"", 400"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1830736049,,3,8220ac607bb1dc8df6b523d421b24731e3ccc010,3bb1f59ee42f6e3418caf9fb261b0ff24414a9f3,webhooks/README.md,,update this link,"-
-## Webhook Salient CompleteView
-[This project](webhook_salient/README.md) uses Stream webhooks to send license plate data to your CompleteView VMS.
+[This article](https://guides.platerecognizer.com/docs/stream/integrations#custom-integrations) will  help you to get started with the custom webhook integrations.","--- 

+++ 

@@ -1,3 +1,3 @@

 ## Webhook Integrations
 
-[This article](https://guides.platerecognizer.com/docs/stream/integrations#custom-integrations) will  help you to get started with the custom webhook integrations.
+[This article](https://guides.platerecognizer.com/docs/stream/integrations/custom) will  help you to get started with the custom webhook integrations."
https://api.github.com/repos/xun082/create-neat/pulls/comments/1528641886,,19,ce23fa78dfaf11626f80f5749501257e98004738,f805998e76fda454324dcbbc4daed50fbf7352f7,packages/core/package.json,,what,"   ""bin"": {
-    ""create-neat"": ""./dist/index.js""
+    ""create-neat"": ""./dist/index.js"",
+    ""c"": ""./dist/index.js""","--- 

+++ 

@@ -15,8 +15,7 @@

     ""directory"": ""packages/core""
   },
   ""bin"": {
-    ""create-neat"": ""./dist/index.js"",
-    ""c"": ""./dist/index.js""
+    ""create-neat"": ""./dist/index.js""
   },
   ""scripts"": {
     ""build"": ""tsc"","
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1933605254,,14,137777db7209447e06bb0774098773f391d4455b,b0ae987b6165ba3f82d32313a8165eb516e35c64,stream/slight-update/README.md,stream/stream-light-update/README.md,what's the total zip size in this example?,"+3. Copy `/tmp/extracted_content` to machine 2 that is offline.
+4. On machine 2, run a script to restore the content and create a new Docker image.
+```shell
+python main.py restore -i platerecognizer/alpr-stream:1.52.0 -o /tmp/extracted_content -t latest3","--- 

+++ 

@@ -1,17 +1,26 @@

-# Stream Slight Update
+# Stream Light Update
 1. Install requirements
 ```shell
 pip install -r requirements.txt
 ```
-2. On machine 1 that is online, run a script to extract the models and code from the Docker image.
+2. On machine 1 that is online, run script to **extract** the models and code from the Docker image.
 ```shell
-python main.py extract -i platerecognizer/alpr-stream:1.53.0 -o /tmp/extracted_content
+python main.py extract -s platerecognizer/alpr-stream:1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o /tmp/extracted_content
 ```
-You will now have `app.tar`, `dist-packages.tar` and `site-packages.tar` in `/tmp/extracted_content`
-3. Copy `/tmp/extracted_content` to machine 2 that is offline.
-4. On machine 2, run a script to restore the content and create a new Docker image.
+You will now have the following folder structure in `/tmp/extracted_content`:
 ```shell
-python main.py restore -i platerecognizer/alpr-stream:1.52.0 -o /tmp/extracted_content -t latest3
+/tmp/extracted_content
+│
+└── update_1.52.0_to_1.53.0
+    ├── app.tar
+    ├── dist-packages.tar
+    └── site-packages.tar
 ```
-Updated image to use will now be called `platerecognizer/alpr-stream:latest3`
+3. Copy the **diff** folder(`/tmp/extracted_content/update_1.52.0_to_1.53.0`) to machine 2 that is offline.
 
+4. On machine 2, run script again to **restore** the content and create a new Docker image.
+```shell
+python main.py restore -s /tmp/extracted_content/update_1.52.0_to_1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o latest3
+```
+Updated image to use will now be called `platerecognizer/alpr-stream:latest3` that you can use to rebuild Stream container
+> We recommend using a new tag so that you can have your previous image available to restore to in case of an issue"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1942545997,,99,0f4cc914a1a010ac4215157b05980d4cda094290,b0ae987b6165ba3f82d32313a8165eb516e35c64,stream/slight-update/main.py,stream/stream-light-update/main.py,".mkdir(parents=True, exist_ok=True)
Incase args.output does not exist yet it also creates it as well. Same goes for all others in the script. ","+def extract_updates(args):
+    source_image_fs = args.output / ""source""
+    dest_image_fs = args.output / ""dest""
+    source_image_fs.mkdir(exist_ok=True)","--- 

+++ 

@@ -13,17 +13,29 @@

     Path(""/usr/local/lib/python3.8/site-packages/""),
     Path(""/usr/local/lib/python3.8/dist-packages/""),
 ]
+CONTAINER_STOP_TIMEOUT = 2
 
 
-def get_python_version(image):
+def get_versions(image) -> tuple[str | None, str | None]:
+    """"""
+    Returns python and stream version from container config ENV
+    :param image:
+    :return:
+    """"""
+    py_version = None
+    stream_version = None
     config = client.api.inspect_image(image)[""Config""]
     for env in config[""Env""]:
         name, value = env.split(""="", maxsplit=1)
         if name == ""PYTHON_VERSION"":
-            return value
+            py_version = value
+        elif name == ""TAG"":
+            stream_version = value
+
+    return py_version, stream_version
 
 
-def archive_image_updates(image, output) -> str | None:
+def archive_image_updates(image, output) -> tuple[str | None, str | None]:
     container = None
     try:
         container = client.containers.run(
@@ -38,13 +50,10 @@

                 for chunk in bits:
                     fp.write(chunk)
             print(f""Successfully created {zip_file}"")
-        return get_python_version(image)
-    except Exception as e:
-        print(f""An error occurred: {e}"")
-        raise
+        return get_versions(image)
     finally:
         if container is not None:
-            container.stop()
+            container.stop(timeout=CONTAINER_STOP_TIMEOUT)
 
 
 def hash_file(filepath):
@@ -94,29 +103,29 @@

 
 
 def extract_updates(args):
-    source_image_fs = args.output / ""source""
-    dest_image_fs = args.output / ""dest""
-    source_image_fs.mkdir(exist_ok=True)
-    dest_image_fs.mkdir(exist_ok=True)
+    with tempfile.TemporaryDirectory() as src, tempfile.TemporaryDirectory() as dest:
+        source_image_fs = Path(src)
+        dest_image_fs = Path(dest)
+        # Download Source Image Files
+        source_python, source_stream = archive_image_updates(
+            args.source, source_image_fs
+        )
+        # Download Source Image Files
+        dest_python, dest_stream = archive_image_updates(args.dest, dest_image_fs)
+        if source_python != dest_python:
+            print(
+                ""WARNING! Update across different python version is not guaranteed to work.""
+                f""You are updating from [{source_python}] to [{dest_python}]""
+            )
+        diff_fs = args.output / f""update_{dest_stream}_to_{source_stream}""
+        diff_fs.mkdir(exist_ok=True, parents=True)
 
-    # Download Source Image Files
-    source_python = archive_image_updates(args.source, source_image_fs)
-    # Download Source Image Files
-    dest_python = archive_image_updates(args.dest, dest_image_fs)
-    if source_python != dest_python:
-        print(
-            ""WARNING! Update across different python version is not guaranteed to work.""
-            f""You are updating from [{source_python}] to [{dest_python}]""
-        )
-    diff_fs = args.output / ""diff""
-    diff_fs.mkdir(exist_ok=True)
-
-    for path in paths_to_copy:
-        create_diff_tar(
-            source_image_fs / f""{path.name}.tar"",
-            dest_image_fs / f""{path.name}.tar"",
-            diff_fs / f""{path.name}.tar"",
-        )
+        for path in paths_to_copy:
+            create_diff_tar(
+                source_image_fs / f""{path.name}.tar"",
+                dest_image_fs / f""{path.name}.tar"",
+                diff_fs / f""{path.name}.tar"",
+            )
 
 
 def restore_updates(args):
@@ -141,11 +150,9 @@

             ""platerecognizer/alpr-stream"", args.output, pause=True, conf=image_config
         )
         print(f""Updated image is platerecognizer/alpr-stream:{args.output}"")
-    except Exception as e:
-        print(f""An error occurred: {e}"")
     finally:
         if container is not None:
-            container.stop()
+            container.stop(timeout=CONTAINER_STOP_TIMEOUT)
 
 
 def main():"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1944053141,,1,6497dc03ca7d304ef0eb7a10ca1ae87f38ecf9de,b0ae987b6165ba3f82d32313a8165eb516e35c64,stream/slight-update/README.md,stream/stream-light-update/README.md,Can you rename the title Stream Light Update? Same thing for the directory name.,"@@ -0,0 +1,37 @@
+# Stream Slight Update","--- 

+++ 

@@ -1,4 +1,4 @@

-# Stream Slight Update
+# Stream Light Update
 1. Install requirements
 ```shell
 pip install -r requirements.txt
@@ -7,31 +7,20 @@

 ```shell
 python main.py extract -s platerecognizer/alpr-stream:1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o /tmp/extracted_content
 ```
-
-You will now have the following folder structure in `/tmp/extracted_content`
+You will now have the following folder structure in `/tmp/extracted_content`:
 ```shell
 /tmp/extracted_content
-├── dest
-│   ├── app.tar
-│   ├── dist-packages.tar
-│   └── site-packages.tar
-├── diff
-│   ├── app.tar
-│   ├── dist-packages.tar
-│   └── site-packages.tar
-└── source
+│
+└── update_1.52.0_to_1.53.0
     ├── app.tar
     ├── dist-packages.tar
     └── site-packages.tar
-
-4 directories, 9 files
 ```
-
-3. Copy the **diff** folder(`/tmp/extracted_content/diff`) to machine 2 that is offline.
+3. Copy the **diff** folder(`/tmp/extracted_content/update_1.52.0_to_1.53.0`) to machine 2 that is offline.
 
 4. On machine 2, run script again to **restore** the content and create a new Docker image.
 ```shell
-python main.py restore -s /tmp/extracted_content/diff -d platerecognizer/alpr-stream:1.52.0 -o latest3
+python main.py restore -s /tmp/extracted_content/update_1.52.0_to_1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o latest3
 ```
 Updated image to use will now be called `platerecognizer/alpr-stream:latest3` that you can use to rebuild Stream container
 > We recommend using a new tag so that you can have your previous image available to restore to in case of an issue"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1944055213,,13,6497dc03ca7d304ef0eb7a10ca1ae87f38ecf9de,b0ae987b6165ba3f82d32313a8165eb516e35c64,stream/slight-update/README.md,stream/stream-light-update/README.md,"By default, let's save only the diff as `./update_1.52.0_to_1.53.0`. Other directories aren't needed by the end user. That's more explicit.","+
+You will now have the following folder structure in `/tmp/extracted_content`
+```shell
+/tmp/extracted_content","--- 

+++ 

@@ -1,4 +1,4 @@

-# Stream Slight Update
+# Stream Light Update
 1. Install requirements
 ```shell
 pip install -r requirements.txt
@@ -7,31 +7,20 @@

 ```shell
 python main.py extract -s platerecognizer/alpr-stream:1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o /tmp/extracted_content
 ```
-
-You will now have the following folder structure in `/tmp/extracted_content`
+You will now have the following folder structure in `/tmp/extracted_content`:
 ```shell
 /tmp/extracted_content
-├── dest
-│   ├── app.tar
-│   ├── dist-packages.tar
-│   └── site-packages.tar
-├── diff
-│   ├── app.tar
-│   ├── dist-packages.tar
-│   └── site-packages.tar
-└── source
+│
+└── update_1.52.0_to_1.53.0
     ├── app.tar
     ├── dist-packages.tar
     └── site-packages.tar
-
-4 directories, 9 files
 ```
-
-3. Copy the **diff** folder(`/tmp/extracted_content/diff`) to machine 2 that is offline.
+3. Copy the **diff** folder(`/tmp/extracted_content/update_1.52.0_to_1.53.0`) to machine 2 that is offline.
 
 4. On machine 2, run script again to **restore** the content and create a new Docker image.
 ```shell
-python main.py restore -s /tmp/extracted_content/diff -d platerecognizer/alpr-stream:1.52.0 -o latest3
+python main.py restore -s /tmp/extracted_content/update_1.52.0_to_1.53.0 -d platerecognizer/alpr-stream:1.52.0 -o latest3
 ```
 Updated image to use will now be called `platerecognizer/alpr-stream:latest3` that you can use to rebuild Stream container
 > We recommend using a new tag so that you can have your previous image available to restore to in case of an issue"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808594788,,16,810055517fbd417e34f2e6f832f3812d2c4640b4,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/src/rollbar.js,,"
I suggest changing
 ```
+var Rollbar = class {
```
 to
```
+class Rollbar {
```","+	return data;
+};
+
+var Rollbar = class {","--- 

+++ 

@@ -4,104 +4,118 @@

 const rollbarUrl = `https://api.rollbar.com/api/1/item/`;
 
 const Frame = ({ fileName, lineNumber, columnNumber, functionName, args }) => {
-	const data = {};
-	data.filename = fileName;
-	data.lineno = lineNumber;
-	data.colno = columnNumber;
-	data.method = functionName;
-	data.args = args;
-	return data;
+  const data = {};
+  data.filename = fileName;
+  data.lineno = lineNumber;
+  data.colno = columnNumber;
+  data.method = functionName;
+  data.args = args;
+  return data;
 };
 
-var Rollbar = class {
-	constructor(token, environment) {
-		if (!token) {
-			throw new Error(""Token is required for Rollbar initialization"");
-		}
-		this.token = token;
-		this.environment = environment != null ? environment : ""production"";
-	}
+class Rollbar {
+  constructor(token, environment) {
+    if (!token) {
+      throw new Error(""Token is required for Rollbar initialization"");
+    }
+    this.token = token;
+    this.environment = environment != null ? environment : ""production"";
+  }
 
-	createTrace(description, exception) {
-		const stack = ErrorStackParser.parse(exception);
-		return {
-			frames: stack.map((stackFrame) => Frame(stackFrame)),
-			exception: {
-				class: exception.name,
-				message: exception.message,
-				description: description,
-			},
-		};
-	}
+  createTrace(description, exception) {
+    let stackFrames = null;
+    try {
+      const stack = ErrorStackParser.parse(exception);
+      stackFrames = stack.map((stackFrame) => Frame(stackFrame));
+    } catch (e) {
+      stackFrames = [
+        Frame({
+          fileName: ""index.js"",
+          lineNumber: null,
+          columnNumber: null,
+          functionName: null,
+          args: null,
+        }),
+      ];
+    }
+    return {
+      frames: stackFrames,
+      exception: {
+        class: exception.name,
+        message: exception.message,
+        description: description,
+      },
+    };
+  }
 
-	createTelemetry(eventLog) {
-		return {
-			level: eventLog[""level""],
-			timestamp_ms: eventLog[""timestamp""],
-			source: ""server"",
-			type: ""log"",
-			body: {
-				message: eventLog[""message""][0],
-			},
-		};
-	}
+  createTelemetry(eventLog) {
+    return {
+      level: eventLog[""level""],
+      timestamp_ms: eventLog[""timestamp""],
+      source: ""server"",
+      type: ""log"",
+      body: {
+        message: eventLog[""message""][0],
+      },
+    };
+  }
 
-	error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
-		const traceChain = exceptions.map((exception) =>
-			this.createTrace(description, exception),
-		);
-		const telemetry = eventLogs.map((eventLog) =>
-			this.createTelemetry(eventLog),
-		);
-		const rollbarData = {
-			environment: this.environment,
-			body: { telemetry: telemetry, trace_chain: traceChain },
-			timestamp: timestamp,
-			code_version: codeVersion,
-			language: ""javascript"",
-		};
+  error(exceptions, description, timestamp, event, eventLogs, codeVersion) {
+    const traceChain = exceptions.map((exception) =>
+      this.createTrace(description, exception),
+    );
+    const telemetry = eventLogs.map((eventLog) =>
+      this.createTelemetry(eventLog),
+    );
+    const rollbarData = {
+      environment: this.environment,
+      body: { telemetry: telemetry, trace_chain: traceChain },
+      timestamp: timestamp,
+      code_version: codeVersion,
+      language: ""javascript"",
+    };
 
-		if (""request"" in event) {
-			const request = event[""request""];
-			const params = request[""cf""];
-			delete params[""tlsClientAuth""];
-			delete params[""tlsExportedAuthenticator""];
-			rollbarData[""request""] = {
-				url: request[""url""],
-				method: request[""method""],
-				headers: request[""headers""],
-				params: params,
-			};
-		}
+    if (""request"" in event) {
+      const request = event[""request""];
+      const params = request[""cf""];
+      delete params[""tlsClientAuth""];
+      delete params[""tlsExportedAuthenticator""];
+      rollbarData[""request""] = {
+        url: request[""url""],
+        method: request[""method""],
+        headers: request[""headers""],
+        params: params,
+      };
+    }
 
-		let init = {
-			method: ""POST"",
-			body: JSON.stringify({ data: rollbarData }),
-			headers: {
-				""Content-type"": ""application/json"",
-				""X-Rollbar-Access-Token"": this.token,
-			},
-		};
-		return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
-	}
-};
+    let init = {
+      method: ""POST"",
+      body: JSON.stringify({ data: rollbarData }),
+      headers: {
+        ""Content-type"": ""application/json"",
+        ""X-Rollbar-Access-Token"": this.token,
+      },
+    };
+    return fetchWithRetry(rollbarUrl, init).then((response) => response.json());
+  }
+}
 
 export default {
-	async tail(events, env, ctx) {
-		const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
-		for (const event of events) {
-			if (event.exceptions.length) {
-				ctx.waitUntil(
-					rollbar.error(
-						event.exceptions,
-						event[""scriptName""],
-						event[""eventTimestamp""],
-						event[""event""],
-						event[""logs""],
-						event[""scriptVersion""][""id""],
-					),
-				);
-			}
-		}
-	},
+  async tail(events, env, ctx) {
+    const rollbar = new Rollbar(env.ROLLBAR_TOKEN, ""production"");
+    for (const event of events) {
+      if (event.exceptions.length) {
+        ctx.waitUntil(
+          rollbar.error(
+            event.exceptions,
+            event[""scriptName""],
+            event[""eventTimestamp""],
+            event[""event""],
+            event[""logs""],
+            event[""scriptVersion""][""id""],
+          ),
+        );
+      }
+    }
+  },
 };"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808856253,,10,941a042b9b77a4ccc2552ed13d147c354efbd643,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/test/index.spec.js,,"This fails:
Expected: """"Hello World!""""
Received: """"Error - Required POST""""","+import { describe, it, expect } from ""vitest"";
+import worker from ""../src"";
+
+describe(""Hello World worker"", () => {","--- 

+++ 

@@ -1,28 +1,28 @@

 import {
-	env,
-	createExecutionContext,
-	waitOnExecutionContext,
-	SELF,
+  env,
+  createExecutionContext,
+  waitOnExecutionContext,
+  SELF,
 } from ""cloudflare:test"";
 import { describe, it, expect } from ""vitest"";
 import worker from ""../src"";
 
 describe(""Hello World worker"", () => {
-	it(""responds with Hello World! (unit style)"", async () => {
-		const request = new Request(""http://example.com"");
-		// Create an empty context to pass to `worker.fetch()`.
-		const ctx = createExecutionContext();
-		const response = await worker.fetch(request, env, ctx);
-		// Wait for all `Promise`s passed to `ctx.waitUntil()` to settle before running test assertions
-		await waitOnExecutionContext(ctx);
-		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
-	});
+  it(""responds with Hello World! (unit style)"", async () => {
+    const request = new Request(""http://example.com"");
+    // Create an empty context to pass to `worker.fetch()`.
+    const ctx = createExecutionContext();
+    const response = await worker.fetch(request, env, ctx);
+    // Wait for all `Promise`s passed to `ctx.waitUntil()` to settle before running test assertions
+    await waitOnExecutionContext(ctx);
+    expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
+  });
 
-	it(""responds with Hello World! (integration style)"", async () => {
-		const request = new Request(""http://example.com"");
-		// Create an empty context to pass to `worker.fetch()`.
-		const ctx = createExecutionContext();
-		const response = await SELF.fetch(request, env, ctx);
-		expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
-	});
+  it(""responds with Hello World! (integration style)"", async () => {
+    const request = new Request(""http://example.com"");
+    // Create an empty context to pass to `worker.fetch()`.
+    const ctx = createExecutionContext();
+    const response = await SELF.fetch(request, env, ctx);
+    expect(await response.text()).toMatchInlineSnapshot(`""Hello World!""`);
+  });
 });"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808863270,,55,941a042b9b77a4ccc2552ed13d147c354efbd643,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/README.md,,Which ones?,"+```
+
+## Local development
+- Create a `.dev.vars` with the env variables required","--- 

+++ 

@@ -52,7 +52,12 @@

 ```
 
 ## Local development
-- Create a `.dev.vars` with the env variables required
+- Create a `.dev.vars` with the env variables required:
+```dotenv
+SNAPSHOT_TOKEN=
+SNAPSHOT_URL= # Optional
+ROLLBAR_TOKEN= # optional
+```
 - Run `npm run dev` in your terminal to start a development server
 - Open a browser tab at http://localhost:8787/ to see your worker in action
 - Test with curl"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808867833,,1082,941a042b9b77a4ccc2552ed13d147c354efbd643,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/package-lock.json,,Could you run `npm audit fix` to bump this package version? (security),"+      ""dev"": true
+    },
+    ""node_modules/braces"": {
+      ""version"": ""3.0.2"",","--- 

+++ 

@@ -1079,12 +1079,12 @@

       ""dev"": true
     },
     ""node_modules/braces"": {
-      ""version"": ""3.0.2"",
-      ""resolved"": ""https://registry.npmjs.org/braces/-/braces-3.0.2.tgz"",
-      ""integrity"": ""sha512-b8um+L1RzM3WDSzvhm6gIz1yfTbBt6YTlcEKAvsmqCZZFw46z626lVj9j1yEPW33H5H+lBQpZMP1k8l+78Ha0A=="",
-      ""dev"": true,
-      ""dependencies"": {
-        ""fill-range"": ""^7.0.1""
+      ""version"": ""3.0.3"",
+      ""resolved"": ""https://registry.npmjs.org/braces/-/braces-3.0.3.tgz"",
+      ""integrity"": ""sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA=="",
+      ""dev"": true,
+      ""dependencies"": {
+        ""fill-range"": ""^7.1.1""
       },
       ""engines"": {
         ""node"": "">=8""
@@ -1330,9 +1330,9 @@

       }
     },
     ""node_modules/fill-range"": {
-      ""version"": ""7.0.1"",
-      ""resolved"": ""https://registry.npmjs.org/fill-range/-/fill-range-7.0.1.tgz"",
-      ""integrity"": ""sha512-qOo9F+dMUmC2Lcb4BbVvnKJxTPjCm+RRpe4gDuGrzkL7mEVl/djYSu2OdQ2Pa302N4oqkSg9ir6jaLWJ2USVpQ=="",
+      ""version"": ""7.1.1"",
+      ""resolved"": ""https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz"",
+      ""integrity"": ""sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg=="",
       ""dev"": true,
       ""dependencies"": {
         ""to-regex-range"": ""^5.0.1"""
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1808868798,,60,941a042b9b77a4ccc2552ed13d147c354efbd643,7645a7ff134a797c1d8f5a6fa166a00bcf62b8fe,snapshot-middleware/README.md,,"I can't test, as I don't have Survision.txt, and this triggers a 500
I suggest changing
 ```
+  	curl -vX POST http://localhost:8787/ -d @Survision.txt --header ""Content-Type: application/json"" --header ""survision-serial-number: sv1-searial-1""
```
 to
```
+  	curl -v http://localhost:8787/ -d @Survision.txt --header ""Content-Type: application/json"" --header ""survision-serial-number: sv1-searial-1""
```
Is `searial` a typo?","+- Open a browser tab at http://localhost:8787/ to see your worker in action
+- Test with curl
+    ```shell
+  	curl -vX POST http://localhost:8787/ -d @Survision.txt --header ""Content-Type: application/json"" --header ""survision-serial-number: sv1-searial-1""","--- 

+++ 

@@ -52,7 +52,12 @@

 ```
 
 ## Local development
-- Create a `.dev.vars` with the env variables required
+- Create a `.dev.vars` with the env variables required:
+```dotenv
+SNAPSHOT_TOKEN=
+SNAPSHOT_URL= # Optional
+ROLLBAR_TOKEN= # optional
+```
 - Run `npm run dev` in your terminal to start a development server
 - Open a browser tab at http://localhost:8787/ to see your worker in action
 - Test with curl"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1906709003,,51,95d04ed7e2449ec3732881a6cff35e91ea579dd5,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/main.py,,nit: isn't datetime.strptime better for this?,"+        #  ""11:49:22"", Format HH/MM/SS
+        hour, minute, second = data[""TimeUtc""].split("":"")
+        lgr.debug([hour, minute, second])
+        created_date = datetime(","--- 

+++ 

@@ -42,21 +42,12 @@

     def enqueue(self, data, params):
         camera_id = data[""CameraName""]
         image_base_64 = data[""ContextImage""]
-        # ""10/01/2022"", Format DD/MM/YYYY
-        month, day, year = data[""DateUtc""].split(""/"")
-        lgr.debug([month, day, year])
-        #  ""11:49:22"", Format HH/MM/SS
-        hour, minute, second = data[""TimeUtc""].split("":"")
-        lgr.debug([hour, minute, second])
-        created_date = datetime(
-            int(year),
-            int(month),
-            int(day),
-            int(hour),
-            int(minute),
-            int(second),
-            tzinfo=timezone.utc,
-        )
+        date_utc = data[""DateUtc""]
+        time_utc = data[""TimeUtc""]
+        created_date = datetime.strptime(
+            f""{date_utc} {time_utc}"", ""%d/%m/%Y %H:%M:%S""
+        ).replace(tzinfo=timezone.utc)
+
         # created_date = datetime.now() uncomment for testing
         if isinstance(self.api, ParkPowApi):
             v_attrs = data[""Attributes""]"
https://api.github.com/repos/parkpow/deep-license-plate-recognition/pulls/comments/1906720994,,169,95d04ed7e2449ec3732881a6cff35e91ea579dd5,f636f5a1ebc8b3c649f79342a8077ff2cd257386,parkpow/genetec/main.py,,"For future scripts, let's use Flask when we need a web server.","+        return all(k in data for k in required_keys)
+
+
+class RequestHandler(BaseHTTPRequestHandler):","--- 

+++ 

@@ -42,21 +42,12 @@

     def enqueue(self, data, params):
         camera_id = data[""CameraName""]
         image_base_64 = data[""ContextImage""]
-        # ""10/01/2022"", Format DD/MM/YYYY
-        month, day, year = data[""DateUtc""].split(""/"")
-        lgr.debug([month, day, year])
-        #  ""11:49:22"", Format HH/MM/SS
-        hour, minute, second = data[""TimeUtc""].split("":"")
-        lgr.debug([hour, minute, second])
-        created_date = datetime(
-            int(year),
-            int(month),
-            int(day),
-            int(hour),
-            int(minute),
-            int(second),
-            tzinfo=timezone.utc,
-        )
+        date_utc = data[""DateUtc""]
+        time_utc = data[""TimeUtc""]
+        created_date = datetime.strptime(
+            f""{date_utc} {time_utc}"", ""%d/%m/%Y %H:%M:%S""
+        ).replace(tzinfo=timezone.utc)
+
         # created_date = datetime.now() uncomment for testing
         if isinstance(self.api, ParkPowApi):
             v_attrs = data[""Attributes""]"
